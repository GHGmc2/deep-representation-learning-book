{"generated": "2025-08-19T06:40:17.375837+00:00", "count": 2611, "entries": [{"page": "Appendix A Optimization Methods", "href": "A1.html#top", "title": "Appendix A Optimization Methods", "snippet": ""}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1", "title": "A.1 Steepest Descent", "snippet": "A.1 Steepest Descent Optimization is concerned with the question of how to find where a function, say L ‚Äã ( Œ∏ ) L(\\theta) italic_L ( italic_Œ∏ ) , reaches its minimum value. Mathematically, this is stated as a problem: arg ‚Äã min Œ∏ ‚àà Œò ‚Å° ‚Ñí ‚Äã ( Œ∏ ) , \\operatorname*{arg\\ min}_{\\theta\\in\\Theta}\\mathcal{L}(\\theta), start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_Œ∏ ‚àà roman_Œò end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ ) , (A.1.1) where Œò \\Theta roman_Œò represents a domain to which the argument ùíô \\bm{x} bold_italic_x is confined. Often (and unleess otherwise mentioned, in thi"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2", "title": "A.2 Computing Gradients via Automatic Differentiation", "snippet": "A.2 Computing Gradients via Automatic Differentiation Above, we discussed several optimization algorithms for deep networks which assumed access to a first-order oracle , i.e., a device which would allow us to compute ‚Ñí ‚Äã ( Œ∏ ) \\mathcal{L}(\\theta) caligraphic_L ( italic_Œ∏ ) and ‚àá ‚Ñí ‚Äã ( Œ∏ ) \\nabla\\mathcal{L}(\\theta) ‚àá caligraphic_L ( italic_Œ∏ ) . For simple functions ‚Ñí \\mathcal{L} caligraphic_L , it is possible to do this by hand. However, for deep neural networks, this quickly becomes tedious, and hinders rapid experimentation. Hence we require a general algorithm which would allow us to effic"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3", "title": "A.3 Game Theory and Minimax Optimization", "snippet": "A.3 Game Theory and Minimax Optimization In certain cases, such as in Chapter 5 , a learning problem cannot be reduced to a single optimization problem but rather represents multiple potentially opposing components of the system try to each minimize their own objective. Examples of this paradigm include distribution learning via generative adversarial networks (GAN) and closed-loop transcription (CTRL). We will denote such a system as a two-player game , where we have two ‚Äúplayers‚Äù (i.e., components) called Player 1 and Player 2 trying to minimize their objectives ‚Ñí 1 \\mathcal{L}^{1} caligraph"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S4", "title": "A.4 Exercises", "snippet": "A.4 Exercises Exercise A.1 . We have shown that for a smooth function f f italic_f , gradient descent converges linearly to the global optimum if it is strongly convex. However, in general nonconvex optimization, we do not have convexity, let alone strong convexity. Fortunately, in some cases, f f italic_f satisfies the so-called Œº \\mu italic_Œº -Polyak-Lojasiewicz (PL) inequality, i.e., there exists a constant Œº > 0 \\mu>0 italic_Œº > 0 such that for all Œ∏ \\theta italic_Œ∏ , 1 2 ‚Äã ‚Äñ ‚àá f ‚Äã ( Œ∏ ) ‚Äñ 2 2 ‚â• Œº ‚Äã ( f ‚Äã ( Œ∏ ) ‚àí f ‚Äã ( Œ∏ ‚ãÜ ) ) , \\displaystyle\\frac{1}{2}\\|\\nabla f(\\theta)\\|_{2}^{2}\\geq\\mu\\l"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS1", "title": "A.1.1 Vanilla Gradient Descent for Smooth Problems", "snippet": "A.1.1 Vanilla Gradient Descent for Smooth Problems The simplest and most widely used method for optimization is gradient descent (GD). It was first introduced by Cauchy in 1847. The idea is very simple: starting from an initial state, we iteratively take small steps such that each step reduces the value of the function ‚Ñí ‚Äã ( Œ∏ ) \\mathcal{L}(\\theta) caligraphic_L ( italic_Œ∏ ) . Suppose that the current state is Œ∏ \\theta italic_Œ∏ . We want to take a small step, say of distance h h italic_h , in a direction, indicated by a vector ùíó \\bm{v} bold_italic_v , to reach a new state Œ∏ + h ‚Äã ùíó \\theta+h\\bm"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS2", "title": "A.1.2 Preconditioned Gradient Descent for Badly-Conditioned Problems", "snippet": "A.1.2 Preconditioned Gradient Descent for Badly-Conditioned Problems Figure A.3 : The negative gradient ‚àí ‚àá ‚Ñí Œª -\\nabla\\mathcal{L}_{\\lambda} - ‚àá caligraphic_L start_POSTSUBSCRIPT italic_Œª end_POSTSUBSCRIPT and pre-conditioned (Newton‚Äôs method step) vector field ‚àí [ ‚àá 2 ‚Ñí Œª ] ‚àí 1 ‚Äã [ ‚àá ‚Ñí Œª ] -[\\nabla^{2}\\mathcal{L}_{\\lambda}]^{-1}[\\nabla\\mathcal{L}_{\\lambda}] - [ ‚àá start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_Œª end_POSTSUBSCRIPT ] start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT [ ‚àá caligraphic_L start_POSTSUBSCRIPT italic_Œª end_POSTSUBSCRIPT ] where Œª = 19 "}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS3", "title": "A.1.3 Proximal Gradient Descent for Non-Smooth Problems", "snippet": "A.1.3 Proximal Gradient Descent for Non-Smooth Problems Even in very toy problems, however, such as LASSO or dictionary learning, the problem is not strongly convex but rather just convex, and the objective is no longer just smooth but rather the sum of a smooth function and a non-smooth regularizer (such as the ‚Ñì 1 \\ell^{1} roman_‚Ñì start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT norm). Such problems are solved by proximal optimization algorithms , which generalize steepest descent to non-smooth objectives. Formally, let us say that ‚Ñí ‚Äã ( Œ∏ ) ‚âê ùíÆ ‚Äã ( Œ∏ ) + ‚Ñõ ‚Äã ( Œ∏ ) \\mathcal{L}(\\theta)\\doteq\\mathc"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS4", "title": "A.1.4 Stochastic Gradient Descent for Large-Scale Problems", "snippet": "A.1.4 Stochastic Gradient Descent for Large-Scale Problems In deep learning, the objective function ‚Ñí \\mathcal{L} caligraphic_L usually cannot be computed exactly, and instead at each optimization step it is estimated using finite samples (say, using a mini-batch). A common way to model this situation is to define a stochastic loss function ‚Ñí œâ ‚Äã ( Œ∏ ) \\mathcal{L}_{\\omega}(\\theta) caligraphic_L start_POSTSUBSCRIPT italic_œâ end_POSTSUBSCRIPT ( italic_Œ∏ ) where œâ \\omega italic_œâ is some ‚Äúsource of randomness‚Äù. For example, œâ \\omega italic_œâ could contain the indices of the samples in a batch ove"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS5", "title": "A.1.5 Putting Everything Together: Adam", "snippet": "A.1.5 Putting Everything Together: Adam The gradient descent scheme proposes an iteration of the form Œ∏ k + 1 = Œ∏ k + h ‚Äã ùíó k , \\theta_{k+1}=\\theta_{k}+h\\bm{v}_{k}, italic_Œ∏ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + italic_h bold_italic_v start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , (A.1.48) where (recall) ùíó k \\bm{v}_{k} bold_italic_v start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT is chosen to be (proportional to) the steepest descent vector in the Euclidean norm: ùíó k = ‚àí ‚àá ‚Ñí ‚Äã ( Œ∏ k ) ‚Äñ ‚àá ‚Ñí ‚Äã ( Œ∏ k ) ‚Äñ 2 ‚àà arg ‚Äã min ùíó ‚àà "}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.SS1", "title": "A.2.1 Differentials", "snippet": "A.2.1 Differentials A full accounting of this subsection is given in the excellent guide [ BEJ25 ] . To motivate differentials, let us first consider the simple example of a differentiable function ‚Ñí : ‚Ñù ‚Üí ‚Ñù \\mathcal{L}\\colon\\mathbb{R}\\to\\mathbb{R} caligraphic_L : blackboard_R ‚Üí blackboard_R acting on a parameter Œ∏ \\theta italic_Œ∏ . We can write ‚Ñí ‚Äã ( Œ∏ ) ‚àí ‚Ñí ‚Äã ( Œ∏ 0 ) = ‚Ñí ‚Ä≤ ‚Äã ( Œ∏ 0 ) ‚ãÖ ( Œ∏ ‚àí Œ∏ 0 ) + o ‚Äã ( | Œ∏ ‚àí Œ∏ 0 | ) . \\mathcal{L}(\\theta)-\\mathcal{L}(\\theta_{0})=\\mathcal{L}^{\\prime}(\\theta_{0})\\cdot(\\theta-\\theta_{0})+o(\\lvert\\theta-\\theta_{0}\\rvert). caligraphic_L ( italic_Œ∏ ) - caligraphi"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.SS2", "title": "A.2.2 Automatic Differentiation", "snippet": "A.2.2 Automatic Differentiation The main idea of AD is to compute the chain rule efficiently. The basic problem we need to cope with is the following. In the optimization section of the appendix, we considered that the parameter space Œò \\Theta roman_Œò was an abstract Euclidean space like ‚Ñù n \\mathbb{R}^{n} blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT . In practice the parameters are really some collection of vectors, matrices, and higher-order objects: Œò = ‚Ñù m √ó n √ó ‚Ñù n √ó ‚Ñù r √ó q √ó p √ó ‚Ñù r √ó q √ó ‚ãØ \\Theta=\\mathbb{R}^{m\\times n}\\times\\mathbb{R}^{n}\\times\\mathbb{R}^{r\\times q\\t"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.SS3", "title": "A.2.3 Back Propagation", "snippet": "A.2.3 Back Propagation In this section, we will discuss algorithmic backpropagation using a simple yet completely practical example. Suppose that we fix an input-label pair ( ùëø , ùíö ) (\\bm{X},\\bm{y}) ( bold_italic_X , bold_italic_y ) , and fix a network architecture f Œ∏ = f Œ∏ L ‚àò ‚ãØ ‚àò f Œ∏ 1 ‚àò f Œ∏ emb f_{\\theta}=f_{\\theta}^{L}\\circ\\cdots\\circ f_{\\theta}^{1}\\circ f_{\\theta}^{\\mathrm{emb}} italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT = italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT ‚àò ‚ãØ ‚àò italic_f start_POSTSUBSCRIPT italic_Œ∏ end"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.SS1", "title": "A.3.1 Learning Stackelberg Equilibria", "snippet": "A.3.1 Learning Stackelberg Equilibria How can we learn Stackelberg equilibria via GDA? In general this is clearly impossible, since learning Stackelberg equilibria via GDA is obviously at least as hard as computing a global minimizer of a loss function (say by setting the shared objective ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) \\mathcal{L}(\\theta,\\eta) caligraphic_L ( italic_Œ∏ , italic_Œ∑ ) to only be a function of Œ∑ \\eta italic_Œ∑ ). As such, we can achieve two types of convergence guarantees: ‚Ä¢ When ‚Ñí \\mathcal{L} caligraphic_L is (strongly) concave in the first argument Œ∏ \\theta italic_Œ∏ and (strongly) convex in the se"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.SS2", "title": "A.3.2 Practical Considerations when Learning Stackelberg Equilibria", "snippet": "A.3.2 Practical Considerations when Learning Stackelberg Equilibria In practice, we do not know how to initialize parameters close to a (differential) Stackelberg equilibrium. Due to symmetries within the objective, including those induced by overparameterization of the neural networks being trained, one can (heuristically) expect that most initializations are close to a Stackelberg equilibrium. Also, we do not know how to compute the step size h h italic_h or the timescale T T italic_T , since they are dependent on properties of the loss ‚Ñí \\mathcal{L} caligraphic_L at the equilibrium. In prac"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#p1", "title": "‚Äú Since the building of all the universe is perfect and is created by the wisdom creator, nothing arises in the universe in which one cannot see the sense of some maximum or minimum. ‚Äù ‚Äì L. Euler", "snippet": "‚Äú Since the building of all the universe is perfect and is created by the wisdom creator, nothing arises in the universe in which one cannot see the sense of some maximum or minimum. ‚Äù ‚Äì L. Euler"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#p2", "title": "In this chapter, we give a brief introduction to some of the most basic but important optimization algorithms used in this book. The purpose is only to help the reader apply these algorithms to proble", "snippet": "In this chapter, we give a brief introduction to some of the most basic but important optimization algorithms used in this book. The purpose is only to help the reader apply these algorithms to problems studied in this book, not to gain a deep understanding about these algorithms. Hence, we will not provide a thorough justification for the algorithms introduced, in terms of performance guarantees."}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.p1", "title": "Optimization is concerned with the question of how to find where a function, say L ‚Äã ( Œ∏ ) L(\\theta) italic_L ( italic_Œ∏ ) , reaches its minimum value. Mathematically, this is stated as a problem: arg", "snippet": "Optimization is concerned with the question of how to find where a function, say L ‚Äã ( Œ∏ ) L(\\theta) italic_L ( italic_Œ∏ ) , reaches its minimum value. Mathematically, this is stated as a problem: arg ‚Äã min Œ∏ ‚àà Œò ‚Å° ‚Ñí ‚Äã ( Œ∏ ) , \\operatorname*{arg\\ min}_{\\theta\\in\\Theta}\\mathcal{L}(\\theta), start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_Œ∏ ‚àà roman_Œò end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ ) , (A.1.1) where Œò \\Theta roman_Œò represents a domain to which the argument ùíô \\bm{x} bold_italic_x is confined. Often (and unleess otherwise mentioned, in this chapter) Œò \\Theta r"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.p2", "title": "The efficiency of finding the (global) minima depends on what information we have about the function ‚Ñí \\mathcal{L} caligraphic_L . For most optimization problems considered in this book, the dimension", "snippet": "The efficiency of finding the (global) minima depends on what information we have about the function ‚Ñí \\mathcal{L} caligraphic_L . For most optimization problems considered in this book, the dimension of Œ∏ \\theta italic_Œ∏ , say n n italic_n , is very large. That makes computing or accessing local information about ‚Ñì \\ell roman_‚Ñì expensive. In particular, since the gradient ‚àá ‚Ñí \\nabla\\mathcal{L} ‚àá caligraphic_L has n n italic_n entries, it is often reasonable to compute; however, the Hessian ‚àá 2 ‚Ñí \\nabla^{2}\\mathcal{L} ‚àá start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT caligraphic_L has n 2 n^{2} it"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS1.p1", "title": "The simplest and most widely used method for optimization is gradient descent (GD). It was first introduced by Cauchy in 1847. The idea is very simple: starting from an initial state, we iteratively t", "snippet": "The simplest and most widely used method for optimization is gradient descent (GD). It was first introduced by Cauchy in 1847. The idea is very simple: starting from an initial state, we iteratively take small steps such that each step reduces the value of the function ‚Ñí ‚Äã ( Œ∏ ) \\mathcal{L}(\\theta) caligraphic_L ( italic_Œ∏ ) ."}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS1.p2", "title": "Suppose that the current state is Œ∏ \\theta italic_Œ∏ . We want to take a small step, say of distance h h italic_h , in a direction, indicated by a vector ùíó \\bm{v} bold_italic_v , to reach a new state Œ∏", "snippet": "Suppose that the current state is Œ∏ \\theta italic_Œ∏ . We want to take a small step, say of distance h h italic_h , in a direction, indicated by a vector ùíó \\bm{v} bold_italic_v , to reach a new state Œ∏ + h ‚Äã ùíó \\theta+h\\bm{v} italic_Œ∏ + italic_h bold_italic_v such that the value of the function decreases: ‚Ñí ‚Äã ( Œ∏ + h ‚Äã ùíó ) ‚â§ ‚Ñí ‚Äã ( Œ∏ ) . \\mathcal{L}(\\theta+h\\bm{v})\\leq\\mathcal{L}(\\theta). caligraphic_L ( italic_Œ∏ + italic_h bold_italic_v ) ‚â§ caligraphic_L ( italic_Œ∏ ) . (A.1.2) To find such a direction ùíó \\bm{v} bold_italic_v , we can approximate ‚Ñí ‚Äã ( Œ∏ + h ‚Äã ùíó ) \\mathcal{L}(\\theta+h\\bm{v}) calig"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS1.SSSx1.p1", "title": "The remaining question is what the step size h h italic_h should be? If we choose h h italic_h to be too small, the value of the function may decrease very slowly, as shown by the plot in the middle i", "snippet": "The remaining question is what the step size h h italic_h should be? If we choose h h italic_h to be too small, the value of the function may decrease very slowly, as shown by the plot in the middle in Figure A.1 . If h h italic_h is too large, the value might not even decrease at all, as shown by the plot on the right in Figure A.1 ."}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS1.SSSx1.p2", "title": "So the step size h h italic_h should be chosen based on the landscape of the function ‚Ñí ‚Äã ( Œ∏ k ) \\mathcal{L}(\\theta_{k}) caligraphic_L ( italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) . Id", "snippet": "So the step size h h italic_h should be chosen based on the landscape of the function ‚Ñí ‚Äã ( Œ∏ k ) \\mathcal{L}(\\theta_{k}) caligraphic_L ( italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) . Ideally, to choose the best step size h h italic_h , we can solve the following optimization problem over a single variable h h italic_h : h = arg ‚Äã min h ‚â• 0 ‚Å° ‚Ñí ‚Äã ( Œ∏ k ‚àí h ‚Äã ‚àá ‚Ñí ‚Äã ( Œ∏ k ) ) . h=\\operatorname*{arg\\ min}_{h\\geq 0}\\mathcal{L}(\\theta_{k}-h\\nabla\\mathcal{L}(\\theta_{k})). italic_h = start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_h ‚â• 0 end_POSTSUBSCRIPT calig"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS1.SSSx1.p3", "title": "Then how should we choose a proper step size h h italic_h ? One common and classical approach is to try to obtain a good approximation of the local landscape around the current state Œ∏ \\theta italic_Œ∏", "snippet": "Then how should we choose a proper step size h h italic_h ? One common and classical approach is to try to obtain a good approximation of the local landscape around the current state Œ∏ \\theta italic_Œ∏ based on some knowledge about the overall landscape of the function ‚Ñí ‚Äã ( Œ∏ ) \\mathcal{L}(\\theta) caligraphic_L ( italic_Œ∏ ) ."}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS1.SSSx1.p4", "title": "Common conditions for the landscape of ‚Ñí ‚Äã ( Œ∏ ) \\mathcal{L}(\\theta) caligraphic_L ( italic_Œ∏ ) include: ‚Ä¢ Œ± \\alpha italic_Œ± -Strong Convexity. Recall that ‚Ñí \\mathcal{L} caligraphic_L is Œ± \\alpha ital", "snippet": "Common conditions for the landscape of ‚Ñí ‚Äã ( Œ∏ ) \\mathcal{L}(\\theta) caligraphic_L ( italic_Œ∏ ) include: ‚Ä¢ Œ± \\alpha italic_Œ± -Strong Convexity. Recall that ‚Ñí \\mathcal{L} caligraphic_L is Œ± \\alpha italic_Œ± -strongly convex if its graph lies above a global quadratic lower bound of slope Œ± \\alpha italic_Œ± , i.e., ‚Ñí ‚Äã ( Œ∏ ) ‚â• l Œ∏ 0 , Œ± ‚Äã ( Œ∏ ) ‚âê ‚Ñí ‚Äã ( Œ∏ 0 ) + ‚ü® ‚àá ‚Ñí ‚Äã ( Œ∏ 0 ) , Œ∏ ‚àí Œ∏ 0 ‚ü© + Œ± 2 ‚Äã ‚Äñ Œ∏ ‚àí Œ∏ 0 ‚Äñ 2 2 \\mathcal{L}(\\theta)\\geq l_{\\theta_{0},\\alpha}(\\theta)\\doteq\\mathcal{L}(\\theta_{0})+\\langle\\nabla\\mathcal{L}(\\theta_{0}),\\theta-\\theta_{0}\\rangle+\\frac{\\alpha}{2}\\|\\theta-\\theta_{0}\\|_{2}^{2}"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.I1.i1.p1", "title": "Œ± \\alpha italic_Œ± -Strong Convexity. Recall that ‚Ñí \\mathcal{L} caligraphic_L is Œ± \\alpha italic_Œ± -strongly convex if its graph lies above a global quadratic lower bound of slope Œ± \\alpha italic_Œ± , i", "snippet": "Œ± \\alpha italic_Œ± -Strong Convexity. Recall that ‚Ñí \\mathcal{L} caligraphic_L is Œ± \\alpha italic_Œ± -strongly convex if its graph lies above a global quadratic lower bound of slope Œ± \\alpha italic_Œ± , i.e., ‚Ñí ‚Äã ( Œ∏ ) ‚â• l Œ∏ 0 , Œ± ‚Äã ( Œ∏ ) ‚âê ‚Ñí ‚Äã ( Œ∏ 0 ) + ‚ü® ‚àá ‚Ñí ‚Äã ( Œ∏ 0 ) , Œ∏ ‚àí Œ∏ 0 ‚ü© + Œ± 2 ‚Äã ‚Äñ Œ∏ ‚àí Œ∏ 0 ‚Äñ 2 2 \\mathcal{L}(\\theta)\\geq l_{\\theta_{0},\\alpha}(\\theta)\\doteq\\mathcal{L}(\\theta_{0})+\\langle\\nabla\\mathcal{L}(\\theta_{0}),\\theta-\\theta_{0}\\rangle+\\frac{\\alpha}{2}\\|\\theta-\\theta_{0}\\|_{2}^{2} caligraphic_L ( italic_Œ∏ ) ‚â• italic_l start_POSTSUBSCRIPT italic_Œ∏ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.I1.i2.p1", "title": "Œ≤ \\beta italic_Œ≤ -Lipschitz Gradient (also called Œ≤ \\beta italic_Œ≤ -Smoothness). Recall that ‚Ñí \\mathcal{L} caligraphic_L has Œ≤ \\beta italic_Œ≤ -Lipschitz gradient if ‚àá ‚Ñí \\nabla\\mathcal{L} ‚àá caligraphic", "snippet": "Œ≤ \\beta italic_Œ≤ -Lipschitz Gradient (also called Œ≤ \\beta italic_Œ≤ -Smoothness). Recall that ‚Ñí \\mathcal{L} caligraphic_L has Œ≤ \\beta italic_Œ≤ -Lipschitz gradient if ‚àá ‚Ñí \\nabla\\mathcal{L} ‚àá caligraphic_L exists and is Œ≤ \\beta italic_Œ≤ -Lipschitz, i.e., ‚Äñ ‚àá ‚Ñí ‚Äã ( Œ∏ ) ‚àí ‚àá ‚Ñí ‚Äã ( Œ∏ 0 ) ‚Äñ 2 ‚â§ Œ≤ ‚Äã ‚Äñ Œ∏ ‚àí Œ∏ 0 ‚Äñ 2 . \\|\\nabla\\mathcal{L}(\\theta)-\\nabla\\mathcal{L}(\\theta_{0})\\|_{2}\\leq\\beta\\|\\theta-\\theta_{0}\\|_{2}. ‚à• ‚àá caligraphic_L ( italic_Œ∏ ) - ‚àá caligraphic_L ( italic_Œ∏ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ‚â§ italic_Œ≤ ‚à• italic_Œ∏ - italic_Œ∏ start_POSTSUBSCR"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmlemma1.p1", "title": "Suppose that u : Œò ‚Üí ‚Ñù u\\colon\\Theta\\to\\mathbb{R} italic_u : roman_Œò ‚Üí blackboard_R is a global upper bound on ‚Ñí \\mathcal{L} caligraphic_L , namely ‚Ñí ‚Äã ( Œ∏ ) ‚â§ u ‚Äã ( Œ∏ ) \\mathcal{L}(\\theta)\\leq u(\\the", "snippet": "Suppose that u : Œò ‚Üí ‚Ñù u\\colon\\Theta\\to\\mathbb{R} italic_u : roman_Œò ‚Üí blackboard_R is a global upper bound on ‚Ñí \\mathcal{L} caligraphic_L , namely ‚Ñí ‚Äã ( Œ∏ ) ‚â§ u ‚Äã ( Œ∏ ) \\mathcal{L}(\\theta)\\leq u(\\theta) caligraphic_L ( italic_Œ∏ ) ‚â§ italic_u ( italic_Œ∏ ) for all Œ∏ ‚àà Œò \\theta\\in\\Theta italic_Œ∏ ‚àà roman_Œò . Suppose that they meet with equality at Œ∏ 0 \\theta_{0} italic_Œ∏ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , i.e., ‚Ñí ‚Äã ( Œ∏ 0 ) = u ‚Äã ( Œ∏ 0 ) \\mathcal{L}(\\theta_{0})=u(\\theta_{0}) caligraphic_L ( italic_Œ∏ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) = italic_u ( italic_Œ∏ start_POSTSUBSCRIPT 0 end_POS"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS1.SSSx1.p5", "title": "We will use this lemma to show that we can use the Lipschitz gradient property to ensure that each gradient step cannot worsen the value of ‚Ñí \\mathcal{L} caligraphic_L . Indeed, at every base point Œ∏ ", "snippet": "We will use this lemma to show that we can use the Lipschitz gradient property to ensure that each gradient step cannot worsen the value of ‚Ñí \\mathcal{L} caligraphic_L . Indeed, at every base point Œ∏ 0 \\theta_{0} italic_Œ∏ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , we have that u Œ∏ 0 , Œ≤ u_{\\theta_{0},\\beta} italic_u start_POSTSUBSCRIPT italic_Œ∏ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_Œ≤ end_POSTSUBSCRIPT is a global upper bound on ‚Ñí \\mathcal{L} caligraphic_L , and u Œ∏ 0 , Œ≤ ‚Äã ( Œ∏ 0 ) = ‚Ñí ‚Äã ( Œ∏ 0 ) u_{\\theta_{0},\\beta}(\\theta_{0})=\\mathcal{L}(\\theta_{0}) italic_u start_POSTSUBSCRIPT ital"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS1.SSSx1.p6", "title": "Now, let us suppose that ‚Ñí \\mathcal{L} caligraphic_L is Œ± \\alpha italic_Œ± -strongly convex, has Œ≤ \\beta italic_Œ≤ -Lipschitz gradient, and has global optimum Œ∏ ‚ãÜ \\theta^{\\star} italic_Œ∏ start_POSTSUPER", "snippet": "Now, let us suppose that ‚Ñí \\mathcal{L} caligraphic_L is Œ± \\alpha italic_Œ± -strongly convex, has Œ≤ \\beta italic_Œ≤ -Lipschitz gradient, and has global optimum Œ∏ ‚ãÜ \\theta^{\\star} italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT . We will show that Œ∏ k \\theta_{k} italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT will converge directly to the unique global optimum Œ∏ ‚ãÜ \\theta^{\\star} italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT , which is a very strong form of convergence. In particular, we will bound ‚Äñ Œ∏ ‚ãÜ ‚àí Œ∏ k ‚Äñ 2 \\|\\theta^{\\star}-\\theta_{k}\\|_{2} ‚à• italic_Œ∏ start_POSTSUPERSCRIP"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS1.SSSx1.p7", "title": "We end this section with a caveat: learning a global optimum is (usually) impractically hard. Under certain conditions, we can ensure that the gradient descent iterates converge to a local optimum . A", "snippet": "We end this section with a caveat: learning a global optimum is (usually) impractically hard. Under certain conditions, we can ensure that the gradient descent iterates converge to a local optimum . Also, under more relaxed conditions, we can ensure local convergence, i.e., that the iterates converge to a (global or local) optimum if the sequence is initialized close enough to the optimum."}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS2.SSSx1.p1", "title": "There are some smooth problems and strongly convex problems on which gradient descent nonetheless does quite poorly. For example, let Œª ‚â• 0 \\lambda\\geq 0 italic_Œª ‚â• 0 and let ‚Ñí Œª : ‚Ñù 2 ‚Üí ‚Ñù \\mathcal{L}", "snippet": "There are some smooth problems and strongly convex problems on which gradient descent nonetheless does quite poorly. For example, let Œª ‚â• 0 \\lambda\\geq 0 italic_Œª ‚â• 0 and let ‚Ñí Œª : ‚Ñù 2 ‚Üí ‚Ñù \\mathcal{L}_{\\lambda}\\colon\\mathbb{R}^{2}\\to\\mathbb{R} caligraphic_L start_POSTSUBSCRIPT italic_Œª end_POSTSUBSCRIPT : blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ‚Üí blackboard_R of the form ‚Ñí Œª ‚Äã ( Œ∏ ) = ‚Ñí Œª ‚Äã ( [ Œ∏ 1 Œ∏ 2 ] ) ‚âê 1 2 ‚Äã { ( 1 + Œª ) ‚Äã Œ∏ 1 2 + Œ∏ 2 2 } = 1 2 ‚Äã Œ∏ ‚ä§ ‚Äã [ 1 + Œª 0 0 1 ] ‚Äã Œ∏ . \\mathcal{L}_{\\lambda}(\\theta)=\\mathcal{L}_{\\lambda}\\left(\\begin{bmatrix}\\theta_{1}\\\\ \\theta_{2}\\end"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS2.SSSx1.p2", "title": "The key lies in the objective‚Äôs curvature , which is given by the Hessian. Suppose that (as a counterfactual) we had a second-order oracle which would allow us to compute ‚Ñí ‚Äã ( Œ∏ ) \\mathcal{L}(\\theta)", "snippet": "The key lies in the objective‚Äôs curvature , which is given by the Hessian. Suppose that (as a counterfactual) we had a second-order oracle which would allow us to compute ‚Ñí ‚Äã ( Œ∏ ) \\mathcal{L}(\\theta) caligraphic_L ( italic_Œ∏ ) , ‚àá ‚Ñí ‚Äã ( Œ∏ ) \\nabla\\mathcal{L}(\\theta) ‚àá caligraphic_L ( italic_Œ∏ ) , and ‚àá 2 ‚Ñí ‚Äã ( Œ∏ ) \\nabla^{2}\\mathcal{L}(\\theta) ‚àá start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT caligraphic_L ( italic_Œ∏ ) . Then, instead of picking a descent direction ùíó \\bm{v} bold_italic_v to optimize the first-order Taylor expansion around Œ∏ \\theta italic_Œ∏ , we could optimize the second-order Tay"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS2.SSSx1.p3", "title": "Let us carry out this computation. The second-order Taylor expansion of ‚Ñí ‚Äã ( Œ∏ + h ‚Äã ùíó ) \\mathcal{L}(\\theta+h\\bm{v}) caligraphic_L ( italic_Œ∏ + italic_h bold_italic_v ) around h = 0 h=0 italic_h = 0 ", "snippet": "Let us carry out this computation. The second-order Taylor expansion of ‚Ñí ‚Äã ( Œ∏ + h ‚Äã ùíó ) \\mathcal{L}(\\theta+h\\bm{v}) caligraphic_L ( italic_Œ∏ + italic_h bold_italic_v ) around h = 0 h=0 italic_h = 0 is ‚Ñí ‚Äã ( Œ∏ + h ‚Äã ùíó ) = ‚Ñí ‚Äã ( Œ∏ ) + h ‚Äã ‚ü® ‚àá ‚Ñí ‚Äã ( Œ∏ ) , ùíó ‚ü© + 1 2 ‚Äã h 2 ‚Äã ‚ü® [ ‚àá 2 ‚Ñí ‚Äã ( Œ∏ ) ] ‚Äã ùíó , ùíó ‚ü© + o ‚Äã ( h 2 ) . \\mathcal{L}(\\theta+h\\bm{v})=\\mathcal{L}(\\theta)+h\\langle\\nabla\\mathcal{L}(\\theta),\\bm{v}\\rangle+\\frac{1}{2}h^{2}\\langle[\\nabla^{2}\\mathcal{L}(\\theta)]\\bm{v},\\bm{v}\\rangle+o(h^{2}). caligraphic_L ( italic_Œ∏ + italic_h bold_italic_v ) = caligraphic_L ( italic_Œ∏ ) + italic_h ‚ü® ‚àá cali"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS2.SSSx2.p1", "title": "In practice, we do not have a second-order oracle which allows us to compute ‚àá 2 ‚Ñí ‚Äã ( Œ∏ ) \\nabla^{2}\\mathcal{L}(\\theta) ‚àá start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT caligraphic_L ( italic_Œ∏ ) . Inst", "snippet": "In practice, we do not have a second-order oracle which allows us to compute ‚àá 2 ‚Ñí ‚Äã ( Œ∏ ) \\nabla^{2}\\mathcal{L}(\\theta) ‚àá start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT caligraphic_L ( italic_Œ∏ ) . Instead, we can attempt to learn an approximation to it alongside the parameter update Œ∏ k + 1 \\theta_{k+1} italic_Œ∏ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT from Œ∏ k \\theta_{k} italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ."}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS2.SSSx2.p2", "title": "How do we learn an approximation to it? We shall find some equations which the Hessian‚Äôs inverse satisfies and then try to update our approximation so that it satisfies the equations. Namely, taking t", "snippet": "How do we learn an approximation to it? We shall find some equations which the Hessian‚Äôs inverse satisfies and then try to update our approximation so that it satisfies the equations. Namely, taking the Taylor series of ‚àá ‚Ñí ‚Äã ( Œ∏ + Œ¥ Œ∏ ) \\nabla\\mathcal{L}(\\theta+\\delta_{\\theta}) ‚àá caligraphic_L ( italic_Œ∏ + italic_Œ¥ start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ) around point Œ∏ \\theta italic_Œ∏ , we obtain ‚àá L ‚Äã ( Œ∏ + Œ¥ Œ∏ ) ‚àí ‚àá ‚Ñí ‚Äã ( Œ∏ ) ‚èü ‚âê Œ¥ ùíà = [ ‚àá 2 ‚Ñí ‚Äã ( Œ∏ ) ] ‚Äã Œ¥ Œ∏ + o ‚Äã ( ‚Äñ Œ¥ Œ∏ ‚Äñ 2 ) . \\underbrace{\\nabla L(\\theta+\\delta_{\\theta})-\\nabla\\mathcal{L}(\\theta)}_{\\doteq\\delta_{\\bm{g}}}=[\\nabl"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS2.SSSx2.p3", "title": "We end this subsection with a caveat: in deep learning, for example, ‚Ñí \\mathcal{L} caligraphic_L is not a convex function and so Newton‚Äôs method (and approximations to it) do not make sense. In this c", "snippet": "We end this subsection with a caveat: in deep learning, for example, ‚Ñí \\mathcal{L} caligraphic_L is not a convex function and so Newton‚Äôs method (and approximations to it) do not make sense. In this case we look at the geometric intuition of Newton‚Äôs method on convex functions, say from Figure A.3 : the inverse Hessian whitens the gradients. Thus instead of a Hessian-approximating preconditioner, we can adjust the above procedures to learn a more general whitening transformation for the gradient. This is the idea behind the original proposal of PSGD [ Li17 ] , which contains more information a"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS3.p1", "title": "Even in very toy problems, however, such as LASSO or dictionary learning, the problem is not strongly convex but rather just convex, and the objective is no longer just smooth but rather the sum of a ", "snippet": "Even in very toy problems, however, such as LASSO or dictionary learning, the problem is not strongly convex but rather just convex, and the objective is no longer just smooth but rather the sum of a smooth function and a non-smooth regularizer (such as the ‚Ñì 1 \\ell^{1} roman_‚Ñì start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT norm). Such problems are solved by proximal optimization algorithms , which generalize steepest descent to non-smooth objectives."}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS3.p2", "title": "Formally, let us say that ‚Ñí ‚Äã ( Œ∏ ) ‚âê ùíÆ ‚Äã ( Œ∏ ) + ‚Ñõ ‚Äã ( Œ∏ ) \\mathcal{L}(\\theta)\\doteq\\mathcal{S}(\\theta)+\\mathcal{R}(\\theta) caligraphic_L ( italic_Œ∏ ) ‚âê caligraphic_S ( italic_Œ∏ ) + caligraphic_R ( i", "snippet": "Formally, let us say that ‚Ñí ‚Äã ( Œ∏ ) ‚âê ùíÆ ‚Äã ( Œ∏ ) + ‚Ñõ ‚Äã ( Œ∏ ) \\mathcal{L}(\\theta)\\doteq\\mathcal{S}(\\theta)+\\mathcal{R}(\\theta) caligraphic_L ( italic_Œ∏ ) ‚âê caligraphic_S ( italic_Œ∏ ) + caligraphic_R ( italic_Œ∏ ) (A.1.33) where ùíÆ \\mathcal{S} caligraphic_S is smooth, say with Œ≤ \\beta italic_Œ≤ -Lipschitz gradient, and ‚Ñõ \\mathcal{R} caligraphic_R is non-smooth (i.e., rough). The proximal gradient algorithm generalizes the steepest descent algorithm, by using the majorization-minimization framework (i.e., Lemma A.1 ) with a different global upper bound. Namely, we construct such an upper bound by ask"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.I2.i1.p1", "title": "is close to the gradient update Œ∏ 0 ‚àí 1 Œ≤ ‚Äã ‚àá ùíÆ ‚Äã ( Œ∏ 0 ) \\theta_{0}-\\frac{1}{\\beta}\\nabla\\mathcal{S}(\\theta_{0}) italic_Œ∏ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT - divide start_ARG 1 end_ARG start_AR", "snippet": "is close to the gradient update Œ∏ 0 ‚àí 1 Œ≤ ‚Äã ‚àá ùíÆ ‚Äã ( Œ∏ 0 ) \\theta_{0}-\\frac{1}{\\beta}\\nabla\\mathcal{S}(\\theta_{0}) italic_Œ∏ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT - divide start_ARG 1 end_ARG start_ARG italic_Œ≤ end_ARG ‚àá caligraphic_S ( italic_Œ∏ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) ;"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.I2.i2.p1", "title": "has a small value of the regularizer ‚Ñõ ‚Äã ( Œ∏ 1 ) \\mathcal{R}(\\theta_{1}) caligraphic_R ( italic_Œ∏ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT )", "snippet": "has a small value of the regularizer ‚Ñõ ‚Äã ( Œ∏ 1 ) \\mathcal{R}(\\theta_{1}) caligraphic_R ( italic_Œ∏ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT )"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS3.p3", "title": "One remaining question is: how can we compute the proximal operator? At first glance, it seems like we have traded one intractable minimization problem for another. Since we have not made any assumpti", "snippet": "One remaining question is: how can we compute the proximal operator? At first glance, it seems like we have traded one intractable minimization problem for another. Since we have not made any assumption on ‚Ñõ \\mathcal{R} caligraphic_R so far, the framework works even when ‚Ñõ \\mathcal{R} caligraphic_R is a very complex function (such as a neural network loss), which would require us to solve a neural network training problem in order to compute a single proximal operator. However, in practice, for simple regularizers ‚Ñõ \\mathcal{R} caligraphic_R such as those we use in this manuscript, there exist"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmexample1.p1", "title": "Let Œì ‚äÜ Œò \\Gamma\\subseteq\\Theta roman_Œì ‚äÜ roman_Œò be a set, and let œá Œì \\chi_{\\Gamma} italic_œá start_POSTSUBSCRIPT roman_Œì end_POSTSUBSCRIPT be the characteristic function on Œì \\Gamma roman_Œì , i.e., ", "snippet": "Let Œì ‚äÜ Œò \\Gamma\\subseteq\\Theta roman_Œì ‚äÜ roman_Œò be a set, and let œá Œì \\chi_{\\Gamma} italic_œá start_POSTSUBSCRIPT roman_Œì end_POSTSUBSCRIPT be the characteristic function on Œì \\Gamma roman_Œì , i.e., œá Œì ‚Äã ( Œ∏ ) ‚âê { 0 , if ‚Äã Œ∏ ‚àà Œì + ‚àû , if ‚Äã Œ∏ ‚àâ Œì . \\chi_{\\Gamma}(\\theta)\\doteq\\begin{cases}0,&\\text{if}\\ \\theta\\in\\Gamma\\\\ +\\infty,&\\text{if}\\ \\theta\\notin\\Gamma.\\end{cases} italic_œá start_POSTSUBSCRIPT roman_Œì end_POSTSUBSCRIPT ( italic_Œ∏ ) ‚âê { start_ROW start_CELL 0 , end_CELL start_CELL if italic_Œ∏ ‚àà roman_Œì end_CELL end_ROW start_ROW start_CELL + ‚àû , end_CELL start_CELL if italic_Œ∏ ‚àâ roman_Œì . "}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmexample2.p1", "title": "The ‚Ñì 1 \\ell^{1} roman_‚Ñì start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT norm has a proximal operator which performs soft thresholding: S h ‚Äã ( Œ∏ ) ‚âê prox h , Œª ‚à• ‚ãÖ ‚à• 1 ‚Å° ( Œ∏ ) = arg ‚Äã min Œ∏ 1 ‚Å° [ 1 2 ‚Äã h", "snippet": "The ‚Ñì 1 \\ell^{1} roman_‚Ñì start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT norm has a proximal operator which performs soft thresholding: S h ‚Äã ( Œ∏ ) ‚âê prox h , Œª ‚à• ‚ãÖ ‚à• 1 ‚Å° ( Œ∏ ) = arg ‚Äã min Œ∏ 1 ‚Å° [ 1 2 ‚Äã h ‚Äã ‚Äñ Œ∏ 1 ‚àí Œ∏ ‚Äñ 2 2 + Œª ‚Äã ‚Äñ Œ∏ ‚Äñ 1 ] S_{h}(\\theta)\\doteq\\operatorname{prox}_{h,\\lambda\\|\\cdot\\|_{1}}(\\theta)=\\operatorname*{arg\\ min}_{\\theta_{1}}\\left[\\frac{1}{2h}\\|\\theta_{1}-\\theta\\|_{2}^{2}+\\lambda\\|\\theta\\|_{1}\\right] italic_S start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ( italic_Œ∏ ) ‚âê roman_prox start_POSTSUBSCRIPT italic_h , italic_Œª ‚à• ‚ãÖ ‚à• start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUB"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmexample3.p1", "title": "In Chapter 4 we use a proximal operator corresponding to the ‚Ñì 1 \\ell^{1} roman_‚Ñì start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT norm plus the characteristic function for the positive orthant ‚Ñù + n ‚âê { ùíô", "snippet": "In Chapter 4 we use a proximal operator corresponding to the ‚Ñì 1 \\ell^{1} roman_‚Ñì start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT norm plus the characteristic function for the positive orthant ‚Ñù + n ‚âê { ùíô ‚àà ‚Ñù n : x i ‚â• 0 ‚Äã ‚àÄ i } \\mathbb{R}_{+}^{n}\\doteq\\{\\bm{x}\\in\\mathbb{R}^{n}\\colon x_{i}\\geq 0\\ \\forall i\\} blackboard_R start_POSTSUBSCRIPT + end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ‚âê { bold_italic_x ‚àà blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT : italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ‚â• 0 ‚àÄ italic_i } , namely T h ‚Äã ( Œ∏ ) ‚âê prox h ,"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS4.p1", "title": "In deep learning, the objective function ‚Ñí \\mathcal{L} caligraphic_L usually cannot be computed exactly, and instead at each optimization step it is estimated using finite samples (say, using a mini-b", "snippet": "In deep learning, the objective function ‚Ñí \\mathcal{L} caligraphic_L usually cannot be computed exactly, and instead at each optimization step it is estimated using finite samples (say, using a mini-batch). A common way to model this situation is to define a stochastic loss function ‚Ñí œâ ‚Äã ( Œ∏ ) \\mathcal{L}_{\\omega}(\\theta) caligraphic_L start_POSTSUBSCRIPT italic_œâ end_POSTSUBSCRIPT ( italic_Œ∏ ) where œâ \\omega italic_œâ is some ‚Äúsource of randomness‚Äù. For example, œâ \\omega italic_œâ could contain the indices of the samples in a batch over which to compute the loss. Then, we would like to minimiz"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS4.p2", "title": "The basic first-order stochastic algorithm is stochastic gradient descent : at each iteration k k italic_k we sample œâ k \\omega_{k} italic_œâ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , define ‚Ñí k", "snippet": "The basic first-order stochastic algorithm is stochastic gradient descent : at each iteration k k italic_k we sample œâ k \\omega_{k} italic_œâ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , define ‚Ñí k ‚âê ‚Ñí œâ k \\mathcal{L}_{k}\\doteq\\mathcal{L}_{\\omega_{k}} caligraphic_L start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ‚âê caligraphic_L start_POSTSUBSCRIPT italic_œâ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT , and perform a gradient step on ‚Ñí k \\mathcal{L}_{k} caligraphic_L start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , i.e., Œ∏ k + 1 = Œ∏ k ‚àí h ‚Äã ‚àá ‚Ñí k ‚Äã ( Œ∏ k ) . \\theta_{k+1}=\\"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS4.p3", "title": "In order to fix this, we can either average the parameters Œ∏ k \\theta_{k} italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT or average the gradients ‚àá ‚Ñí k ‚Äã ( Œ∏ k ) \\nabla\\mathcal{L}_{k}(\\theta_", "snippet": "In order to fix this, we can either average the parameters Œ∏ k \\theta_{k} italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT or average the gradients ‚àá ‚Ñí k ‚Äã ( Œ∏ k ) \\nabla\\mathcal{L}_{k}(\\theta_{k}) ‚àá caligraphic_L start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) over time. If we average the parameters Œ∏ k \\theta_{k} italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , then (using Figure A.4 as a mental model) the issue of pinballing is straightforwardly not possible, since the average iterate will grow closer to the center. "}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS4.p4", "title": "In practice, instead of using an arithmetic average, we take an exponentially moving average (EMA) of the parameters (this is called Polyak momentum ) or of the gradients (this is called Nesterov mome", "snippet": "In practice, instead of using an arithmetic average, we take an exponentially moving average (EMA) of the parameters (this is called Polyak momentum ) or of the gradients (this is called Nesterov momentum ). Nesterov momentum is more popular and we will study it here."}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS4.p5", "title": "A stochastic gradient descent iteration with Nesterov momentum is as follows: ùíà k \\displaystyle\\bm{g}_{k} bold_italic_g start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = Œ≤ ‚Äã ùíà k ‚àí 1 + ( 1 ‚àí Œ≤ ) ‚Äã ‚àá ‚Ñí k", "snippet": "A stochastic gradient descent iteration with Nesterov momentum is as follows: ùíà k \\displaystyle\\bm{g}_{k} bold_italic_g start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = Œ≤ ‚Äã ùíà k ‚àí 1 + ( 1 ‚àí Œ≤ ) ‚Äã ‚àá ‚Ñí k ‚Äã ( Œ∏ k ) \\displaystyle=\\beta\\bm{g}_{k-1}+(1-\\beta)\\nabla\\mathcal{L}_{k}(\\theta_{k}) = italic_Œ≤ bold_italic_g start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT + ( 1 - italic_Œ≤ ) ‚àá caligraphic_L start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) (A.1.46) Œ∏ k + 1 \\displaystyle\\theta_{k+1} italic_Œ∏ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUB"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS4.p6", "title": "We end with a caveat: one can show that Polyak momentum and Nesterov momentum are equivalent, for certain choices of parameter settings. Then it is also possible to show that a decaying learning rate ", "snippet": "We end with a caveat: one can show that Polyak momentum and Nesterov momentum are equivalent, for certain choices of parameter settings. Then it is also possible to show that a decaying learning rate schedule (i.e., the learning rate h h italic_h depends on the iteration k k italic_k , and its limit is h k ‚Üí 0 h_{k}\\to 0 italic_h start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ‚Üí 0 as k ‚Üí ‚àû k\\to\\infty italic_k ‚Üí ‚àû ) with plain SGD (or PSGD) can mimic the effect of momentum. Namely, [ DCM+23 ] shows that if the SGD algorithm lasts K K italic_K iterations, the gradient norms are bounded ‚Äñ ‚àá ‚Ñí ‚Äã ( "}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS5.p1", "title": "The gradient descent scheme proposes an iteration of the form Œ∏ k + 1 = Œ∏ k + h ‚Äã ùíó k , \\theta_{k+1}=\\theta_{k}+h\\bm{v}_{k}, italic_Œ∏ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = italic_Œ∏ star", "snippet": "The gradient descent scheme proposes an iteration of the form Œ∏ k + 1 = Œ∏ k + h ‚Äã ùíó k , \\theta_{k+1}=\\theta_{k}+h\\bm{v}_{k}, italic_Œ∏ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + italic_h bold_italic_v start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , (A.1.48) where (recall) ùíó k \\bm{v}_{k} bold_italic_v start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT is chosen to be (proportional to) the steepest descent vector in the Euclidean norm: ùíó k = ‚àí ‚àá ‚Ñí ‚Äã ( Œ∏ k ) ‚Äñ ‚àá ‚Ñí ‚Äã ( Œ∏ k ) ‚Äñ 2 ‚àà arg ‚Äã min ùíó ‚àà ‚Ñù n ‚Äñ ùíó ‚Äñ 2 = 1 ‚Å° ‚ü® ‚àá ‚Ñí ‚Äã ( Œ∏ k ) , ùíó ‚ü© "}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS5.p2", "title": "Another way to view Adam, which partially explains its empirical success, is that it dynamically updates the learning rates for each parameter based on the squared gradients. In particular, notice tha", "snippet": "Another way to view Adam, which partially explains its empirical success, is that it dynamically updates the learning rates for each parameter based on the squared gradients. In particular, notice that we can write Œ∏ k + 1 = Œ∏ k ‚àí Œ∑ k ‚äô ùíà k where Œ∑ k = h ‚Äã ùíî k ‚äô ( ‚àí 1 2 ) \\theta_{k+1}=\\theta_{k}-\\eta_{k}\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\scalebox{0.8}{$\\scriptstyle\\odot$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\odot$}}$}}{\\raisebox{1.3pt}{$\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scaleb"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.p1", "title": "Above, we discussed several optimization algorithms for deep networks which assumed access to a first-order oracle , i.e., a device which would allow us to compute ‚Ñí ‚Äã ( Œ∏ ) \\mathcal{L}(\\theta) caligr", "snippet": "Above, we discussed several optimization algorithms for deep networks which assumed access to a first-order oracle , i.e., a device which would allow us to compute ‚Ñí ‚Äã ( Œ∏ ) \\mathcal{L}(\\theta) caligraphic_L ( italic_Œ∏ ) and ‚àá ‚Ñí ‚Äã ( Œ∏ ) \\nabla\\mathcal{L}(\\theta) ‚àá caligraphic_L ( italic_Œ∏ ) . For simple functions ‚Ñí \\mathcal{L} caligraphic_L , it is possible to do this by hand. However, for deep neural networks, this quickly becomes tedious, and hinders rapid experimentation. Hence we require a general algorithm which would allow us to efficiently compute the gradients of arbitrary (sub)differe"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.p2", "title": "In this section, we introduce the basics of automatic differentiation (AD or autodiff ), which is a computationally efficient way to compute gradients and Jacobians of general functions f : ‚Ñù m ‚Üí ‚Ñù n ", "snippet": "In this section, we introduce the basics of automatic differentiation (AD or autodiff ), which is a computationally efficient way to compute gradients and Jacobians of general functions f : ‚Ñù m ‚Üí ‚Ñù n f:\\mathbb{R}^{m}\\to\\mathbb{R}^{n} italic_f : blackboard_R start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ‚Üí blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT . We will show how this leads to the backpropagation algorithm for computing gradients of loss functions involving neural networks. A summary of the structure of this section is as follows: 1. We introduce differentials , a c"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.I1.i1.p1", "title": "We introduce differentials , a convenient formalism for calculating and organizing the derivatives of functions between high-dimensional parameter spaces (which may themselves be products of other spa", "snippet": "We introduce differentials , a convenient formalism for calculating and organizing the derivatives of functions between high-dimensional parameter spaces (which may themselves be products of other spaces involving matrices, tensors, etc.)."}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.I1.i2.p1", "title": "We describe the basics of forward-mode and reverse-mode automatic differentiation, which involves considerations that are important for efficient computation of gradients/Jacobians for different kinds", "snippet": "We describe the basics of forward-mode and reverse-mode automatic differentiation, which involves considerations that are important for efficient computation of gradients/Jacobians for different kinds of functions arising in machine learning applications."}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.I1.i3.p1", "title": "We describe backpropagation in the special case of a loss function applied to a stack-of-layers neural network as an instantiation of reverse-mode automatic differentiation.", "snippet": "We describe backpropagation in the special case of a loss function applied to a stack-of-layers neural network as an instantiation of reverse-mode automatic differentiation."}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.p3", "title": "Our treatment will err on the mathematical side, to give the reader a deep understanding of the underlying mathematics. The reader should ensure to couple this understanding with a strong grasp of pra", "snippet": "Our treatment will err on the mathematical side, to give the reader a deep understanding of the underlying mathematics. The reader should ensure to couple this understanding with a strong grasp of practical aspects of automatic differentiation for deep learning, for example as manifested in the outstanding tutorial of [ Kar22a ] ."}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.SS1.p1", "title": "A full accounting of this subsection is given in the excellent guide [ BEJ25 ] . To motivate differentials, let us first consider the simple example of a differentiable function ‚Ñí : ‚Ñù ‚Üí ‚Ñù \\mathcal{L}\\", "snippet": "A full accounting of this subsection is given in the excellent guide [ BEJ25 ] . To motivate differentials, let us first consider the simple example of a differentiable function ‚Ñí : ‚Ñù ‚Üí ‚Ñù \\mathcal{L}\\colon\\mathbb{R}\\to\\mathbb{R} caligraphic_L : blackboard_R ‚Üí blackboard_R acting on a parameter Œ∏ \\theta italic_Œ∏ . We can write ‚Ñí ‚Äã ( Œ∏ ) ‚àí ‚Ñí ‚Äã ( Œ∏ 0 ) = ‚Ñí ‚Ä≤ ‚Äã ( Œ∏ 0 ) ‚ãÖ ( Œ∏ ‚àí Œ∏ 0 ) + o ‚Äã ( | Œ∏ ‚àí Œ∏ 0 | ) . \\mathcal{L}(\\theta)-\\mathcal{L}(\\theta_{0})=\\mathcal{L}^{\\prime}(\\theta_{0})\\cdot(\\theta-\\theta_{0})+o(\\lvert\\theta-\\theta_{0}\\rvert). caligraphic_L ( italic_Œ∏ ) - caligraphic_L ( italic_Œ∏ start"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.SS1.p2", "title": "Let‚Äôs see how this works for a higher dimensions, i.e., ‚Ñí : ‚Ñù n ‚Üí ‚Ñù \\mathcal{L}\\colon\\mathbb{R}^{n}\\to\\mathbb{R} caligraphic_L : blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ‚Üí black", "snippet": "Let‚Äôs see how this works for a higher dimensions, i.e., ‚Ñí : ‚Ñù n ‚Üí ‚Ñù \\mathcal{L}\\colon\\mathbb{R}^{n}\\to\\mathbb{R} caligraphic_L : blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ‚Üí blackboard_R . Then we still have d ‚Äã ‚Ñí = ‚Ñí ‚Ä≤ ‚Äã ( Œ∏ ) ‚ãÖ d ‚Äã Œ∏ \\mathrm{d}\\mathcal{L}=\\mathcal{L}^{\\prime}(\\theta)\\cdot\\mathrm{d}\\theta roman_d caligraphic_L = caligraphic_L start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_Œ∏ ) ‚ãÖ roman_d italic_Œ∏ (A.2.4) for some notion of a derivative ‚Ñí ‚Ä≤ ‚Äã ( Œ∏ ) \\mathcal{L}^{\\prime}(\\theta) caligraphic_L start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_Œ∏ ) . Sin"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.SS1.p3", "title": "Now consider a higher-order tensor function ‚Ñí : ‚Ñù m √ó n ‚Üí ‚Ñù p √ó q \\mathcal{L}\\colon\\mathbb{R}^{m\\times n}\\to\\mathbb{R}^{p\\times q} caligraphic_L : blackboard_R start_POSTSUPERSCRIPT italic_m √ó italic_", "snippet": "Now consider a higher-order tensor function ‚Ñí : ‚Ñù m √ó n ‚Üí ‚Ñù p √ó q \\mathcal{L}\\colon\\mathbb{R}^{m\\times n}\\to\\mathbb{R}^{p\\times q} caligraphic_L : blackboard_R start_POSTSUPERSCRIPT italic_m √ó italic_n end_POSTSUPERSCRIPT ‚Üí blackboard_R start_POSTSUPERSCRIPT italic_p √ó italic_q end_POSTSUPERSCRIPT . Then our basic linearization equation is insufficient for this case: d ‚Äã ‚Ñí = ‚Ñí ‚Ä≤ ‚Äã ( Œ∏ ) ‚ãÖ d ‚Äã Œ∏ \\mathrm{d}\\mathcal{L}=\\mathcal{L}^{\\prime}(\\theta)\\cdot\\mathrm{d}\\theta roman_d caligraphic_L = caligraphic_L start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_Œ∏ ) ‚ãÖ roman_d italic_Œ∏ does not make se"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.SS1.p4", "title": "Namely, we consider ‚Ñí ‚Ä≤ ‚Äã ( Œ∏ ) \\mathcal{L}^{\\prime}(\\theta) caligraphic_L start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_Œ∏ ) as a linear transformation whose input is Œ∏ \\theta italic_Œ∏ -space a", "snippet": "Namely, we consider ‚Ñí ‚Ä≤ ‚Äã ( Œ∏ ) \\mathcal{L}^{\\prime}(\\theta) caligraphic_L start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_Œ∏ ) as a linear transformation whose input is Œ∏ \\theta italic_Œ∏ -space and whose output is ‚Ñí \\mathcal{L} caligraphic_L -space, which takes in a small change in Œ∏ \\theta italic_Œ∏ and outputs the corresponding small change in ‚Ñí \\mathcal{L} caligraphic_L . Namely, we can write d ‚Äã ‚Ñí = ‚Ñí ‚Ä≤ ‚Äã ( Œ∏ ) ‚Äã [ d ‚Äã Œ∏ ] . \\mathrm{d}\\mathcal{L}=\\mathcal{L}^{\\prime}(\\theta)[\\mathrm{d}\\theta]. roman_d caligraphic_L = caligraphic_L start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_Œ∏ "}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmtheorem1.p1", "title": "Suppose ‚Ñí = f ‚àò g \\mathcal{L}=f\\circ g caligraphic_L = italic_f ‚àò italic_g where f f italic_f and g g italic_g are differentiable. Then d ‚Äã ‚Ñí = f ‚Ä≤ ‚Äã ( g ‚Äã ( Œ∏ ) ) ‚Äã g ‚Ä≤ ‚Äã ( Œ∏ ) ‚Äã [ d ‚Äã Œ∏ ] , \\mathrm{", "snippet": "Suppose ‚Ñí = f ‚àò g \\mathcal{L}=f\\circ g caligraphic_L = italic_f ‚àò italic_g where f f italic_f and g g italic_g are differentiable. Then d ‚Äã ‚Ñí = f ‚Ä≤ ‚Äã ( g ‚Äã ( Œ∏ ) ) ‚Äã g ‚Ä≤ ‚Äã ( Œ∏ ) ‚Äã [ d ‚Äã Œ∏ ] , \\mathrm{d}\\mathcal{L}=f^{\\prime}(g(\\theta))g^{\\prime}(\\theta)[\\mathrm{d}\\theta], roman_d caligraphic_L = italic_f start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_g ( italic_Œ∏ ) ) italic_g start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_Œ∏ ) [ roman_d italic_Œ∏ ] , (A.2.6) where (as usual) multiplication indicates composition of linear operators. In particular, ‚Ñí ‚Ä≤ ‚Äã ( Œ∏ ) = f ‚Ä≤ ‚Äã ( g ‚Äã ( Œ∏ ) ) ‚Äã g"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.SS1.p5", "title": "It is productive to think of the multivariate chain rule in functorial terms: composition of functions gets ‚Äòturned into‚Äô matrix multiplication of Jacobians (composition of linear operators!). We illu", "snippet": "It is productive to think of the multivariate chain rule in functorial terms: composition of functions gets ‚Äòturned into‚Äô matrix multiplication of Jacobians (composition of linear operators!). We illustrate the power of this result and this perspective through several examples."}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmexample4.p1", "title": "Consider the function f ‚Äã ( ùëø ) = ùëæ ‚Äã ùëø + ùíÉ ‚Äã ùüè ‚ä§ f(\\bm{X})=\\bm{W}\\bm{X}+\\bm{b}\\bm{1}^{\\top} italic_f ( bold_italic_X ) = bold_italic_W bold_italic_X + bold_italic_b bold_1 start_POSTSUPERSCRIPT ‚ä§ end", "snippet": "Consider the function f ‚Äã ( ùëø ) = ùëæ ‚Äã ùëø + ùíÉ ‚Äã ùüè ‚ä§ f(\\bm{X})=\\bm{W}\\bm{X}+\\bm{b}\\bm{1}^{\\top} italic_f ( bold_italic_X ) = bold_italic_W bold_italic_X + bold_italic_b bold_1 start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT . Then d ‚Äã f = f ‚Äã ( ùëø + d ‚Äã ùëø ) ‚àí f ‚Äã ( ùëø ) = [ ùëæ ‚Äã ( ùëø + d ‚Äã ùëø ) + ùíÉ ‚Äã ùüè ‚ä§ ] ‚àí [ ùëæ ‚Äã ùëø + ùíÉ ‚Äã ùüè ‚ä§ ] = ùëæ ‚Äã d ‚Äã ùëø . \\mathrm{d}f=f(\\bm{X}+\\mathrm{d}\\bm{X})-f(\\bm{X})=[\\bm{W}(\\bm{X}+\\mathrm{d}\\bm{X})+\\bm{b}\\bm{1}^{\\top}]-[\\bm{W}\\bm{X}+\\bm{b}\\bm{1}^{\\top}]=\\bm{W}\\mathrm{d}\\bm{X}. roman_d italic_f = italic_f ( bold_italic_X + roman_d bold_italic_X ) - italic_f ( bold_italic_X ) = [ bol"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmexample5.p1", "title": "Consider the function f = g ‚Äã h f=gh italic_f = italic_g italic_h where g , h g,h italic_g , italic_h are differentiable functions whose outputs can multiply together. Then f = p ‚àò v f=p\\circ v italic", "snippet": "Consider the function f = g ‚Äã h f=gh italic_f = italic_g italic_h where g , h g,h italic_g , italic_h are differentiable functions whose outputs can multiply together. Then f = p ‚àò v f=p\\circ v italic_f = italic_p ‚àò italic_v where v = ( g , h ) v=(g,h) italic_v = ( italic_g , italic_h ) and p ‚Äã ( a , b ) = a ‚Äã b p(a,b)=ab italic_p ( italic_a , italic_b ) = italic_a italic_b . Applying the chain rule we have d ‚Äã f = p ‚Ä≤ ‚Äã ( v ‚Äã ( x ) ) ‚Äã v ‚Ä≤ ‚Äã ( x ) ‚Äã [ d ‚Äã x ] . \\mathrm{d}f=p^{\\prime}(v(x))v^{\\prime}(x)[\\mathrm{d}x]. roman_d italic_f = italic_p start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( ita"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmexample6.p1", "title": "Consider the function f ‚Äã ( ùë® ) = ùë® ‚ä§ ‚Äã ùë® ‚Äã ùë© ‚Äã ùë® f(\\bm{A})=\\bm{A}^{\\top}\\bm{A}\\bm{B}\\bm{A} italic_f ( bold_italic_A ) = bold_italic_A start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_A bold_it", "snippet": "Consider the function f ‚Äã ( ùë® ) = ùë® ‚ä§ ‚Äã ùë® ‚Äã ùë© ‚Äã ùë® f(\\bm{A})=\\bm{A}^{\\top}\\bm{A}\\bm{B}\\bm{A} italic_f ( bold_italic_A ) = bold_italic_A start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_A bold_italic_B bold_italic_A where ùë® \\bm{A} bold_italic_A is a matrix and ùë© \\bm{B} bold_italic_B is a constant matrix. Then, letting f = g ‚Äã h f=gh italic_f = italic_g italic_h where g ‚Äã ( ùë® ) = ùë® ‚ä§ ‚Äã ùë® g(\\bm{A})=\\bm{A}^{\\top}\\bm{A} italic_g ( bold_italic_A ) = bold_italic_A start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_A and h ‚Äã ( ùë® ) = ùë© ‚Äã ùë® h(\\bm{A})=\\bm{B}\\bm{A} italic_h ( bold_italic_A ) = "}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmexample7.p1", "title": "Consider the function f : ‚Ñù m √ó n √ó k ‚Üí ‚Ñù m √ó n f\\colon\\mathbb{R}^{m\\times n\\times k}\\to\\mathbb{R}^{m\\times n} italic_f : blackboard_R start_POSTSUPERSCRIPT italic_m √ó italic_n √ó italic_k end_POSTSUPE", "snippet": "Consider the function f : ‚Ñù m √ó n √ó k ‚Üí ‚Ñù m √ó n f\\colon\\mathbb{R}^{m\\times n\\times k}\\to\\mathbb{R}^{m\\times n} italic_f : blackboard_R start_POSTSUPERSCRIPT italic_m √ó italic_n √ó italic_k end_POSTSUPERSCRIPT ‚Üí blackboard_R start_POSTSUPERSCRIPT italic_m √ó italic_n end_POSTSUPERSCRIPT given by f ‚Äã ( ùë® ) i ‚Äã j = ‚àë t = 1 k A i ‚Äã j ‚Äã t . f(\\bm{A})_{ij}=\\sum_{t=1}^{k}A_{ijt}. italic_f ( bold_italic_A ) start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = ‚àë start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_A start_POSTSUBSCRIPT italic_i"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.SS1.p6", "title": "This gives us all the technology we need to compute differentials of everything. The last thing we cover in this section is a method to compute gradients using the differential. Namely, for a function", "snippet": "This gives us all the technology we need to compute differentials of everything. The last thing we cover in this section is a method to compute gradients using the differential. Namely, for a function ‚Ñí \\mathcal{L} caligraphic_L whose output is a scalar, the gradient ‚àá ‚Ñí \\nabla\\mathcal{L} ‚àá caligraphic_L is defined as d ‚Äã ‚Ñí = ‚Ñí ‚Ä≤ ‚Äã ( Œ∏ ) ‚Äã [ d ‚Äã Œ∏ ] = ‚ü® ‚àá ‚Ñí ‚Äã ( Œ∏ ) , d ‚Äã Œ∏ ‚ü© , \\mathrm{d}\\mathcal{L}=\\mathcal{L}^{\\prime}(\\theta)[\\mathrm{d}\\theta]=\\langle\\nabla\\mathcal{L}(\\theta),\\mathrm{d}\\theta\\rangle, roman_d caligraphic_L = caligraphic_L start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_Œ∏ "}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.SS2.p1", "title": "The main idea of AD is to compute the chain rule efficiently. The basic problem we need to cope with is the following. In the optimization section of the appendix, we considered that the parameter spa", "snippet": "The main idea of AD is to compute the chain rule efficiently. The basic problem we need to cope with is the following. In the optimization section of the appendix, we considered that the parameter space Œò \\Theta roman_Œò was an abstract Euclidean space like ‚Ñù n \\mathbb{R}^{n} blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT . In practice the parameters are really some collection of vectors, matrices, and higher-order objects: Œò = ‚Ñù m √ó n √ó ‚Ñù n √ó ‚Ñù r √ó q √ó p √ó ‚Ñù r √ó q √ó ‚ãØ \\Theta=\\mathbb{R}^{m\\times n}\\times\\mathbb{R}^{n}\\times\\mathbb{R}^{r\\times q\\times p}\\times\\mathbb{R}^{r\\times"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.SS2.p2", "title": "Let us do a simple example to start. Let ‚Ñí \\mathcal{L} caligraphic_L be defined by ‚Ñí = a ‚àò b ‚àò c \\mathcal{L}=a\\circ b\\circ c caligraphic_L = italic_a ‚àò italic_b ‚àò italic_c where a , b , c a,b,c italic", "snippet": "Let us do a simple example to start. Let ‚Ñí \\mathcal{L} caligraphic_L be defined by ‚Ñí = a ‚àò b ‚àò c \\mathcal{L}=a\\circ b\\circ c caligraphic_L = italic_a ‚àò italic_b ‚àò italic_c where a , b , c a,b,c italic_a , italic_b , italic_c are differentiable. Then the chain rule gives ‚Ñí ‚Ä≤ ‚Äã ( Œ∏ ) = a ‚Ä≤ ‚Äã ( b ‚Äã ( c ‚Äã ( Œ∏ ) ) ) ‚Äã b ‚Ä≤ ‚Äã ( c ‚Äã ( Œ∏ ) ) ‚Äã c ‚Ä≤ ‚Äã ( Œ∏ ) . \\mathcal{L}^{\\prime}(\\theta)=a^{\\prime}(b(c(\\theta)))b^{\\prime}(c(\\theta))c^{\\prime}(\\theta). caligraphic_L start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_Œ∏ ) = italic_a start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_b ( italic_c ( itali"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.SS2.p3", "title": "More generally, suppose that f = f L ‚àò ‚ãØ ‚àò f 1 f=f^{L}\\circ\\cdots\\circ f^{1} italic_f = italic_f start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT ‚àò ‚ãØ ‚àò italic_f start_POSTSUPERSCRIPT 1 end_POSTSUPER", "snippet": "More generally, suppose that f = f L ‚àò ‚ãØ ‚àò f 1 f=f^{L}\\circ\\cdots\\circ f^{1} italic_f = italic_f start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT ‚àò ‚ãØ ‚àò italic_f start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT where each f ‚Ñì : ‚Ñù d ‚Ñì ‚àí 1 ‚Üí ‚Ñù d ‚Ñì f^{\\ell}\\colon\\mathbb{R}^{d^{\\ell-1}}\\to\\mathbb{R}^{d^{\\ell}} italic_f start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT : blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUPERSCRIPT roman_‚Ñì - 1 end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT ‚Üí blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT end_POSTSUPERSCRIPT ,"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.I2.i1.p1", "title": "If the function to optimize has more outputs than inputs (i.e., d L > d 0 d^{L}>d^{0} italic_d start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT > italic_d start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT", "snippet": "If the function to optimize has more outputs than inputs (i.e., d L > d 0 d^{L}>d^{0} italic_d start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT > italic_d start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT ), use forward-mode AD ."}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.I2.i2.p1", "title": "If the function to optimize has more inputs than outputs (i.e., d 0 > d L d^{0}>d^{L} italic_d start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT > italic_d start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT", "snippet": "If the function to optimize has more inputs than outputs (i.e., d 0 > d L d^{0}>d^{L} italic_d start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT > italic_d start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT ), use reverse-mode AD ."}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.SS3.p1", "title": "In this section, we will discuss algorithmic backpropagation using a simple yet completely practical example. Suppose that we fix an input-label pair ( ùëø , ùíö ) (\\bm{X},\\bm{y}) ( bold_italic_X , bold_i", "snippet": "In this section, we will discuss algorithmic backpropagation using a simple yet completely practical example. Suppose that we fix an input-label pair ( ùëø , ùíö ) (\\bm{X},\\bm{y}) ( bold_italic_X , bold_italic_y ) , and fix a network architecture f Œ∏ = f Œ∏ L ‚àò ‚ãØ ‚àò f Œ∏ 1 ‚àò f Œ∏ emb f_{\\theta}=f_{\\theta}^{L}\\circ\\cdots\\circ f_{\\theta}^{1}\\circ f_{\\theta}^{\\mathrm{emb}} italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT = italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT ‚àò ‚ãØ ‚àò italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_PO"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.SS3.p2", "title": "To carry out this computation, let us make some changes to the notation. ‚Ä¢ First, let us change the notation to emphasize the dependency structure between the variables. Namely, ùíÅ 1 ‚âê f emb ‚Äã ( ùëø , Œ∏ ", "snippet": "To carry out this computation, let us make some changes to the notation. ‚Ä¢ First, let us change the notation to emphasize the dependency structure between the variables. Namely, ùíÅ 1 ‚âê f emb ‚Äã ( ùëø , Œ∏ emb ) \\displaystyle\\bm{Z}^{1}\\doteq f^{\\mathrm{emb}}(\\bm{X},\\theta^{\\mathrm{emb}}) bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ‚âê italic_f start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT ( bold_italic_X , italic_Œ∏ start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT ) (A.2.41) ùíÅ ‚Ñì + 1 ‚âê f ‚Ñì ‚Äã ( ùíÅ ‚Ñì , Œ∏ ‚Ñì ) , ‚àÄ ‚Ñì ‚àà { 1 , ‚Ä¶ , L } , \\displaystyle\\bm{Z}^{\\ell+1}\\doteq f^{\\ell}(\\bm{Z}^{"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.I3.i1.p1", "title": "First, let us change the notation to emphasize the dependency structure between the variables. Namely, ùíÅ 1 ‚âê f emb ‚Äã ( ùëø , Œ∏ emb ) \\displaystyle\\bm{Z}^{1}\\doteq f^{\\mathrm{emb}}(\\bm{X},\\theta^{\\mathrm", "snippet": "First, let us change the notation to emphasize the dependency structure between the variables. Namely, ùíÅ 1 ‚âê f emb ‚Äã ( ùëø , Œ∏ emb ) \\displaystyle\\bm{Z}^{1}\\doteq f^{\\mathrm{emb}}(\\bm{X},\\theta^{\\mathrm{emb}}) bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ‚âê italic_f start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT ( bold_italic_X , italic_Œ∏ start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT ) (A.2.41) ùíÅ ‚Ñì + 1 ‚âê f ‚Ñì ‚Äã ( ùíÅ ‚Ñì , Œ∏ ‚Ñì ) , ‚àÄ ‚Ñì ‚àà { 1 , ‚Ä¶ , L } , \\displaystyle\\bm{Z}^{\\ell+1}\\doteq f^{\\ell}(\\bm{Z}^{\\ell},\\theta^{\\ell}),\\qquad\\forall\\ell\\in\\{1,\\dots,L\\}, bold_italic_Z start"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.I3.i2.p1", "title": "Then, instead of having the derivative be f ‚Ä≤ f^{\\prime} italic_f start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT , we explicitly notate the independent variable and write the derivative as d ‚Äã f d ‚Äã Œ∏ 1 ", "snippet": "Then, instead of having the derivative be f ‚Ä≤ f^{\\prime} italic_f start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT , we explicitly notate the independent variable and write the derivative as d ‚Äã f d ‚Äã Œ∏ 1 \\frac{\\mathrm{d}f}{\\mathrm{d}\\theta^{1}} divide start_ARG roman_d italic_f end_ARG start_ARG roman_d italic_Œ∏ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT end_ARG , for example. This is because there are many variables in our model and we only care about one at a time."}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.SS3.p3", "title": "Now let us compute the differentials w.r.t. some Œ∏ ‚Ñì \\theta^{\\ell} italic_Œ∏ start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT : d ‚Äã ‚Ñí \\displaystyle\\mathrm{d}\\mathcal{L} roman_d caligraphic_L = d ‚Äã ‚Ñí d", "snippet": "Now let us compute the differentials w.r.t. some Œ∏ ‚Ñì \\theta^{\\ell} italic_Œ∏ start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT : d ‚Äã ‚Ñí \\displaystyle\\mathrm{d}\\mathcal{L} roman_d caligraphic_L = d ‚Äã ‚Ñí d ‚Äã ùíÅ ‚Ñì + 1 ‚ãÖ d ‚Äã ùíÅ ‚Ñì + 1 \\displaystyle=\\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}\\bm{Z}^{\\ell+1}}\\cdot\\mathrm{d}\\bm{Z}^{\\ell+1} = divide start_ARG roman_d caligraphic_L end_ARG start_ARG roman_d bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì + 1 end_POSTSUPERSCRIPT end_ARG ‚ãÖ roman_d bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì + 1 end_POSTSUPERSCRIPT (A.2.54) = d ‚Äã ‚Ñí d ‚Äã ùíÅ ‚Ñì + 1 ‚ãÖ d ‚Äã ( f ‚Ñì ‚Äã ( ùíÅ ‚Ñì , Œ∏ ‚Ñì"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.SS3.p4", "title": "This gives us a computationally efficient algorithm to find all gradients in the whole network.", "snippet": "This gives us a computationally efficient algorithm to find all gradients in the whole network."}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.SS3.p5", "title": "We‚Äôll finish this section by computing the adjoint derivative for a simple layer.", "snippet": "We‚Äôll finish this section by computing the adjoint derivative for a simple layer."}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmexample8.p1", "title": "Consider the ‚Äúlinear‚Äù (affine) layer f ‚Ñì f^{\\ell} italic_f start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT f ‚Ñì ‚Äã ( ùíÅ , ùëæ ‚Ñì , ùíÉ ‚Ñì ) ‚âê ùëæ ‚Ñì ‚Äã ùíÅ + ùíÉ ‚Ñì ‚Äã ùüè ‚ä§ = [ ùëæ ‚Ñì ùíÉ ‚Ñì ] . f^{\\ell}(\\bm{Z},\\bm{W}^{\\ell}", "snippet": "Consider the ‚Äúlinear‚Äù (affine) layer f ‚Ñì f^{\\ell} italic_f start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT f ‚Ñì ‚Äã ( ùíÅ , ùëæ ‚Ñì , ùíÉ ‚Ñì ) ‚âê ùëæ ‚Ñì ‚Äã ùíÅ + ùíÉ ‚Ñì ‚Äã ùüè ‚ä§ = [ ùëæ ‚Ñì ùíÉ ‚Ñì ] . f^{\\ell}(\\bm{Z},\\bm{W}^{\\ell},\\bm{b}^{\\ell})\\doteq\\bm{W}^{\\ell}\\bm{Z}+\\bm{b}^{\\ell}\\bm{1}^{\\top}=\\begin{bmatrix}\\bm{W}^{\\ell}&\\bm{b}^{\\ell}\\end{bmatrix}. italic_f start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ( bold_italic_Z , bold_italic_W start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT , bold_italic_b start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ) ‚âê bold_italic_W start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT "}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.SS3.p6", "title": "Note that as a simple application of chain rule, both backpropagation and automatic differentiation work over general ‚Äúcomputational graphs‚Äù, i.e., compositions of (simple) functions. We give all exam", "snippet": "Note that as a simple application of chain rule, both backpropagation and automatic differentiation work over general ‚Äúcomputational graphs‚Äù, i.e., compositions of (simple) functions. We give all examples as neural network layers because this is the most common example in practice."}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.p1", "title": "In certain cases, such as in Chapter 5 , a learning problem cannot be reduced to a single optimization problem but rather represents multiple potentially opposing components of the system try to each ", "snippet": "In certain cases, such as in Chapter 5 , a learning problem cannot be reduced to a single optimization problem but rather represents multiple potentially opposing components of the system try to each minimize their own objective. Examples of this paradigm include distribution learning via generative adversarial networks (GAN) and closed-loop transcription (CTRL). We will denote such a system as a two-player game , where we have two ‚Äúplayers‚Äù (i.e., components) called Player 1 and Player 2 trying to minimize their objectives ‚Ñí 1 \\mathcal{L}^{1} caligraphic_L start_POSTSUPERSCRIPT 1 end_POSTSUPE"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.p2", "title": "Our first, very preliminary example is as follows. Suppose that there exists functions u ‚Äã ( Œ∏ ) u(\\theta) italic_u ( italic_Œ∏ ) and v ‚Äã ( Œ∑ ) v(\\eta) italic_v ( italic_Œ∑ ) such that ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) = ‚àí", "snippet": "Our first, very preliminary example is as follows. Suppose that there exists functions u ‚Äã ( Œ∏ ) u(\\theta) italic_u ( italic_Œ∏ ) and v ‚Äã ( Œ∑ ) v(\\eta) italic_v ( italic_Œ∑ ) such that ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) = ‚àí u ‚Äã ( Œ∏ ) + v ‚Äã ( Œ∑ ) . \\mathcal{L}(\\theta,\\eta)=-u(\\theta)+v(\\eta). caligraphic_L ( italic_Œ∏ , italic_Œ∑ ) = - italic_u ( italic_Œ∏ ) + italic_v ( italic_Œ∑ ) . (A.3.1) Then both players‚Äô objectives are independent of the other player, and the players should try to achieve their respective optima: Œ∏ ‚ãÜ ‚àà arg ‚Äã min Œ∏ ‚àà Œò ‚Å° u ‚Äã ( Œ∏ ) , Œ∑ ‚ãÜ ‚àà arg ‚Äã min Œ∑ ‚àà H ‚Å° v ‚Äã ( Œ∑ ) . \\theta^{\\star}\\in\\operatorname"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.p3", "title": "In this book, the relevant game-theoretic formalism is a Stackelberg game (variously called sequential game). In this formalism, one player (without loss of generality Player 1, and also described as ", "snippet": "In this book, the relevant game-theoretic formalism is a Stackelberg game (variously called sequential game). In this formalism, one player (without loss of generality Player 1, and also described as a leader ) picks their parameters before the other (i.e., Player 2, also described as a follower ), and the follower can use the full knowledge of the leader‚Äôs choice to make their own choice. The correct notion of equilibrium for a Stackelberg game is a Stackelberg equilibrium . To explain this equilibrium, note that since Player 2 (i.e., the follower) can choose Œ∑ \\eta italic_Œ∑ reactively to the"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.p4", "title": "Note that ùíÆ ‚Äã ( Œ∏ ) = arg ‚Äã min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) , \\mathcal{S}(\\theta)=\\operatorname*{arg\\ min}_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta), caligraphic_S ( italic_Œ∏ ) = start_OPERATOR roman_arg ", "snippet": "Note that ùíÆ ‚Äã ( Œ∏ ) = arg ‚Äã min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) , \\mathcal{S}(\\theta)=\\operatorname*{arg\\ min}_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta), caligraphic_S ( italic_Œ∏ ) = start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_Œ∑ ‚àà roman_H end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ , italic_Œ∑ ) , (A.3.5) so it holds min Œ∑ ‚àà ùíÆ ‚Äã ( Œ∏ ) ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) = min Œ∑ ‚àà arg ‚Äã min Œ∑ ‚Ä≤ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ‚Ä≤ ) ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) = min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) . \\min_{\\eta\\in\\mathcal{S}(\\theta)}\\mathcal{L}(\\theta,\\eta)=\\min_{\\eta\\in\\operatorname*{arg\\ min}_{\\eta^{\\prime}\\in\\mathrm{H}}\\mathcal{L"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.p5", "title": "In the rest of the section, we will briefly discuss some algorithmic approaches to learn Stackelberg equilibria. The intuition you should have is that learning an equilibrium is like letting the diffe", "snippet": "In the rest of the section, we will briefly discuss some algorithmic approaches to learn Stackelberg equilibria. The intuition you should have is that learning an equilibrium is like letting the different parts of the system automatically figure out tradeoffs between the different objectives they want to optimize."}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.p6", "title": "We end this section with a caveat: in two-player zero-sum games, if it holds that max Œ∏ ‚àà Œò ‚Å° min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) = min Œ∑ ‚àà H ‚Å° max Œ∏ ‚àà Œò ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) \\max_{\\theta\\in\\Theta}\\min_{\\eta\\in\\math", "snippet": "We end this section with a caveat: in two-player zero-sum games, if it holds that max Œ∏ ‚àà Œò ‚Å° min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) = min Œ∑ ‚àà H ‚Å° max Œ∏ ‚àà Œò ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) \\max_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta)=\\min_{\\eta\\in\\mathrm{H}}\\max_{\\theta\\in\\Theta}\\mathcal{L}(\\theta,\\eta) roman_max start_POSTSUBSCRIPT italic_Œ∏ ‚àà roman_Œò end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT italic_Œ∑ ‚àà roman_H end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ , italic_Œ∑ ) = roman_min start_POSTSUBSCRIPT italic_Œ∑ ‚àà roman_H end_POSTSUBSCRIPT roman_max start_POSTSUBSCRIPT italic_Œ∏ ‚àà roman_Œò end_POST"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.p7", "title": "Suppose that indeed max Œ∏ ‚àà Œò ‚Å° min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) = min Œ∑ ‚àà H ‚Å° max Œ∏ ‚àà Œò ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) . \\max_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta)=\\min_{\\eta\\in\\mathrm{H}}\\max_", "snippet": "Suppose that indeed max Œ∏ ‚àà Œò ‚Å° min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) = min Œ∑ ‚àà H ‚Å° max Œ∏ ‚àà Œò ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) . \\max_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta)=\\min_{\\eta\\in\\mathrm{H}}\\max_{\\theta\\in\\Theta}\\mathcal{L}(\\theta,\\eta). roman_max start_POSTSUBSCRIPT italic_Œ∏ ‚àà roman_Œò end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT italic_Œ∑ ‚àà roman_H end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ , italic_Œ∑ ) = roman_min start_POSTSUBSCRIPT italic_Œ∑ ‚àà roman_H end_POSTSUBSCRIPT roman_max start_POSTSUBSCRIPT italic_Œ∏ ‚àà roman_Œò end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ , italic_Œ∑ ) . (A.3.9)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.p8", "title": "First suppose that ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) (\\theta^{\\star},\\eta^{\\star}) ( italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT , italic_Œ∑ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ) is a saddle point. We wil", "snippet": "First suppose that ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) (\\theta^{\\star},\\eta^{\\star}) ( italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT , italic_Œ∑ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ) is a saddle point. We will show it is a Stackelberg equilibrium. By definition we have min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ) = ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) = max Œ∏ ‚àà Œò ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ‚ãÜ ) . \\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta^{\\star},\\eta)=\\mathcal{L}(\\theta^{\\star},\\eta^{\\star})=\\max_{\\theta\\in\\Theta}\\mathcal{L}(\\theta,\\eta^{\\star}). roman_min start_POSTSUBSCRIPT italic_Œ∑ ‚àà roman_H end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ start_POSTSU"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.p9", "title": "Now let ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) (\\theta^{\\star},\\eta^{\\star}) ( italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT , italic_Œ∑ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ) be a Stackelberg equilibrium. We cla", "snippet": "Now let ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) (\\theta^{\\star},\\eta^{\\star}) ( italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT , italic_Œ∑ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ) be a Stackelberg equilibrium. We claim that it is a saddle point, which completes the proof. By the definition of minimax equilibrium, max Œ∏ ‚àà Œò ‚Å° min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) = ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) . \\max_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta)=\\mathcal{L}(\\theta^{\\star},\\eta^{\\star}). roman_max start_POSTSUBSCRIPT italic_Œ∏ ‚àà roman_Œò end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT italic_Œ∑ ‚àà roman_H end_POSTSUBS"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.p10", "title": "Conditions under which the min ‚Å° max = max ‚Å° min \\min\\max=\\max\\min roman_min roman_max = roman_max roman_min equality holds are given by so-called minimax theorems ; the most famous of these is a theo", "snippet": "Conditions under which the min ‚Å° max = max ‚Å° min \\min\\max=\\max\\min roman_min roman_max = roman_max roman_min equality holds are given by so-called minimax theorems ; the most famous of these is a theorem of von Neumann. However, in the cases we think about, this property usually does not hold."}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.SS1.p1", "title": "How can we learn Stackelberg equilibria via GDA? In general this is clearly impossible, since learning Stackelberg equilibria via GDA is obviously at least as hard as computing a global minimizer of a", "snippet": "How can we learn Stackelberg equilibria via GDA? In general this is clearly impossible, since learning Stackelberg equilibria via GDA is obviously at least as hard as computing a global minimizer of a loss function (say by setting the shared objective ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) \\mathcal{L}(\\theta,\\eta) caligraphic_L ( italic_Œ∏ , italic_Œ∑ ) to only be a function of Œ∑ \\eta italic_Œ∑ ). As such, we can achieve two types of convergence guarantees: ‚Ä¢ When ‚Ñí \\mathcal{L} caligraphic_L is (strongly) concave in the first argument Œ∏ \\theta italic_Œ∏ and (strongly) convex in the second argument Œ∑ \\eta italic_Œ∑ (as well"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.I1.i1.p1", "title": "When ‚Ñí \\mathcal{L} caligraphic_L is (strongly) concave in the first argument Œ∏ \\theta italic_Œ∏ and (strongly) convex in the second argument Œ∑ \\eta italic_Œ∑ (as well as having Lipschitz gradients in bo", "snippet": "When ‚Ñí \\mathcal{L} caligraphic_L is (strongly) concave in the first argument Œ∏ \\theta italic_Œ∏ and (strongly) convex in the second argument Œ∑ \\eta italic_Œ∑ (as well as having Lipschitz gradients in both arguments), we can achieve exponentially fast convergence to a Stackelberg equilibrium."}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.I1.i2.p1", "title": "When ‚Ñí \\mathcal{L} caligraphic_L is not concave or convex in either argument, we can achieve local convergence guarantees : namely, if we initialize the parameter values near a (local) Stackelberg equ", "snippet": "When ‚Ñí \\mathcal{L} caligraphic_L is not concave or convex in either argument, we can achieve local convergence guarantees : namely, if we initialize the parameter values near a (local) Stackelberg equilibrium and the optimization geometry is good then we can learn that equilibrium efficiently."}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.SS1.p2", "title": "The former situation is exactly analogous to the case of single-player optimization, where we proved that gradient descent converges exponentially fast for strongly convex objectives which have Lipsch", "snippet": "The former situation is exactly analogous to the case of single-player optimization, where we proved that gradient descent converges exponentially fast for strongly convex objectives which have Lipschitz gradient. The latter situation is also analogous to the case of single-player optimization, although we did not cover it in depth due to technical difficulty; indeed there exist local convergence guarantees for nonconvex objectives which have locally nice geometry."}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.SS1.p3", "title": "The algorithm in these two cases is the same algorithm, called Gradient Descent-Ascent (GDA). To motivate GDA, suppose we are trying to learn Œ∏ ‚ãÜ \\theta^{\\star} italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_PO", "snippet": "The algorithm in these two cases is the same algorithm, called Gradient Descent-Ascent (GDA). To motivate GDA, suppose we are trying to learn Œ∏ ‚ãÜ \\theta^{\\star} italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT . We could do gradient ascent on the function Œ∏ ‚Ü¶ min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) \\theta\\mapsto\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta) italic_Œ∏ ‚Ü¶ roman_min start_POSTSUBSCRIPT italic_Œ∑ ‚àà roman_H end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ , italic_Œ∑ ) . But then we would need to take the derivative in Œ∏ \\theta italic_Œ∏ of this function. To see how to do this, suppose that ‚Ñí \\mathca"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.SS1.p4", "title": "This method is often not done in practice, as it requires T + 1 T+1 italic_T + 1 total gradient descent iterations to update Œ∏ \\theta italic_Œ∏ once. Instead, we use the so-called (simultaneous) Gradie", "snippet": "This method is often not done in practice, as it requires T + 1 T+1 italic_T + 1 total gradient descent iterations to update Œ∏ \\theta italic_Œ∏ once. Instead, we use the so-called (simultaneous) Gradient Descent-Ascent (GDA) iteration Œ∏ k + 1 \\displaystyle\\theta_{k+1} italic_Œ∏ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = Œ∏ k + h ‚Äã ‚àá Œ∏ ‚Ñí ‚Äã ( Œ∏ k , Œ∑ k ) \\displaystyle=\\theta_{k}+h\\nabla_{\\theta}\\mathcal{L}(\\theta_{k},\\eta_{k}) = italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + italic_h ‚àá start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ start_POSTSUBSCRIPT i"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.SS1.p5", "title": "It is crucial to pick T T italic_T sensibly. How can we do that? In the sequel, we discuss two configurations of T T italic_T which lead to convergence of GDA to a Stackelberg equilibrium under differ", "snippet": "It is crucial to pick T T italic_T sensibly. How can we do that? In the sequel, we discuss two configurations of T T italic_T which lead to convergence of GDA to a Stackelberg equilibrium under different assumptions."}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.SS1.SSSx1.p1", "title": "If T = 1 T=1 italic_T = 1 (i.e., named one-timescale because both Œ∏ \\theta italic_Œ∏ and Œ∑ \\eta italic_Œ∑ updates are of the same scale), then the GDA algorithm becomes Œ∏ k + 1 = Œ∏ k + h ‚Äã ‚àá Œ∏ ‚Ñí ‚Äã ( Œ∏ k", "snippet": "If T = 1 T=1 italic_T = 1 (i.e., named one-timescale because both Œ∏ \\theta italic_Œ∏ and Œ∑ \\eta italic_Œ∑ updates are of the same scale), then the GDA algorithm becomes Œ∏ k + 1 = Œ∏ k + h ‚Äã ‚àá Œ∏ ‚Ñí ‚Äã ( Œ∏ k , Œ∑ k ) , Œ∑ k + 1 = Œ∑ k ‚àí h ‚Äã ‚àá Œ∑ ‚Ñí ‚Äã ( Œ∏ k , Œ∑ k ) . \\theta_{k+1}=\\theta_{k}+h\\nabla_{\\theta}\\mathcal{L}(\\theta_{k},\\eta_{k}),\\qquad\\eta_{k+1}=\\eta_{k}-h\\nabla_{\\eta}\\mathcal{L}(\\theta_{k},\\eta_{k}). italic_Œ∏ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + italic_h ‚àá start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT caligraphic_L ( ital"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.SS1.SSSx2.p1", "title": "Strong convexity/concavity is a global property, and none of the games we look into in this book have objectives which are globally strongly concave/strongly convex. In this case, the best we can hope", "snippet": "Strong convexity/concavity is a global property, and none of the games we look into in this book have objectives which are globally strongly concave/strongly convex. In this case, the best we can hope for is local convergence to Stackelberg equilibria: if the parameters are initialized close to a Stackelberg equilibrium, then GDA can converge onto it, given an appropriate step size h h italic_h and timescale T T italic_T ."}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.SS1.SSSx2.p2", "title": "In fact, our results also hold for a version of the local Stackelberg equilibrium called the differential Stackelberg equilibrium , which was introduced in [ FCR19 ] (though we use the precise definit", "snippet": "In fact, our results also hold for a version of the local Stackelberg equilibrium called the differential Stackelberg equilibrium , which was introduced in [ FCR19 ] (though we use the precise definition in [ LFD+22 ] ), and which we define as follows. A point ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) (\\theta^{\\star},\\eta^{\\star}) ( italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT , italic_Œ∑ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ) is a differential Stackelberg equilibrium if: ‚Ä¢ ‚àá Œ∑ ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) = ùüé \\nabla_{\\eta}\\mathcal{L}(\\theta^{\\star},\\eta^{\\star})=\\bm{0} ‚àá start_POSTSUBSCRIPT italic_Œ∑ end_POSTSUBSCRIPT"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.I2.i1.p1", "title": "‚àá Œ∑ ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) = ùüé \\nabla_{\\eta}\\mathcal{L}(\\theta^{\\star},\\eta^{\\star})=\\bm{0} ‚àá start_POSTSUBSCRIPT italic_Œ∑ end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSC", "snippet": "‚àá Œ∑ ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) = ùüé \\nabla_{\\eta}\\mathcal{L}(\\theta^{\\star},\\eta^{\\star})=\\bm{0} ‚àá start_POSTSUBSCRIPT italic_Œ∑ end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT , italic_Œ∑ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ) = bold_0 ;"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.I2.i2.p1", "title": "‚àá Œ∑ 2 ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) \\nabla_{\\eta}^{2}\\mathcal{L}(\\theta^{\\star},\\eta^{\\star}) ‚àá start_POSTSUBSCRIPT italic_Œ∑ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT caligraphic_L ( italic_Œ∏ ", "snippet": "‚àá Œ∑ 2 ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) \\nabla_{\\eta}^{2}\\mathcal{L}(\\theta^{\\star},\\eta^{\\star}) ‚àá start_POSTSUBSCRIPT italic_Œ∑ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT caligraphic_L ( italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT , italic_Œ∑ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ) is symmetric positive definite;"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.I2.i3.p1", "title": "‚àá Œ∏ ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) = ùüé \\nabla_{\\theta}\\mathcal{L}(\\theta^{\\star},\\eta^{\\star})=\\bm{0} ‚àá start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPER", "snippet": "‚àá Œ∏ ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) = ùüé \\nabla_{\\theta}\\mathcal{L}(\\theta^{\\star},\\eta^{\\star})=\\bm{0} ‚àá start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT , italic_Œ∑ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ) = bold_0 ;"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.I2.i4.p1", "title": "( ‚àá Œ∏ 2 ‚Ñí + [ d d ‚Äã Œ∏ ‚Äã ‚àá Œ∑ ‚Ñí ] ‚Äã [ ‚àá Œ∑ 2 ‚Ñí ] ‚àí 1 ‚Äã [ d d ‚Äã Œ∑ ‚Äã ‚àá Œ∏ ‚Ñí ] ) ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) (\\nabla_{\\theta}^{2}\\mathcal{L}+[\\frac{\\mathrm{d}}{\\mathrm{d}\\theta}\\nabla_{\\eta}\\mathcal{L}][\\nabla_{\\eta}^{2", "snippet": "( ‚àá Œ∏ 2 ‚Ñí + [ d d ‚Äã Œ∏ ‚Äã ‚àá Œ∑ ‚Ñí ] ‚Äã [ ‚àá Œ∑ 2 ‚Ñí ] ‚àí 1 ‚Äã [ d d ‚Äã Œ∑ ‚Äã ‚àá Œ∏ ‚Ñí ] ) ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) (\\nabla_{\\theta}^{2}\\mathcal{L}+[\\frac{\\mathrm{d}}{\\mathrm{d}\\theta}\\nabla_{\\eta}\\mathcal{L}][\\nabla_{\\eta}^{2}\\mathcal{L}]^{-1}[\\frac{\\mathrm{d}}{\\mathrm{d}\\eta}\\nabla_{\\theta}\\mathcal{L}])(\\theta^{\\star},\\eta^{\\star}) ( ‚àá start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT caligraphic_L + [ divide start_ARG roman_d end_ARG start_ARG roman_d italic_Œ∏ end_ARG ‚àá start_POSTSUBSCRIPT italic_Œ∑ end_POSTSUBSCRIPT caligraphic_L ] [ ‚àá start_POSTSUBSCRIPT italic_Œ∑ end_POSTSUB"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.SS1.SSSx2.p3", "title": "Analogously to the notion of strict local optimum in single-player optimization (where we require ‚àá 2 ‚Ñí ‚Äã ( Œ∏ ‚ãÜ ) \\nabla^{2}\\mathcal{L}(\\theta^{\\star}) ‚àá start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ca", "snippet": "Analogously to the notion of strict local optimum in single-player optimization (where we require ‚àá 2 ‚Ñí ‚Äã ( Œ∏ ‚ãÜ ) \\nabla^{2}\\mathcal{L}(\\theta^{\\star}) ‚àá start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT caligraphic_L ( italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ) to be positive semidefinite), the definition of differential Stackelberg equilibrium implies that ‚Ñí ‚Äã ( Œ∏ , ‚ãÖ ) \\mathcal{L}(\\theta,\\cdot) caligraphic_L ( italic_Œ∏ , ‚ãÖ ) is locally (strictly) convex in a neighborhood of the equilibrium, and that min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( ‚ãÖ , Œ∑ ) \\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\cdot,\\eta) roman_min st"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.SS1.SSSx2.p4", "title": "In this context, we present the result from [ LFD+22 ] . Let ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) (\\theta^{\\star},\\eta^{\\star}) ( italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT , italic_Œ∑ start_POSTSUPERSCRIPT ‚ãÜ end_P", "snippet": "In this context, we present the result from [ LFD+22 ] . Let ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) (\\theta^{\\star},\\eta^{\\star}) ( italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT , italic_Œ∑ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ) be a differential Stackelberg equilibrium. Suppose that ‚Ñí \\mathcal{L} caligraphic_L has Lipschitz gradients, i.e., max ‚Å° { ‚Äñ ‚àá Œ∏ 2 ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) ‚Äñ 2 , ‚Äñ ‚àá Œ∑ 2 ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) ‚Äñ 2 , ‚Äñ d d ‚Äã Œ∑ ‚Äã ‚àá Œ∏ ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) ‚Äñ 2 } ‚â§ Œ≤ . \\max\\left\\{\\|\\nabla_{\\theta}^{2}\\mathcal{L}(\\theta^{\\star},\\eta^{\\star})\\|_{2},\\left\\|\\nabla_{\\eta}^{2}\\mathcal{L}(\\theta^{\\star},\\eta^{\\star})\\right\\"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.SS2.p1", "title": "In practice, we do not know how to initialize parameters close to a (differential) Stackelberg equilibrium. Due to symmetries within the objective, including those induced by overparameterization of t", "snippet": "In practice, we do not know how to initialize parameters close to a (differential) Stackelberg equilibrium. Due to symmetries within the objective, including those induced by overparameterization of the neural networks being trained, one can (heuristically) expect that most initializations are close to a Stackelberg equilibrium. Also, we do not know how to compute the step size h h italic_h or the timescale T T italic_T , since they are dependent on properties of the loss ‚Ñí \\mathcal{L} caligraphic_L at the equilibrium. In practice, there are some common approaches: ‚Ä¢ Take T = 1 T=1 italic_T = "}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.I3.i1.p1", "title": "Take T = 1 T=1 italic_T = 1 (equal step-sizes), and use updates for Œ∏ \\theta italic_Œ∏ and Œ∑ \\eta italic_Œ∑ that are derived from a learning-rate-adaptive optimizer like Adam (as opposed to vanilla GD).", "snippet": "Take T = 1 T=1 italic_T = 1 (equal step-sizes), and use updates for Œ∏ \\theta italic_Œ∏ and Œ∑ \\eta italic_Œ∑ that are derived from a learning-rate-adaptive optimizer like Adam (as opposed to vanilla GD). Here, you hope (but do not know ) that the optimizer can adjust the learning rates to learn a good equilibrium."}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.I3.i2.p1", "title": "Take T T italic_T to be some constant like T = 10 6 T=10^{6} italic_T = 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT which implies that Œ∑ \\eta italic_Œ∑ equilibrates 10 6 10^{6} 10 start_POSTSUPERSCR", "snippet": "Take T T italic_T to be some constant like T = 10 6 T=10^{6} italic_T = 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT which implies that Œ∑ \\eta italic_Œ∑ equilibrates 10 6 10^{6} 10 start_POSTSUPERSCRIPT 6 end_POSTSUPERSCRIPT times as fast as Œ∏ \\theta italic_Œ∏ . Here you can also use Adam-style updates, and hope that it fixes the time scale."}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.I3.i3.p1", "title": "Let T T italic_T depend on the iteration k k italic_k , and let T k ‚Üí ‚àû T_{k}\\to\\infty italic_T start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ‚Üí ‚àû as k ‚Üí ‚àû k\\to\\infty italic_k ‚Üí ‚àû . This schedule was ", "snippet": "Let T T italic_T depend on the iteration k k italic_k , and let T k ‚Üí ‚àû T_{k}\\to\\infty italic_T start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ‚Üí ‚àû as k ‚Üí ‚àû k\\to\\infty italic_k ‚Üí ‚àû . This schedule was studied (also in the case of noise) by Borkar in [ Bor97 ] ."}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmexercise1.p1", "title": "We have shown that for a smooth function f f italic_f , gradient descent converges linearly to the global optimum if it is strongly convex. However, in general nonconvex optimization, we do not have c", "snippet": "We have shown that for a smooth function f f italic_f , gradient descent converges linearly to the global optimum if it is strongly convex. However, in general nonconvex optimization, we do not have convexity, let alone strong convexity. Fortunately, in some cases, f f italic_f satisfies the so-called Œº \\mu italic_Œº -Polyak-Lojasiewicz (PL) inequality, i.e., there exists a constant Œº > 0 \\mu>0 italic_Œº > 0 such that for all Œ∏ \\theta italic_Œ∏ , 1 2 ‚Äã ‚Äñ ‚àá f ‚Äã ( Œ∏ ) ‚Äñ 2 2 ‚â• Œº ‚Äã ( f ‚Äã ( Œ∏ ) ‚àí f ‚Äã ( Œ∏ ‚ãÜ ) ) , \\displaystyle\\frac{1}{2}\\|\\nabla f(\\theta)\\|_{2}^{2}\\geq\\mu\\left(f(\\theta)-f(\\theta^{\\star"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmexercise1.p2", "title": "Please show that under the PL inequality and the assumption that f f italic_f is Œ≤ \\beta italic_Œ≤ -smooth, gradient descent ( A.1.12 ) converges linearly to Œ∏ ‚àó \\theta^{*} italic_Œ∏ start_POSTSUPERSCRI", "snippet": "Please show that under the PL inequality and the assumption that f f italic_f is Œ≤ \\beta italic_Œ≤ -smooth, gradient descent ( A.1.12 ) converges linearly to Œ∏ ‚àó \\theta^{*} italic_Œ∏ start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT ."}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmexercise2.p1", "title": "Compute the differential and adjoint derivative of the softmax function, defined as follows. softmax ‚Å° ( [ x 1 ‚ãÆ x n ] ) = 1 ‚àë i = 1 n e x i ‚Äã [ x 1 ‚ãÆ x n ] . \\operatorname{\\mathrm{softmax}}\\left(\\beg", "snippet": "Compute the differential and adjoint derivative of the softmax function, defined as follows. softmax ‚Å° ( [ x 1 ‚ãÆ x n ] ) = 1 ‚àë i = 1 n e x i ‚Äã [ x 1 ‚ãÆ x n ] . \\operatorname{\\mathrm{softmax}}\\left(\\begin{bmatrix}x_{1}\\\\ \\vdots\\\\ x_{n}\\end{bmatrix}\\right)=\\frac{1}{\\sum_{i=1}^{n}e^{x_{i}}}\\begin{bmatrix}x_{1}\\\\ \\vdots\\\\ x_{n}\\end{bmatrix}. roman_softmax ( [ start_ARG start_ROW start_CELL italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL ‚ãÆ end_CELL end_ROW start_ROW start_CELL italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] ) "}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmexercise3.p1", "title": "Carry through the backpropagation computation for a L L italic_L -layer MLP, as defined in Section 7.2.3 .", "snippet": "Carry through the backpropagation computation for a L L italic_L -layer MLP, as defined in Section 7.2.3 ."}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmexercise4.p1", "title": "Carry through the backpropagation computation for a L L italic_L -layer transformer, as defined in Section 7.2.3 .", "snippet": "Carry through the backpropagation computation for a L L italic_L -layer transformer, as defined in Section 7.2.3 ."}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmexercise5.p1", "title": "Carry through the backpropagation computation for an autoencoder with L L italic_L encoder layers and L L italic_L decoder layers (without necessarily specifying an architecture).", "snippet": "Carry through the backpropagation computation for an autoencoder with L L italic_L encoder layers and L L italic_L decoder layers (without necessarily specifying an architecture)."}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmlemma1", "title": "Lemma A.1 (Majorization-Minimization) .", "snippet": "Lemma A.1 (Majorization-Minimization) . Suppose that u : Œò ‚Üí ‚Ñù u\\colon\\Theta\\to\\mathbb{R} italic_u : roman_Œò ‚Üí blackboard_R is a global upper bound on ‚Ñí \\mathcal{L} caligraphic_L , namely ‚Ñí ‚Äã ( Œ∏ ) ‚â§ u ‚Äã ( Œ∏ ) \\mathcal{L}(\\theta)\\leq u(\\theta) caligraphic_L ( italic_Œ∏ ) ‚â§ italic_u ( italic_Œ∏ ) for all Œ∏ ‚àà Œò \\theta\\in\\Theta italic_Œ∏ ‚àà roman_Œò . Suppose that they meet with equality at Œ∏ 0 \\theta_{0} italic_Œ∏ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , i.e., ‚Ñí ‚Äã ( Œ∏ 0 ) = u ‚Äã ( Œ∏ 0 ) \\mathcal{L}(\\theta_{0})=u(\\theta_{0}) caligraphic_L ( italic_Œ∏ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) = italic_u "}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmexample1", "title": "Example A.1 .", "snippet": "Example A.1 . Let Œì ‚äÜ Œò \\Gamma\\subseteq\\Theta roman_Œì ‚äÜ roman_Œò be a set, and let œá Œì \\chi_{\\Gamma} italic_œá start_POSTSUBSCRIPT roman_Œì end_POSTSUBSCRIPT be the characteristic function on Œì \\Gamma roman_Œì , i.e., œá Œì ‚Äã ( Œ∏ ) ‚âê { 0 , if ‚Äã Œ∏ ‚àà Œì + ‚àû , if ‚Äã Œ∏ ‚àâ Œì . \\chi_{\\Gamma}(\\theta)\\doteq\\begin{cases}0,&\\text{if}\\ \\theta\\in\\Gamma\\\\ +\\infty,&\\text{if}\\ \\theta\\notin\\Gamma.\\end{cases} italic_œá start_POSTSUBSCRIPT roman_Œì end_POSTSUBSCRIPT ( italic_Œ∏ ) ‚âê { start_ROW start_CELL 0 , end_CELL start_CELL if italic_Œ∏ ‚àà roman_Œì end_CELL end_ROW start_ROW start_CELL + ‚àû , end_CELL start_CELL if italic_"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmexample2", "title": "Example A.2 .", "snippet": "Example A.2 . The ‚Ñì 1 \\ell^{1} roman_‚Ñì start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT norm has a proximal operator which performs soft thresholding: S h ‚Äã ( Œ∏ ) ‚âê prox h , Œª ‚à• ‚ãÖ ‚à• 1 ‚Å° ( Œ∏ ) = arg ‚Äã min Œ∏ 1 ‚Å° [ 1 2 ‚Äã h ‚Äã ‚Äñ Œ∏ 1 ‚àí Œ∏ ‚Äñ 2 2 + Œª ‚Äã ‚Äñ Œ∏ ‚Äñ 1 ] S_{h}(\\theta)\\doteq\\operatorname{prox}_{h,\\lambda\\|\\cdot\\|_{1}}(\\theta)=\\operatorname*{arg\\ min}_{\\theta_{1}}\\left[\\frac{1}{2h}\\|\\theta_{1}-\\theta\\|_{2}^{2}+\\lambda\\|\\theta\\|_{1}\\right] italic_S start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ( italic_Œ∏ ) ‚âê roman_prox start_POSTSUBSCRIPT italic_h , italic_Œª ‚à• ‚ãÖ ‚à• start_POSTSUBSCRIPT 1 end_POSTSUBSCRI"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmexample3", "title": "Example A.3 .", "snippet": "Example A.3 . In Chapter 4 we use a proximal operator corresponding to the ‚Ñì 1 \\ell^{1} roman_‚Ñì start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT norm plus the characteristic function for the positive orthant ‚Ñù + n ‚âê { ùíô ‚àà ‚Ñù n : x i ‚â• 0 ‚Äã ‚àÄ i } \\mathbb{R}_{+}^{n}\\doteq\\{\\bm{x}\\in\\mathbb{R}^{n}\\colon x_{i}\\geq 0\\ \\forall i\\} blackboard_R start_POSTSUBSCRIPT + end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ‚âê { bold_italic_x ‚àà blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT : italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ‚â• 0 ‚àÄ italic_i } , namely T h ‚Äã ( "}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmtheorem1", "title": "Theorem A.1 (Differential Chain Rule) .", "snippet": "Theorem A.1 (Differential Chain Rule) . Suppose ‚Ñí = f ‚àò g \\mathcal{L}=f\\circ g caligraphic_L = italic_f ‚àò italic_g where f f italic_f and g g italic_g are differentiable. Then d ‚Äã ‚Ñí = f ‚Ä≤ ‚Äã ( g ‚Äã ( Œ∏ ) ) ‚Äã g ‚Ä≤ ‚Äã ( Œ∏ ) ‚Äã [ d ‚Äã Œ∏ ] , \\mathrm{d}\\mathcal{L}=f^{\\prime}(g(\\theta))g^{\\prime}(\\theta)[\\mathrm{d}\\theta], roman_d caligraphic_L = italic_f start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_g ( italic_Œ∏ ) ) italic_g start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_Œ∏ ) [ roman_d italic_Œ∏ ] , (A.2.6) where (as usual) multiplication indicates composition of linear operators. In particula"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmexample4", "title": "Example A.4 .", "snippet": "Example A.4 . Consider the function f ‚Äã ( ùëø ) = ùëæ ‚Äã ùëø + ùíÉ ‚Äã ùüè ‚ä§ f(\\bm{X})=\\bm{W}\\bm{X}+\\bm{b}\\bm{1}^{\\top} italic_f ( bold_italic_X ) = bold_italic_W bold_italic_X + bold_italic_b bold_1 start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT . Then d ‚Äã f = f ‚Äã ( ùëø + d ‚Äã ùëø ) ‚àí f ‚Äã ( ùëø ) = [ ùëæ ‚Äã ( ùëø + d ‚Äã ùëø ) + ùíÉ ‚Äã ùüè ‚ä§ ] ‚àí [ ùëæ ‚Äã ùëø + ùíÉ ‚Äã ùüè ‚ä§ ] = ùëæ ‚Äã d ‚Äã ùëø . \\mathrm{d}f=f(\\bm{X}+\\mathrm{d}\\bm{X})-f(\\bm{X})=[\\bm{W}(\\bm{X}+\\mathrm{d}\\bm{X})+\\bm{b}\\bm{1}^{\\top}]-[\\bm{W}\\bm{X}+\\bm{b}\\bm{1}^{\\top}]=\\bm{W}\\mathrm{d}\\bm{X}. roman_d italic_f = italic_f ( bold_italic_X + roman_d bold_italic_X ) - italic_f ( bold_ital"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmexample5", "title": "Example A.5 .", "snippet": "Example A.5 . Consider the function f = g ‚Äã h f=gh italic_f = italic_g italic_h where g , h g,h italic_g , italic_h are differentiable functions whose outputs can multiply together. Then f = p ‚àò v f=p\\circ v italic_f = italic_p ‚àò italic_v where v = ( g , h ) v=(g,h) italic_v = ( italic_g , italic_h ) and p ‚Äã ( a , b ) = a ‚Äã b p(a,b)=ab italic_p ( italic_a , italic_b ) = italic_a italic_b . Applying the chain rule we have d ‚Äã f = p ‚Ä≤ ‚Äã ( v ‚Äã ( x ) ) ‚Äã v ‚Ä≤ ‚Äã ( x ) ‚Äã [ d ‚Äã x ] . \\mathrm{d}f=p^{\\prime}(v(x))v^{\\prime}(x)[\\mathrm{d}x]. roman_d italic_f = italic_p start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUP"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmexample6", "title": "Example A.6 .", "snippet": "Example A.6 . Consider the function f ‚Äã ( ùë® ) = ùë® ‚ä§ ‚Äã ùë® ‚Äã ùë© ‚Äã ùë® f(\\bm{A})=\\bm{A}^{\\top}\\bm{A}\\bm{B}\\bm{A} italic_f ( bold_italic_A ) = bold_italic_A start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_A bold_italic_B bold_italic_A where ùë® \\bm{A} bold_italic_A is a matrix and ùë© \\bm{B} bold_italic_B is a constant matrix. Then, letting f = g ‚Äã h f=gh italic_f = italic_g italic_h where g ‚Äã ( ùë® ) = ùë® ‚ä§ ‚Äã ùë® g(\\bm{A})=\\bm{A}^{\\top}\\bm{A} italic_g ( bold_italic_A ) = bold_italic_A start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_A and h ‚Äã ( ùë® ) = ùë© ‚Äã ùë® h(\\bm{A})=\\bm{B}\\bm{A} italic_h ( bold"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmexample7", "title": "Example A.7 .", "snippet": "Example A.7 . Consider the function f : ‚Ñù m √ó n √ó k ‚Üí ‚Ñù m √ó n f\\colon\\mathbb{R}^{m\\times n\\times k}\\to\\mathbb{R}^{m\\times n} italic_f : blackboard_R start_POSTSUPERSCRIPT italic_m √ó italic_n √ó italic_k end_POSTSUPERSCRIPT ‚Üí blackboard_R start_POSTSUPERSCRIPT italic_m √ó italic_n end_POSTSUPERSCRIPT given by f ‚Äã ( ùë® ) i ‚Äã j = ‚àë t = 1 k A i ‚Äã j ‚Äã t . f(\\bm{A})_{ij}=\\sum_{t=1}^{k}A_{ijt}. italic_f ( bold_italic_A ) start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = ‚àë start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_A start_POSTSUBS"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmexample8", "title": "Example A.8 .", "snippet": "Example A.8 . Consider the ‚Äúlinear‚Äù (affine) layer f ‚Ñì f^{\\ell} italic_f start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT f ‚Ñì ‚Äã ( ùíÅ , ùëæ ‚Ñì , ùíÉ ‚Ñì ) ‚âê ùëæ ‚Ñì ‚Äã ùíÅ + ùíÉ ‚Ñì ‚Äã ùüè ‚ä§ = [ ùëæ ‚Ñì ùíÉ ‚Ñì ] . f^{\\ell}(\\bm{Z},\\bm{W}^{\\ell},\\bm{b}^{\\ell})\\doteq\\bm{W}^{\\ell}\\bm{Z}+\\bm{b}^{\\ell}\\bm{1}^{\\top}=\\begin{bmatrix}\\bm{W}^{\\ell}&\\bm{b}^{\\ell}\\end{bmatrix}. italic_f start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ( bold_italic_Z , bold_italic_W start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT , bold_italic_b start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ) ‚âê bold_italic_W start_POSTSUPERSCRIPT roman_‚Ñì end_PO"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmexercise1", "title": "Exercise A.1 .", "snippet": "Exercise A.1 . We have shown that for a smooth function f f italic_f , gradient descent converges linearly to the global optimum if it is strongly convex. However, in general nonconvex optimization, we do not have convexity, let alone strong convexity. Fortunately, in some cases, f f italic_f satisfies the so-called Œº \\mu italic_Œº -Polyak-Lojasiewicz (PL) inequality, i.e., there exists a constant Œº > 0 \\mu>0 italic_Œº > 0 such that for all Œ∏ \\theta italic_Œ∏ , 1 2 ‚Äã ‚Äñ ‚àá f ‚Äã ( Œ∏ ) ‚Äñ 2 2 ‚â• Œº ‚Äã ( f ‚Äã ( Œ∏ ) ‚àí f ‚Äã ( Œ∏ ‚ãÜ ) ) , \\displaystyle\\frac{1}{2}\\|\\nabla f(\\theta)\\|_{2}^{2}\\geq\\mu\\left(f(\\theta)-"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmexercise2", "title": "Exercise A.2 .", "snippet": "Exercise A.2 . Compute the differential and adjoint derivative of the softmax function, defined as follows. softmax ‚Å° ( [ x 1 ‚ãÆ x n ] ) = 1 ‚àë i = 1 n e x i ‚Äã [ x 1 ‚ãÆ x n ] . \\operatorname{\\mathrm{softmax}}\\left(\\begin{bmatrix}x_{1}\\\\ \\vdots\\\\ x_{n}\\end{bmatrix}\\right)=\\frac{1}{\\sum_{i=1}^{n}e^{x_{i}}}\\begin{bmatrix}x_{1}\\\\ \\vdots\\\\ x_{n}\\end{bmatrix}. roman_softmax ( [ start_ARG start_ROW start_CELL italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL ‚ãÆ end_CELL end_ROW start_ROW start_CELL italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_CELL end_R"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmexercise3", "title": "Exercise A.3 .", "snippet": "Exercise A.3 . Carry through the backpropagation computation for a L L italic_L -layer MLP, as defined in Section 7.2.3 ."}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmexercise4", "title": "Exercise A.4 .", "snippet": "Exercise A.4 . Carry through the backpropagation computation for a L L italic_L -layer transformer, as defined in Section 7.2.3 ."}, {"page": "Appendix A Optimization Methods", "href": "A1.html#Thmexercise5", "title": "Exercise A.5 .", "snippet": "Exercise A.5 . Carry through the backpropagation computation for an autoencoder with L L italic_L encoder layers and L L italic_L decoder layers (without necessarily specifying an architecture)."}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E1", "title": "arg ‚Äã min Œ∏ ‚àà Œò ‚Å° ‚Ñí ‚Äã ( Œ∏ ) , \\operatorname*{arg\\ min}_{\\theta\\in\\Theta}\\mathcal{L}(\\theta), start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_Œ∏ ‚àà roman_Œò end_POSTSUBSCRIPT ca", "snippet": "arg ‚Äã min Œ∏ ‚àà Œò ‚Å° ‚Ñí ‚Äã ( Œ∏ ) , \\operatorname*{arg\\ min}_{\\theta\\in\\Theta}\\mathcal{L}(\\theta), start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_Œ∏ ‚àà roman_Œò end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ ) , (A.1.1)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E2", "title": "‚Ñí ‚Äã ( Œ∏ + h ‚Äã ùíó ) ‚â§ ‚Ñí ‚Äã ( Œ∏ ) . \\mathcal{L}(\\theta+h\\bm{v})\\leq\\mathcal{L}(\\theta). caligraphic_L ( italic_Œ∏ + italic_h bold_italic_v ) ‚â§ caligraphic_L ( italic_Œ∏ ) . (A.1.2)", "snippet": "‚Ñí ‚Äã ( Œ∏ + h ‚Äã ùíó ) ‚â§ ‚Ñí ‚Äã ( Œ∏ ) . \\mathcal{L}(\\theta+h\\bm{v})\\leq\\mathcal{L}(\\theta). caligraphic_L ( italic_Œ∏ + italic_h bold_italic_v ) ‚â§ caligraphic_L ( italic_Œ∏ ) . (A.1.2)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E3", "title": "‚Ñí ‚Äã ( Œ∏ + h ‚Äã ùíó ) = ‚Ñí ‚Äã ( Œ∏ ) + h ‚Äã ‚ü® ‚àá ‚Ñí ‚Äã ( Œ∏ ) , ùíó ‚ü© + o ‚Äã ( h ) , \\mathcal{L}(\\theta+h\\bm{v})=\\mathcal{L}(\\theta)+h\\langle\\nabla\\mathcal{L}(\\theta),\\bm{v}\\rangle+o(h), caligraphic_L ( italic_Œ∏ + i", "snippet": "‚Ñí ‚Äã ( Œ∏ + h ‚Äã ùíó ) = ‚Ñí ‚Äã ( Œ∏ ) + h ‚Äã ‚ü® ‚àá ‚Ñí ‚Äã ( Œ∏ ) , ùíó ‚ü© + o ‚Äã ( h ) , \\mathcal{L}(\\theta+h\\bm{v})=\\mathcal{L}(\\theta)+h\\langle\\nabla\\mathcal{L}(\\theta),\\bm{v}\\rangle+o(h), caligraphic_L ( italic_Œ∏ + italic_h bold_italic_v ) = caligraphic_L ( italic_Œ∏ ) + italic_h ‚ü® ‚àá caligraphic_L ( italic_Œ∏ ) , bold_italic_v ‚ü© + italic_o ( italic_h ) , (A.1.3)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E4", "title": "arg ‚Äã min ùíó ‚àà ‚Ñù d ‚Äñ ùíó ‚Äñ 2 = 1 ‚Å° [ ‚Ñí ‚Äã ( Œ∏ ) + h ‚Äã ‚ü® ‚àá ‚Ñí ‚Äã ( Œ∏ ) , ùíó ‚ü© ] = arg ‚Äã min ùíó ‚àà ‚Ñù d ‚Äñ ùíó ‚Äñ 2 = 1 ‚Å° ‚ü® ‚àá ‚Ñí ‚Äã ( Œ∏ ) , ùíó ‚ü© = ‚àí ‚àá ‚Ñí ‚Äã ( Œ∏ ) ‚Äñ ‚àá ‚Ñí ‚Äã ( Œ∏ ) ‚Äñ 2 , \\operatorname*{arg\\ min}_{\\begin{subar", "snippet": "arg ‚Äã min ùíó ‚àà ‚Ñù d ‚Äñ ùíó ‚Äñ 2 = 1 ‚Å° [ ‚Ñí ‚Äã ( Œ∏ ) + h ‚Äã ‚ü® ‚àá ‚Ñí ‚Äã ( Œ∏ ) , ùíó ‚ü© ] = arg ‚Äã min ùíó ‚àà ‚Ñù d ‚Äñ ùíó ‚Äñ 2 = 1 ‚Å° ‚ü® ‚àá ‚Ñí ‚Äã ( Œ∏ ) , ùíó ‚ü© = ‚àí ‚àá ‚Ñí ‚Äã ( Œ∏ ) ‚Äñ ‚àá ‚Ñí ‚Äã ( Œ∏ ) ‚Äñ 2 , \\operatorname*{arg\\ min}_{\\begin{subarray}{c}\\bm{v}\\in\\mathbb{R}^{d}\\\\ \\|\\bm{v}\\|_{2}=1\\end{subarray}}[\\mathcal{L}(\\theta)+h\\langle\\nabla\\mathcal{L}(\\theta),\\bm{v}\\rangle]=\\operatorname*{arg\\ min}_{\\begin{subarray}{c}\\bm{v}\\in\\mathbb{R}^{d}\\\\ \\|\\bm{v}\\|_{2}=1\\end{subarray}}\\langle\\nabla\\mathcal{L}(\\theta),\\bm{v}\\rangle=-\\frac{\\nabla\\mathcal{L}(\\theta)}{\\|\\nabla\\mathcal{L}(\\theta)\\|_{2}}, start_OPERATOR roman_arg roman_min end_OPERATOR"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E5", "title": "Œ∏ k + 1 = Œ∏ k ‚àí h ‚Äã ‚àá ‚Ñí ‚Äã ( Œ∏ k ) . \\theta_{k+1}=\\theta_{k}-h\\nabla\\mathcal{L}(\\theta_{k}). italic_Œ∏ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POST", "snippet": "Œ∏ k + 1 = Œ∏ k ‚àí h ‚Äã ‚àá ‚Ñí ‚Äã ( Œ∏ k ) . \\theta_{k+1}=\\theta_{k}-h\\nabla\\mathcal{L}(\\theta_{k}). italic_Œ∏ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - italic_h ‚àá caligraphic_L ( italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) . (A.1.5)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E6", "title": "h = arg ‚Äã min h ‚â• 0 ‚Å° ‚Ñí ‚Äã ( Œ∏ k ‚àí h ‚Äã ‚àá ‚Ñí ‚Äã ( Œ∏ k ) ) . h=\\operatorname*{arg\\ min}_{h\\geq 0}\\mathcal{L}(\\theta_{k}-h\\nabla\\mathcal{L}(\\theta_{k})). italic_h = start_OPERATOR roman_arg roman_min end_OP", "snippet": "h = arg ‚Äã min h ‚â• 0 ‚Å° ‚Ñí ‚Äã ( Œ∏ k ‚àí h ‚Äã ‚àá ‚Ñí ‚Äã ( Œ∏ k ) ) . h=\\operatorname*{arg\\ min}_{h\\geq 0}\\mathcal{L}(\\theta_{k}-h\\nabla\\mathcal{L}(\\theta_{k})). italic_h = start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_h ‚â• 0 end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - italic_h ‚àá caligraphic_L ( italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ) . (A.1.6)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E7", "title": "‚Ñí ‚Äã ( Œ∏ ) ‚â• l Œ∏ 0 , Œ± ‚Äã ( Œ∏ ) ‚âê ‚Ñí ‚Äã ( Œ∏ 0 ) + ‚ü® ‚àá ‚Ñí ‚Äã ( Œ∏ 0 ) , Œ∏ ‚àí Œ∏ 0 ‚ü© + Œ± 2 ‚Äã ‚Äñ Œ∏ ‚àí Œ∏ 0 ‚Äñ 2 2 \\mathcal{L}(\\theta)\\geq l_{\\theta_{0},\\alpha}(\\theta)\\doteq\\mathcal{L}(\\theta_{0})+\\langle\\nabla\\mathc", "snippet": "‚Ñí ‚Äã ( Œ∏ ) ‚â• l Œ∏ 0 , Œ± ‚Äã ( Œ∏ ) ‚âê ‚Ñí ‚Äã ( Œ∏ 0 ) + ‚ü® ‚àá ‚Ñí ‚Äã ( Œ∏ 0 ) , Œ∏ ‚àí Œ∏ 0 ‚ü© + Œ± 2 ‚Äã ‚Äñ Œ∏ ‚àí Œ∏ 0 ‚Äñ 2 2 \\mathcal{L}(\\theta)\\geq l_{\\theta_{0},\\alpha}(\\theta)\\doteq\\mathcal{L}(\\theta_{0})+\\langle\\nabla\\mathcal{L}(\\theta_{0}),\\theta-\\theta_{0}\\rangle+\\frac{\\alpha}{2}\\|\\theta-\\theta_{0}\\|_{2}^{2} caligraphic_L ( italic_Œ∏ ) ‚â• italic_l start_POSTSUBSCRIPT italic_Œ∏ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_Œ± end_POSTSUBSCRIPT ( italic_Œ∏ ) ‚âê caligraphic_L ( italic_Œ∏ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) + ‚ü® ‚àá caligraphic_L ( italic_Œ∏ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) , italic_Œ∏ - italic_"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E8", "title": "‚Äñ ‚àá ‚Ñí ‚Äã ( Œ∏ ) ‚àí ‚àá ‚Ñí ‚Äã ( Œ∏ 0 ) ‚Äñ 2 ‚â§ Œ≤ ‚Äã ‚Äñ Œ∏ ‚àí Œ∏ 0 ‚Äñ 2 . \\|\\nabla\\mathcal{L}(\\theta)-\\nabla\\mathcal{L}(\\theta_{0})\\|_{2}\\leq\\beta\\|\\theta-\\theta_{0}\\|_{2}. ‚à• ‚àá caligraphic_L ( italic_Œ∏ ) - ‚àá caligraphi", "snippet": "‚Äñ ‚àá ‚Ñí ‚Äã ( Œ∏ ) ‚àí ‚àá ‚Ñí ‚Äã ( Œ∏ 0 ) ‚Äñ 2 ‚â§ Œ≤ ‚Äã ‚Äñ Œ∏ ‚àí Œ∏ 0 ‚Äñ 2 . \\|\\nabla\\mathcal{L}(\\theta)-\\nabla\\mathcal{L}(\\theta_{0})\\|_{2}\\leq\\beta\\|\\theta-\\theta_{0}\\|_{2}. ‚à• ‚àá caligraphic_L ( italic_Œ∏ ) - ‚àá caligraphic_L ( italic_Œ∏ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ‚â§ italic_Œ≤ ‚à• italic_Œ∏ - italic_Œ∏ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT . (A.1.8)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E9", "title": "‚Ñí ‚Äã ( Œ∏ ) ‚â§ u Œ∏ 0 , Œ≤ ‚Äã ( Œ∏ ) ‚âê ‚Ñí ‚Äã ( Œ∏ 0 ) + ‚ü® ‚àá ‚Ñí ‚Äã ( Œ∏ 0 ) , Œ∏ ‚àí Œ∏ 0 ‚ü© + Œ≤ 2 ‚Äã ‚Äñ Œ∏ ‚àí Œ∏ 0 ‚Äñ 2 2 . \\mathcal{L}(\\theta)\\leq u_{\\theta_{0},\\beta}(\\theta)\\doteq\\mathcal{L}(\\theta_{0})+\\langle\\nabla\\math", "snippet": "‚Ñí ‚Äã ( Œ∏ ) ‚â§ u Œ∏ 0 , Œ≤ ‚Äã ( Œ∏ ) ‚âê ‚Ñí ‚Äã ( Œ∏ 0 ) + ‚ü® ‚àá ‚Ñí ‚Äã ( Œ∏ 0 ) , Œ∏ ‚àí Œ∏ 0 ‚ü© + Œ≤ 2 ‚Äã ‚Äñ Œ∏ ‚àí Œ∏ 0 ‚Äñ 2 2 . \\mathcal{L}(\\theta)\\leq u_{\\theta_{0},\\beta}(\\theta)\\doteq\\mathcal{L}(\\theta_{0})+\\langle\\nabla\\mathcal{L}(\\theta_{0}),\\theta-\\theta_{0}\\rangle+\\frac{\\beta}{2}\\|\\theta-\\theta_{0}\\|_{2}^{2}. caligraphic_L ( italic_Œ∏ ) ‚â§ italic_u start_POSTSUBSCRIPT italic_Œ∏ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_Œ≤ end_POSTSUBSCRIPT ( italic_Œ∏ ) ‚âê caligraphic_L ( italic_Œ∏ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) + ‚ü® ‚àá caligraphic_L ( italic_Œ∏ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) , italic_Œ∏ - italic"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E10", "title": "Œ∏ 1 ‚àà arg ‚Äã min Œ∏ ‚àà Œò ‚Å° u ‚Äã ( Œ∏ ) ‚üπ ‚Ñí ‚Äã ( Œ∏ 1 ) ‚â§ u ‚Äã ( Œ∏ 1 ) ‚â§ u ‚Äã ( Œ∏ 0 ) = ‚Ñí ‚Äã ( Œ∏ 0 ) . \\theta_{1}\\in\\operatorname*{arg\\ min}_{\\theta\\in\\Theta}u(\\theta)\\implies\\mathcal{L}(\\theta_{1})\\leq u(\\theta", "snippet": "Œ∏ 1 ‚àà arg ‚Äã min Œ∏ ‚àà Œò ‚Å° u ‚Äã ( Œ∏ ) ‚üπ ‚Ñí ‚Äã ( Œ∏ 1 ) ‚â§ u ‚Äã ( Œ∏ 1 ) ‚â§ u ‚Äã ( Œ∏ 0 ) = ‚Ñí ‚Äã ( Œ∏ 0 ) . \\theta_{1}\\in\\operatorname*{arg\\ min}_{\\theta\\in\\Theta}u(\\theta)\\implies\\mathcal{L}(\\theta_{1})\\leq u(\\theta_{1})\\leq u(\\theta_{0})=\\mathcal{L}(\\theta_{0}). italic_Œ∏ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ‚àà start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_Œ∏ ‚àà roman_Œò end_POSTSUBSCRIPT italic_u ( italic_Œ∏ ) ‚üπ caligraphic_L ( italic_Œ∏ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ‚â§ italic_u ( italic_Œ∏ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ‚â§ italic_u ( italic_Œ∏ start_POSTSUBSCRIP"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E11", "title": "if Œ∏ minimizes u Œ∏ 0 , Œ≤ then ‚Ñí ‚Äã ( Œ∏ ) ‚â§ u Œ∏ 0 , Œ≤ ‚Äã ( Œ∏ ) ‚â§ u Œ∏ 0 , Œ≤ ‚Äã ( Œ∏ 0 ) = ‚Ñí ‚Äã ( Œ∏ 0 ) . \\text{if $\\theta$ minimizes $u_{\\theta_{0},\\beta}$ then}\\quad\\mathcal{L}(\\theta)\\leq u_{\\theta_{0},\\be", "snippet": "if Œ∏ minimizes u Œ∏ 0 , Œ≤ then ‚Ñí ‚Äã ( Œ∏ ) ‚â§ u Œ∏ 0 , Œ≤ ‚Äã ( Œ∏ ) ‚â§ u Œ∏ 0 , Œ≤ ‚Äã ( Œ∏ 0 ) = ‚Ñí ‚Äã ( Œ∏ 0 ) . \\text{if $\\theta$ minimizes $u_{\\theta_{0},\\beta}$ then}\\quad\\mathcal{L}(\\theta)\\leq u_{\\theta_{0},\\beta}(\\theta)\\leq u_{\\theta_{0},\\beta}(\\theta_{0})=\\mathcal{L}(\\theta_{0}). if italic_Œ∏ minimizes italic_u start_POSTSUBSCRIPT italic_Œ∏ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_Œ≤ end_POSTSUBSCRIPT then caligraphic_L ( italic_Œ∏ ) ‚â§ italic_u start_POSTSUBSCRIPT italic_Œ∏ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_Œ≤ end_POSTSUBSCRIPT ( italic_Œ∏ ) ‚â§ italic_u start_POSTSUBSCRIPT italic_Œ∏ sta"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E12", "title": "Œ∏ k + 1 = Œ∏ k ‚àí 1 Œ≤ ‚Äã ‚àá ‚Ñí ‚Äã ( Œ∏ k ) ‚üπ ‚Ñí ‚Äã ( Œ∏ k + 1 ) ‚â§ ‚Ñí ‚Äã ( Œ∏ k ) . \\theta_{k+1}=\\theta_{k}-\\frac{1}{\\beta}\\nabla\\mathcal{L}(\\theta_{k})\\implies\\mathcal{L}(\\theta_{k+1})\\leq\\mathcal{L}(\\theta_{k}). ", "snippet": "Œ∏ k + 1 = Œ∏ k ‚àí 1 Œ≤ ‚Äã ‚àá ‚Ñí ‚Äã ( Œ∏ k ) ‚üπ ‚Ñí ‚Äã ( Œ∏ k + 1 ) ‚â§ ‚Ñí ‚Äã ( Œ∏ k ) . \\theta_{k+1}=\\theta_{k}-\\frac{1}{\\beta}\\nabla\\mathcal{L}(\\theta_{k})\\implies\\mathcal{L}(\\theta_{k+1})\\leq\\mathcal{L}(\\theta_{k}). italic_Œ∏ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - divide start_ARG 1 end_ARG start_ARG italic_Œ≤ end_ARG ‚àá caligraphic_L ( italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ‚üπ caligraphic_L ( italic_Œ∏ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT ) ‚â§ caligraphic_L ( italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCR"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E21", "title": "‚Äñ Œ∏ ‚ãÜ ‚àí Œ∏ k + 1 ‚Äñ 2 2 ‚â§ ( 1 ‚àí Œ± / Œ≤ ) k + 1 ‚Äã ‚Äñ Œ∏ ‚ãÜ ‚àí Œ∏ 0 ‚Äñ 2 2 , \\|\\theta^{\\star}-\\theta_{k+1}\\|_{2}^{2}\\leq(1-\\alpha/\\beta)^{k+1}\\|\\theta^{\\star}-\\theta_{0}\\|_{2}^{2}, ‚à• italic_Œ∏ start_POSTSUPERSCRI", "snippet": "‚Äñ Œ∏ ‚ãÜ ‚àí Œ∏ k + 1 ‚Äñ 2 2 ‚â§ ( 1 ‚àí Œ± / Œ≤ ) k + 1 ‚Äã ‚Äñ Œ∏ ‚ãÜ ‚àí Œ∏ 0 ‚Äñ 2 2 , \\|\\theta^{\\star}-\\theta_{k+1}\\|_{2}^{2}\\leq(1-\\alpha/\\beta)^{k+1}\\|\\theta^{\\star}-\\theta_{0}\\|_{2}^{2}, ‚à• italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT - italic_Œ∏ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ‚â§ ( 1 - italic_Œ± / italic_Œ≤ ) start_POSTSUPERSCRIPT italic_k + 1 end_POSTSUPERSCRIPT ‚à• italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT - italic_Œ∏ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBS"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E22", "title": "‚Ñí Œª ‚Äã ( Œ∏ ) = ‚Ñí Œª ‚Äã ( [ Œ∏ 1 Œ∏ 2 ] ) ‚âê 1 2 ‚Äã { ( 1 + Œª ) ‚Äã Œ∏ 1 2 + Œ∏ 2 2 } = 1 2 ‚Äã Œ∏ ‚ä§ ‚Äã [ 1 + Œª 0 0 1 ] ‚Äã Œ∏ . \\mathcal{L}_{\\lambda}(\\theta)=\\mathcal{L}_{\\lambda}\\left(\\begin{bmatrix}\\theta_{1}\\\\ \\thet", "snippet": "‚Ñí Œª ‚Äã ( Œ∏ ) = ‚Ñí Œª ‚Äã ( [ Œ∏ 1 Œ∏ 2 ] ) ‚âê 1 2 ‚Äã { ( 1 + Œª ) ‚Äã Œ∏ 1 2 + Œ∏ 2 2 } = 1 2 ‚Äã Œ∏ ‚ä§ ‚Äã [ 1 + Œª 0 0 1 ] ‚Äã Œ∏ . \\mathcal{L}_{\\lambda}(\\theta)=\\mathcal{L}_{\\lambda}\\left(\\begin{bmatrix}\\theta_{1}\\\\ \\theta_{2}\\end{bmatrix}\\right)\\doteq\\frac{1}{2}\\left\\{(1+\\lambda)\\theta_{1}^{2}+\\theta_{2}^{2}\\right\\}=\\frac{1}{2}\\theta^{\\top}\\begin{bmatrix}1+\\lambda&0\\\\ 0&1\\end{bmatrix}\\theta. caligraphic_L start_POSTSUBSCRIPT italic_Œª end_POSTSUBSCRIPT ( italic_Œ∏ ) = caligraphic_L start_POSTSUBSCRIPT italic_Œª end_POSTSUBSCRIPT ( [ start_ARG start_ROW start_CELL italic_Œ∏ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E23", "title": "‚Ñí ‚Äã ( Œ∏ + h ‚Äã ùíó ) = ‚Ñí ‚Äã ( Œ∏ ) + h ‚Äã ‚ü® ‚àá ‚Ñí ‚Äã ( Œ∏ ) , ùíó ‚ü© + 1 2 ‚Äã h 2 ‚Äã ‚ü® [ ‚àá 2 ‚Ñí ‚Äã ( Œ∏ ) ] ‚Äã ùíó , ùíó ‚ü© + o ‚Äã ( h 2 ) . \\mathcal{L}(\\theta+h\\bm{v})=\\mathcal{L}(\\theta)+h\\langle\\nabla\\mathcal{L}(\\theta),\\b", "snippet": "‚Ñí ‚Äã ( Œ∏ + h ‚Äã ùíó ) = ‚Ñí ‚Äã ( Œ∏ ) + h ‚Äã ‚ü® ‚àá ‚Ñí ‚Äã ( Œ∏ ) , ùíó ‚ü© + 1 2 ‚Äã h 2 ‚Äã ‚ü® [ ‚àá 2 ‚Ñí ‚Äã ( Œ∏ ) ] ‚Äã ùíó , ùíó ‚ü© + o ‚Äã ( h 2 ) . \\mathcal{L}(\\theta+h\\bm{v})=\\mathcal{L}(\\theta)+h\\langle\\nabla\\mathcal{L}(\\theta),\\bm{v}\\rangle+\\frac{1}{2}h^{2}\\langle[\\nabla^{2}\\mathcal{L}(\\theta)]\\bm{v},\\bm{v}\\rangle+o(h^{2}). caligraphic_L ( italic_Œ∏ + italic_h bold_italic_v ) = caligraphic_L ( italic_Œ∏ ) + italic_h ‚ü® ‚àá caligraphic_L ( italic_Œ∏ ) , bold_italic_v ‚ü© + divide start_ARG 1 end_ARG start_ARG 2 end_ARG italic_h start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ‚ü® [ ‚àá start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT caligraphi"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E25", "title": "arg ‚Äã min ùíó ‚àà ‚Ñù n ‚Å° [ ‚ü® ‚àá ‚Ñí ‚Äã ( Œ∏ ) , ùíó ‚ü© + 1 2 ‚Äã h ‚Äã ‚ü® [ ‚àá 2 ‚Ñí ‚Äã ( Œ∏ ) ] ‚Äã ùíó , ùíó ‚ü© ] = ‚àí 1 h ‚Äã [ ‚àá 2 ‚Ñí ‚Äã ( Œ∏ ) ] ‚àí 1 ‚Äã [ ‚àá ‚Ñí ‚Äã ( Œ∏ ) ] . \\operatorname*{arg\\ min}_{\\bm{v}\\in\\mathbb{R}^{n}}\\left[\\langl", "snippet": "arg ‚Äã min ùíó ‚àà ‚Ñù n ‚Å° [ ‚ü® ‚àá ‚Ñí ‚Äã ( Œ∏ ) , ùíó ‚ü© + 1 2 ‚Äã h ‚Äã ‚ü® [ ‚àá 2 ‚Ñí ‚Äã ( Œ∏ ) ] ‚Äã ùíó , ùíó ‚ü© ] = ‚àí 1 h ‚Äã [ ‚àá 2 ‚Ñí ‚Äã ( Œ∏ ) ] ‚àí 1 ‚Äã [ ‚àá ‚Ñí ‚Äã ( Œ∏ ) ] . \\operatorname*{arg\\ min}_{\\bm{v}\\in\\mathbb{R}^{n}}\\left[\\langle\\nabla\\mathcal{L}(\\theta),\\bm{v}\\rangle+\\frac{1}{2}h\\langle[\\nabla^{2}\\mathcal{L}(\\theta)]\\bm{v},\\bm{v}\\rangle\\right]=-\\frac{1}{h}[\\nabla^{2}\\mathcal{L}(\\theta)]^{-1}[\\nabla\\mathcal{L}(\\theta)]. start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT bold_italic_v ‚àà blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT end_POSTSUBSCRIPT [ ‚ü® ‚àá caligraphic_L ( italic_Œ∏ ) , bol"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E26", "title": "Œ∏ k + 1 = Œ∏ k ‚àí [ ‚àá 2 ‚Ñí ‚Äã ( Œ∏ k ) ] ‚àí 1 ‚Äã [ ‚àá ‚Ñí ‚Äã ( Œ∏ k ) ] , \\theta_{k+1}=\\theta_{k}-[\\nabla^{2}\\mathcal{L}(\\theta_{k})]^{-1}[\\nabla\\mathcal{L}(\\theta_{k})], italic_Œ∏ start_POSTSUBSCRIPT italic_k + 1", "snippet": "Œ∏ k + 1 = Œ∏ k ‚àí [ ‚àá 2 ‚Ñí ‚Äã ( Œ∏ k ) ] ‚àí 1 ‚Äã [ ‚àá ‚Ñí ‚Äã ( Œ∏ k ) ] , \\theta_{k+1}=\\theta_{k}-[\\nabla^{2}\\mathcal{L}(\\theta_{k})]^{-1}[\\nabla\\mathcal{L}(\\theta_{k})], italic_Œ∏ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - [ ‚àá start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT caligraphic_L ( italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ] start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT [ ‚àá caligraphic_L ( italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ] , (A.1.26)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E27", "title": "Œ∏ k + 1 = Œ∏ k ‚àí h ‚Äã [ ‚àá 2 ‚Ñí ‚Äã ( Œ∏ k ) ] ‚àí 1 ‚Äã [ ‚àá ‚Ñí ‚Äã ( Œ∏ k ) ] , \\theta_{k+1}=\\theta_{k}-h[\\nabla^{2}\\mathcal{L}(\\theta_{k})]^{-1}[\\nabla\\mathcal{L}(\\theta_{k})], italic_Œ∏ start_POSTSUBSCRIPT italic_", "snippet": "Œ∏ k + 1 = Œ∏ k ‚àí h ‚Äã [ ‚àá 2 ‚Ñí ‚Äã ( Œ∏ k ) ] ‚àí 1 ‚Äã [ ‚àá ‚Ñí ‚Äã ( Œ∏ k ) ] , \\theta_{k+1}=\\theta_{k}-h[\\nabla^{2}\\mathcal{L}(\\theta_{k})]^{-1}[\\nabla\\mathcal{L}(\\theta_{k})], italic_Œ∏ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - italic_h [ ‚àá start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT caligraphic_L ( italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ] start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT [ ‚àá caligraphic_L ( italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ] , (A.1.27)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E28", "title": "‚àá L ‚Äã ( Œ∏ + Œ¥ Œ∏ ) ‚àí ‚àá ‚Ñí ‚Äã ( Œ∏ ) ‚èü ‚âê Œ¥ ùíà = [ ‚àá 2 ‚Ñí ‚Äã ( Œ∏ ) ] ‚Äã Œ¥ Œ∏ + o ‚Äã ( ‚Äñ Œ¥ Œ∏ ‚Äñ 2 ) . \\underbrace{\\nabla L(\\theta+\\delta_{\\theta})-\\nabla\\mathcal{L}(\\theta)}_{\\doteq\\delta_{\\bm{g}}}=[\\nabla^{2}\\math", "snippet": "‚àá L ‚Äã ( Œ∏ + Œ¥ Œ∏ ) ‚àí ‚àá ‚Ñí ‚Äã ( Œ∏ ) ‚èü ‚âê Œ¥ ùíà = [ ‚àá 2 ‚Ñí ‚Äã ( Œ∏ ) ] ‚Äã Œ¥ Œ∏ + o ‚Äã ( ‚Äñ Œ¥ Œ∏ ‚Äñ 2 ) . \\underbrace{\\nabla L(\\theta+\\delta_{\\theta})-\\nabla\\mathcal{L}(\\theta)}_{\\doteq\\delta_{\\bm{g}}}=[\\nabla^{2}\\mathcal{L}(\\theta)]\\delta_{\\theta}+o(\\|\\delta_{\\theta}\\|_{2}). under‚èü start_ARG ‚àá italic_L ( italic_Œ∏ + italic_Œ¥ start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ) - ‚àá caligraphic_L ( italic_Œ∏ ) end_ARG start_POSTSUBSCRIPT ‚âê italic_Œ¥ start_POSTSUBSCRIPT bold_italic_g end_POSTSUBSCRIPT end_POSTSUBSCRIPT = [ ‚àá start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT caligraphic_L ( italic_Œ∏ ) ] italic_Œ¥ start_POSTSUBS"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E29", "title": "Œ¥ ùíà ‚âà [ ‚àá 2 ‚Ñí ‚Äã ( Œ∏ ) ] ‚Äã Œ¥ Œ∏ ‚üπ Œ¥ Œ∏ ‚âà [ ‚àá 2 ‚Ñí ‚Äã ( Œ∏ ) ] ‚àí 1 ‚Äã Œ¥ ùíà \\delta_{\\bm{g}}\\approx[\\nabla^{2}\\mathcal{L}(\\theta)]\\delta_{\\theta}\\implies\\delta_{\\theta}\\approx[\\nabla^{2}\\mathcal{L}(\\theta)]^{-1}", "snippet": "Œ¥ ùíà ‚âà [ ‚àá 2 ‚Ñí ‚Äã ( Œ∏ ) ] ‚Äã Œ¥ Œ∏ ‚üπ Œ¥ Œ∏ ‚âà [ ‚àá 2 ‚Ñí ‚Äã ( Œ∏ ) ] ‚àí 1 ‚Äã Œ¥ ùíà \\delta_{\\bm{g}}\\approx[\\nabla^{2}\\mathcal{L}(\\theta)]\\delta_{\\theta}\\implies\\delta_{\\theta}\\approx[\\nabla^{2}\\mathcal{L}(\\theta)]^{-1}\\delta_{\\bm{g}} italic_Œ¥ start_POSTSUBSCRIPT bold_italic_g end_POSTSUBSCRIPT ‚âà [ ‚àá start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT caligraphic_L ( italic_Œ∏ ) ] italic_Œ¥ start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ‚üπ italic_Œ¥ start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ‚âà [ ‚àá start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT caligraphic_L ( italic_Œ∏ ) ] start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT itali"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E30", "title": "Œ¥ Œ∏ ‚âà P ‚Äã Œ¥ ùíà , \\delta_{\\theta}\\approx P\\delta_{\\bm{g}}, italic_Œ¥ start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ‚âà italic_P italic_Œ¥ start_POSTSUBSCRIPT bold_italic_g end_POSTSUBSCRIPT , (A.1.30)", "snippet": "Œ¥ Œ∏ ‚âà P ‚Äã Œ¥ ùíà , \\delta_{\\theta}\\approx P\\delta_{\\bm{g}}, italic_Œ¥ start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ‚âà italic_P italic_Œ¥ start_POSTSUBSCRIPT bold_italic_g end_POSTSUBSCRIPT , (A.1.30)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E33", "title": "‚Ñí ‚Äã ( Œ∏ ) ‚âê ùíÆ ‚Äã ( Œ∏ ) + ‚Ñõ ‚Äã ( Œ∏ ) \\mathcal{L}(\\theta)\\doteq\\mathcal{S}(\\theta)+\\mathcal{R}(\\theta) caligraphic_L ( italic_Œ∏ ) ‚âê caligraphic_S ( italic_Œ∏ ) + caligraphic_R ( italic_Œ∏ ) (A.1.33)", "snippet": "‚Ñí ‚Äã ( Œ∏ ) ‚âê ùíÆ ‚Äã ( Œ∏ ) + ‚Ñõ ‚Äã ( Œ∏ ) \\mathcal{L}(\\theta)\\doteq\\mathcal{S}(\\theta)+\\mathcal{R}(\\theta) caligraphic_L ( italic_Œ∏ ) ‚âê caligraphic_S ( italic_Œ∏ ) + caligraphic_R ( italic_Œ∏ ) (A.1.33)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E34", "title": "‚Ñí ‚Äã ( Œ∏ 1 ) = ùíÆ ‚Äã ( Œ∏ 1 ) + ‚Ñõ ‚Äã ( Œ∏ 1 ) ‚â§ u Œ∏ 0 , Œ≤ ‚Äã ( Œ∏ 1 ) ‚âê ùíÆ ‚Äã ( Œ∏ 0 ) + ‚ü® ‚àá ùíÆ ‚Äã ( Œ∏ 0 ) , Œ∏ 1 ‚àí Œ∏ 0 ‚ü© + Œ≤ 2 ‚Äã ‚Äñ Œ∏ 1 ‚àí Œ∏ 0 ‚Äñ 2 2 + ‚Ñõ ‚Äã ( Œ∏ 1 ) . \\mathcal{L}(\\theta_{1})=\\mathcal{S}(\\theta_{1})+\\m", "snippet": "‚Ñí ‚Äã ( Œ∏ 1 ) = ùíÆ ‚Äã ( Œ∏ 1 ) + ‚Ñõ ‚Äã ( Œ∏ 1 ) ‚â§ u Œ∏ 0 , Œ≤ ‚Äã ( Œ∏ 1 ) ‚âê ùíÆ ‚Äã ( Œ∏ 0 ) + ‚ü® ‚àá ùíÆ ‚Äã ( Œ∏ 0 ) , Œ∏ 1 ‚àí Œ∏ 0 ‚ü© + Œ≤ 2 ‚Äã ‚Äñ Œ∏ 1 ‚àí Œ∏ 0 ‚Äñ 2 2 + ‚Ñõ ‚Äã ( Œ∏ 1 ) . \\mathcal{L}(\\theta_{1})=\\mathcal{S}(\\theta_{1})+\\mathcal{R}(\\theta_{1})\\leq u_{\\theta_{0},\\beta}(\\theta_{1})\\doteq\\mathcal{S}(\\theta_{0})+\\langle\\nabla\\mathcal{S}(\\theta_{0}),\\theta_{1}-\\theta_{0}\\rangle+\\frac{\\beta}{2}\\|\\theta_{1}-\\theta_{0}\\|_{2}^{2}+\\mathcal{R}(\\theta_{1}). caligraphic_L ( italic_Œ∏ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) = caligraphic_S ( italic_Œ∏ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) + caligraphic_R ( italic_Œ∏ start_POS"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E35", "title": "arg ‚Äã min Œ∏ 1 ‚Å° u Œ∏ 0 , Œ≤ ‚Äã ( Œ∏ 1 ) = arg ‚Äã min Œ∏ 1 ‚Å° [ Œ≤ 2 ‚Äã ‚Äñ Œ∏ 1 ‚àí ( Œ∏ 0 ‚àí 1 Œ≤ ‚Äã ‚àá ùíÆ ‚Äã ( Œ∏ 0 ) ) ‚Äñ 2 2 + ‚Ñõ ‚Äã ( Œ∏ 1 ) ] . \\operatorname*{arg\\ min}_{\\theta_{1}}u_{\\theta_{0},\\beta}(\\theta_{1})=\\opera", "snippet": "arg ‚Äã min Œ∏ 1 ‚Å° u Œ∏ 0 , Œ≤ ‚Äã ( Œ∏ 1 ) = arg ‚Äã min Œ∏ 1 ‚Å° [ Œ≤ 2 ‚Äã ‚Äñ Œ∏ 1 ‚àí ( Œ∏ 0 ‚àí 1 Œ≤ ‚Äã ‚àá ùíÆ ‚Äã ( Œ∏ 0 ) ) ‚Äñ 2 2 + ‚Ñõ ‚Äã ( Œ∏ 1 ) ] . \\operatorname*{arg\\ min}_{\\theta_{1}}u_{\\theta_{0},\\beta}(\\theta_{1})=\\operatorname*{arg\\ min}_{\\theta_{1}}\\left[\\frac{\\beta}{2}\\left\\|\\theta_{1}-\\left(\\theta_{0}-\\frac{1}{\\beta}\\nabla\\mathcal{S}(\\theta_{0})\\right)\\right\\|_{2}^{2}+\\mathcal{R}(\\theta_{1})\\right]. start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_Œ∏ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_u start_POSTSUBSCRIPT italic_Œ∏ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT "}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E36", "title": "prox h , ‚Ñõ ‚Å° ( Œ∏ ) ‚âê arg ‚Äã min Œ∏ 1 ‚Å° [ 1 2 ‚Äã h ‚Äã ‚Äñ Œ∏ 1 ‚àí Œ∏ ‚Äñ 2 2 + ‚Ñõ ‚Äã ( Œ∏ ) ] . \\operatorname{prox}_{h,\\mathcal{R}}(\\theta)\\doteq\\operatorname*{arg\\ min}_{\\theta_{1}}\\left[\\frac{1}{2h}\\|\\theta_{1}-\\t", "snippet": "prox h , ‚Ñõ ‚Å° ( Œ∏ ) ‚âê arg ‚Äã min Œ∏ 1 ‚Å° [ 1 2 ‚Äã h ‚Äã ‚Äñ Œ∏ 1 ‚àí Œ∏ ‚Äñ 2 2 + ‚Ñõ ‚Äã ( Œ∏ ) ] . \\operatorname{prox}_{h,\\mathcal{R}}(\\theta)\\doteq\\operatorname*{arg\\ min}_{\\theta_{1}}\\left[\\frac{1}{2h}\\|\\theta_{1}-\\theta\\|_{2}^{2}+\\mathcal{R}(\\theta)\\right]. roman_prox start_POSTSUBSCRIPT italic_h , caligraphic_R end_POSTSUBSCRIPT ( italic_Œ∏ ) ‚âê start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_Œ∏ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ divide start_ARG 1 end_ARG start_ARG 2 italic_h end_ARG ‚à• italic_Œ∏ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT - italic_Œ∏ ‚à• start_POSTS"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E37", "title": "Œ∏ k + 1 = prox h , ‚Ñõ ‚Å° ( Œ∏ k ‚àí h ‚Äã ‚àá ùíÆ ‚Äã ( Œ∏ k ) ) . \\theta_{k+1}=\\operatorname{prox}_{h,\\mathcal{R}}(\\theta_{k}-h\\nabla\\mathcal{S}(\\theta_{k})). italic_Œ∏ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBS", "snippet": "Œ∏ k + 1 = prox h , ‚Ñõ ‚Å° ( Œ∏ k ‚àí h ‚Äã ‚àá ùíÆ ‚Äã ( Œ∏ k ) ) . \\theta_{k+1}=\\operatorname{prox}_{h,\\mathcal{R}}(\\theta_{k}-h\\nabla\\mathcal{S}(\\theta_{k})). italic_Œ∏ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = roman_prox start_POSTSUBSCRIPT italic_h , caligraphic_R end_POSTSUBSCRIPT ( italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - italic_h ‚àá caligraphic_S ( italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ) . (A.1.37)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E38", "title": "œá Œì ‚Äã ( Œ∏ ) ‚âê { 0 , if ‚Äã Œ∏ ‚àà Œì + ‚àû , if ‚Äã Œ∏ ‚àâ Œì . \\chi_{\\Gamma}(\\theta)\\doteq\\begin{cases}0,&\\text{if}\\ \\theta\\in\\Gamma\\\\ +\\infty,&\\text{if}\\ \\theta\\notin\\Gamma.\\end{cases} italic_œá start_POSTSUBSCRIP", "snippet": "œá Œì ‚Äã ( Œ∏ ) ‚âê { 0 , if ‚Äã Œ∏ ‚àà Œì + ‚àû , if ‚Äã Œ∏ ‚àâ Œì . \\chi_{\\Gamma}(\\theta)\\doteq\\begin{cases}0,&\\text{if}\\ \\theta\\in\\Gamma\\\\ +\\infty,&\\text{if}\\ \\theta\\notin\\Gamma.\\end{cases} italic_œá start_POSTSUBSCRIPT roman_Œì end_POSTSUBSCRIPT ( italic_Œ∏ ) ‚âê { start_ROW start_CELL 0 , end_CELL start_CELL if italic_Œ∏ ‚àà roman_Œì end_CELL end_ROW start_ROW start_CELL + ‚àû , end_CELL start_CELL if italic_Œ∏ ‚àâ roman_Œì . end_CELL end_ROW (A.1.38)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E39", "title": "prox h , œá Œì ‚Å° ( Œ∏ ) = arg ‚Äã min Œ∏ 1 ‚àà Œì ‚Å° 1 2 ‚Äã ‚Äñ Œ∏ 1 ‚àí Œ∏ ‚Äñ 2 2 = arg ‚Äã min Œ∏ 1 ‚àà Œì ‚Å° ‚Äñ Œ∏ 1 ‚àí Œ∏ ‚Äñ 2 . \\operatorname{prox}_{h,\\chi_{\\Gamma}}(\\theta)=\\operatorname*{arg\\ min}_{\\theta_{1}\\in\\Gamma}\\frac", "snippet": "prox h , œá Œì ‚Å° ( Œ∏ ) = arg ‚Äã min Œ∏ 1 ‚àà Œì ‚Å° 1 2 ‚Äã ‚Äñ Œ∏ 1 ‚àí Œ∏ ‚Äñ 2 2 = arg ‚Äã min Œ∏ 1 ‚àà Œì ‚Å° ‚Äñ Œ∏ 1 ‚àí Œ∏ ‚Äñ 2 . \\operatorname{prox}_{h,\\chi_{\\Gamma}}(\\theta)=\\operatorname*{arg\\ min}_{\\theta_{1}\\in\\Gamma}\\frac{1}{2}\\|\\theta_{1}-\\theta\\|_{2}^{2}=\\operatorname*{arg\\ min}_{\\theta_{1}\\in\\Gamma}\\|\\theta_{1}-\\theta\\|_{2}. roman_prox start_POSTSUBSCRIPT italic_h , italic_œá start_POSTSUBSCRIPT roman_Œì end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_Œ∏ ) = start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_Œ∏ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ‚àà roman_Œì end_POSTSUBSCRIPT divide start_ARG"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E40", "title": "S h ‚Äã ( Œ∏ ) ‚âê prox h , Œª ‚à• ‚ãÖ ‚à• 1 ‚Å° ( Œ∏ ) = arg ‚Äã min Œ∏ 1 ‚Å° [ 1 2 ‚Äã h ‚Äã ‚Äñ Œ∏ 1 ‚àí Œ∏ ‚Äñ 2 2 + Œª ‚Äã ‚Äñ Œ∏ ‚Äñ 1 ] S_{h}(\\theta)\\doteq\\operatorname{prox}_{h,\\lambda\\|\\cdot\\|_{1}}(\\theta)=\\operatorname*{arg\\ min}_", "snippet": "S h ‚Äã ( Œ∏ ) ‚âê prox h , Œª ‚à• ‚ãÖ ‚à• 1 ‚Å° ( Œ∏ ) = arg ‚Äã min Œ∏ 1 ‚Å° [ 1 2 ‚Äã h ‚Äã ‚Äñ Œ∏ 1 ‚àí Œ∏ ‚Äñ 2 2 + Œª ‚Äã ‚Äñ Œ∏ ‚Äñ 1 ] S_{h}(\\theta)\\doteq\\operatorname{prox}_{h,\\lambda\\|\\cdot\\|_{1}}(\\theta)=\\operatorname*{arg\\ min}_{\\theta_{1}}\\left[\\frac{1}{2h}\\|\\theta_{1}-\\theta\\|_{2}^{2}+\\lambda\\|\\theta\\|_{1}\\right] italic_S start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ( italic_Œ∏ ) ‚âê roman_prox start_POSTSUBSCRIPT italic_h , italic_Œª ‚à• ‚ãÖ ‚à• start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_Œ∏ ) = start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_Œ∏ start_POSTSUBSCRIPT 1 end_POSTSUB"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E41", "title": "S h ‚Äã ( Œ∏ ) i = { Œ∏ i ‚àí h ‚Äã Œª , if ‚Äã Œ∏ i ‚â• h ‚Äã Œª 0 , if ‚Äã Œ∏ i ‚àà [ ‚àí h ‚Äã Œª , h ‚Äã Œª ] Œ∏ i + h ‚Äã Œª , if ‚Äã Œ∏ i ‚â§ ‚àí h ‚Äã Œª = { max ‚Å° { | Œ∏ i | ‚àí h ‚Äã Œª , 0 } ‚Äã sign ‚Å° ( Œ∏ i ) , if ‚Äã | Œ∏ i | ‚â• h ‚Äã Œª 0 , if ‚Äã ", "snippet": "S h ‚Äã ( Œ∏ ) i = { Œ∏ i ‚àí h ‚Äã Œª , if ‚Äã Œ∏ i ‚â• h ‚Äã Œª 0 , if ‚Äã Œ∏ i ‚àà [ ‚àí h ‚Äã Œª , h ‚Äã Œª ] Œ∏ i + h ‚Äã Œª , if ‚Äã Œ∏ i ‚â§ ‚àí h ‚Äã Œª = { max ‚Å° { | Œ∏ i | ‚àí h ‚Äã Œª , 0 } ‚Äã sign ‚Å° ( Œ∏ i ) , if ‚Äã | Œ∏ i | ‚â• h ‚Äã Œª 0 , if ‚Äã | Œ∏ i | < h ‚Äã Œª . S_{h}(\\theta)_{i}=\\begin{cases}\\theta_{i}-h\\lambda,&\\text{if}\\ \\theta_{i}\\geq h\\lambda\\\\ 0,&\\text{if}\\ \\theta_{i}\\in[-h\\lambda,h\\lambda]\\\\ \\theta_{i}+h\\lambda,&\\text{if}\\ \\theta_{i}\\leq-h\\lambda\\end{cases}=\\begin{cases}\\max\\{\\lvert\\theta_{i}\\rvert-h\\lambda,0\\}\\operatorname{sign}(\\theta_{i}),&\\text{if}\\ \\lvert\\theta_{i}\\rvert\\geq h\\lambda\\\\ 0,&\\text{if}\\ \\lvert\\theta_{i}\\rvert<h\\l"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E42", "title": "T h ‚Äã ( Œ∏ ) ‚âê prox h , Œª ‚à• ‚ãÖ ‚à• 1 + œá ‚Ñù + n ‚Å° ( Œ∏ ) = arg ‚Äã min Œ∏ 1 ‚àà ‚Ñù + n ‚Å° [ 1 2 ‚Äã h ‚Äã ‚Äñ Œ∏ 1 ‚àí Œ∏ ‚Äñ 2 2 + Œª ‚Äã ‚Äñ Œ∏ ‚Äñ 1 ] , T_{h}(\\theta)\\doteq\\operatorname{prox}_{h,\\lambda\\|\\cdot\\|_{1}+\\chi_{\\mathbb{", "snippet": "T h ‚Äã ( Œ∏ ) ‚âê prox h , Œª ‚à• ‚ãÖ ‚à• 1 + œá ‚Ñù + n ‚Å° ( Œ∏ ) = arg ‚Äã min Œ∏ 1 ‚àà ‚Ñù + n ‚Å° [ 1 2 ‚Äã h ‚Äã ‚Äñ Œ∏ 1 ‚àí Œ∏ ‚Äñ 2 2 + Œª ‚Äã ‚Äñ Œ∏ ‚Äñ 1 ] , T_{h}(\\theta)\\doteq\\operatorname{prox}_{h,\\lambda\\|\\cdot\\|_{1}+\\chi_{\\mathbb{R}_{+}^{n}}}(\\theta)=\\operatorname*{arg\\ min}_{\\theta_{1}\\in\\mathbb{R}_{+}^{n}}\\left[\\frac{1}{2h}\\|\\theta_{1}-\\theta\\|_{2}^{2}+\\lambda\\|\\theta\\|_{1}\\right], italic_T start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ( italic_Œ∏ ) ‚âê roman_prox start_POSTSUBSCRIPT italic_h , italic_Œª ‚à• ‚ãÖ ‚à• start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + italic_œá start_POSTSUBSCRIPT blackboard_R start_POSTSUBSCRIPT + end_POSTS"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E43", "title": "T h ‚Äã ( Œ∏ ) i ‚âê max ‚Å° { Œ∏ i ‚àí h ‚Äã Œª , 0 } . T_{h}(\\theta)_{i}\\doteq\\max\\{\\theta_{i}-h\\lambda,0\\}. italic_T start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ( italic_Œ∏ ) start_POSTSUBSCRIPT italic_i end_", "snippet": "T h ‚Äã ( Œ∏ ) i ‚âê max ‚Å° { Œ∏ i ‚àí h ‚Äã Œª , 0 } . T_{h}(\\theta)_{i}\\doteq\\max\\{\\theta_{i}-h\\lambda,0\\}. italic_T start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT ( italic_Œ∏ ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ‚âê roman_max { italic_Œ∏ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_h italic_Œª , 0 } . (A.1.43)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E44", "title": "Œ∏ k + 1 = Œ∏ k ‚àí h ‚Äã ‚àá ‚Ñí k ‚Äã ( Œ∏ k ) . \\theta_{k+1}=\\theta_{k}-h\\nabla\\mathcal{L}_{k}(\\theta_{k}). italic_Œ∏ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = italic_Œ∏ start_POSTSUBSCRIPT italic_k en", "snippet": "Œ∏ k + 1 = Œ∏ k ‚àí h ‚Äã ‚àá ‚Ñí k ‚Äã ( Œ∏ k ) . \\theta_{k+1}=\\theta_{k}-h\\nabla\\mathcal{L}_{k}(\\theta_{k}). italic_Œ∏ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - italic_h ‚àá caligraphic_L start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) . (A.1.44)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E45", "title": "‚Ñí œâ ‚Äã ( Œ∏ ) ‚âê 1 2 ‚Äã ‚Äñ Œ∏ ‚àí Œæ œâ ‚Äñ 2 2 . \\mathcal{L}_{\\omega}(\\theta)\\doteq\\frac{1}{2}\\|\\theta-\\xi_{\\omega}\\|_{2}^{2}. caligraphic_L start_POSTSUBSCRIPT italic_œâ end_POSTSUBSCRIPT ( italic_Œ∏ ) ‚âê divide s", "snippet": "‚Ñí œâ ‚Äã ( Œ∏ ) ‚âê 1 2 ‚Äã ‚Äñ Œ∏ ‚àí Œæ œâ ‚Äñ 2 2 . \\mathcal{L}_{\\omega}(\\theta)\\doteq\\frac{1}{2}\\|\\theta-\\xi_{\\omega}\\|_{2}^{2}. caligraphic_L start_POSTSUBSCRIPT italic_œâ end_POSTSUBSCRIPT ( italic_Œ∏ ) ‚âê divide start_ARG 1 end_ARG start_ARG 2 end_ARG ‚à• italic_Œ∏ - italic_Œæ start_POSTSUBSCRIPT italic_œâ end_POSTSUBSCRIPT ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . (A.1.45)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E48", "title": "Œ∏ k + 1 = Œ∏ k + h ‚Äã ùíó k , \\theta_{k+1}=\\theta_{k}+h\\bm{v}_{k}, italic_Œ∏ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + italic_h bold_ita", "snippet": "Œ∏ k + 1 = Œ∏ k + h ‚Äã ùíó k , \\theta_{k+1}=\\theta_{k}+h\\bm{v}_{k}, italic_Œ∏ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + italic_h bold_italic_v start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , (A.1.48)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E49", "title": "ùíó k = ‚àí ‚àá ‚Ñí ‚Äã ( Œ∏ k ) ‚Äñ ‚àá ‚Ñí ‚Äã ( Œ∏ k ) ‚Äñ 2 ‚àà arg ‚Äã min ùíó ‚àà ‚Ñù n ‚Äñ ùíó ‚Äñ 2 = 1 ‚Å° ‚ü® ‚àá ‚Ñí ‚Äã ( Œ∏ k ) , ùíó ‚ü© . \\bm{v}_{k}=-\\frac{\\nabla\\mathcal{L}(\\theta_{k})}{\\|\\nabla\\mathcal{L}(\\theta_{k})\\|_{2}}\\in\\operatorn", "snippet": "ùíó k = ‚àí ‚àá ‚Ñí ‚Äã ( Œ∏ k ) ‚Äñ ‚àá ‚Ñí ‚Äã ( Œ∏ k ) ‚Äñ 2 ‚àà arg ‚Äã min ùíó ‚àà ‚Ñù n ‚Äñ ùíó ‚Äñ 2 = 1 ‚Å° ‚ü® ‚àá ‚Ñí ‚Äã ( Œ∏ k ) , ùíó ‚ü© . \\bm{v}_{k}=-\\frac{\\nabla\\mathcal{L}(\\theta_{k})}{\\|\\nabla\\mathcal{L}(\\theta_{k})\\|_{2}}\\in\\operatorname*{arg\\ min}_{\\begin{subarray}{c}\\bm{v}\\in\\mathbb{R}^{n}\\\\ \\|\\bm{v}\\|_{2}=1\\end{subarray}}\\langle\\nabla\\mathcal{L}(\\theta_{k}),\\bm{v}\\rangle. bold_italic_v start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = - divide start_ARG ‚àá caligraphic_L ( italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) end_ARG start_ARG ‚à• ‚àá caligraphic_L ( italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ‚à• "}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E50", "title": "ùíó k ‚àà arg ‚Äã min ùíó ‚àà ‚Ñù n ‚Äñ ùíó ‚Äñ = 1 ‚Å° ‚ü® ‚àá ‚Ñí ‚Äã ( Œ∏ k ) , ùíó ‚ü© . \\bm{v}_{k}\\in\\operatorname*{arg\\ min}_{\\begin{subarray}{c}\\bm{v}\\in\\mathbb{R}^{n}\\\\ \\|\\bm{v}\\|=1\\end{subarray}}\\langle\\nabla\\mathcal{L}(\\the", "snippet": "ùíó k ‚àà arg ‚Äã min ùíó ‚àà ‚Ñù n ‚Äñ ùíó ‚Äñ = 1 ‚Å° ‚ü® ‚àá ‚Ñí ‚Äã ( Œ∏ k ) , ùíó ‚ü© . \\bm{v}_{k}\\in\\operatorname*{arg\\ min}_{\\begin{subarray}{c}\\bm{v}\\in\\mathbb{R}^{n}\\\\ \\|\\bm{v}\\|=1\\end{subarray}}\\langle\\nabla\\mathcal{L}(\\theta_{k}),\\bm{v}\\rangle. bold_italic_v start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ‚àà start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT start_ARG start_ROW start_CELL bold_italic_v ‚àà blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT end_CELL end_ROW start_ROW start_CELL ‚à• bold_italic_v ‚à• = 1 end_CELL end_ROW end_ARG end_POSTSUBSCRIPT ‚ü® ‚àá caligraphic_L ( italic_Œ∏ sta"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E51", "title": "ùíó k = ‚àí sign ‚Å° ( ‚àá ‚Ñí ‚Äã ( Œ∏ k ) ) ‚àà arg ‚Äã min ùíó ‚àà ‚Ñù n ‚Äñ ùíó ‚Äñ ‚àû = 1 ‚Å° ‚ü® ‚àá ‚Ñí ‚Äã ( Œ∏ k ) , ùíó ‚ü© , \\bm{v}_{k}=-\\operatorname{sign}(\\nabla\\mathcal{L}(\\theta_{k}))\\in\\operatorname*{arg\\ min}_{\\begin{subarray}{c", "snippet": "ùíó k = ‚àí sign ‚Å° ( ‚àá ‚Ñí ‚Äã ( Œ∏ k ) ) ‚àà arg ‚Äã min ùíó ‚àà ‚Ñù n ‚Äñ ùíó ‚Äñ ‚àû = 1 ‚Å° ‚ü® ‚àá ‚Ñí ‚Äã ( Œ∏ k ) , ùíó ‚ü© , \\bm{v}_{k}=-\\operatorname{sign}(\\nabla\\mathcal{L}(\\theta_{k}))\\in\\operatorname*{arg\\ min}_{\\begin{subarray}{c}\\bm{v}\\in\\mathbb{R}^{n}\\\\ \\|\\bm{v}\\|_{\\infty}=1\\end{subarray}}\\langle\\nabla\\mathcal{L}(\\theta_{k}),\\bm{v}\\rangle, bold_italic_v start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = - roman_sign ( ‚àá caligraphic_L ( italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ) ‚àà start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT start_ARG start_ROW start_CELL bold_italic_v ‚àà blackboard_R"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E52", "title": "Œ∏ k + 1 = Œ∏ k ‚àí h ‚Äã sign ‚Å° ( ‚àá ‚Ñí ‚Äã ( Œ∏ k ) ) . \\theta_{k+1}=\\theta_{k}-h\\operatorname{sign}(\\nabla\\mathcal{L}(\\theta_{k})). italic_Œ∏ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = italic_Œ∏ start", "snippet": "Œ∏ k + 1 = Œ∏ k ‚àí h ‚Äã sign ‚Å° ( ‚àá ‚Ñí ‚Äã ( Œ∏ k ) ) . \\theta_{k+1}=\\theta_{k}-h\\operatorname{sign}(\\nabla\\mathcal{L}(\\theta_{k})). italic_Œ∏ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT - italic_h roman_sign ( ‚àá caligraphic_L ( italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ) . (A.1.52)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E53", "title": "sign ‚Å° ( x ) = x | x | = x x 2 . \\operatorname{sign}(x)=\\frac{x}{\\lvert x\\rvert}=\\frac{x}{\\sqrt{x^{2}}}. roman_sign ( italic_x ) = divide start_ARG italic_x end_ARG start_ARG | italic_x | end_ARG = di", "snippet": "sign ‚Å° ( x ) = x | x | = x x 2 . \\operatorname{sign}(x)=\\frac{x}{\\lvert x\\rvert}=\\frac{x}{\\sqrt{x^{2}}}. roman_sign ( italic_x ) = divide start_ARG italic_x end_ARG start_ARG | italic_x | end_ARG = divide start_ARG italic_x end_ARG start_ARG square-root start_ARG italic_x start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG end_ARG . (A.1.53)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E54", "title": "sign ‚Å° ( ùíô ) = ùíô ‚äò [ ùíô ‚äô 2 ] ‚äô ( 1 / 2 ) . \\operatorname{sign}(\\bm{x})=\\bm{x}\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\oslash$}}{\\scalebox{0.8}{$\\te", "snippet": "sign ‚Å° ( ùíô ) = ùíô ‚äò [ ùíô ‚äô 2 ] ‚äô ( 1 / 2 ) . \\operatorname{sign}(\\bm{x})=\\bm{x}\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\oslash$}}{\\scalebox{0.8}{$\\textstyle\\oslash$}}{\\scalebox{0.8}{$\\scriptstyle\\oslash$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\oslash$}}$}}{\\raisebox{1.3pt}{$\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\oslash$}}{\\scalebox{0.8}{$\\textstyle\\oslash$}}{\\scalebox{0.8}{$\\scriptstyle\\oslash$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\oslash$}}$}}{\\raisebox{0.75pt}{$\\scriptstyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\oslash$}}{\\scalebox{0.8}{$\\t"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E55", "title": "Œ∏ k + 1 = Œ∏ k ‚àí h ‚Äã ( [ ‚àá ‚Ñí ‚Äã ( Œ∏ k ) ] ‚äò [ ‚àá ‚Ñí ‚Äã ( Œ∏ k ) ‚äô 2 ] ‚äô 1 2 ) . \\theta_{k+1}=\\theta_{k}-h([\\nabla\\mathcal{L}(\\theta_{k})]\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scal", "snippet": "Œ∏ k + 1 = Œ∏ k ‚àí h ‚Äã ( [ ‚àá ‚Ñí ‚Äã ( Œ∏ k ) ] ‚äò [ ‚àá ‚Ñí ‚Äã ( Œ∏ k ) ‚äô 2 ] ‚äô 1 2 ) . \\theta_{k+1}=\\theta_{k}-h([\\nabla\\mathcal{L}(\\theta_{k})]\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\oslash$}}{\\scalebox{0.8}{$\\textstyle\\oslash$}}{\\scalebox{0.8}{$\\scriptstyle\\oslash$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\oslash$}}$}}{\\raisebox{1.3pt}{$\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\oslash$}}{\\scalebox{0.8}{$\\textstyle\\oslash$}}{\\scalebox{0.8}{$\\scriptstyle\\oslash$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\oslash$}}$}}{\\raisebox{0.75pt}{$\\scriptstyle\\mathchoice{\\sca"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.E59", "title": "Œ∏ k + 1 = Œ∏ k ‚àí Œ∑ k ‚äô ùíà k where Œ∑ k = h ‚Äã ùíî k ‚äô ( ‚àí 1 2 ) \\theta_{k+1}=\\theta_{k}-\\eta_{k}\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebo", "snippet": "Œ∏ k + 1 = Œ∏ k ‚àí Œ∑ k ‚äô ùíà k where Œ∑ k = h ‚Äã ùíî k ‚äô ( ‚àí 1 2 ) \\theta_{k+1}=\\theta_{k}-\\eta_{k}\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\scalebox{0.8}{$\\scriptstyle\\odot$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\odot$}}$}}{\\raisebox{1.3pt}{$\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\scalebox{0.8}{$\\scriptstyle\\odot$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\odot$}}$}}{\\raisebox{0.75pt}{$\\scriptstyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textst"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E1", "title": "‚Ñí ‚Äã ( Œ∏ ) ‚àí ‚Ñí ‚Äã ( Œ∏ 0 ) = ‚Ñí ‚Ä≤ ‚Äã ( Œ∏ 0 ) ‚ãÖ ( Œ∏ ‚àí Œ∏ 0 ) + o ‚Äã ( | Œ∏ ‚àí Œ∏ 0 | ) . \\mathcal{L}(\\theta)-\\mathcal{L}(\\theta_{0})=\\mathcal{L}^{\\prime}(\\theta_{0})\\cdot(\\theta-\\theta_{0})+o(\\lvert\\theta-\\theta", "snippet": "‚Ñí ‚Äã ( Œ∏ ) ‚àí ‚Ñí ‚Äã ( Œ∏ 0 ) = ‚Ñí ‚Ä≤ ‚Äã ( Œ∏ 0 ) ‚ãÖ ( Œ∏ ‚àí Œ∏ 0 ) + o ‚Äã ( | Œ∏ ‚àí Œ∏ 0 | ) . \\mathcal{L}(\\theta)-\\mathcal{L}(\\theta_{0})=\\mathcal{L}^{\\prime}(\\theta_{0})\\cdot(\\theta-\\theta_{0})+o(\\lvert\\theta-\\theta_{0}\\rvert). caligraphic_L ( italic_Œ∏ ) - caligraphic_L ( italic_Œ∏ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) = caligraphic_L start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_Œ∏ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) ‚ãÖ ( italic_Œ∏ - italic_Œ∏ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) + italic_o ( | italic_Œ∏ - italic_Œ∏ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT | ) . (A.2.1)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E2", "title": "Œ¥ ‚Äã ‚Ñí = ‚Ñí ‚Ä≤ ‚Äã ( Œ∏ 0 ) ‚ãÖ Œ¥ ‚Äã Œ∏ + o ‚Äã ( | Œ¥ ‚Äã Œ∏ | ) . \\delta\\mathcal{L}=\\mathcal{L}^{\\prime}(\\theta_{0})\\cdot\\delta\\theta+o(\\lvert\\delta\\theta\\rvert). italic_Œ¥ caligraphic_L = caligraphic_L start_POSTSU", "snippet": "Œ¥ ‚Äã ‚Ñí = ‚Ñí ‚Ä≤ ‚Äã ( Œ∏ 0 ) ‚ãÖ Œ¥ ‚Äã Œ∏ + o ‚Äã ( | Œ¥ ‚Äã Œ∏ | ) . \\delta\\mathcal{L}=\\mathcal{L}^{\\prime}(\\theta_{0})\\cdot\\delta\\theta+o(\\lvert\\delta\\theta\\rvert). italic_Œ¥ caligraphic_L = caligraphic_L start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_Œ∏ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) ‚ãÖ italic_Œ¥ italic_Œ∏ + italic_o ( | italic_Œ¥ italic_Œ∏ | ) . (A.2.2)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E3", "title": "d ‚Äã ‚Ñí = ‚Ñí ‚Ä≤ ‚Äã ( Œ∏ ) ‚ãÖ d ‚Äã Œ∏ , \\mathrm{d}\\mathcal{L}=\\mathcal{L}^{\\prime}(\\theta)\\cdot\\mathrm{d}\\theta, roman_d caligraphic_L = caligraphic_L start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_Œ∏ ) ‚ãÖ ", "snippet": "d ‚Äã ‚Ñí = ‚Ñí ‚Ä≤ ‚Äã ( Œ∏ ) ‚ãÖ d ‚Äã Œ∏ , \\mathrm{d}\\mathcal{L}=\\mathcal{L}^{\\prime}(\\theta)\\cdot\\mathrm{d}\\theta, roman_d caligraphic_L = caligraphic_L start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_Œ∏ ) ‚ãÖ roman_d italic_Œ∏ , (A.2.3)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E4", "title": "d ‚Äã ‚Ñí = ‚Ñí ‚Ä≤ ‚Äã ( Œ∏ ) ‚ãÖ d ‚Äã Œ∏ \\mathrm{d}\\mathcal{L}=\\mathcal{L}^{\\prime}(\\theta)\\cdot\\mathrm{d}\\theta roman_d caligraphic_L = caligraphic_L start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_Œ∏ ) ‚ãÖ rom", "snippet": "d ‚Äã ‚Ñí = ‚Ñí ‚Ä≤ ‚Äã ( Œ∏ ) ‚ãÖ d ‚Äã Œ∏ \\mathrm{d}\\mathcal{L}=\\mathcal{L}^{\\prime}(\\theta)\\cdot\\mathrm{d}\\theta roman_d caligraphic_L = caligraphic_L start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_Œ∏ ) ‚ãÖ roman_d italic_Œ∏ (A.2.4)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E5", "title": "d ‚Äã ‚Ñí = ‚Ñí ‚Ä≤ ‚Äã ( Œ∏ ) ‚Äã [ d ‚Äã Œ∏ ] . \\mathrm{d}\\mathcal{L}=\\mathcal{L}^{\\prime}(\\theta)[\\mathrm{d}\\theta]. roman_d caligraphic_L = caligraphic_L start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_Œ∏ ) [", "snippet": "d ‚Äã ‚Ñí = ‚Ñí ‚Ä≤ ‚Äã ( Œ∏ ) ‚Äã [ d ‚Äã Œ∏ ] . \\mathrm{d}\\mathcal{L}=\\mathcal{L}^{\\prime}(\\theta)[\\mathrm{d}\\theta]. roman_d caligraphic_L = caligraphic_L start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_Œ∏ ) [ roman_d italic_Œ∏ ] . (A.2.5)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E6", "title": "d ‚Äã ‚Ñí = f ‚Ä≤ ‚Äã ( g ‚Äã ( Œ∏ ) ) ‚Äã g ‚Ä≤ ‚Äã ( Œ∏ ) ‚Äã [ d ‚Äã Œ∏ ] , \\mathrm{d}\\mathcal{L}=f^{\\prime}(g(\\theta))g^{\\prime}(\\theta)[\\mathrm{d}\\theta], roman_d caligraphic_L = italic_f start_POSTSUPERSCRIPT ‚Ä≤ end_PO", "snippet": "d ‚Äã ‚Ñí = f ‚Ä≤ ‚Äã ( g ‚Äã ( Œ∏ ) ) ‚Äã g ‚Ä≤ ‚Äã ( Œ∏ ) ‚Äã [ d ‚Äã Œ∏ ] , \\mathrm{d}\\mathcal{L}=f^{\\prime}(g(\\theta))g^{\\prime}(\\theta)[\\mathrm{d}\\theta], roman_d caligraphic_L = italic_f start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_g ( italic_Œ∏ ) ) italic_g start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_Œ∏ ) [ roman_d italic_Œ∏ ] , (A.2.6)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E7", "title": "‚Ñí ‚Ä≤ ‚Äã ( Œ∏ ) = f ‚Ä≤ ‚Äã ( g ‚Äã ( Œ∏ ) ) ‚Äã g ‚Ä≤ ‚Äã ( Œ∏ ) \\mathcal{L}^{\\prime}(\\theta)=f^{\\prime}(g(\\theta))g^{\\prime}(\\theta) caligraphic_L start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_Œ∏ ) = italic_f s", "snippet": "‚Ñí ‚Ä≤ ‚Äã ( Œ∏ ) = f ‚Ä≤ ‚Äã ( g ‚Äã ( Œ∏ ) ) ‚Äã g ‚Ä≤ ‚Äã ( Œ∏ ) \\mathcal{L}^{\\prime}(\\theta)=f^{\\prime}(g(\\theta))g^{\\prime}(\\theta) caligraphic_L start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_Œ∏ ) = italic_f start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_g ( italic_Œ∏ ) ) italic_g start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_Œ∏ ) (A.2.7)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E8", "title": "d ‚Äã f = f ‚Äã ( ùëø + d ‚Äã ùëø ) ‚àí f ‚Äã ( ùëø ) = [ ùëæ ‚Äã ( ùëø + d ‚Äã ùëø ) + ùíÉ ‚Äã ùüè ‚ä§ ] ‚àí [ ùëæ ‚Äã ùëø + ùíÉ ‚Äã ùüè ‚ä§ ] = ùëæ ‚Äã d ‚Äã ùëø . \\mathrm{d}f=f(\\bm{X}+\\mathrm{d}\\bm{X})-f(\\bm{X})=[\\bm{W}(\\bm{X}+\\mathrm{d}\\bm{X})+\\bm{b}\\bm{", "snippet": "d ‚Äã f = f ‚Äã ( ùëø + d ‚Äã ùëø ) ‚àí f ‚Äã ( ùëø ) = [ ùëæ ‚Äã ( ùëø + d ‚Äã ùëø ) + ùíÉ ‚Äã ùüè ‚ä§ ] ‚àí [ ùëæ ‚Äã ùëø + ùíÉ ‚Äã ùüè ‚ä§ ] = ùëæ ‚Äã d ‚Äã ùëø . \\mathrm{d}f=f(\\bm{X}+\\mathrm{d}\\bm{X})-f(\\bm{X})=[\\bm{W}(\\bm{X}+\\mathrm{d}\\bm{X})+\\bm{b}\\bm{1}^{\\top}]-[\\bm{W}\\bm{X}+\\bm{b}\\bm{1}^{\\top}]=\\bm{W}\\mathrm{d}\\bm{X}. roman_d italic_f = italic_f ( bold_italic_X + roman_d bold_italic_X ) - italic_f ( bold_italic_X ) = [ bold_italic_W ( bold_italic_X + roman_d bold_italic_X ) + bold_italic_b bold_1 start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ] - [ bold_italic_W bold_italic_X + bold_italic_b bold_1 start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ] ="}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E9", "title": "f ‚Ä≤ ‚Äã ( ùëø ) ‚Äã [ d ‚Äã ùëø ] = ùëæ ‚Äã d ‚Äã ùëø ‚üπ f ‚Ä≤ ‚Äã ( ùëø ) = ùëæ . f^{\\prime}(\\bm{X})[\\mathrm{d}\\bm{X}]=\\bm{W}\\mathrm{d}\\bm{X}\\implies f^{\\prime}(\\bm{X})=\\bm{W}. italic_f start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCR", "snippet": "f ‚Ä≤ ‚Äã ( ùëø ) ‚Äã [ d ‚Äã ùëø ] = ùëæ ‚Äã d ‚Äã ùëø ‚üπ f ‚Ä≤ ‚Äã ( ùëø ) = ùëæ . f^{\\prime}(\\bm{X})[\\mathrm{d}\\bm{X}]=\\bm{W}\\mathrm{d}\\bm{X}\\implies f^{\\prime}(\\bm{X})=\\bm{W}. italic_f start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( bold_italic_X ) [ roman_d bold_italic_X ] = bold_italic_W roman_d bold_italic_X ‚üπ italic_f start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( bold_italic_X ) = bold_italic_W . (A.2.9)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E12", "title": "d ‚Äã f = p ‚Ä≤ ‚Äã ( v ‚Äã ( x ) ) ‚Äã v ‚Ä≤ ‚Äã ( x ) ‚Äã [ d ‚Äã x ] . \\mathrm{d}f=p^{\\prime}(v(x))v^{\\prime}(x)[\\mathrm{d}x]. roman_d italic_f = italic_p start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_v ( ita", "snippet": "d ‚Äã f = p ‚Ä≤ ‚Äã ( v ‚Äã ( x ) ) ‚Äã v ‚Ä≤ ‚Äã ( x ) ‚Äã [ d ‚Äã x ] . \\mathrm{d}f=p^{\\prime}(v(x))v^{\\prime}(x)[\\mathrm{d}x]. roman_d italic_f = italic_p start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_v ( italic_x ) ) italic_v start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_x ) [ roman_d italic_x ] . (A.2.12)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E13", "title": "d ‚Äã v = v ‚Ä≤ ‚Äã ( x ) ‚Äã [ d ‚Äã x ] = v ‚Äã ( x + d ‚Äã x ) ‚àí v ‚Äã ( x ) = [ g ‚Äã ( x + d ‚Äã x ) ‚àí g ‚Äã ( x ) h ‚Äã ( x + d ‚Äã x ) ‚àí h ‚Äã ( x ) ] = [ g ‚Ä≤ ‚Äã ( x ) ‚Äã [ d ‚Äã x ] h ‚Ä≤ ‚Äã ( x ) ‚Äã [ d ‚Äã x ] ] . \\mathrm{d}v=v^", "snippet": "d ‚Äã v = v ‚Ä≤ ‚Äã ( x ) ‚Äã [ d ‚Äã x ] = v ‚Äã ( x + d ‚Äã x ) ‚àí v ‚Äã ( x ) = [ g ‚Äã ( x + d ‚Äã x ) ‚àí g ‚Äã ( x ) h ‚Äã ( x + d ‚Äã x ) ‚àí h ‚Äã ( x ) ] = [ g ‚Ä≤ ‚Äã ( x ) ‚Äã [ d ‚Äã x ] h ‚Ä≤ ‚Äã ( x ) ‚Äã [ d ‚Äã x ] ] . \\mathrm{d}v=v^{\\prime}(x)[\\mathrm{d}x]=v(x+\\mathrm{d}x)-v(x)=\\begin{bmatrix}g(x+\\mathrm{d}x)-g(x)\\\\ h(x+\\mathrm{d}x)-h(x)\\end{bmatrix}=\\begin{bmatrix}g^{\\prime}(x)[\\mathrm{d}x]\\\\ h^{\\prime}(x)[\\mathrm{d}x]\\end{bmatrix}. roman_d italic_v = italic_v start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_x ) [ roman_d italic_x ] = italic_v ( italic_x + roman_d italic_x ) - italic_v ( italic_x ) = [ start_ARG start_R"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E16", "title": "p ‚Ä≤ ‚Äã ( a , b ) ‚Äã [ d ‚Äã a , d ‚Äã b ] = ( d ‚Äã a ) ‚Äã b + ( d ‚Äã b ) ‚Äã a . p^{\\prime}(a,b)[\\mathrm{d}a,\\mathrm{d}b]=(\\mathrm{d}a)b+(\\mathrm{d}b)a. italic_p start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( ita", "snippet": "p ‚Ä≤ ‚Äã ( a , b ) ‚Äã [ d ‚Äã a , d ‚Äã b ] = ( d ‚Äã a ) ‚Äã b + ( d ‚Äã b ) ‚Äã a . p^{\\prime}(a,b)[\\mathrm{d}a,\\mathrm{d}b]=(\\mathrm{d}a)b+(\\mathrm{d}b)a. italic_p start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_a , italic_b ) [ roman_d italic_a , roman_d italic_b ] = ( roman_d italic_a ) italic_b + ( roman_d italic_b ) italic_a . (A.2.16)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E19", "title": "f ‚Ä≤ ‚Äã ( x ) ‚Äã [ d ‚Äã x ] = ( g ‚Ä≤ ‚Äã ( x ) ‚Äã [ d ‚Äã x ] ) ‚Äã h ‚Äã ( x ) + g ‚Äã ( x ) ‚Äã ( h ‚Ä≤ ‚Äã ( x ) ‚Äã [ d ‚Äã x ] ) . f^{\\prime}(x)[\\mathrm{d}x]=(g^{\\prime}(x)[\\mathrm{d}x])h(x)+g(x)(h^{\\prime}(x)[\\mathrm{d}x", "snippet": "f ‚Ä≤ ‚Äã ( x ) ‚Äã [ d ‚Äã x ] = ( g ‚Ä≤ ‚Äã ( x ) ‚Äã [ d ‚Äã x ] ) ‚Äã h ‚Äã ( x ) + g ‚Äã ( x ) ‚Äã ( h ‚Ä≤ ‚Äã ( x ) ‚Äã [ d ‚Äã x ] ) . f^{\\prime}(x)[\\mathrm{d}x]=(g^{\\prime}(x)[\\mathrm{d}x])h(x)+g(x)(h^{\\prime}(x)[\\mathrm{d}x]). italic_f start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_x ) [ roman_d italic_x ] = ( italic_g start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_x ) [ roman_d italic_x ] ) italic_h ( italic_x ) + italic_g ( italic_x ) ( italic_h start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_x ) [ roman_d italic_x ] ) . (A.2.19)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E20", "title": "f ‚Ä≤ ‚Äã ( x ) ‚Äã [ d ‚Äã x ] = ( g ‚Ä≤ ‚Äã ( x ) ‚Äã h ‚Äã ( x ) + g ‚Äã ( x ) ‚Äã h ‚Ä≤ ‚Äã ( x ) ) ‚Äã [ d ‚Äã x ] ‚üπ f ‚Ä≤ ‚Äã ( x ) = g ‚Ä≤ ‚Äã ( x ) ‚Äã h ‚Äã ( x ) + g ‚Äã ( x ) ‚Äã h ‚Ä≤ ‚Äã ( x ) f^{\\prime}(x)[\\mathrm{d}x]=(g^{\\prime}(x)h", "snippet": "f ‚Ä≤ ‚Äã ( x ) ‚Äã [ d ‚Äã x ] = ( g ‚Ä≤ ‚Äã ( x ) ‚Äã h ‚Äã ( x ) + g ‚Äã ( x ) ‚Äã h ‚Ä≤ ‚Äã ( x ) ) ‚Äã [ d ‚Äã x ] ‚üπ f ‚Ä≤ ‚Äã ( x ) = g ‚Ä≤ ‚Äã ( x ) ‚Äã h ‚Äã ( x ) + g ‚Äã ( x ) ‚Äã h ‚Ä≤ ‚Äã ( x ) f^{\\prime}(x)[\\mathrm{d}x]=(g^{\\prime}(x)h(x)+g(x)h^{\\prime}(x))[\\mathrm{d}x]\\implies f^{\\prime}(x)=g^{\\prime}(x)h(x)+g(x)h^{\\prime}(x) italic_f start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_x ) [ roman_d italic_x ] = ( italic_g start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_x ) italic_h ( italic_x ) + italic_g ( italic_x ) italic_h start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_x ) ) [ roman_d italic_x ] ‚üπ italic_f sta"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E23", "title": "f ‚Äã ( ùë® ) i ‚Äã j = ‚àë t = 1 k A i ‚Äã j ‚Äã t . f(\\bm{A})_{ij}=\\sum_{t=1}^{k}A_{ijt}. italic_f ( bold_italic_A ) start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = ‚àë start_POSTSUBSCRIPT italic_t = 1 ", "snippet": "f ‚Äã ( ùë® ) i ‚Äã j = ‚àë t = 1 k A i ‚Äã j ‚Äã t . f(\\bm{A})_{ij}=\\sum_{t=1}^{k}A_{ijt}. italic_f ( bold_italic_A ) start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = ‚àë start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_A start_POSTSUBSCRIPT italic_i italic_j italic_t end_POSTSUBSCRIPT . (A.2.23)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E24", "title": "d ‚Äã f i ‚Äã j = [ f ‚Äã ( ùë® + d ‚Äã ùë® ) ‚àí f ‚Äã ( ùë® ) ] i ‚Äã j = ‚àë t = 1 k d ‚Äã ùë® i ‚Äã j ‚Äã t = ùüè k ‚ä§ ‚Äã ( d ‚Äã ùë® ) i ‚Äã j . \\mathrm{d}f_{ij}=[f(\\bm{A}+\\mathrm{d}\\bm{A})-f(\\bm{A})]_{ij}=\\sum_{t=1}^{k}\\mathrm{d}\\bm{A", "snippet": "d ‚Äã f i ‚Äã j = [ f ‚Äã ( ùë® + d ‚Äã ùë® ) ‚àí f ‚Äã ( ùë® ) ] i ‚Äã j = ‚àë t = 1 k d ‚Äã ùë® i ‚Äã j ‚Äã t = ùüè k ‚ä§ ‚Äã ( d ‚Äã ùë® ) i ‚Äã j . \\mathrm{d}f_{ij}=[f(\\bm{A}+\\mathrm{d}\\bm{A})-f(\\bm{A})]_{ij}=\\sum_{t=1}^{k}\\mathrm{d}\\bm{A}_{ijt}=\\bm{1}_{k}^{\\top}(\\mathrm{d}\\bm{A})_{ij}. roman_d italic_f start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = [ italic_f ( bold_italic_A + roman_d bold_italic_A ) - italic_f ( bold_italic_A ) ] start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = ‚àë start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT roman_d bold_italic_A start_P"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E25", "title": "( f ‚Ä≤ ‚Äã ( ùë® ) ‚Äã [ d ‚Äã ùë® ] ) i ‚Äã j = ùüè k ‚ä§ ‚Äã ( d ‚Äã ùë® ) i ‚Äã j , (f^{\\prime}(\\bm{A})[\\mathrm{d}\\bm{A}])_{ij}=\\bm{1}_{k}^{\\top}(\\mathrm{d}\\bm{A})_{ij}, ( italic_f start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRI", "snippet": "( f ‚Ä≤ ‚Äã ( ùë® ) ‚Äã [ d ‚Äã ùë® ] ) i ‚Äã j = ùüè k ‚ä§ ‚Äã ( d ‚Äã ùë® ) i ‚Äã j , (f^{\\prime}(\\bm{A})[\\mathrm{d}\\bm{A}])_{ij}=\\bm{1}_{k}^{\\top}(\\mathrm{d}\\bm{A})_{ij}, ( italic_f start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( bold_italic_A ) [ roman_d bold_italic_A ] ) start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = bold_1 start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ( roman_d bold_italic_A ) start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT , (A.2.25)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E26", "title": "d ‚Äã ‚Ñí = ‚Ñí ‚Ä≤ ‚Äã ( Œ∏ ) ‚Äã [ d ‚Äã Œ∏ ] = ‚ü® ‚àá ‚Ñí ‚Äã ( Œ∏ ) , d ‚Äã Œ∏ ‚ü© , \\mathrm{d}\\mathcal{L}=\\mathcal{L}^{\\prime}(\\theta)[\\mathrm{d}\\theta]=\\langle\\nabla\\mathcal{L}(\\theta),\\mathrm{d}\\theta\\rangle, roman_d calig", "snippet": "d ‚Äã ‚Ñí = ‚Ñí ‚Ä≤ ‚Äã ( Œ∏ ) ‚Äã [ d ‚Äã Œ∏ ] = ‚ü® ‚àá ‚Ñí ‚Äã ( Œ∏ ) , d ‚Äã Œ∏ ‚ü© , \\mathrm{d}\\mathcal{L}=\\mathcal{L}^{\\prime}(\\theta)[\\mathrm{d}\\theta]=\\langle\\nabla\\mathcal{L}(\\theta),\\mathrm{d}\\theta\\rangle, roman_d caligraphic_L = caligraphic_L start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_Œ∏ ) [ roman_d italic_Œ∏ ] = ‚ü® ‚àá caligraphic_L ( italic_Œ∏ ) , roman_d italic_Œ∏ ‚ü© , (A.2.26)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E27", "title": "‚Ñí ‚Ä≤ ‚Äã ( Œ∏ ) = a ‚Ä≤ ‚Äã ( b ‚Äã ( c ‚Äã ( Œ∏ ) ) ) ‚Äã b ‚Ä≤ ‚Äã ( c ‚Äã ( Œ∏ ) ) ‚Äã c ‚Ä≤ ‚Äã ( Œ∏ ) . \\mathcal{L}^{\\prime}(\\theta)=a^{\\prime}(b(c(\\theta)))b^{\\prime}(c(\\theta))c^{\\prime}(\\theta). caligraphic_L start_POSTSU", "snippet": "‚Ñí ‚Ä≤ ‚Äã ( Œ∏ ) = a ‚Ä≤ ‚Äã ( b ‚Äã ( c ‚Äã ( Œ∏ ) ) ) ‚Äã b ‚Ä≤ ‚Äã ( c ‚Äã ( Œ∏ ) ) ‚Äã c ‚Ä≤ ‚Äã ( Œ∏ ) . \\mathcal{L}^{\\prime}(\\theta)=a^{\\prime}(b(c(\\theta)))b^{\\prime}(c(\\theta))c^{\\prime}(\\theta). caligraphic_L start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_Œ∏ ) = italic_a start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_b ( italic_c ( italic_Œ∏ ) ) ) italic_b start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_c ( italic_Œ∏ ) ) italic_c start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_Œ∏ ) . (A.2.27)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E28", "title": "c ‚Ä≤ ‚Äã ( Œ∏ ) ‚üπ b ‚Ä≤ ‚Äã ( c ‚Äã ( Œ∏ ) ) ‚Äã c ‚Ä≤ ‚Äã ( Œ∏ ) ‚üπ a ‚Ä≤ ‚Äã ( b ‚Äã ( c ‚Äã ( Œ∏ ) ) ) ‚Äã b ‚Ä≤ ‚Äã ( c ‚Äã ( Œ∏ ) ) ‚Äã c ‚Ä≤ ‚Äã ( Œ∏ ) c^{\\prime}(\\theta)\\implies b^{\\prime}(c(\\theta))c^{\\prime}(\\theta)\\implies a^{\\prime}(", "snippet": "c ‚Ä≤ ‚Äã ( Œ∏ ) ‚üπ b ‚Ä≤ ‚Äã ( c ‚Äã ( Œ∏ ) ) ‚Äã c ‚Ä≤ ‚Äã ( Œ∏ ) ‚üπ a ‚Ä≤ ‚Äã ( b ‚Äã ( c ‚Äã ( Œ∏ ) ) ) ‚Äã b ‚Ä≤ ‚Äã ( c ‚Äã ( Œ∏ ) ) ‚Äã c ‚Ä≤ ‚Äã ( Œ∏ ) c^{\\prime}(\\theta)\\implies b^{\\prime}(c(\\theta))c^{\\prime}(\\theta)\\implies a^{\\prime}(b(c(\\theta)))b^{\\prime}(c(\\theta))c^{\\prime}(\\theta) italic_c start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_Œ∏ ) ‚üπ italic_b start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_c ( italic_Œ∏ ) ) italic_c start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_Œ∏ ) ‚üπ italic_a start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_b ( italic_c ( italic_Œ∏ ) ) ) italic_b start_POSTSUPERSCRIPT ‚Ä≤ end_POS"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E29", "title": "a ‚Ä≤ ‚Äã ( b ‚Äã ( c ‚Äã ( Œ∏ ) ) ) ‚üπ a ‚Ä≤ ‚Äã ( b ‚Äã ( c ‚Äã ( Œ∏ ) ) ) ‚Äã b ‚Ä≤ ‚Äã ( c ‚Äã ( Œ∏ ) ) ‚üπ a ‚Ä≤ ‚Äã ( b ‚Äã ( c ‚Äã ( Œ∏ ) ) ) ‚Äã b ‚Ä≤ ‚Äã ( c ‚Äã ( Œ∏ ) ) ‚Äã c ‚Ä≤ ‚Äã ( Œ∏ ) , a^{\\prime}(b(c(\\theta)))\\implies a^{\\prime}(b(c(\\the", "snippet": "a ‚Ä≤ ‚Äã ( b ‚Äã ( c ‚Äã ( Œ∏ ) ) ) ‚üπ a ‚Ä≤ ‚Äã ( b ‚Äã ( c ‚Äã ( Œ∏ ) ) ) ‚Äã b ‚Ä≤ ‚Äã ( c ‚Äã ( Œ∏ ) ) ‚üπ a ‚Ä≤ ‚Äã ( b ‚Äã ( c ‚Äã ( Œ∏ ) ) ) ‚Äã b ‚Ä≤ ‚Äã ( c ‚Äã ( Œ∏ ) ) ‚Äã c ‚Ä≤ ‚Äã ( Œ∏ ) , a^{\\prime}(b(c(\\theta)))\\implies a^{\\prime}(b(c(\\theta)))b^{\\prime}(c(\\theta))\\implies a^{\\prime}(b(c(\\theta)))b^{\\prime}(c(\\theta))c^{\\prime}(\\theta), italic_a start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_b ( italic_c ( italic_Œ∏ ) ) ) ‚üπ italic_a start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_b ( italic_c ( italic_Œ∏ ) ) ) italic_b start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_c ( italic_Œ∏ ) ) ‚üπ italic_a start_POSTSUPERSCRIPT ‚Ä≤ "}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E30", "title": "f ‚Ä≤ ‚Äã ( ùíô ) = a ‚Ä≤ ‚Äã ( b ‚Äã ( c ‚Äã ( ùíô ) ) ) ‚Äã b ‚Ä≤ ‚Äã ( c ‚Äã ( ùíô ) ) ‚Äã c ‚Ä≤ ‚Äã ( ùíô ) f^{\\prime}(\\bm{x})=a^{\\prime}(b(c(\\bm{x})))b^{\\prime}(c(\\bm{x}))c^{\\prime}(\\bm{x}) italic_f start_POSTSUPERSCRIPT ‚Ä≤ end_PO", "snippet": "f ‚Ä≤ ‚Äã ( ùíô ) = a ‚Ä≤ ‚Äã ( b ‚Äã ( c ‚Äã ( ùíô ) ) ) ‚Äã b ‚Ä≤ ‚Äã ( c ‚Äã ( ùíô ) ) ‚Äã c ‚Ä≤ ‚Äã ( ùíô ) f^{\\prime}(\\bm{x})=a^{\\prime}(b(c(\\bm{x})))b^{\\prime}(c(\\bm{x}))c^{\\prime}(\\bm{x}) italic_f start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( bold_italic_x ) = italic_a start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_b ( italic_c ( bold_italic_x ) ) ) italic_b start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_c ( bold_italic_x ) ) italic_c start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( bold_italic_x ) (A.2.30)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E40", "title": "‚Ñí ‚Äã ( Œ∏ ) ‚âê ùñ´ ‚Äã ( ùíö , ùíö ^ Œ∏ ‚Äã ( ùëø ) ) , \\mathcal{L}(\\theta)\\doteq\\mathsf{L}(\\bm{y},\\hat{\\bm{y}}_{\\theta}(\\bm{X})), caligraphic_L ( italic_Œ∏ ) ‚âê sansserif_L ( bold_italic_y , over^ start_ARG bold_itali", "snippet": "‚Ñí ‚Äã ( Œ∏ ) ‚âê ùñ´ ‚Äã ( ùíö , ùíö ^ Œ∏ ‚Äã ( ùëø ) ) , \\mathcal{L}(\\theta)\\doteq\\mathsf{L}(\\bm{y},\\hat{\\bm{y}}_{\\theta}(\\bm{X})), caligraphic_L ( italic_Œ∏ ) ‚âê sansserif_L ( bold_italic_y , over^ start_ARG bold_italic_y end_ARG start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( bold_italic_X ) ) , (A.2.40)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E82", "title": "f ‚Ñì ‚Äã ( ùíÅ , ùëæ ‚Ñì , ùíÉ ‚Ñì ) ‚âê ùëæ ‚Ñì ‚Äã ùíÅ + ùíÉ ‚Ñì ‚Äã ùüè ‚ä§ = [ ùëæ ‚Ñì ùíÉ ‚Ñì ] . f^{\\ell}(\\bm{Z},\\bm{W}^{\\ell},\\bm{b}^{\\ell})\\doteq\\bm{W}^{\\ell}\\bm{Z}+\\bm{b}^{\\ell}\\bm{1}^{\\top}=\\begin{bmatrix}\\bm{W}^{\\ell}&\\bm{b}^{\\ell", "snippet": "f ‚Ñì ‚Äã ( ùíÅ , ùëæ ‚Ñì , ùíÉ ‚Ñì ) ‚âê ùëæ ‚Ñì ‚Äã ùíÅ + ùíÉ ‚Ñì ‚Äã ùüè ‚ä§ = [ ùëæ ‚Ñì ùíÉ ‚Ñì ] . f^{\\ell}(\\bm{Z},\\bm{W}^{\\ell},\\bm{b}^{\\ell})\\doteq\\bm{W}^{\\ell}\\bm{Z}+\\bm{b}^{\\ell}\\bm{1}^{\\top}=\\begin{bmatrix}\\bm{W}^{\\ell}&\\bm{b}^{\\ell}\\end{bmatrix}. italic_f start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ( bold_italic_Z , bold_italic_W start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT , bold_italic_b start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ) ‚âê bold_italic_W start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT bold_italic_Z + bold_italic_b start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT bold_1 start_POSTSUPERSCRIPT "}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E85", "title": "d ‚Äã f ‚Ñì d ‚Äã ( ùëæ ‚Ñì , ùíÉ ‚Ñì ) ‚Äã [ d ‚Äã ùëæ ‚Ñì , d ‚Äã ùíÉ ‚Ñì ] = ( d ‚Äã ùëæ ‚Ñì ) ‚Äã ùíÅ + ( d ‚Äã ùíÉ ‚Ñì ) ‚Äã ùüè ‚ä§ , \\frac{\\mathrm{d}f^{\\ell}}{\\mathrm{d}(\\bm{W}^{\\ell},\\bm{b}^{\\ell})}[\\mathrm{d}\\bm{W}^{\\ell},\\mathrm{d}\\bm{b}^{\\", "snippet": "d ‚Äã f ‚Ñì d ‚Äã ( ùëæ ‚Ñì , ùíÉ ‚Ñì ) ‚Äã [ d ‚Äã ùëæ ‚Ñì , d ‚Äã ùíÉ ‚Ñì ] = ( d ‚Äã ùëæ ‚Ñì ) ‚Äã ùíÅ + ( d ‚Äã ùíÉ ‚Ñì ) ‚Äã ùüè ‚ä§ , \\frac{\\mathrm{d}f^{\\ell}}{\\mathrm{d}(\\bm{W}^{\\ell},\\bm{b}^{\\ell})}[\\mathrm{d}\\bm{W}^{\\ell},\\mathrm{d}\\bm{b}^{\\ell}]=(\\mathrm{d}\\bm{W}^{\\ell})\\bm{Z}+(\\mathrm{d}\\bm{b}^{\\ell})\\bm{1}^{\\top}, divide start_ARG roman_d italic_f start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT end_ARG start_ARG roman_d ( bold_italic_W start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT , bold_italic_b start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ) end_ARG [ roman_d bold_italic_W start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCR"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S2.E86", "title": "T ‚Äã [ ùë® , ùíñ ] = ùë® ‚Äã ùíÅ + ùíñ ‚Äã ùüè ‚ä§ . T[\\bm{A},\\bm{u}]=\\bm{A}\\bm{Z}+\\bm{u}\\bm{1}^{\\top}. italic_T [ bold_italic_A , bold_italic_u ] = bold_italic_A bold_italic_Z + bold_italic_u bold_1 start_POSTSUPERSCRI", "snippet": "T ‚Äã [ ùë® , ùíñ ] = ùë® ‚Äã ùíÅ + ùíñ ‚Äã ùüè ‚ä§ . T[\\bm{A},\\bm{u}]=\\bm{A}\\bm{Z}+\\bm{u}\\bm{1}^{\\top}. italic_T [ bold_italic_A , bold_italic_u ] = bold_italic_A bold_italic_Z + bold_italic_u bold_1 start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT . (A.2.86)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E1", "title": "‚Ñí ‚Äã ( Œ∏ , Œ∑ ) = ‚àí u ‚Äã ( Œ∏ ) + v ‚Äã ( Œ∑ ) . \\mathcal{L}(\\theta,\\eta)=-u(\\theta)+v(\\eta). caligraphic_L ( italic_Œ∏ , italic_Œ∑ ) = - italic_u ( italic_Œ∏ ) + italic_v ( italic_Œ∑ ) . (A.3.1)", "snippet": "‚Ñí ‚Äã ( Œ∏ , Œ∑ ) = ‚àí u ‚Äã ( Œ∏ ) + v ‚Äã ( Œ∑ ) . \\mathcal{L}(\\theta,\\eta)=-u(\\theta)+v(\\eta). caligraphic_L ( italic_Œ∏ , italic_Œ∑ ) = - italic_u ( italic_Œ∏ ) + italic_v ( italic_Œ∑ ) . (A.3.1)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E2", "title": "Œ∏ ‚ãÜ ‚àà arg ‚Äã min Œ∏ ‚àà Œò ‚Å° u ‚Äã ( Œ∏ ) , Œ∑ ‚ãÜ ‚àà arg ‚Äã min Œ∑ ‚àà H ‚Å° v ‚Äã ( Œ∑ ) . \\theta^{\\star}\\in\\operatorname*{arg\\ min}_{\\theta\\in\\Theta}u(\\theta),\\qquad\\eta^{\\star}\\in\\operatorname*{arg\\ min}_{\\eta\\in\\math", "snippet": "Œ∏ ‚ãÜ ‚àà arg ‚Äã min Œ∏ ‚àà Œò ‚Å° u ‚Äã ( Œ∏ ) , Œ∑ ‚ãÜ ‚àà arg ‚Äã min Œ∑ ‚àà H ‚Å° v ‚Äã ( Œ∑ ) . \\theta^{\\star}\\in\\operatorname*{arg\\ min}_{\\theta\\in\\Theta}u(\\theta),\\qquad\\eta^{\\star}\\in\\operatorname*{arg\\ min}_{\\eta\\in\\mathrm{H}}v(\\eta). italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ‚àà start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_Œ∏ ‚àà roman_Œò end_POSTSUBSCRIPT italic_u ( italic_Œ∏ ) , italic_Œ∑ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ‚àà start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_Œ∑ ‚àà roman_H end_POSTSUBSCRIPT italic_v ( italic_Œ∑ ) . (A.3.2)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E3", "title": "Œ∏ ‚ãÜ ‚àà arg ‚Äã max Œ∏ ‚àà Œò ‚Å° min Œ∑ ‚àà ùíÆ ‚Äã ( Œ∏ ) ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) , Œ∑ ‚ãÜ ‚àà arg ‚Äã min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ) . \\theta^{\\star}\\in\\operatorname*{arg\\ max}_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathcal{S}(\\theta)}\\mathc", "snippet": "Œ∏ ‚ãÜ ‚àà arg ‚Äã max Œ∏ ‚àà Œò ‚Å° min Œ∑ ‚àà ùíÆ ‚Äã ( Œ∏ ) ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) , Œ∑ ‚ãÜ ‚àà arg ‚Äã min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ) . \\theta^{\\star}\\in\\operatorname*{arg\\ max}_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathcal{S}(\\theta)}\\mathcal{L}(\\theta,\\eta),\\qquad\\eta^{\\star}\\in\\operatorname*{arg\\ min}_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta^{\\star},\\eta). italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ‚àà start_OPERATOR roman_arg roman_max end_OPERATOR start_POSTSUBSCRIPT italic_Œ∏ ‚àà roman_Œò end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT italic_Œ∑ ‚àà caligraphic_S ( italic_Œ∏ ) end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ , italic_Œ∑ "}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E4", "title": "Œ∏ ‚ãÜ ‚àà arg ‚Äã max Œ∏ ‚àà Œò ‚Å° min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) , Œ∑ ‚ãÜ ‚àà arg ‚Äã min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ) , \\theta^{\\star}\\in\\operatorname*{arg\\ max}_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta", "snippet": "Œ∏ ‚ãÜ ‚àà arg ‚Äã max Œ∏ ‚àà Œò ‚Å° min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) , Œ∑ ‚ãÜ ‚àà arg ‚Äã min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ) , \\theta^{\\star}\\in\\operatorname*{arg\\ max}_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta),\\qquad\\eta^{\\star}\\in\\operatorname*{arg\\ min}_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta^{\\star},\\eta), italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ‚àà start_OPERATOR roman_arg roman_max end_OPERATOR start_POSTSUBSCRIPT italic_Œ∏ ‚àà roman_Œò end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT italic_Œ∑ ‚àà roman_H end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ , italic_Œ∑ ) , italic_Œ∑ start_POSTSUPERSCRIPT ‚ãÜ"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E5", "title": "ùíÆ ‚Äã ( Œ∏ ) = arg ‚Äã min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) , \\mathcal{S}(\\theta)=\\operatorname*{arg\\ min}_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta), caligraphic_S ( italic_Œ∏ ) = start_OPERATOR roman_arg roman_min ", "snippet": "ùíÆ ‚Äã ( Œ∏ ) = arg ‚Äã min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) , \\mathcal{S}(\\theta)=\\operatorname*{arg\\ min}_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta), caligraphic_S ( italic_Œ∏ ) = start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_Œ∑ ‚àà roman_H end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ , italic_Œ∑ ) , (A.3.5)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E6", "title": "min Œ∑ ‚àà ùíÆ ‚Äã ( Œ∏ ) ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) = min Œ∑ ‚àà arg ‚Äã min Œ∑ ‚Ä≤ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ‚Ä≤ ) ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) = min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) . \\min_{\\eta\\in\\mathcal{S}(\\theta)}\\mathcal{L}(\\theta,\\eta)=\\min_{\\eta\\in\\op", "snippet": "min Œ∑ ‚àà ùíÆ ‚Äã ( Œ∏ ) ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) = min Œ∑ ‚àà arg ‚Äã min Œ∑ ‚Ä≤ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ‚Ä≤ ) ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) = min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) . \\min_{\\eta\\in\\mathcal{S}(\\theta)}\\mathcal{L}(\\theta,\\eta)=\\min_{\\eta\\in\\operatorname*{arg\\ min}_{\\eta^{\\prime}\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta^{\\prime})}\\mathcal{L}(\\theta,\\eta)=\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta). roman_min start_POSTSUBSCRIPT italic_Œ∑ ‚àà caligraphic_S ( italic_Œ∏ ) end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ , italic_Œ∑ ) = roman_min start_POSTSUBSCRIPT italic_Œ∑ ‚àà start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E7", "title": "max Œ∏ ‚àà Œò ‚Å° min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) = min Œ∑ ‚àà H ‚Å° max Œ∏ ‚àà Œò ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) \\max_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta)=\\min_{\\eta\\in\\mathrm{H}}\\max_{\\theta\\in\\Theta}\\math", "snippet": "max Œ∏ ‚àà Œò ‚Å° min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) = min Œ∑ ‚àà H ‚Å° max Œ∏ ‚àà Œò ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) \\max_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta)=\\min_{\\eta\\in\\mathrm{H}}\\max_{\\theta\\in\\Theta}\\mathcal{L}(\\theta,\\eta) roman_max start_POSTSUBSCRIPT italic_Œ∏ ‚àà roman_Œò end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT italic_Œ∑ ‚àà roman_H end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ , italic_Œ∑ ) = roman_min start_POSTSUBSCRIPT italic_Œ∑ ‚àà roman_H end_POSTSUBSCRIPT roman_max start_POSTSUBSCRIPT italic_Œ∏ ‚àà roman_Œò end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ , italic_Œ∑ ) (A.3.7)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E8", "title": "Œ∏ ‚ãÜ ‚àà arg ‚Äã max Œ∏ ‚àà Œò ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ‚ãÜ ) , Œ∑ ‚ãÜ ‚àà arg ‚Äã min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ) , \\theta^{\\star}\\in\\operatorname*{arg\\ max}_{\\theta\\in\\Theta}\\mathcal{L}(\\theta,\\eta^{\\star}),\\qquad\\eta^{\\star}\\in\\op", "snippet": "Œ∏ ‚ãÜ ‚àà arg ‚Äã max Œ∏ ‚àà Œò ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ‚ãÜ ) , Œ∑ ‚ãÜ ‚àà arg ‚Äã min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ) , \\theta^{\\star}\\in\\operatorname*{arg\\ max}_{\\theta\\in\\Theta}\\mathcal{L}(\\theta,\\eta^{\\star}),\\qquad\\eta^{\\star}\\in\\operatorname*{arg\\ min}_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta^{\\star},\\eta), italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ‚àà start_OPERATOR roman_arg roman_max end_OPERATOR start_POSTSUBSCRIPT italic_Œ∏ ‚àà roman_Œò end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ , italic_Œ∑ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ) , italic_Œ∑ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ‚àà start_OPERATOR roman_arg r"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.Ex1", "title": "max Œ∏ ‚àà Œò ‚Å° min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) . \\max_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta). roman_max start_POSTSUBSCRIPT italic_Œ∏ ‚àà roman_Œò end_POSTSUBSCRIPT roman_min start_POSTS", "snippet": "max Œ∏ ‚àà Œò ‚Å° min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) . \\max_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta). roman_max start_POSTSUBSCRIPT italic_Œ∏ ‚àà roman_Œò end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT italic_Œ∑ ‚àà roman_H end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ , italic_Œ∑ ) ."}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E9", "title": "max Œ∏ ‚àà Œò ‚Å° min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) = min Œ∑ ‚àà H ‚Å° max Œ∏ ‚àà Œò ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) . \\max_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta)=\\min_{\\eta\\in\\mathrm{H}}\\max_{\\theta\\in\\Theta}\\ma", "snippet": "max Œ∏ ‚àà Œò ‚Å° min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) = min Œ∑ ‚àà H ‚Å° max Œ∏ ‚àà Œò ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) . \\max_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta)=\\min_{\\eta\\in\\mathrm{H}}\\max_{\\theta\\in\\Theta}\\mathcal{L}(\\theta,\\eta). roman_max start_POSTSUBSCRIPT italic_Œ∏ ‚àà roman_Œò end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT italic_Œ∑ ‚àà roman_H end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ , italic_Œ∑ ) = roman_min start_POSTSUBSCRIPT italic_Œ∑ ‚àà roman_H end_POSTSUBSCRIPT roman_max start_POSTSUBSCRIPT italic_Œ∏ ‚àà roman_Œò end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ , italic_Œ∑ ) . (A.3.9)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E10", "title": "min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ) = ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) = max Œ∏ ‚àà Œò ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ‚ãÜ ) . \\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta^{\\star},\\eta)=\\mathcal{L}(\\theta^{\\star},\\eta^{\\star})=\\max_{\\theta\\in\\Theta}", "snippet": "min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ) = ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) = max Œ∏ ‚àà Œò ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ‚ãÜ ) . \\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta^{\\star},\\eta)=\\mathcal{L}(\\theta^{\\star},\\eta^{\\star})=\\max_{\\theta\\in\\Theta}\\mathcal{L}(\\theta,\\eta^{\\star}). roman_min start_POSTSUBSCRIPT italic_Œ∑ ‚àà roman_H end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT , italic_Œ∑ ) = caligraphic_L ( italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT , italic_Œ∑ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ) = roman_max start_POSTSUBSCRIPT italic_Œ∏ ‚àà roman_Œò end_POSTSUBSCRIPT caligraphic_L ( ital"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E11", "title": "min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) ‚â§ ‚Ñí ‚Äã ( Œ∏ , Œ∑ ‚ãÜ ) ‚â§ ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) . \\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta)\\leq\\mathcal{L}(\\theta,\\eta^{\\star})\\leq\\mathcal{L}(\\theta^{\\star},\\eta^{\\star}). roma", "snippet": "min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) ‚â§ ‚Ñí ‚Äã ( Œ∏ , Œ∑ ‚ãÜ ) ‚â§ ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) . \\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta)\\leq\\mathcal{L}(\\theta,\\eta^{\\star})\\leq\\mathcal{L}(\\theta^{\\star},\\eta^{\\star}). roman_min start_POSTSUBSCRIPT italic_Œ∑ ‚àà roman_H end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ , italic_Œ∑ ) ‚â§ caligraphic_L ( italic_Œ∏ , italic_Œ∑ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ) ‚â§ caligraphic_L ( italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT , italic_Œ∑ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ) . (A.3.11)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E12", "title": "max Œ∏ ‚àà Œò ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) ‚â§ ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) . \\max_{\\theta\\in\\Theta}\\mathcal{L}(\\theta,\\eta)\\leq\\mathcal{L}(\\theta^{\\star},\\eta^{\\star}). roman_max start_POSTSUBSCRIPT italic_Œ∏ ‚àà roman_Œò end_POSTSUB", "snippet": "max Œ∏ ‚àà Œò ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) ‚â§ ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) . \\max_{\\theta\\in\\Theta}\\mathcal{L}(\\theta,\\eta)\\leq\\mathcal{L}(\\theta^{\\star},\\eta^{\\star}). roman_max start_POSTSUBSCRIPT italic_Œ∏ ‚àà roman_Œò end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ , italic_Œ∑ ) ‚â§ caligraphic_L ( italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT , italic_Œ∑ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ) . (A.3.12)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E13", "title": "‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) ‚â§ ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ) ‚â§ max Œ∏ ‚àà Œò ‚Å° ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ) ‚üπ ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) ‚â§ min Œ∑ ‚àà H ‚Å° max Œ∏ ‚àà Œò ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) . \\mathcal{L}(\\theta^{\\star},\\eta^{\\star})\\leq\\mathcal{L}(\\theta^{\\star},\\", "snippet": "‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) ‚â§ ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ) ‚â§ max Œ∏ ‚àà Œò ‚Å° ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ) ‚üπ ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) ‚â§ min Œ∑ ‚àà H ‚Å° max Œ∏ ‚àà Œò ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) . \\mathcal{L}(\\theta^{\\star},\\eta^{\\star})\\leq\\mathcal{L}(\\theta^{\\star},\\eta)\\leq\\max_{\\theta\\in\\Theta}\\mathcal{L}(\\theta^{\\star},\\eta)\\implies\\mathcal{L}(\\theta^{\\star},\\eta^{\\star})\\leq\\min_{\\eta\\in\\mathrm{H}}\\max_{\\theta\\in\\Theta}\\mathcal{L}(\\theta,\\eta). caligraphic_L ( italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT , italic_Œ∑ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ) ‚â§ caligraphic_L ( italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT , italic_Œ∑ ) ‚â§ ro"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E14", "title": "max Œ∏ ‚àà Œò ‚Å° min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) = ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) = min Œ∑ ‚àà H ‚Å° max Œ∏ ‚àà Œò ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) . \\max_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta)=\\mathcal{L}(\\theta^{\\star},\\e", "snippet": "max Œ∏ ‚àà Œò ‚Å° min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) = ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) = min Œ∑ ‚àà H ‚Å° max Œ∏ ‚àà Œò ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) . \\max_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta)=\\mathcal{L}(\\theta^{\\star},\\eta^{\\star})=\\min_{\\eta\\in\\mathrm{H}}\\max_{\\theta\\in\\Theta}\\mathcal{L}(\\theta,\\eta). roman_max start_POSTSUBSCRIPT italic_Œ∏ ‚àà roman_Œò end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT italic_Œ∑ ‚àà roman_H end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ , italic_Œ∑ ) = caligraphic_L ( italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT , italic_Œ∑ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ) = roman_min star"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E15", "title": "Œ∏ ‚ãÜ ‚àà arg ‚Äã max Œ∏ ‚àà Œò ‚Å° min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) . \\theta^{\\star}\\in\\operatorname*{arg\\ max}_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta). italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_PO", "snippet": "Œ∏ ‚ãÜ ‚àà arg ‚Äã max Œ∏ ‚àà Œò ‚Å° min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) . \\theta^{\\star}\\in\\operatorname*{arg\\ max}_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta). italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ‚àà start_OPERATOR roman_arg roman_max end_OPERATOR start_POSTSUBSCRIPT italic_Œ∏ ‚àà roman_Œò end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT italic_Œ∑ ‚àà roman_H end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ , italic_Œ∑ ) . (A.3.15)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E16", "title": "max Œ∏ ‚àà Œò ‚Å° min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) = ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) . \\max_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta)=\\mathcal{L}(\\theta^{\\star},\\eta^{\\star}). roman_max start_POSTSUBSCRI", "snippet": "max Œ∏ ‚àà Œò ‚Å° min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) = ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) . \\max_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta)=\\mathcal{L}(\\theta^{\\star},\\eta^{\\star}). roman_max start_POSTSUBSCRIPT italic_Œ∏ ‚àà roman_Œò end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT italic_Œ∑ ‚àà roman_H end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ , italic_Œ∑ ) = caligraphic_L ( italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT , italic_Œ∑ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ) . (A.3.16)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E17", "title": "max Œ∏ ‚àà Œò ‚Å° min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) = ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) = min Œ∑ ‚àà H ‚Å° max Œ∏ ‚àà Œò ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) . \\max_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta)=\\mathcal{L}(\\theta^{\\star},\\e", "snippet": "max Œ∏ ‚àà Œò ‚Å° min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) = ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) = min Œ∑ ‚àà H ‚Å° max Œ∏ ‚àà Œò ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) . \\max_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta)=\\mathcal{L}(\\theta^{\\star},\\eta^{\\star})=\\min_{\\eta\\in\\mathrm{H}}\\max_{\\theta\\in\\Theta}\\mathcal{L}(\\theta,\\eta). roman_max start_POSTSUBSCRIPT italic_Œ∏ ‚àà roman_Œò end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT italic_Œ∑ ‚àà roman_H end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ , italic_Œ∑ ) = caligraphic_L ( italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT , italic_Œ∑ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ) = roman_min star"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E18", "title": "Œ∏ ‚ãÜ ‚àà arg ‚Äã max Œ∏ ‚àà Œò ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ‚ãÜ ) , Œ∑ ‚ãÜ ‚àà arg ‚Äã min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ) . \\theta^{\\star}\\in\\operatorname*{arg\\ max}_{\\theta\\in\\Theta}\\mathcal{L}(\\theta,\\eta^{\\star}),\\qquad\\eta^{\\star}\\in\\op", "snippet": "Œ∏ ‚ãÜ ‚àà arg ‚Äã max Œ∏ ‚àà Œò ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ‚ãÜ ) , Œ∑ ‚ãÜ ‚àà arg ‚Äã min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ) . \\theta^{\\star}\\in\\operatorname*{arg\\ max}_{\\theta\\in\\Theta}\\mathcal{L}(\\theta,\\eta^{\\star}),\\qquad\\eta^{\\star}\\in\\operatorname*{arg\\ min}_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta^{\\star},\\eta). italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ‚àà start_OPERATOR roman_arg roman_max end_OPERATOR start_POSTSUBSCRIPT italic_Œ∏ ‚àà roman_Œò end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ , italic_Œ∑ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ) , italic_Œ∑ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ‚àà start_OPERATOR roman_arg r"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E19", "title": "max Œ∏ ‚àà Œò ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ‚ãÜ ) = ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) . \\max_{\\theta\\in\\Theta}\\mathcal{L}(\\theta,\\eta^{\\star})=\\mathcal{L}(\\theta^{\\star},\\eta^{\\star}). roman_max start_POSTSUBSCRIPT italic_Œ∏ ‚àà roman_Œò end_", "snippet": "max Œ∏ ‚àà Œò ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ‚ãÜ ) = ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) . \\max_{\\theta\\in\\Theta}\\mathcal{L}(\\theta,\\eta^{\\star})=\\mathcal{L}(\\theta^{\\star},\\eta^{\\star}). roman_max start_POSTSUBSCRIPT italic_Œ∏ ‚àà roman_Œò end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ , italic_Œ∑ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ) = caligraphic_L ( italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT , italic_Œ∑ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ) . (A.3.19)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E20", "title": "‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) ‚â§ max Œ∏ ‚àà Œò ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ‚ãÜ ) , \\mathcal{L}(\\theta^{\\star},\\eta^{\\star})\\leq\\max_{\\theta\\in\\Theta}\\mathcal{L}(\\theta,\\eta^{\\star}), caligraphic_L ( italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ e", "snippet": "‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) ‚â§ max Œ∏ ‚àà Œò ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ‚ãÜ ) , \\mathcal{L}(\\theta^{\\star},\\eta^{\\star})\\leq\\max_{\\theta\\in\\Theta}\\mathcal{L}(\\theta,\\eta^{\\star}), caligraphic_L ( italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT , italic_Œ∑ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ) ‚â§ roman_max start_POSTSUBSCRIPT italic_Œ∏ ‚àà roman_Œò end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ , italic_Œ∑ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ) , (A.3.20)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E21", "title": "‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) = max Œ∏ ‚àà Œò ‚Å° min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) ‚â§ max Œ∏ ‚àà Œò ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ‚ãÜ ) . \\mathcal{L}(\\theta^{\\star},\\eta^{\\star})=\\max_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta", "snippet": "‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) = max Œ∏ ‚àà Œò ‚Å° min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) ‚â§ max Œ∏ ‚àà Œò ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ‚ãÜ ) . \\mathcal{L}(\\theta^{\\star},\\eta^{\\star})=\\max_{\\theta\\in\\Theta}\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta)\\leq\\max_{\\theta\\in\\Theta}\\mathcal{L}(\\theta,\\eta^{\\star}). caligraphic_L ( italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT , italic_Œ∑ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ) = roman_max start_POSTSUBSCRIPT italic_Œ∏ ‚àà roman_Œò end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT italic_Œ∑ ‚àà roman_H end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ , italic_Œ∑ ) ‚â§ roman_max start_POSTSUBSCRIPT italic_"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E22", "title": "‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) = max Œ∏ ‚àà Œò ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ‚ãÜ ) , \\mathcal{L}(\\theta^{\\star},\\eta^{\\star})=\\max_{\\theta\\in\\Theta}\\mathcal{L}(\\theta,\\eta^{\\star}), caligraphic_L ( italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_", "snippet": "‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) = max Œ∏ ‚àà Œò ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ‚ãÜ ) , \\mathcal{L}(\\theta^{\\star},\\eta^{\\star})=\\max_{\\theta\\in\\Theta}\\mathcal{L}(\\theta,\\eta^{\\star}), caligraphic_L ( italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT , italic_Œ∑ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ) = roman_max start_POSTSUBSCRIPT italic_Œ∏ ‚àà roman_Œò end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ , italic_Œ∑ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ) , (A.3.22)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E23", "title": "‚àá Œ∏ [ min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) ] = ‚àá Œ∏ ‚Ñí ‚Äã ( Œ∏ , stop_grad ‚Äã ( Œ∑ ‚ãÜ ‚Äã ( Œ∏ ) ) ) , \\nabla_{\\theta}\\left[\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta)\\right]=\\nabla_{\\theta}\\mathcal{L}(\\theta,\\texttt", "snippet": "‚àá Œ∏ [ min Œ∑ ‚àà H ‚Å° ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) ] = ‚àá Œ∏ ‚Ñí ‚Äã ( Œ∏ , stop_grad ‚Äã ( Œ∑ ‚ãÜ ‚Äã ( Œ∏ ) ) ) , \\nabla_{\\theta}\\left[\\min_{\\eta\\in\\mathrm{H}}\\mathcal{L}(\\theta,\\eta)\\right]=\\nabla_{\\theta}\\mathcal{L}(\\theta,\\texttt{stop\\_grad}(\\eta^{\\star}(\\theta))), ‚àá start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT [ roman_min start_POSTSUBSCRIPT italic_Œ∑ ‚àà roman_H end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ , italic_Œ∑ ) ] = ‚àá start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ , stop_grad ( italic_Œ∑ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ( italic_Œ∏ ) ) ) , (A.3.23)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E28", "title": "Œ∏ k + 1 = Œ∏ k + h ‚Äã ‚àá Œ∏ ‚Ñí ‚Äã ( Œ∏ k , Œ∑ k ) , Œ∑ k + 1 = Œ∑ k ‚àí h ‚Äã ‚àá Œ∑ ‚Ñí ‚Äã ( Œ∏ k , Œ∑ k ) . \\theta_{k+1}=\\theta_{k}+h\\nabla_{\\theta}\\mathcal{L}(\\theta_{k},\\eta_{k}),\\qquad\\eta_{k+1}=\\eta_{k}-h\\nabla_{\\eta", "snippet": "Œ∏ k + 1 = Œ∏ k + h ‚Äã ‚àá Œ∏ ‚Ñí ‚Äã ( Œ∏ k , Œ∑ k ) , Œ∑ k + 1 = Œ∑ k ‚àí h ‚Äã ‚àá Œ∑ ‚Ñí ‚Äã ( Œ∏ k , Œ∑ k ) . \\theta_{k+1}=\\theta_{k}+h\\nabla_{\\theta}\\mathcal{L}(\\theta_{k},\\eta_{k}),\\qquad\\eta_{k+1}=\\eta_{k}-h\\nabla_{\\eta}\\mathcal{L}(\\theta_{k},\\eta_{k}). italic_Œ∏ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + italic_h ‚àá start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_Œ∑ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) , italic_Œ∑ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUB"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E29", "title": "‚àá 2 ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) = [ ‚àá Œ∏ 2 ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) d d ‚Äã Œ∑ ‚Äã ‚àá Œ∏ 2 ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) d d ‚Äã Œ∏ ‚Äã ‚àá Œ∑ ‚Äã Œ∏ 2 ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) ‚àá Œ∑ 2 ‚Ñí ‚Äã ( Œ∏ ) ] \\nabla^{2}\\mathcal{L}(\\theta,\\eta)=\\begin{bmatrix}\\nabla_{\\theta}^{2}\\mathca", "snippet": "‚àá 2 ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) = [ ‚àá Œ∏ 2 ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) d d ‚Äã Œ∑ ‚Äã ‚àá Œ∏ 2 ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) d d ‚Äã Œ∏ ‚Äã ‚àá Œ∑ ‚Äã Œ∏ 2 ‚Ñí ‚Äã ( Œ∏ , Œ∑ ) ‚àá Œ∑ 2 ‚Ñí ‚Äã ( Œ∏ ) ] \\nabla^{2}\\mathcal{L}(\\theta,\\eta)=\\begin{bmatrix}\\nabla_{\\theta}^{2}\\mathcal{L}(\\theta,\\eta)&\\frac{\\mathrm{d}}{\\mathrm{d}\\eta}\\nabla_{\\theta}^{2}\\mathcal{L}(\\theta,\\eta)\\\\ \\frac{\\mathrm{d}}{\\mathrm{d}\\theta}\\nabla_{\\eta\\theta}^{2}\\mathcal{L}(\\theta,\\eta)&\\nabla_{\\eta}^{2}\\mathcal{L}(\\theta)\\end{bmatrix} ‚àá start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT caligraphic_L ( italic_Œ∏ , italic_Œ∑ ) = [ start_ARG start_ROW start_CELL ‚àá start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT st"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E30", "title": "max ‚Å° { ‚Äñ ‚àá Œ∏ 2 ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) ‚Äñ 2 , ‚Äñ ‚àá Œ∑ 2 ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) ‚Äñ 2 , ‚Äñ d d ‚Äã Œ∑ ‚Äã ‚àá Œ∏ ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) ‚Äñ 2 } ‚â§ Œ≤ . \\max\\left\\{\\|\\nabla_{\\theta}^{2}\\mathcal{L}(\\theta^{\\star},\\eta^{\\star})\\|_{2},\\le", "snippet": "max ‚Å° { ‚Äñ ‚àá Œ∏ 2 ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) ‚Äñ 2 , ‚Äñ ‚àá Œ∑ 2 ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) ‚Äñ 2 , ‚Äñ d d ‚Äã Œ∑ ‚Äã ‚àá Œ∏ ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) ‚Äñ 2 } ‚â§ Œ≤ . \\max\\left\\{\\|\\nabla_{\\theta}^{2}\\mathcal{L}(\\theta^{\\star},\\eta^{\\star})\\|_{2},\\left\\|\\nabla_{\\eta}^{2}\\mathcal{L}(\\theta^{\\star},\\eta^{\\star})\\right\\|_{2},\\left\\|\\frac{\\mathrm{d}}{\\mathrm{d}\\eta}\\nabla_{\\theta}\\mathcal{L}(\\theta^{\\star},\\eta^{\\star})\\right\\|_{2}\\right\\}\\leq\\beta. roman_max { ‚à• ‚àá start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT caligraphic_L ( italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT , italic_Œ∑ start_POSTSUP"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E31", "title": "Œº Œ∑ = Œª min ‚Äã ( ‚àá Œ∑ 2 ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) ) , Œº Œ∏ = min ‚Å° { Œ≤ , ‚àí ( ‚àá Œ∏ 2 ‚Ñí + [ d d ‚Äã Œ∏ ‚Äã ‚àá Œ∑ ‚Ñí ] ‚Äã [ ‚àá Œ∑ 2 ‚Ñí ] ‚àí 1 ‚Äã [ d d ‚Äã Œ∑ ‚Äã ‚àá Œ∏ ‚Ñí ] ) ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) } . \\mu_{\\eta}=\\lambda_{\\min}(\\nabla_{\\eta}^{2", "snippet": "Œº Œ∑ = Œª min ‚Äã ( ‚àá Œ∑ 2 ‚Ñí ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) ) , Œº Œ∏ = min ‚Å° { Œ≤ , ‚àí ( ‚àá Œ∏ 2 ‚Ñí + [ d d ‚Äã Œ∏ ‚Äã ‚àá Œ∑ ‚Ñí ] ‚Äã [ ‚àá Œ∑ 2 ‚Ñí ] ‚àí 1 ‚Äã [ d d ‚Äã Œ∑ ‚Äã ‚àá Œ∏ ‚Ñí ] ) ‚Äã ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) } . \\mu_{\\eta}=\\lambda_{\\min}(\\nabla_{\\eta}^{2}\\mathcal{L}(\\theta^{\\star},\\eta^{\\star})),\\qquad\\mu_{\\theta}=\\min\\left\\{\\beta,-\\left(\\nabla_{\\theta}^{2}\\mathcal{L}+\\left[\\frac{\\mathrm{d}}{\\mathrm{d}\\theta}\\nabla_{\\eta}\\mathcal{L}\\right]\\left[\\nabla_{\\eta}^{2}\\mathcal{L}\\right]^{-1}\\left[\\frac{\\mathrm{d}}{\\mathrm{d}\\eta}\\nabla_{\\theta}\\mathcal{L}\\right]\\right)(\\theta^{\\star},\\eta^{\\star})\\right\\}. italic_Œº start_POSTSUBSCRIPT italic_Œ∑ end_POSTS"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E32", "title": "Œ∫ Œ∑ = L / Œº Œ∑ , Œ∫ Œ∏ = L / Œº Œ∏ . \\kappa_{\\eta}=L/\\mu_{\\eta},\\qquad\\kappa_{\\theta}=L/\\mu_{\\theta}. italic_Œ∫ start_POSTSUBSCRIPT italic_Œ∑ end_POSTSUBSCRIPT = italic_L / italic_Œº start_POSTSUBSCRIPT itali", "snippet": "Œ∫ Œ∑ = L / Œº Œ∑ , Œ∫ Œ∏ = L / Œº Œ∏ . \\kappa_{\\eta}=L/\\mu_{\\eta},\\qquad\\kappa_{\\theta}=L/\\mu_{\\theta}. italic_Œ∫ start_POSTSUBSCRIPT italic_Œ∑ end_POSTSUBSCRIPT = italic_L / italic_Œº start_POSTSUBSCRIPT italic_Œ∑ end_POSTSUBSCRIPT , italic_Œ∫ start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT = italic_L / italic_Œº start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT . (A.3.32)"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.E35", "title": "‚Äñ ( Œ∏ k , Œ∑ k ) ‚àí ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) ‚Äñ 2 ‚â§ c 0 ‚Äã ( 1 ‚àí c 1 T ‚Äã Œ∫ Œ∏ ) k ‚Äã ‚Äñ ( Œ∏ 0 , Œ∑ 0 ) ‚àí ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) ‚Äñ 2 , \\|(\\theta_{k},\\eta_{k})-(\\theta^{\\star},\\eta^{\\star})\\|_{2}\\leq c_{0}\\left(1-\\frac{c_{1}}{T\\ka", "snippet": "‚Äñ ( Œ∏ k , Œ∑ k ) ‚àí ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) ‚Äñ 2 ‚â§ c 0 ‚Äã ( 1 ‚àí c 1 T ‚Äã Œ∫ Œ∏ ) k ‚Äã ‚Äñ ( Œ∏ 0 , Œ∑ 0 ) ‚àí ( Œ∏ ‚ãÜ , Œ∑ ‚ãÜ ) ‚Äñ 2 , \\|(\\theta_{k},\\eta_{k})-(\\theta^{\\star},\\eta^{\\star})\\|_{2}\\leq c_{0}\\left(1-\\frac{c_{1}}{T\\kappa_{\\theta}}\\right)^{k}\\|(\\theta_{0},\\eta_{0})-(\\theta^{\\star},\\eta^{\\star})\\|_{2}, ‚à• ( italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_Œ∑ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) - ( italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT , italic_Œ∑ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ) ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ‚â§ italic_c start_POSTSUBSCRIPT 0 end_PO"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S4.E1", "title": "softmax ‚Å° ( [ x 1 ‚ãÆ x n ] ) = 1 ‚àë i = 1 n e x i ‚Äã [ x 1 ‚ãÆ x n ] . \\operatorname{\\mathrm{softmax}}\\left(\\begin{bmatrix}x_{1}\\\\ \\vdots\\\\ x_{n}\\end{bmatrix}\\right)=\\frac{1}{\\sum_{i=1}^{n}e^{x_{i}}}\\begin", "snippet": "softmax ‚Å° ( [ x 1 ‚ãÆ x n ] ) = 1 ‚àë i = 1 n e x i ‚Äã [ x 1 ‚ãÆ x n ] . \\operatorname{\\mathrm{softmax}}\\left(\\begin{bmatrix}x_{1}\\\\ \\vdots\\\\ x_{n}\\end{bmatrix}\\right)=\\frac{1}{\\sum_{i=1}^{n}e^{x_{i}}}\\begin{bmatrix}x_{1}\\\\ \\vdots\\\\ x_{n}\\end{bmatrix}. roman_softmax ( [ start_ARG start_ROW start_CELL italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL ‚ãÆ end_CELL end_ROW start_ROW start_CELL italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] ) = divide start_ARG 1 end_ARG start_ARG ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT s"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx100", "title": "‚Äñ Œ∏ ‚ãÜ ‚àí Œ∏ k + 1 ‚Äñ 2 2 \\displaystyle\\|\\theta^{\\star}-\\theta_{k+1}\\|_{2}^{2} ‚à• italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT - italic_Œ∏ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT ‚à• start_", "snippet": "‚Äñ Œ∏ ‚ãÜ ‚àí Œ∏ k + 1 ‚Äñ 2 2 \\displaystyle\\|\\theta^{\\star}-\\theta_{k+1}\\|_{2}^{2} ‚à• italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT - italic_Œ∏ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ‚â§ ‚Äñ Œ∏ ‚ãÜ ‚àí Œ∏ k + h ‚Äã ‚àá ‚Ñí ‚Äã ( Œ∏ k ) ‚Äñ 2 2 \\displaystyle\\leq\\|\\theta^{\\star}-\\theta_{k}+h\\nabla\\mathcal{L}(\\theta_{k})\\|_{2}^{2} ‚â§ ‚à• italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT - italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + italic_h ‚àá caligraphic_L ( italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCR"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx101", "title": "‚Äñ Œ∏ ‚ãÜ ‚àí Œ∏ k + 1 ‚Äñ 2 2 \\displaystyle\\|\\theta^{\\star}-\\theta_{k+1}\\|_{2}^{2} ‚à• italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT - italic_Œ∏ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT ‚à• start_", "snippet": "‚Äñ Œ∏ ‚ãÜ ‚àí Œ∏ k + 1 ‚Äñ 2 2 \\displaystyle\\|\\theta^{\\star}-\\theta_{k+1}\\|_{2}^{2} ‚à• italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT - italic_Œ∏ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ‚â§ ( 1 ‚àí Œ± ‚Äã h ) ‚Äã ‚Äñ Œ∏ ‚ãÜ ‚àí Œ∏ k ‚Äñ 2 2 ‚â§ ( 1 ‚àí Œ± ‚Äã h ) 2 ‚Äã ‚Äñ Œ∏ ‚ãÜ ‚àí Œ∏ k ‚àí 1 ‚Äñ 2 2 ‚â§ ‚ãØ \\displaystyle\\leq(1-\\alpha h)\\|\\theta^{\\star}-\\theta_{k}\\|_{2}^{2}\\leq(1-\\alpha h)^{2}\\|\\theta^{\\star}-\\theta_{k-1}\\|_{2}^{2}\\leq\\cdots ‚â§ ( 1 - italic_Œ± italic_h ) ‚à• italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT - italic_Œ∏ start_POST"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx102", "title": "arg ‚Äã min ùíó ‚àà ‚Ñù n ‚Äñ ùíó ‚Äñ 2 = 1 ‚Å° [ ‚Ñí ‚Äã ( Œ∏ ) + h ‚Äã ‚ü® ‚àá ‚Ñí ‚Äã ( Œ∏ ) , ùíó ‚ü© + 1 2 ‚Äã h 2 ‚Äã ‚ü® [ ‚àá 2 ‚Ñí ‚Äã ( Œ∏ ) ] ‚Äã ùíó , ùíó ‚ü© ] \\displaystyle\\operatorname*{arg\\ min}_{\\begin{subarray}{c}\\bm{v}\\in\\mathbb{R}^{n}\\\\ ", "snippet": "arg ‚Äã min ùíó ‚àà ‚Ñù n ‚Äñ ùíó ‚Äñ 2 = 1 ‚Å° [ ‚Ñí ‚Äã ( Œ∏ ) + h ‚Äã ‚ü® ‚àá ‚Ñí ‚Äã ( Œ∏ ) , ùíó ‚ü© + 1 2 ‚Äã h 2 ‚Äã ‚ü® [ ‚àá 2 ‚Ñí ‚Äã ( Œ∏ ) ] ‚Äã ùíó , ùíó ‚ü© ] \\displaystyle\\operatorname*{arg\\ min}_{\\begin{subarray}{c}\\bm{v}\\in\\mathbb{R}^{n}\\\\ \\|\\bm{v}\\|_{2}=1\\end{subarray}}\\left[\\mathcal{L}(\\theta)+h\\langle\\nabla\\mathcal{L}(\\theta),\\bm{v}\\rangle+\\frac{1}{2}h^{2}\\langle[\\nabla^{2}\\mathcal{L}(\\theta)]\\bm{v},\\bm{v}\\rangle\\right] start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT start_ARG start_ROW start_CELL bold_italic_v ‚àà blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT end_CELL end_ROW start_ROW start_C"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx103", "title": "P k \\displaystyle P_{k} italic_P start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = PreconditionerUpdate ‚Äã ( P k ‚àí 1 ; Œ∏ k , ‚àá ‚Ñí ‚Äã ( Œ∏ k ) ) \\displaystyle=\\mathrm{PreconditionerUpdate}(P_{k-1};\\theta_{k", "snippet": "P k \\displaystyle P_{k} italic_P start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = PreconditionerUpdate ‚Äã ( P k ‚àí 1 ; Œ∏ k , ‚àá ‚Ñí ‚Äã ( Œ∏ k ) ) \\displaystyle=\\mathrm{PreconditionerUpdate}(P_{k-1};\\theta_{k},\\nabla\\mathcal{L}(\\theta_{k})) = roman_PreconditionerUpdate ( italic_P start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT ; italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , ‚àá caligraphic_L ( italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ) (A.1.31) Œ∏ k + 1 \\displaystyle\\theta_{k+1} italic_Œ∏ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = Œ∏ k ‚àí h ‚Äã P k ‚Äã ‚àá ‚Ñí ‚Äã ( Œ∏ k ) . \\"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx104", "title": "ùíà k \\displaystyle\\bm{g}_{k} bold_italic_g start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = Œ≤ ‚Äã ùíà k ‚àí 1 + ( 1 ‚àí Œ≤ ) ‚Äã ‚àá ‚Ñí k ‚Äã ( Œ∏ k ) \\displaystyle=\\beta\\bm{g}_{k-1}+(1-\\beta)\\nabla\\mathcal{L}_{k}(\\the", "snippet": "ùíà k \\displaystyle\\bm{g}_{k} bold_italic_g start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = Œ≤ ‚Äã ùíà k ‚àí 1 + ( 1 ‚àí Œ≤ ) ‚Äã ‚àá ‚Ñí k ‚Äã ( Œ∏ k ) \\displaystyle=\\beta\\bm{g}_{k-1}+(1-\\beta)\\nabla\\mathcal{L}_{k}(\\theta_{k}) = italic_Œ≤ bold_italic_g start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT + ( 1 - italic_Œ≤ ) ‚àá caligraphic_L start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) (A.1.46) Œ∏ k + 1 \\displaystyle\\theta_{k+1} italic_Œ∏ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = Œ∏ k ‚àí h ‚Äã ùíà k . \\displaystyle=\\theta_{k}-h\\bm{g}_{k}. = italic_Œ∏ star"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx105", "title": "ùíà k \\displaystyle\\bm{g}_{k} bold_italic_g start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = Œ≤ 1 ‚Äã ùíà k ‚àí 1 + ( 1 ‚àí Œ≤ 1 ) ‚Äã ‚àá ‚Ñí k ‚Äã ( Œ∏ k ) \\displaystyle=\\beta^{1}\\bm{g}_{k-1}+(1-\\beta^{1})\\nabla\\mathcal", "snippet": "ùíà k \\displaystyle\\bm{g}_{k} bold_italic_g start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = Œ≤ 1 ‚Äã ùíà k ‚àí 1 + ( 1 ‚àí Œ≤ 1 ) ‚Äã ‚àá ‚Ñí k ‚Äã ( Œ∏ k ) \\displaystyle=\\beta^{1}\\bm{g}_{k-1}+(1-\\beta^{1})\\nabla\\mathcal{L}_{k}(\\theta_{k}) = italic_Œ≤ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT bold_italic_g start_POSTSUBSCRIPT italic_k - 1 end_POSTSUBSCRIPT + ( 1 - italic_Œ≤ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) ‚àá caligraphic_L start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) (A.1.56) ùíî k \\displaystyle\\bm{s}_{k} bold_italic_s start_POSTSUBSCRIPT ita"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx106", "title": "d ‚Äã g \\displaystyle\\mathrm{d}g roman_d italic_g = g ‚Äã ( ùëæ + d ‚Äã ùëæ , ùíÉ + d ‚Äã ùíÉ ) ‚àí g ‚Äã ( ùëæ , ùíÉ ) = [ ( ùëæ + d ‚Äã ùëæ ) ‚Äã ùëø + ( ùíÉ + d ‚Äã ùíÉ ) ‚Äã ùüè ‚ä§ ] ‚àí [ ùëæ ‚Äã ùëø + ùíÉ ] \\displaystyle=g(\\bm{W}+\\mathrm{d}\\bm{W},\\b", "snippet": "d ‚Äã g \\displaystyle\\mathrm{d}g roman_d italic_g = g ‚Äã ( ùëæ + d ‚Äã ùëæ , ùíÉ + d ‚Äã ùíÉ ) ‚àí g ‚Äã ( ùëæ , ùíÉ ) = [ ( ùëæ + d ‚Äã ùëæ ) ‚Äã ùëø + ( ùíÉ + d ‚Äã ùíÉ ) ‚Äã ùüè ‚ä§ ] ‚àí [ ùëæ ‚Äã ùëø + ùíÉ ] \\displaystyle=g(\\bm{W}+\\mathrm{d}\\bm{W},\\bm{b}+\\mathrm{d}\\bm{b})-g(\\bm{W},\\bm{b})=[(\\bm{W}+\\mathrm{d}\\bm{W})\\bm{X}+(\\bm{b}+\\mathrm{d}\\bm{b})\\bm{1}^{\\top}]-[\\bm{W}\\bm{X}+\\bm{b}] = italic_g ( bold_italic_W + roman_d bold_italic_W , bold_italic_b + roman_d bold_italic_b ) - italic_g ( bold_italic_W , bold_italic_b ) = [ ( bold_italic_W + roman_d bold_italic_W ) bold_italic_X + ( bold_italic_b + roman_d bold_italic_b ) bold_1 start_POSTSUPERS"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx107", "title": "d ‚Äã p \\displaystyle\\mathrm{d}p roman_d italic_p = p ‚Ä≤ ‚Äã ( a , b ) ‚Äã [ d ‚Äã a , d ‚Äã b ] = p ‚Äã ( a + d ‚Äã a , b + d ‚Äã b ) ‚àí p ‚Äã ( a , b ) = ( a + d ‚Äã a ) ‚Äã ( b + d ‚Äã b ) ‚àí a ‚Äã b \\displaystyle=p^{\\prime}(a", "snippet": "d ‚Äã p \\displaystyle\\mathrm{d}p roman_d italic_p = p ‚Ä≤ ‚Äã ( a , b ) ‚Äã [ d ‚Äã a , d ‚Äã b ] = p ‚Äã ( a + d ‚Äã a , b + d ‚Äã b ) ‚àí p ‚Äã ( a , b ) = ( a + d ‚Äã a ) ‚Äã ( b + d ‚Äã b ) ‚àí a ‚Äã b \\displaystyle=p^{\\prime}(a,b)[\\mathrm{d}a,\\mathrm{d}b]=p(a+\\mathrm{d}a,b+\\mathrm{d}b)-p(a,b)=(a+\\mathrm{d}a)(b+\\mathrm{d}b)-ab = italic_p start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_a , italic_b ) [ roman_d italic_a , roman_d italic_b ] = italic_p ( italic_a + roman_d italic_a , italic_b + roman_d italic_b ) - italic_p ( italic_a , italic_b ) = ( italic_a + roman_d italic_a ) ( italic_b + roman_d italic_b ) - ital"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx108", "title": "f ‚Ä≤ ‚Äã ( x ) ‚Äã [ d ‚Äã x ] \\displaystyle f^{\\prime}(x)[\\mathrm{d}x] italic_f start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_x ) [ roman_d italic_x ] = p ‚Ä≤ ‚Äã ( v ‚Äã ( x ) ) ‚Äã v ‚Ä≤ ‚Äã ( x ) ‚Äã [ d ‚Äã x ] ", "snippet": "f ‚Ä≤ ‚Äã ( x ) ‚Äã [ d ‚Äã x ] \\displaystyle f^{\\prime}(x)[\\mathrm{d}x] italic_f start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_x ) [ roman_d italic_x ] = p ‚Ä≤ ‚Äã ( v ‚Äã ( x ) ) ‚Äã v ‚Ä≤ ‚Äã ( x ) ‚Äã [ d ‚Äã x ] = p ‚Ä≤ ‚Äã ( g ‚Äã ( x ) , h ‚Äã ( x ) ) ‚Äã [ g ‚Ä≤ ‚Äã ( x ) ‚Äã [ d ‚Äã x ] , h ‚Ä≤ ‚Äã ( x ) ‚Äã [ d ‚Äã x ] ] \\displaystyle=p^{\\prime}(v(x))v^{\\prime}(x)[\\mathrm{d}x]=p^{\\prime}(g(x),h(x))[g^{\\prime}(x)[\\mathrm{d}x],h^{\\prime}(x)[\\mathrm{d}x]] = italic_p start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_v ( italic_x ) ) italic_v start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_x ) [ roman_d italic_x ] = italic"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx109", "title": "f ‚Ä≤ ‚Äã ( ùë® ) ‚Äã [ d ‚Äã ùë® ] \\displaystyle f^{\\prime}(\\bm{A})[\\mathrm{d}\\bm{A}] italic_f start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( bold_italic_A ) [ roman_d bold_italic_A ] = ( g ‚Ä≤ ‚Äã ( ùë® ) ‚Äã [ d ‚Äã ùë® ] ", "snippet": "f ‚Ä≤ ‚Äã ( ùë® ) ‚Äã [ d ‚Äã ùë® ] \\displaystyle f^{\\prime}(\\bm{A})[\\mathrm{d}\\bm{A}] italic_f start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( bold_italic_A ) [ roman_d bold_italic_A ] = ( g ‚Ä≤ ‚Äã ( ùë® ) ‚Äã [ d ‚Äã ùë® ] ) ‚Äã h ‚Äã ( ùë® ) + g ‚Äã ( ùë® ) ‚Äã ( h ‚Ä≤ ‚Äã ( ùë® ) ‚Äã [ d ‚Äã ùë® ] ) \\displaystyle=(g^{\\prime}(\\bm{A})[\\mathrm{d}\\bm{A}])h(\\bm{A})+g(\\bm{A})(h^{\\prime}(\\bm{A})[\\mathrm{d}\\bm{A}]) = ( italic_g start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( bold_italic_A ) [ roman_d bold_italic_A ] ) italic_h ( bold_italic_A ) + italic_g ( bold_italic_A ) ( italic_h start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( bold_italic_A ) [ "}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx110", "title": "computing ‚Äã c ‚Ä≤ ‚Äã ( ùíô ) ‚àà ‚Ñù q √ó p ‚Äã takes negligible time \\displaystyle\\text{computing}\\ c^{\\prime}(\\bm{x})\\in\\mathbb{R}^{q\\times p}\\ \\text{takes negligible time} computing italic_c start_POSTSUPERSCR", "snippet": "computing ‚Äã c ‚Ä≤ ‚Äã ( ùíô ) ‚àà ‚Ñù q √ó p ‚Äã takes negligible time \\displaystyle\\text{computing}\\ c^{\\prime}(\\bm{x})\\in\\mathbb{R}^{q\\times p}\\ \\text{takes negligible time} computing italic_c start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( bold_italic_x ) ‚àà blackboard_R start_POSTSUPERSCRIPT italic_q √ó italic_p end_POSTSUPERSCRIPT takes negligible time (A.2.31) computing ‚Äã b ‚Ä≤ ‚Äã ( c ‚Äã ( ùíô ) ) ‚Äã c ‚Ä≤ ‚Äã ( x ) ‚àà ‚Ñù r √ó p ‚Äã takes ùí™ ‚Äã ( p ‚Äã q ‚Äã r ) time \\displaystyle\\text{computing}\\ b^{\\prime}(c(\\bm{x}))c^{\\prime}(x)\\in\\mathbb{R}^{r\\times p}\\ \\text{takes $\\mathcal{O}(pqr)$ time} computing italic_b start_POSTSUP"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx111", "title": "computing ‚Äã a ‚Ä≤ ‚Äã ( b ‚Äã ( c ‚Äã ( ùíô ) ) ) ‚àà ‚Ñù s √ó r ‚Äã takes negligible time \\displaystyle\\text{computing}\\ a^{\\prime}(b(c(\\bm{x})))\\in\\mathbb{R}^{s\\times r}\\ \\text{takes negligible time} computing itali", "snippet": "computing ‚Äã a ‚Ä≤ ‚Äã ( b ‚Äã ( c ‚Äã ( ùíô ) ) ) ‚àà ‚Ñù s √ó r ‚Äã takes negligible time \\displaystyle\\text{computing}\\ a^{\\prime}(b(c(\\bm{x})))\\in\\mathbb{R}^{s\\times r}\\ \\text{takes negligible time} computing italic_a start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_b ( italic_c ( bold_italic_x ) ) ) ‚àà blackboard_R start_POSTSUPERSCRIPT italic_s √ó italic_r end_POSTSUPERSCRIPT takes negligible time (A.2.34) computing ‚Äã a ‚Ä≤ ‚Äã ( b ‚Äã ( c ‚Äã ( ùíô ) ) ) ‚Äã b ‚Ä≤ ‚Äã ( c ‚Äã ( ùíô ) ) ‚àà ‚Ñù s √ó q ‚Äã takes ùí™ ‚Äã ( q ‚Äã r ‚Äã s ) time \\displaystyle\\text{computing}\\ a^{\\prime}(b(c(\\bm{x})))b^{\\prime}(c(\\bm{x}))\\in\\mathbb{R}^{s\\time"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx112", "title": "ùíÅ Œ∏ 1 ‚Äã ( ùëø ) ‚âê f Œ∏ emb ‚Äã ( ùëø ) , \\displaystyle\\bm{Z}_{\\theta}^{1}(\\bm{X})\\doteq f_{\\theta}^{\\mathrm{emb}}(\\bm{X}), bold_italic_Z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1", "snippet": "ùíÅ Œ∏ 1 ‚Äã ( ùëø ) ‚âê f Œ∏ emb ‚Äã ( ùëø ) , \\displaystyle\\bm{Z}_{\\theta}^{1}(\\bm{X})\\doteq f_{\\theta}^{\\mathrm{emb}}(\\bm{X}), bold_italic_Z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( bold_italic_X ) ‚âê italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT ( bold_italic_X ) , (A.2.37) ùíÅ Œ∏ ‚Ñì + 1 ‚Äã ( ùëø ) ‚âê f Œ∏ ‚Ñì ‚Äã ( ùíÅ Œ∏ ‚Ñì ‚Äã ( ùëø ) ) , ‚àÄ ‚Ñì ‚àà { 1 , ‚Ä¶ , L } , \\displaystyle\\bm{Z}_{\\theta}^{\\ell+1}(\\bm{X})\\doteq f_{\\theta}^{\\ell}(\\bm{Z}_{\\theta}^{\\ell}(\\bm{X})),\\quad\\forall\\ell\\in\\{1,\\dots,L\\}, bold_italic_Z st"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx113", "title": "ùíÅ 1 ‚âê f emb ‚Äã ( ùëø , Œ∏ emb ) \\displaystyle\\bm{Z}^{1}\\doteq f^{\\mathrm{emb}}(\\bm{X},\\theta^{\\mathrm{emb}}) bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ‚âê italic_f start_POSTSUPERSCRIPT roma", "snippet": "ùíÅ 1 ‚âê f emb ‚Äã ( ùëø , Œ∏ emb ) \\displaystyle\\bm{Z}^{1}\\doteq f^{\\mathrm{emb}}(\\bm{X},\\theta^{\\mathrm{emb}}) bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ‚âê italic_f start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT ( bold_italic_X , italic_Œ∏ start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT ) (A.2.41) ùíÅ ‚Ñì + 1 ‚âê f ‚Ñì ‚Äã ( ùíÅ ‚Ñì , Œ∏ ‚Ñì ) , ‚àÄ ‚Ñì ‚àà { 1 , ‚Ä¶ , L } , \\displaystyle\\bm{Z}^{\\ell+1}\\doteq f^{\\ell}(\\bm{Z}^{\\ell},\\theta^{\\ell}),\\qquad\\forall\\ell\\in\\{1,\\dots,L\\}, bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì + 1 end_POSTSUPERSCRIPT ‚âê italic_f start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPER"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx114", "title": "d ‚Äã ‚Ñí \\displaystyle\\mathrm{d}\\mathcal{L} roman_d caligraphic_L = d ‚Äã ùñ´ \\displaystyle=\\mathrm{d}\\mathsf{L} = roman_d sansserif_L (A.2.45) = d ‚Äã ùñ´ d ‚Äã ùíö ^ ‚ãÖ d ‚Äã ùíö ^ \\displaystyle=\\frac{\\mathrm{d}\\mathsf", "snippet": "d ‚Äã ‚Ñí \\displaystyle\\mathrm{d}\\mathcal{L} roman_d caligraphic_L = d ‚Äã ùñ´ \\displaystyle=\\mathrm{d}\\mathsf{L} = roman_d sansserif_L (A.2.45) = d ‚Äã ùñ´ d ‚Äã ùíö ^ ‚ãÖ d ‚Äã ùíö ^ \\displaystyle=\\frac{\\mathrm{d}\\mathsf{L}}{\\mathrm{d}\\hat{\\bm{y}}}\\cdot\\mathrm{d}\\hat{\\bm{y}} = divide start_ARG roman_d sansserif_L end_ARG start_ARG roman_d over^ start_ARG bold_italic_y end_ARG end_ARG ‚ãÖ roman_d over^ start_ARG bold_italic_y end_ARG (A.2.46) = d ‚Äã ùñ´ d ‚Äã ùíö ^ ‚ãÖ d ‚Äã ( h ‚Äã ( ùíÅ L + 1 , Œ∏ head ) ) \\displaystyle=\\frac{\\mathrm{d}\\mathsf{L}}{\\mathrm{d}\\hat{\\bm{y}}}\\cdot\\mathrm{d}{(h(\\bm{Z}^{L+1},\\theta^{\\mathrm{head}}))} = "}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx115", "title": "d ‚Äã ‚Ñí \\displaystyle\\mathrm{d}\\mathcal{L} roman_d caligraphic_L = d ‚Äã ùñ´ d ‚Äã ùíö ^ ‚ãÖ d ‚Äã h d ‚Äã Œ∏ head ‚ãÖ d ‚Äã Œ∏ head \\displaystyle=\\frac{\\mathrm{d}\\mathsf{L}}{\\mathrm{d}\\hat{\\bm{y}}}\\cdot\\frac{\\mathrm{d}h}{", "snippet": "d ‚Äã ‚Ñí \\displaystyle\\mathrm{d}\\mathcal{L} roman_d caligraphic_L = d ‚Äã ùñ´ d ‚Äã ùíö ^ ‚ãÖ d ‚Äã h d ‚Äã Œ∏ head ‚ãÖ d ‚Äã Œ∏ head \\displaystyle=\\frac{\\mathrm{d}\\mathsf{L}}{\\mathrm{d}\\hat{\\bm{y}}}\\cdot\\frac{\\mathrm{d}h}{\\mathrm{d}\\theta^{\\mathrm{head}}}\\cdot\\mathrm{d}\\theta^{\\mathrm{head}} = divide start_ARG roman_d sansserif_L end_ARG start_ARG roman_d over^ start_ARG bold_italic_y end_ARG end_ARG ‚ãÖ divide start_ARG roman_d italic_h end_ARG start_ARG roman_d italic_Œ∏ start_POSTSUPERSCRIPT roman_head end_POSTSUPERSCRIPT end_ARG ‚ãÖ roman_d italic_Œ∏ start_POSTSUPERSCRIPT roman_head end_POSTSUPERSCRIPT (A.2.49) = [ ‚àá"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx116", "title": "d ‚Äã ‚Ñí \\displaystyle\\mathrm{d}\\mathcal{L} roman_d caligraphic_L = d ‚Äã ‚Ñí d ‚Äã ùíÅ ‚Ñì + 1 ‚ãÖ d ‚Äã ùíÅ ‚Ñì + 1 \\displaystyle=\\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}\\bm{Z}^{\\ell+1}}\\cdot\\mathrm{d}\\bm{Z}^{\\ell+1} = d", "snippet": "d ‚Äã ‚Ñí \\displaystyle\\mathrm{d}\\mathcal{L} roman_d caligraphic_L = d ‚Äã ‚Ñí d ‚Äã ùíÅ ‚Ñì + 1 ‚ãÖ d ‚Äã ùíÅ ‚Ñì + 1 \\displaystyle=\\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}\\bm{Z}^{\\ell+1}}\\cdot\\mathrm{d}\\bm{Z}^{\\ell+1} = divide start_ARG roman_d caligraphic_L end_ARG start_ARG roman_d bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì + 1 end_POSTSUPERSCRIPT end_ARG ‚ãÖ roman_d bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì + 1 end_POSTSUPERSCRIPT (A.2.54) = d ‚Äã ‚Ñí d ‚Äã ùíÅ ‚Ñì + 1 ‚ãÖ d ‚Äã ( f ‚Ñì ‚Äã ( ùíÅ ‚Ñì , Œ∏ ‚Ñì ) ) \\displaystyle=\\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}\\bm{Z}^{\\ell+1}}\\cdot\\mathrm{d}{(f^{\\ell}(\\bm{Z}^{\\ell},\\theta^{\\ell})"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx117", "title": "d ‚Äã ‚Ñí \\displaystyle\\mathrm{d}\\mathcal{L} roman_d caligraphic_L = d ‚Äã ‚Ñí d ‚Äã ùíÅ ‚Ñì + 1 ‚ãÖ d ‚Äã ùíÅ ‚Ñì + 1 \\displaystyle=\\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}\\bm{Z}^{\\ell+1}}\\cdot\\mathrm{d}\\bm{Z}^{\\ell+1} = d", "snippet": "d ‚Äã ‚Ñí \\displaystyle\\mathrm{d}\\mathcal{L} roman_d caligraphic_L = d ‚Äã ‚Ñí d ‚Äã ùíÅ ‚Ñì + 1 ‚ãÖ d ‚Äã ùíÅ ‚Ñì + 1 \\displaystyle=\\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}\\bm{Z}^{\\ell+1}}\\cdot\\mathrm{d}\\bm{Z}^{\\ell+1} = divide start_ARG roman_d caligraphic_L end_ARG start_ARG roman_d bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì + 1 end_POSTSUPERSCRIPT end_ARG ‚ãÖ roman_d bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì + 1 end_POSTSUPERSCRIPT (A.2.62) = d ‚Äã ‚Ñí d ‚Äã ùíÅ ‚Ñì + 1 ‚ãÖ d ‚Äã ( f ‚Ñì ‚Äã ( ùíÅ ‚Ñì , Œ∏ ‚Ñì ) ) \\displaystyle=\\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}\\bm{Z}^{\\ell+1}}\\cdot\\mathrm{d}{(f^{\\ell}(\\bm{Z}^{\\ell},\\theta^{\\ell})"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx118", "title": "d ‚Äã ‚Ñí \\displaystyle\\mathrm{d}\\mathcal{L} roman_d caligraphic_L = d ‚Äã ùñ´ \\displaystyle=\\mathrm{d}\\mathsf{L} = roman_d sansserif_L (A.2.68) = d ‚Äã ùñ´ d ‚Äã ùíö ^ ‚ãÖ d ‚Äã ùíö ^ \\displaystyle=\\frac{\\mathrm{d}\\mathsf", "snippet": "d ‚Äã ‚Ñí \\displaystyle\\mathrm{d}\\mathcal{L} roman_d caligraphic_L = d ‚Äã ùñ´ \\displaystyle=\\mathrm{d}\\mathsf{L} = roman_d sansserif_L (A.2.68) = d ‚Äã ùñ´ d ‚Äã ùíö ^ ‚ãÖ d ‚Äã ùíö ^ \\displaystyle=\\frac{\\mathrm{d}\\mathsf{L}}{\\mathrm{d}\\hat{\\bm{y}}}\\cdot\\mathrm{d}\\hat{\\bm{y}} = divide start_ARG roman_d sansserif_L end_ARG start_ARG roman_d over^ start_ARG bold_italic_y end_ARG end_ARG ‚ãÖ roman_d over^ start_ARG bold_italic_y end_ARG (A.2.69) = d ‚Äã ùñ´ d ‚Äã ùíö ^ ‚ãÖ d ‚Äã h ‚Äã ( ùíÅ L + 1 , Œ∏ head ) \\displaystyle=\\frac{\\mathrm{d}\\mathsf{L}}{\\mathrm{d}\\hat{\\bm{y}}}\\cdot\\mathrm{d}{h(\\bm{Z}^{L+1},\\theta^{\\mathrm{head}})} = divide"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx119", "title": "‚àá ùíÅ L + 1 ‚Ñí \\displaystyle\\nabla_{\\bm{Z}^{L+1}}\\mathcal{L} ‚àá start_POSTSUBSCRIPT bold_italic_Z start_POSTSUPERSCRIPT italic_L + 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT caligraphic_L = ( d ‚Äã h d ‚Äã ùíÅ L +", "snippet": "‚àá ùíÅ L + 1 ‚Ñí \\displaystyle\\nabla_{\\bm{Z}^{L+1}}\\mathcal{L} ‚àá start_POSTSUBSCRIPT bold_italic_Z start_POSTSUPERSCRIPT italic_L + 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT caligraphic_L = ( d ‚Äã h d ‚Äã ùíÅ L + 1 ) ‚àó ‚Äã ‚àá ùíö ^ ùñ´ \\displaystyle=\\left(\\frac{\\mathrm{d}h}{\\mathrm{d}\\bm{Z}^{L+1}}\\right)^{*}\\nabla_{\\hat{\\bm{y}}}\\mathsf{L} = ( divide start_ARG roman_d italic_h end_ARG start_ARG roman_d bold_italic_Z start_POSTSUPERSCRIPT italic_L + 1 end_POSTSUPERSCRIPT end_ARG ) start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT ‚àá start_POSTSUBSCRIPT over^ start_ARG bold_italic_y end_ARG end_POSTSUBSCRIPT sansserif_L ("}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx120", "title": "d ‚Äã f ‚Ñì \\displaystyle\\mathrm{d}f^{\\ell} roman_d italic_f start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT = [ ( ùëæ ‚Ñì + d ‚Äã ùëæ ‚Ñì ) ‚Äã ùíÅ + ( ùíÉ ‚Ñì + d ‚Äã ùíÉ ‚Ñì ) ‚Äã ùüè ‚ä§ ] ‚àí [ ùëæ ‚Ñì ‚Äã ùíÅ + ùíÉ ‚Ñì ] \\displaystyle=[(\\bm", "snippet": "d ‚Äã f ‚Ñì \\displaystyle\\mathrm{d}f^{\\ell} roman_d italic_f start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT = [ ( ùëæ ‚Ñì + d ‚Äã ùëæ ‚Ñì ) ‚Äã ùíÅ + ( ùíÉ ‚Ñì + d ‚Äã ùíÉ ‚Ñì ) ‚Äã ùüè ‚ä§ ] ‚àí [ ùëæ ‚Ñì ‚Äã ùíÅ + ùíÉ ‚Ñì ] \\displaystyle=[(\\bm{W}^{\\ell}+\\mathrm{d}\\bm{W}^{\\ell})\\bm{Z}+(\\bm{b}^{\\ell}+\\mathrm{d}\\bm{b}^{\\ell})\\bm{1}^{\\top}]-[\\bm{W}^{\\ell}\\bm{Z}+\\bm{b}^{\\ell}] = [ ( bold_italic_W start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT + roman_d bold_italic_W start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ) bold_italic_Z + ( bold_italic_b start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT + roman_d bold_italic_b start_POSTSUPERS"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx121", "title": "‚ü® T ‚Äã [ ùë® , ùíñ ] , ùë© ‚ü© ‚Ñù m √ó n \\displaystyle\\langle T[\\bm{A},\\bm{u}],\\bm{B}\\rangle_{\\mathbb{R}^{m\\times n}} ‚ü® italic_T [ bold_italic_A , bold_italic_u ] , bold_italic_B ‚ü© start_POSTSUBSCRIPT blackboard", "snippet": "‚ü® T ‚Äã [ ùë® , ùíñ ] , ùë© ‚ü© ‚Ñù m √ó n \\displaystyle\\langle T[\\bm{A},\\bm{u}],\\bm{B}\\rangle_{\\mathbb{R}^{m\\times n}} ‚ü® italic_T [ bold_italic_A , bold_italic_u ] , bold_italic_B ‚ü© start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_m √ó italic_n end_POSTSUPERSCRIPT end_POSTSUBSCRIPT = tr ‚Å° ( ( ùë® ‚Äã ùíÅ + ùíñ ‚Äã ùüè ‚ä§ ) ‚Äã ùë© ‚ä§ ) \\displaystyle=\\operatorname{tr}((\\bm{A}\\bm{Z}+\\bm{u}\\bm{1}^{\\top})\\bm{B}^{\\top}) = roman_tr ( ( bold_italic_A bold_italic_Z + bold_italic_u bold_1 start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ) bold_italic_B start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ) (A.2.87) = tr ‚Å° ( ùë® ‚Äã ùíÅ ‚Äã ùë©"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx122", "title": "Œ∑ k + 1 = Œ∑ k + 1 T + 1 ; Œ∑ k + 1 t + 1 = Œ∑ k + 1 t ‚àí h ‚Äã ‚àá Œ∑ ‚Ñí ‚Äã ( Œ∏ k , Œ∑ k + 1 t ) , ‚àÄ t ‚àà [ T ] ; Œ∑ k + 1 1 = Œ∑ k \\displaystyle\\eta_{k+1}=\\eta_{k+1}^{T+1};\\qquad\\eta_{k+1}^{t+1}=\\eta_{k+1}^{t}-h\\n", "snippet": "Œ∑ k + 1 = Œ∑ k + 1 T + 1 ; Œ∑ k + 1 t + 1 = Œ∑ k + 1 t ‚àí h ‚Äã ‚àá Œ∑ ‚Ñí ‚Äã ( Œ∏ k , Œ∑ k + 1 t ) , ‚àÄ t ‚àà [ T ] ; Œ∑ k + 1 1 = Œ∑ k \\displaystyle\\eta_{k+1}=\\eta_{k+1}^{T+1};\\qquad\\eta_{k+1}^{t+1}=\\eta_{k+1}^{t}-h\\nabla_{\\eta}\\mathcal{L}(\\theta_{k},\\eta_{k+1}^{t}),\\quad\\forall t\\in[T];\\qquad\\eta_{k+1}^{1}=\\eta_{k} italic_Œ∑ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = italic_Œ∑ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T + 1 end_POSTSUPERSCRIPT ; italic_Œ∑ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t + 1 end_POSTSUPERSCRIPT = ita"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx123", "title": "Œ∏ k + 1 \\displaystyle\\theta_{k+1} italic_Œ∏ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = Œ∏ k + h ‚Äã ‚àá Œ∏ ‚Ñí ‚Äã ( Œ∏ k , Œ∑ k ) \\displaystyle=\\theta_{k}+h\\nabla_{\\theta}\\mathcal{L}(\\theta_{k},\\eta_{k}", "snippet": "Œ∏ k + 1 \\displaystyle\\theta_{k+1} italic_Œ∏ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = Œ∏ k + h ‚Äã ‚àá Œ∏ ‚Ñí ‚Äã ( Œ∏ k , Œ∑ k ) \\displaystyle=\\theta_{k}+h\\nabla_{\\theta}\\mathcal{L}(\\theta_{k},\\eta_{k}) = italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + italic_h ‚àá start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_Œ∑ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) (A.3.26) Œ∑ k + 1 \\displaystyle\\eta_{k+1} italic_Œ∑ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = Œ∑ k ‚àí T ‚Äã h ‚Äã ‚àá Œ∑ ‚Ñí ‚Äã ( Œ∏ k , Œ∑ k ) , \\displa"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx124", "title": "Œ∏ k + 1 \\displaystyle\\theta_{k+1} italic_Œ∏ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = Œ∏ k + 1 4 ‚Äã Œ≤ ‚Äã T ‚Äã ‚àá Œ∏ ‚Ñí ‚Äã ( Œ∏ k , Œ∑ k ) , \\displaystyle=\\theta_{k}+\\frac{1}{4\\beta T}\\nabla_{\\theta}\\m", "snippet": "Œ∏ k + 1 \\displaystyle\\theta_{k+1} italic_Œ∏ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = Œ∏ k + 1 4 ‚Äã Œ≤ ‚Äã T ‚Äã ‚àá Œ∏ ‚Ñí ‚Äã ( Œ∏ k , Œ∑ k ) , \\displaystyle=\\theta_{k}+\\frac{1}{4\\beta T}\\nabla_{\\theta}\\mathcal{L}(\\theta_{k},\\eta_{k}), = italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + divide start_ARG 1 end_ARG start_ARG 4 italic_Œ≤ italic_T end_ARG ‚àá start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT caligraphic_L ( italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_Œ∑ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) , (A.3.33) Œ∑ k + 1 \\displaystyle\\eta_{k+1} italic_Œ∑ start_POST"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#A2.S3.EGx125", "title": "1 2 ‚Äã ‚Äñ ‚àá f ‚Äã ( Œ∏ ) ‚Äñ 2 2 ‚â• Œº ‚Äã ( f ‚Äã ( Œ∏ ) ‚àí f ‚Äã ( Œ∏ ‚ãÜ ) ) , \\displaystyle\\frac{1}{2}\\|\\nabla f(\\theta)\\|_{2}^{2}\\geq\\mu\\left(f(\\theta)-f(\\theta^{\\star})\\right), divide start_ARG 1 end_ARG start_ARG ", "snippet": "1 2 ‚Äã ‚Äñ ‚àá f ‚Äã ( Œ∏ ) ‚Äñ 2 2 ‚â• Œº ‚Äã ( f ‚Äã ( Œ∏ ) ‚àí f ‚Äã ( Œ∏ ‚ãÜ ) ) , \\displaystyle\\frac{1}{2}\\|\\nabla f(\\theta)\\|_{2}^{2}\\geq\\mu\\left(f(\\theta)-f(\\theta^{\\star})\\right), divide start_ARG 1 end_ARG start_ARG 2 end_ARG ‚à• ‚àá italic_f ( italic_Œ∏ ) ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ‚â• italic_Œº ( italic_f ( italic_Œ∏ ) - italic_f ( italic_Œ∏ start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ) ) ,"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS1.SSSx1", "title": "Step-Size Selection", "snippet": "Step-Size Selection The remaining question is what the step size h h italic_h should be? If we choose h h italic_h to be too small, the value of the function may decrease very slowly, as shown by the plot in the middle in Figure A.1 . If h h italic_h is too large, the value might not even decrease at all, as shown by the plot on the right in Figure A.1 . Figure A.1 : The effect of step size h h italic_h on the convergence of the gradient descent method. So the step size h h italic_h should be chosen based on the landscape of the function ‚Ñí ‚Äã ( Œ∏ k ) \\mathcal{L}(\\theta_{k}) caligraphic_L ( ital"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS2.SSSx1", "title": "Newton‚Äôs Method", "snippet": "Newton‚Äôs Method There are some smooth problems and strongly convex problems on which gradient descent nonetheless does quite poorly. For example, let Œª ‚â• 0 \\lambda\\geq 0 italic_Œª ‚â• 0 and let ‚Ñí Œª : ‚Ñù 2 ‚Üí ‚Ñù \\mathcal{L}_{\\lambda}\\colon\\mathbb{R}^{2}\\to\\mathbb{R} caligraphic_L start_POSTSUBSCRIPT italic_Œª end_POSTSUBSCRIPT : blackboard_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ‚Üí blackboard_R of the form ‚Ñí Œª ‚Äã ( Œ∏ ) = ‚Ñí Œª ‚Äã ( [ Œ∏ 1 Œ∏ 2 ] ) ‚âê 1 2 ‚Äã { ( 1 + Œª ) ‚Äã Œ∏ 1 2 + Œ∏ 2 2 } = 1 2 ‚Äã Œ∏ ‚ä§ ‚Äã [ 1 + Œª 0 0 1 ] ‚Äã Œ∏ . \\mathcal{L}_{\\lambda}(\\theta)=\\mathcal{L}_{\\lambda}\\left(\\begin{bmatrix}\\theta_{1}\\"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S1.SS2.SSSx2", "title": "PGD", "snippet": "PGD In practice, we do not have a second-order oracle which allows us to compute ‚àá 2 ‚Ñí ‚Äã ( Œ∏ ) \\nabla^{2}\\mathcal{L}(\\theta) ‚àá start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT caligraphic_L ( italic_Œ∏ ) . Instead, we can attempt to learn an approximation to it alongside the parameter update Œ∏ k + 1 \\theta_{k+1} italic_Œ∏ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT from Œ∏ k \\theta_{k} italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT . How do we learn an approximation to it? We shall find some equations which the Hessian‚Äôs inverse satisfies and then try to update our approximation so tha"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.SS1.SSSx1", "title": "Convergence of One-Timescale GDA to Stackelberg Equilibrium", "snippet": "Convergence of One-Timescale GDA to Stackelberg Equilibrium If T = 1 T=1 italic_T = 1 (i.e., named one-timescale because both Œ∏ \\theta italic_Œ∏ and Œ∑ \\eta italic_Œ∑ updates are of the same scale), then the GDA algorithm becomes Œ∏ k + 1 = Œ∏ k + h ‚Äã ‚àá Œ∏ ‚Ñí ‚Äã ( Œ∏ k , Œ∑ k ) , Œ∑ k + 1 = Œ∑ k ‚àí h ‚Äã ‚àá Œ∑ ‚Ñí ‚Äã ( Œ∏ k , Œ∑ k ) . \\theta_{k+1}=\\theta_{k}+h\\nabla_{\\theta}\\mathcal{L}(\\theta_{k},\\eta_{k}),\\qquad\\eta_{k+1}=\\eta_{k}-h\\nabla_{\\eta}\\mathcal{L}(\\theta_{k},\\eta_{k}). italic_Œ∏ start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = italic_Œ∏ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + italic_h ‚àá start_P"}, {"page": "Appendix A Optimization Methods", "href": "A1.html#S3.SS1.SSSx2", "title": "Local Convergence of Two-Timescale GDA to Stackelberg Equilibrium", "snippet": "Local Convergence of Two-Timescale GDA to Stackelberg Equilibrium Strong convexity/concavity is a global property, and none of the games we look into in this book have objectives which are globally strongly concave/strongly convex. In this case, the best we can hope for is local convergence to Stackelberg equilibria: if the parameters are initialized close to a Stackelberg equilibrium, then GDA can converge onto it, given an appropriate step size h h italic_h and timescale T T italic_T . In fact, our results also hold for a version of the local Stackelberg equilibrium called the differential S"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#top", "title": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "snippet": ""}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S1", "title": "B.1 Differential Entropy of Low-Dimensional Distributions", "snippet": "B.1 Differential Entropy of Low-Dimensional Distributions In this short appendix we discuss the differential entropy of low-dimensional distributions. By definition, the differential entropy of a random variable ùíô \\bm{x} bold_italic_x which does not have a density is ‚àí ‚àû -\\infty - ‚àû ; this includes all random variables supported on low-dimensional sets. The objective of this section is to discuss why this is a ‚Äúmorally correct‚Äù value. In fact, let ùíô \\bm{x} bold_italic_x be any random variable such that Assumption B.1 holds, the support ùíÆ \\mathcal{S} caligraphic_S of ùíô \\bm{x} bold_italic_x has "}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2", "title": "B.2 Diffusion and Denoising Processes", "snippet": "B.2 Diffusion and Denoising Processes In the main body ( Chapter 3 ), we considered a random variable ùíô \\bm{x} bold_italic_x , and a stochastic process defined by ( 3.2.1 ), i.e., ùíô t = ùíô + t ‚Äã ùíà , ‚àÄ t ‚àà [ 0 , T ] \\bm{x}_{t}=\\bm{x}+t\\bm{g},\\qquad\\forall t\\in[0,T] bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_x + italic_t bold_italic_g , ‚àÄ italic_t ‚àà [ 0 , italic_T ] (B.2.1) where ùíà ‚àº ùí© ‚Å° ( ùüé , ùë∞ ) \\bm{g}\\sim\\operatorname{\\mathcal{N}}(\\bm{0},\\bm{I}) bold_italic_g ‚àº caligraphic_N ( bold_0 , bold_italic_I ) independently of ùíô \\bm{x} bold_italic_x . The structure of th"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3", "title": "B.3 Lossy Coding and Sphere Packing", "snippet": "B.3 Lossy Coding and Sphere Packing In this section, we prove Theorem 3.6 . Following our conventions throughout this appendix, we write ùíÆ = Supp ‚Å° ( ùíô ) \\mathcal{S}=\\operatorname{Supp}(\\bm{x}) caligraphic_S = roman_Supp ( bold_italic_x ) for the compact support of the random variable ùíô \\bm{x} bold_italic_x . As foreshadowed, we will make a regularity assumption on the support set ùíÆ \\mathcal{S} caligraphic_S to prove Theorem 3.6 . One possibility for proceeding under minimal assumptions would be to instantiate the results of [ RBK18 , RKB23 ] in our setting, since these results apply to sets ùíÆ"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS1", "title": "B.2.1 Diffusion Process Increases Entropy Over Time", "snippet": "B.2.1 Diffusion Process Increases Entropy Over Time In this section appendix we provide a proof of Theorem B.2 . For convenience, we restate it as follows. Theorem B.2 (Diffusion Increases Entropy) . Let ùê± \\bm{x} bold_italic_x be any random variable such that Assumptions B.1 and B.2 hold, and let ( ùê± t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ‚àà [ 0 , italic_T ] end_POSTSUBSCRIPT be the stochastic process ( B.2.1 ). Then h ‚Äã ( ùíô s ) < h ‚Äã ( ùíô t ) , ‚àÄ s , t : 0 ‚â§ s < t ‚â§ T . h(\\bm{x}_{s})<h(\\bm{x}_{t}),"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS2", "title": "B.2.2 Denoising Process Reduces Entropy Over Time", "snippet": "B.2.2 Denoising Process Reduces Entropy Over Time Recall that in Section 3.2.1 we start with the random variable ùíô T \\bm{x}_{T} bold_italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT and iteratively denoise it using iterations of the form ùíô ^ s ‚âê ùîº ‚Å° [ ùíô s ‚à£ ùíô t = ùíô ^ t ] = s t ‚Äã ùíô ^ t + ( 1 ‚àí s t ) ‚Äã ùíô ¬Ø ‚àó ‚Äã ( t , ùíô ^ t ) . \\hat{\\bm{x}}_{s}\\doteq\\operatorname{\\mathbb{E}}[\\bm{x}_{s}\\mid\\bm{x}_{t}=\\hat{\\bm{x}}_{t}]=\\frac{s}{t}\\hat{\\bm{x}}_{t}+\\left(1-\\frac{s}{t}\\right)\\bar{\\bm{x}}^{\\ast}(t,\\hat{\\bm{x}}_{t}). over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_s end_POSTSUBSCR"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS3", "title": "B.2.3 Technical Lemmas and Intermediate Results", "snippet": "B.2.3 Technical Lemmas and Intermediate Results In this subsection we present technical results which power our main two conceptual theorems. Our presentation will be more or less standard for mathematics; we will start with the higher-level results first, and gradually move back to the more incremental results. The higher-level results will use the incremental results, and in this way we have an easy-to-read dependency ordering of the results: no result depends on those before it. Results which do not depend on each other are generally ordered by the place they appear in the above pair of pro"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.SS1", "title": "B.3.1 Proof of Relationship Between Rate Distortion and Covering", "snippet": "B.3.1 Proof of Relationship Between Rate Distortion and Covering We briefly sketch the proof, then proceed to establishing three fundamental lemmas, then give the proof. The proof will depend on notions introduced in the sketch below. Obtaining an upper bound on the rate distortion function ( 3.3.3 ) is straightforward: by the rate characterization (i.e., the rate distortion function is the minimum rate of a code for ùíô \\bm{x} bold_italic_x with expected squared ‚Ñì 2 \\ell^{2} roman_‚Ñì start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT distortion œµ \\epsilon italic_œµ ), upper bounding ‚Ñõ œµ ‚Äã ( ùíô ) \\mathcal"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.SS2", "title": "B.3.2 Proof of Lemma B.6", "snippet": "B.3.2 Proof of Lemma B.6 (Proof of Lemma B.6 ). It suffices to show that any code for ùíô \\bm{x} bold_italic_x with expected squared distortion œµ 2 \\epsilon^{2} italic_œµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT produces a code for ùíô Œ¥ \\bm{x}_{\\delta} bold_italic_x start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT with the same rate and distortion not much larger, for a suitable choice of Œ¥ \\delta italic_Œ¥ . So fix such a code for ùíô \\bm{x} bold_italic_x , achieving rate R R italic_R and expected squared distortion œµ 2 \\epsilon^{2} italic_œµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . We write ùíô "}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#p1", "title": "‚Äú The increase of disorder or entropy with time is one example of what is called an arrow of time, something that distinguishes the past from the future, giving a direction to time. ‚Äù ‚Äì A Brief Histor", "snippet": "‚Äú The increase of disorder or entropy with time is one example of what is called an arrow of time, something that distinguishes the past from the future, giving a direction to time. ‚Äù ‚Äì A Brief History of Time, Stephen Hawking"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#p2", "title": "In this appendix we provide proofs for several facts, mentioned in Chapter 3 , which are related to differential entropy, how it evolves under diffusion processes, and its connections to lossy coding.", "snippet": "In this appendix we provide proofs for several facts, mentioned in Chapter 3 , which are related to differential entropy, how it evolves under diffusion processes, and its connections to lossy coding. We will make the following mild assumption about the random variable representing the data source, denoted ùíô \\bm{x} bold_italic_x ."}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmassumption1.p1", "title": "ùíô \\bm{x} bold_italic_x is supported on a compact set ùíÆ ‚äÜ ‚Ñù D \\mathcal{S}\\subseteq\\mathbb{R}^{D} caligraphic_S ‚äÜ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT of radius at most R R it", "snippet": "ùíô \\bm{x} bold_italic_x is supported on a compact set ùíÆ ‚äÜ ‚Ñù D \\mathcal{S}\\subseteq\\mathbb{R}^{D} caligraphic_S ‚äÜ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT of radius at most R R italic_R , i.e., R ‚âê sup ùùÉ ‚àà ùíÆ ‚Äñ ùùÉ ‚Äñ 2 R\\doteq\\sup_{\\bm{\\xi}\\in\\mathcal{S}}\\|\\bm{\\xi}\\|_{2} italic_R ‚âê roman_sup start_POSTSUBSCRIPT bold_italic_Œæ ‚àà caligraphic_S end_POSTSUBSCRIPT ‚à• bold_italic_Œæ ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ."}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#p3", "title": "In particular, since compact sets in Euclidean space are bounded, it holds R < ‚àû R<\\infty italic_R < ‚àû . We will consistently use the notation B r ‚Äã ( ùùÉ ) ‚âê { ùíñ ‚àà ‚Ñù D : ‚Äñ ùùÉ ‚àí ùíñ ‚Äñ 2 ‚â§ r } B_{r}(\\bm{\\xi", "snippet": "In particular, since compact sets in Euclidean space are bounded, it holds R < ‚àû R<\\infty italic_R < ‚àû . We will consistently use the notation B r ‚Äã ( ùùÉ ) ‚âê { ùíñ ‚àà ‚Ñù D : ‚Äñ ùùÉ ‚àí ùíñ ‚Äñ 2 ‚â§ r } B_{r}(\\bm{\\xi})\\doteq\\{\\bm{u}\\in\\mathbb{R}^{D}\\colon\\|\\bm{\\xi}-\\bm{u}\\|_{2}\\leq r\\} italic_B start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ( bold_italic_Œæ ) ‚âê { bold_italic_u ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT : ‚à• bold_italic_Œæ - bold_italic_u ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ‚â§ italic_r } to denote the Euclidean ball of radius r r italic_r centered at ùùÉ \\bm{\\xi} bold_it"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#p4", "title": "Notice that this assumption holds for (almost) all variables we care about in practice, as it is (often) imposed by a normalization step during data pre-processing.", "snippet": "Notice that this assumption holds for (almost) all variables we care about in practice, as it is (often) imposed by a normalization step during data pre-processing."}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S1.p1", "title": "In this short appendix we discuss the differential entropy of low-dimensional distributions. By definition, the differential entropy of a random variable ùíô \\bm{x} bold_italic_x which does not have a d", "snippet": "In this short appendix we discuss the differential entropy of low-dimensional distributions. By definition, the differential entropy of a random variable ùíô \\bm{x} bold_italic_x which does not have a density is ‚àí ‚àû -\\infty - ‚àû ; this includes all random variables supported on low-dimensional sets. The objective of this section is to discuss why this is a ‚Äúmorally correct‚Äù value."}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S1.p2", "title": "In fact, let ùíô \\bm{x} bold_italic_x be any random variable such that Assumption B.1 holds, the support ùíÆ \\mathcal{S} caligraphic_S of ùíô \\bm{x} bold_italic_x has 0 volume. 1 1 1 Formally this means tha", "snippet": "In fact, let ùíô \\bm{x} bold_italic_x be any random variable such that Assumption B.1 holds, the support ùíÆ \\mathcal{S} caligraphic_S of ùíô \\bm{x} bold_italic_x has 0 volume. 1 1 1 Formally this means that ùíÆ \\mathcal{S} caligraphic_S is Borel measurable with Borel measure 0 . We will consider the case that ùíô \\bm{x} bold_italic_x is uniform on ùíÆ \\mathcal{S} caligraphic_S . 2 2 2 Say, w.r.t. the Hausdorff measure on ùíÆ \\mathcal{S} caligraphic_S . Our goal is to compute h ‚Äã ( ùíô ) h(\\bm{x}) italic_h ( bold_italic_x ) ."}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S1.p3", "title": "In this case, ùíô \\bm{x} bold_italic_x would not have a density; in the counterfactual world where we did not know h ‚Äã ( ùíô ) = ‚àí ‚àû h(\\bm{x})=-\\infty italic_h ( bold_italic_x ) = - ‚àû , we could not direc", "snippet": "In this case, ùíô \\bm{x} bold_italic_x would not have a density; in the counterfactual world where we did not know h ‚Äã ( ùíô ) = ‚àí ‚àû h(\\bm{x})=-\\infty italic_h ( bold_italic_x ) = - ‚àû , we could not directly define it using the standard definition of differential entropy. Instead, as in the rest of analysis and information theory it would be reasonable to consider the limit of entropies of successively better approximations ùíô Œµ \\bm{x}_{\\varepsilon} bold_italic_x start_POSTSUBSCRIPT italic_Œµ end_POSTSUBSCRIPT of ùíô \\bm{x} bold_italic_x which have densities, i.e., h ‚Äã ( ùíô ) ‚Äã ‚Äú=‚Äù ‚Äã lim Œµ ‚Üò 0 h ‚Äã ( ùíô "}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S1.p4", "title": "To this end, the basic idea is to take an Œµ \\varepsilon italic_Œµ -thickening of ùíÆ \\mathcal{S} caligraphic_S , say ùíÆ Œµ \\mathcal{S}_{\\varepsilon} caligraphic_S start_POSTSUBSCRIPT italic_Œµ end_POSTSUBSC", "snippet": "To this end, the basic idea is to take an Œµ \\varepsilon italic_Œµ -thickening of ùíÆ \\mathcal{S} caligraphic_S , say ùíÆ Œµ \\mathcal{S}_{\\varepsilon} caligraphic_S start_POSTSUBSCRIPT italic_Œµ end_POSTSUBSCRIPT defined as S Œµ = ‚ãÉ ùùÉ ‚àà ùíÆ B Œµ ‚Äã ( ùùÉ ) S_{\\varepsilon}=\\bigcup_{\\bm{\\xi}\\in\\mathcal{S}}B_{\\varepsilon}(\\bm{\\xi}) italic_S start_POSTSUBSCRIPT italic_Œµ end_POSTSUBSCRIPT = ‚ãÉ start_POSTSUBSCRIPT bold_italic_Œæ ‚àà caligraphic_S end_POSTSUBSCRIPT italic_B start_POSTSUBSCRIPT italic_Œµ end_POSTSUBSCRIPT ( bold_italic_Œæ ) (B.1.2) and visualized in Figure B.1 ."}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S1.p5", "title": "We will work with random variables whose support is ùíÆ Œµ \\mathcal{S}_{\\varepsilon} caligraphic_S start_POSTSUBSCRIPT italic_Œµ end_POSTSUBSCRIPT , which is fully-dimensional, and take the limit as Œµ ‚Üí 0", "snippet": "We will work with random variables whose support is ùíÆ Œµ \\mathcal{S}_{\\varepsilon} caligraphic_S start_POSTSUBSCRIPT italic_Œµ end_POSTSUBSCRIPT , which is fully-dimensional, and take the limit as Œµ ‚Üí 0 \\varepsilon\\to 0 italic_Œµ ‚Üí 0 . Indeed, define ùíô Œµ ‚àº ùí∞ ‚Å° ( ùíÆ Œµ ) \\bm{x}_{\\varepsilon}\\sim\\operatorname{\\mathcal{U}}(\\mathcal{S}_{\\varepsilon}) bold_italic_x start_POSTSUBSCRIPT italic_Œµ end_POSTSUBSCRIPT ‚àº caligraphic_U ( caligraphic_S start_POSTSUBSCRIPT italic_Œµ end_POSTSUBSCRIPT ) . Since ùíÆ Œµ \\mathcal{S}_{\\varepsilon} caligraphic_S start_POSTSUBSCRIPT italic_Œµ end_POSTSUBSCRIPT has positive vo"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S1.p6", "title": "The above calculation is actually a corollary of a much more famous and celebrated set of results about the maximum possible entropy of ùíô \\bm{x} bold_italic_x subject to certain constraints on the dis", "snippet": "The above calculation is actually a corollary of a much more famous and celebrated set of results about the maximum possible entropy of ùíô \\bm{x} bold_italic_x subject to certain constraints on the distribution of ùíô \\bm{x} bold_italic_x . We would be remiss to not provide the results here; the proofs are provided in Chapter 2 of [ PW22 ] , for example."}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmtheorem1.p1", "title": "Let ùê± \\bm{x} bold_italic_x be a random variable on ‚Ñù D \\mathbb{R}^{D} blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT . 1. If ùíô \\bm{x} bold_italic_x is supported on a compact set ùíÆ ‚äÜ ‚Ñù", "snippet": "Let ùê± \\bm{x} bold_italic_x be a random variable on ‚Ñù D \\mathbb{R}^{D} blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT . 1. If ùíô \\bm{x} bold_italic_x is supported on a compact set ùíÆ ‚äÜ ‚Ñù D \\mathcal{S}\\subseteq\\mathbb{R}^{D} caligraphic_S ‚äÜ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT (i.e., Assumption B.1 ) then h ‚Äã ( ùíô ) ‚â§ h ‚Äã ( ùí∞ ‚Å° ( ùíÆ ) ) = log ‚Å° vol ‚Å° ( ùíÆ ) . h(\\bm{x})\\leq h(\\operatorname{\\mathcal{U}}(\\mathcal{S}))=\\log\\operatorname{vol}(\\mathcal{S}). italic_h ( bold_italic_x ) ‚â§ italic_h ( caligraphic_U ( caligraphic_S ) ) = roman_log roman_vol ( caligraph"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S1.I1.i1.p1", "title": "If ùíô \\bm{x} bold_italic_x is supported on a compact set ùíÆ ‚äÜ ‚Ñù D \\mathcal{S}\\subseteq\\mathbb{R}^{D} caligraphic_S ‚äÜ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT (i.e., Assumption B.1", "snippet": "If ùíô \\bm{x} bold_italic_x is supported on a compact set ùíÆ ‚äÜ ‚Ñù D \\mathcal{S}\\subseteq\\mathbb{R}^{D} caligraphic_S ‚äÜ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT (i.e., Assumption B.1 ) then h ‚Äã ( ùíô ) ‚â§ h ‚Äã ( ùí∞ ‚Å° ( ùíÆ ) ) = log ‚Å° vol ‚Å° ( ùíÆ ) . h(\\bm{x})\\leq h(\\operatorname{\\mathcal{U}}(\\mathcal{S}))=\\log\\operatorname{vol}(\\mathcal{S}). italic_h ( bold_italic_x ) ‚â§ italic_h ( caligraphic_U ( caligraphic_S ) ) = roman_log roman_vol ( caligraphic_S ) . (B.1.9)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S1.I1.i2.p1", "title": "If ùíô \\bm{x} bold_italic_x has finite covariance such that, for a PSD matrix ùö∫ ‚àà ùñØùñ≤ùñ£ ‚Äã ( D ) \\bm{\\Sigma}\\in\\mathsf{PSD}(D) bold_Œ£ ‚àà sansserif_PSD ( italic_D ) , it holds Cov ‚Å° ( ùíô ) ‚™Ø ùö∫ \\operatorname{C", "snippet": "If ùíô \\bm{x} bold_italic_x has finite covariance such that, for a PSD matrix ùö∫ ‚àà ùñØùñ≤ùñ£ ‚Äã ( D ) \\bm{\\Sigma}\\in\\mathsf{PSD}(D) bold_Œ£ ‚àà sansserif_PSD ( italic_D ) , it holds Cov ‚Å° ( ùíô ) ‚™Ø ùö∫ \\operatorname{Cov}(\\bm{x})\\preceq\\bm{\\Sigma} roman_Cov ( bold_italic_x ) ‚™Ø bold_Œ£ (w.r.t. the PSD ordering, i.e., ùö∫ ‚àí Cov ‚Å° ( ùíô ) \\bm{\\Sigma}-\\operatorname{Cov}(\\bm{x}) bold_Œ£ - roman_Cov ( bold_italic_x ) is PSD), then h ‚Äã ( ùíô ) ‚â§ h ‚Äã ( ùí© ‚Å° ( ùüé , ùö∫ ) ) = 1 2 ‚Äã log ‚Å° ( ( 2 ‚Äã œÄ ‚Äã e ) D ‚Äã det ùö∫ ) . h(\\bm{x})\\leq h(\\operatorname{\\mathcal{N}}(\\bm{0},\\bm{\\Sigma}))=\\frac{1}{2}\\log((2\\pi e)^{D}\\det\\bm{\\Sigma}). italic_"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S1.I1.i3.p1", "title": "If ùíô \\bm{x} bold_italic_x has finite second moment such that, for a constant a ‚â• 0 a\\geq 0 italic_a ‚â• 0 , it holds ùîº ‚Å° ‚Äñ ùíô ‚Äñ 2 2 ‚â§ a \\operatorname{\\mathbb{E}}\\|\\bm{x}\\|_{2}^{2}\\leq a blackboard_E ‚à• bo", "snippet": "If ùíô \\bm{x} bold_italic_x has finite second moment such that, for a constant a ‚â• 0 a\\geq 0 italic_a ‚â• 0 , it holds ùîº ‚Å° ‚Äñ ùíô ‚Äñ 2 2 ‚â§ a \\operatorname{\\mathbb{E}}\\|\\bm{x}\\|_{2}^{2}\\leq a blackboard_E ‚à• bold_italic_x ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ‚â§ italic_a , then h ‚Äã ( ùíô ) ‚â§ h ‚Äã ( ùí© ‚Å° ( ùüé , a D ‚Äã ùë∞ ) ) = D 2 ‚Äã log ‚Å° 2 ‚Äã œÄ ‚Äã e ‚Äã a D . h(\\bm{x})\\leq h\\left(\\operatorname{\\mathcal{N}}\\left(\\bm{0},\\frac{a}{D}\\bm{I}\\right)\\right)=\\frac{D}{2}\\log\\frac{2\\pi ea}{D}. italic_h ( bold_italic_x ) ‚â§ italic_h ( caligraphic_N ( bold_0 , divide start_ARG ital"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.p1", "title": "In the main body ( Chapter 3 ), we considered a random variable ùíô \\bm{x} bold_italic_x , and a stochastic process defined by ( 3.2.1 ), i.e., ùíô t = ùíô + t ‚Äã ùíà , ‚àÄ t ‚àà [ 0 , T ] \\bm{x}_{t}=\\bm{x}+t\\bm{g", "snippet": "In the main body ( Chapter 3 ), we considered a random variable ùíô \\bm{x} bold_italic_x , and a stochastic process defined by ( 3.2.1 ), i.e., ùíô t = ùíô + t ‚Äã ùíà , ‚àÄ t ‚àà [ 0 , T ] \\bm{x}_{t}=\\bm{x}+t\\bm{g},\\qquad\\forall t\\in[0,T] bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_x + italic_t bold_italic_g , ‚àÄ italic_t ‚àà [ 0 , italic_T ] (B.2.1) where ùíà ‚àº ùí© ‚Å° ( ùüé , ùë∞ ) \\bm{g}\\sim\\operatorname{\\mathcal{N}}(\\bm{0},\\bm{I}) bold_italic_g ‚àº caligraphic_N ( bold_0 , bold_italic_I ) independently of ùíô \\bm{x} bold_italic_x ."}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.p2", "title": "The structure of this section is as follows. In Section B.2.1 we provide a formal theorem and crisp proof which shows that under Equation B.2.1 the entropy increases, i.e., d d ‚Äã t ‚Äã h ‚Äã ( ùíô t ) > 0 \\", "snippet": "The structure of this section is as follows. In Section B.2.1 we provide a formal theorem and crisp proof which shows that under Equation B.2.1 the entropy increases, i.e., d d ‚Äã t ‚Äã h ‚Äã ( ùíô t ) > 0 \\frac{\\mathrm{d}}{\\mathrm{d}t}h(\\bm{x}_{t})>0 divide start_ARG roman_d end_ARG start_ARG roman_d italic_t end_ARG italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) > 0 . In Section B.2.2 we provide a formal theorem and crisp proof which shows that under Equation B.2.1 , the entropy decreases during denoising, i.e., h ‚Äã ( ùîº ‚Å° [ ùíô s ‚à£ ùíô t ] ) < h ‚Äã ( ùíô t ) h(\\operatorname{\\mat"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.p3", "title": "Before we start, we introduce some key notations. First, let œÜ t \\varphi_{t} italic_œÜ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT be the density of ùí© ‚Å° ( ùüé , t 2 ‚Äã ùë∞ ) \\operatorname{\\mathcal{N}}(\\b", "snippet": "Before we start, we introduce some key notations. First, let œÜ t \\varphi_{t} italic_œÜ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT be the density of ùí© ‚Å° ( ùüé , t 2 ‚Äã ùë∞ ) \\operatorname{\\mathcal{N}}(\\bm{0},t^{2}\\bm{I}) caligraphic_N ( bold_0 , italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) , i.e., œÜ t ‚Äã ( ùùÉ ) ‚âê 1 ( 2 ‚Äã œÄ ) D / 2 ‚Äã t D ‚Äã exp ‚Å° ( ‚àí ‚Äñ ùùÉ ‚Äñ 2 2 2 ‚Äã t 2 ) . \\varphi_{t}(\\bm{\\xi})\\doteq\\frac{1}{(2\\pi)^{D/2}t^{D}}\\exp\\left(-\\frac{\\|\\bm{\\xi}\\|_{2}^{2}}{2t^{2}}\\right). italic_œÜ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) ‚âê divide start_ARG 1 end_"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.p4", "title": "However, we also need to add an assumption about the smoothness of the distribution of ùíô \\bm{x} bold_italic_x , which will eliminate some technical problems that occur around t = 0 t=0 italic_t = 0 wi", "snippet": "However, we also need to add an assumption about the smoothness of the distribution of ùíô \\bm{x} bold_italic_x , which will eliminate some technical problems that occur around t = 0 t=0 italic_t = 0 with low-dimensional distributions. 3 3 3 As then various quantities become highly irregular and dealing with them would require significant additional analysis. Despite this, we expect that our results hold under milder assumptions with additional work. For now, let us assume:"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmassumption2.p1", "title": "ùíô \\bm{x} bold_italic_x has a twice continuously differentiable density, denoted p p italic_p .", "snippet": "ùíô \\bm{x} bold_italic_x has a twice continuously differentiable density, denoted p p italic_p ."}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS1.p1", "title": "In this section appendix we provide a proof of Theorem B.2 . For convenience, we restate it as follows.", "snippet": "In this section appendix we provide a proof of Theorem B.2 . For convenience, we restate it as follows."}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmtheorem2.p1", "title": "Let ùê± \\bm{x} bold_italic_x be any random variable such that Assumptions B.1 and B.2 hold, and let ( ùê± t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSU", "snippet": "Let ùê± \\bm{x} bold_italic_x be any random variable such that Assumptions B.1 and B.2 hold, and let ( ùê± t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ‚àà [ 0 , italic_T ] end_POSTSUBSCRIPT be the stochastic process ( B.2.1 ). Then h ‚Äã ( ùíô s ) < h ‚Äã ( ùíô t ) , ‚àÄ s , t : 0 ‚â§ s < t ‚â§ T . h(\\bm{x}_{s})<h(\\bm{x}_{t}),\\qquad\\forall s,t\\colon 0\\leq s<t\\leq T. italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) < italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , ‚àÄ itali"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS1.p2", "title": "Before we start, we must ask: when does the inequality in ( B.2.4 ) make sense? We will show in Lemma B.1 that under our assumptions, the differential entropy is well-defined, is never + ‚àû +\\infty + ‚àû", "snippet": "Before we start, we must ask: when does the inequality in ( B.2.4 ) make sense? We will show in Lemma B.1 that under our assumptions, the differential entropy is well-defined, is never + ‚àû +\\infty + ‚àû , and for t > 0 t>0 italic_t > 0 is finite, so the (strict) inequality in ( B.2.4 ) makes sense."}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS1.p3", "title": "The question of well-definedness aside, the crux of this proof is to show that the density p t p_{t} italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT of ùíô t \\bm{x}_{t} bold_italic_x start_POSTS", "snippet": "The question of well-definedness aside, the crux of this proof is to show that the density p t p_{t} italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT of ùíô t \\bm{x}_{t} bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT satisfies a particular partial differential equation, which is very similar to the heat equation . The heat equation is a famous PDE which describes the diffusion of heat through space. This intuitively should make sense, and paints a mental picture: as the time t t italic_t increases, the probability from the original (perhaps tightly concentrated) ùíô \\bm{x} bol"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS1.p4", "title": "Such PDEs for p t p_{t} italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , known as Fokker-Planck equations for more general stochastic processes, are very powerful tools, as they allow us to d", "snippet": "Such PDEs for p t p_{t} italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , known as Fokker-Planck equations for more general stochastic processes, are very powerful tools, as they allow us to describe the instantaneous temporal derivatives of p t p_{t} italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT in terms of the instantaneous spatial derivatives of p t p_{t} italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , and vice versa, providing a concise description of the regularity and dynamics of p t p_{t} italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . Once we obtai"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS1.p5", "title": "The description of the PDE involves a mathematical object called the Laplacian Œî \\Delta roman_Œî . Recall from your multivariable calculus class that the Laplacian operating on a differentiable-in-time", "snippet": "The description of the PDE involves a mathematical object called the Laplacian Œî \\Delta roman_Œî . Recall from your multivariable calculus class that the Laplacian operating on a differentiable-in-time and twice-differentiable-in-space function f : ( 0 , T ) √ó ‚Ñù D ‚Üí ‚Ñù f\\colon(0,T)\\times\\mathbb{R}^{D}\\to\\mathbb{R} italic_f : ( 0 , italic_T ) √ó blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT ‚Üí blackboard_R is given by Œî ‚Äã f t ‚Äã ( ùùÉ ) = tr ‚Å° ( ‚àá 2 f t ‚Äã ( ùùÉ ) ) = ‚àë i = 1 D ‚àÇ 2 f t ‚àÇ Œæ i 2 ‚Äã ( ùùÉ ) . \\Delta f_{t}(\\bm{\\xi})=\\operatorname{tr}(\\nabla^{2}f_{t}(\\bm{\\xi}))=\\sum_{i=1}^{D}\\f"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS1.p6", "title": "Namely, from using the integral representation of p t p_{t} italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and differentiating under the integral, we can compute the derivatives of p t p_{t} ", "snippet": "Namely, from using the integral representation of p t p_{t} italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and differentiating under the integral, we can compute the derivatives of p t p_{t} italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT (which we do in Proposition B.1 ) and observe that p t p_{t} italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT satisfies the heat-like PDE ‚àÇ p t ‚àÇ t ‚Äã ( ùùÉ ) = t ‚Äã Œî ‚Äã p t ‚Äã ( ùùÉ ) . \\frac{\\partial p_{t}}{\\partial t}(\\bm{\\xi})=t\\Delta p_{t}(\\bm{\\xi}). divide start_ARG ‚àÇ italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS1.p7", "title": "To complete the proof we just use the fundamental theorem of calculus h ‚Äã ( ùíô t ) = h ‚Äã ( ùíô s ) + ‚à´ s t d d ‚Äã u ‚Äã h ‚Äã ( ùíô u ) ‚Äã d u > h ‚Äã ( ùíô s ) , h(\\bm{x}_{t})=h(\\bm{x}_{s})+\\int_{s}^{t}\\frac{\\mathr", "snippet": "To complete the proof we just use the fundamental theorem of calculus h ‚Äã ( ùíô t ) = h ‚Äã ( ùíô s ) + ‚à´ s t d d ‚Äã u ‚Äã h ‚Äã ( ùíô u ) ‚Äã d u > h ‚Äã ( ùíô s ) , h(\\bm{x}_{t})=h(\\bm{x}_{s})+\\int_{s}^{t}\\frac{\\mathrm{d}}{\\mathrm{d}u}h(\\bm{x}_{u})\\mathrm{d}u>h(\\bm{x}_{s}), italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) + ‚à´ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT divide start_ARG roman_d end_ARG start_ARG roman_d italic_u end_ARG italic_h ( bold_italic"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS2.p1", "title": "Recall that in Section 3.2.1 we start with the random variable ùíô T \\bm{x}_{T} bold_italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT and iteratively denoise it using iterations of the form ùíô ^ s", "snippet": "Recall that in Section 3.2.1 we start with the random variable ùíô T \\bm{x}_{T} bold_italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT and iteratively denoise it using iterations of the form ùíô ^ s ‚âê ùîº ‚Å° [ ùíô s ‚à£ ùíô t = ùíô ^ t ] = s t ‚Äã ùíô ^ t + ( 1 ‚àí s t ) ‚Äã ùíô ¬Ø ‚àó ‚Äã ( t , ùíô ^ t ) . \\hat{\\bm{x}}_{s}\\doteq\\operatorname{\\mathbb{E}}[\\bm{x}_{s}\\mid\\bm{x}_{t}=\\hat{\\bm{x}}_{t}]=\\frac{s}{t}\\hat{\\bm{x}}_{t}+\\left(1-\\frac{s}{t}\\right)\\bar{\\bm{x}}^{\\ast}(t,\\hat{\\bm{x}}_{t}). over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ‚âê blackboard_E [ bold_italic_x start_POSTSUBSC"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS2.p2", "title": "Before we go about doing this, we make several remarks about the problem statement. First, Tweedie‚Äôs formula ( 3.2.20 ) says that ùíô ¬Ø ‚àó ‚Äã ( t , ùíô t ) = ùíô t + t 2 ‚Äã ‚àá p t ‚Äã ( ùíô t ) , \\bar{\\bm{x}}^{\\ast", "snippet": "Before we go about doing this, we make several remarks about the problem statement. First, Tweedie‚Äôs formula ( 3.2.20 ) says that ùíô ¬Ø ‚àó ‚Äã ( t , ùíô t ) = ùíô t + t 2 ‚Äã ‚àá p t ‚Äã ( ùíô t ) , \\bar{\\bm{x}}^{\\ast}(t,\\bm{x}_{t})=\\bm{x}_{t}+t^{2}\\nabla p_{t}(\\bm{x}_{t}), over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ‚àá italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_i"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS2.p3", "title": "We want to prove that h ‚Äã ( ùîº ‚Å° [ ùíô s ‚à£ ùíô t ] ) < h ‚Äã ( ùíô t ) h(\\operatorname{\\mathbb{E}}[\\bm{x}_{s}\\mid\\bm{x}_{t}])<h(\\bm{x}_{t}) italic_h ( blackboard_E [ bold_italic_x start_POSTSUBSCRIPT italic_s ", "snippet": "We want to prove that h ‚Äã ( ùîº ‚Å° [ ùíô s ‚à£ ùíô t ] ) < h ‚Äã ( ùíô t ) h(\\operatorname{\\mathbb{E}}[\\bm{x}_{s}\\mid\\bm{x}_{t}])<h(\\bm{x}_{t}) italic_h ( blackboard_E [ bold_italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] ) < italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , i.e., formally:"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmtheorem3.p1", "title": "Let ùê± \\bm{x} bold_italic_x be any random variable such that Assumptions B.1 and B.2 hold, and let ( ùê± t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSU", "snippet": "Let ùê± \\bm{x} bold_italic_x be any random variable such that Assumptions B.1 and B.2 hold, and let ( ùê± t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ‚àà [ 0 , italic_T ] end_POSTSUBSCRIPT be the stochastic process ( B.2.1 ). Then h ( ùîº [ ùíô s ‚à£ ùíô t ] ) < h ( ùíô t ) , ‚àÄ s , t ‚àà [ 0 , T ] : 0 < t ‚â§ R 2 ‚Äã D , 0 ‚â§ s < t ‚ãÖ min { 1 , R 2 / D ‚àí 2 ‚Äã t 2 R 2 / D ‚àí t 2 } . h(\\operatorname{\\mathbb{E}}[\\bm{x}_{s}\\mid\\bm{x}_{t}])<h(\\bm{x}_{t}),\\qquad\\forall s,t\\in[0,T]\\colon\\quad 0<t\\leq\\frac{R}{\\sqrt{2D}},\\quad 0\\leq s<t"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS2.p4", "title": "This proof uses two main ideas: 1. First, write down a density for ùîº ‚Å° [ ùíô s ‚à£ ùíô t ] \\operatorname{\\mathbb{E}}[\\bm{x}_{s}\\mid\\bm{x}_{t}] blackboard_E [ bold_italic_x start_POSTSUBSCRIPT italic_s end_P", "snippet": "This proof uses two main ideas: 1. First, write down a density for ùîº ‚Å° [ ùíô s ‚à£ ùíô t ] \\operatorname{\\mathbb{E}}[\\bm{x}_{s}\\mid\\bm{x}_{t}] blackboard_E [ bold_italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] using a change-of-variables formula. 2. Second, bound this density to control the entropy. The change of variables is justified by Corollary B.1 , which was originally derived in [ Gri11 ] ."}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.I1.i1.p1", "title": "First, write down a density for ùîº ‚Å° [ ùíô s ‚à£ ùíô t ] \\operatorname{\\mathbb{E}}[\\bm{x}_{s}\\mid\\bm{x}_{t}] blackboard_E [ bold_italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ‚à£ bold_italic_x start_", "snippet": "First, write down a density for ùîº ‚Å° [ ùíô s ‚à£ ùíô t ] \\operatorname{\\mathbb{E}}[\\bm{x}_{s}\\mid\\bm{x}_{t}] blackboard_E [ bold_italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] using a change-of-variables formula."}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.I1.i2.p1", "title": "Second, bound this density to control the entropy.", "snippet": "Second, bound this density to control the entropy."}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS2.p5", "title": "We execute these ideas now. From Corollary B.1 , we obtain that the function ùíô ¬Ø \\bar{\\bm{x}} over¬Ø start_ARG bold_italic_x end_ARG defined as ùíô ¬Ø ‚Äã ( ùùÉ ) ‚âê ùîº ‚Å° [ ùíô s ‚à£ ùíô t = ùùÉ ] \\bar{\\bm{x}}(\\bm{\\xi}", "snippet": "We execute these ideas now. From Corollary B.1 , we obtain that the function ùíô ¬Ø \\bar{\\bm{x}} over¬Ø start_ARG bold_italic_x end_ARG defined as ùíô ¬Ø ‚Äã ( ùùÉ ) ‚âê ùîº ‚Å° [ ùíô s ‚à£ ùíô t = ùùÉ ] \\bar{\\bm{x}}(\\bm{\\xi})\\doteq\\operatorname{\\mathbb{E}}[\\bm{x}_{s}\\mid\\bm{x}_{t}=\\bm{\\xi}] over¬Ø start_ARG bold_italic_x end_ARG ( bold_italic_Œæ ) ‚âê blackboard_E [ bold_italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_Œæ ] is differentiable, injective, and thus invertible on its range, which we henceforth denote ùí≥ ‚äÜ ‚Ñù D \\mathcal{X}\\subset"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS2.p6", "title": "By concavity, one has ‚àí x ‚Äã log ‚Å° x ‚â§ 1 ‚àí x -x\\log x\\leq 1-x - italic_x roman_log italic_x ‚â§ 1 - italic_x for every x ‚â• 0 x\\geq 0 italic_x ‚â• 0 . Hence h ‚Äã ( ùíô ¬Ø ‚Äã ( ùíô t ) ) ‚àí h ‚Äã ( ùíô t ) \\displaystyle", "snippet": "By concavity, one has ‚àí x ‚Äã log ‚Å° x ‚â§ 1 ‚àí x -x\\log x\\leq 1-x - italic_x roman_log italic_x ‚â§ 1 - italic_x for every x ‚â• 0 x\\geq 0 italic_x ‚â• 0 . Hence h ‚Äã ( ùíô ¬Ø ‚Äã ( ùíô t ) ) ‚àí h ‚Äã ( ùíô t ) \\displaystyle h(\\bar{\\bm{x}}(\\bm{x}_{t}))-h(\\bm{x}_{t}) italic_h ( over¬Ø start_ARG bold_italic_x end_ARG ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ) - italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = ‚àí ‚à´ ùí≥ ( p t ‚àò ùíô ¬Ø ‚àí 1 ) ‚Äã ( ùùÉ ) det ( ùíô ¬Ø ‚Ä≤ ‚Äã ( ùíô ¬Ø ‚àí 1 ‚Äã ( ùùÉ ) ) ) ‚Äã log ‚Å° ( 1 det ( ùíô ¬Ø ‚Ä≤ ‚Äã ( ùíô ¬Ø ‚àí 1 ‚Äã ( ùùÉ ) ) ) ) ‚Äã d ùùÉ \\displaystyle=-\\int_{\\mathcal{X}}\\frac{"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS2.p7", "title": "Applying this bound, we obtain ‚à´ ‚Ñù D p t ‚Äã ( ùùÉ ) ‚Äã ( 1 + ( 1 ‚àí s t ) ‚Äã t 2 D ‚Äã Œî ‚Äã log ‚Å° p t ‚Äã ( ùùÉ ) ) D ‚Äã d ùùÉ \\displaystyle\\int_{\\mathbb{R}^{D}}p_{t}(\\bm{\\xi})\\left(1+\\frac{\\left(1-\\frac{s}{t}\\right)", "snippet": "Applying this bound, we obtain ‚à´ ‚Ñù D p t ‚Äã ( ùùÉ ) ‚Äã ( 1 + ( 1 ‚àí s t ) ‚Äã t 2 D ‚Äã Œî ‚Äã log ‚Å° p t ‚Äã ( ùùÉ ) ) D ‚Äã d ùùÉ \\displaystyle\\int_{\\mathbb{R}^{D}}p_{t}(\\bm{\\xi})\\left(1+\\frac{\\left(1-\\frac{s}{t}\\right)t^{2}}{D}\\Delta\\log p_{t}(\\bm{\\xi})\\right)^{D}\\mathrm{d}\\bm{\\xi} ‚à´ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) ( 1 + divide start_ARG ( 1 - divide start_ARG italic_s end_ARG start_ARG italic_t end_ARG ) italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG star"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS2.p8", "title": "Notice that the bounds for s s italic_s and t t italic_t depend on the radius R R italic_R of the data distribution, and are not so general as the bounds in Theorem B.2 . The result is actually ‚Äúas ge", "snippet": "Notice that the bounds for s s italic_s and t t italic_t depend on the radius R R italic_R of the data distribution, and are not so general as the bounds in Theorem B.2 . The result is actually ‚Äúas general as needed‚Äù in the following sense. Note that if ùíô \\bm{x} bold_italic_x has a twice continuously differentiable density supported on the ball of radius R R italic_R centered at ùüé \\bm{0} bold_0 , then it does for 2 ‚Äã R 2R 2 italic_R , and 3 ‚Äã R 3R 3 italic_R , and so on, i.e., for any ball of radius R ‚Ä≤ > R R^{\\prime}>R italic_R start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT > italic_R . Thus one"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS3.p1", "title": "In this subsection we present technical results which power our main two conceptual theorems. Our presentation will be more or less standard for mathematics; we will start with the higher-level result", "snippet": "In this subsection we present technical results which power our main two conceptual theorems. Our presentation will be more or less standard for mathematics; we will start with the higher-level results first, and gradually move back to the more incremental results. The higher-level results will use the incremental results, and in this way we have an easy-to-read dependency ordering of the results: no result depends on those before it. Results which do not depend on each other are generally ordered by the place they appear in the above pair of proofs."}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS3.SSSx1.p1", "title": "We first show that the entropy exists along the stochastic process and is finite.", "snippet": "We first show that the entropy exists along the stochastic process and is finite."}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmlemma1.p1", "title": "Let ùê± \\bm{x} bold_italic_x be any random variable, and let ( ùê± t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ", "snippet": "Let ùê± \\bm{x} bold_italic_x be any random variable, and let ( ùê± t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ‚àà [ 0 , italic_T ] end_POSTSUBSCRIPT be the stochastic process ( B.2.1 ). 1. For t > 0 t>0 italic_t > 0 , the differential entropy h ‚Äã ( ùíô t ) h(\\bm{x}_{t}) italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) exists and is > ‚àí ‚àû >-\\infty > - ‚àû . 2. If in addition Assumption B.1 holds for ùíô \\bm{x} bold_italic_x , then h ‚Äã ( ùíô ) < ‚àû h(\\bm{x})<\\infty italic_h ( bold_italic_x ) <"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.I2.i1.p1", "title": "For t > 0 t>0 italic_t > 0 , the differential entropy h ‚Äã ( ùíô t ) h(\\bm{x}_{t}) italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) exists and is > ‚àí ‚àû >-\\infty > - ‚àû .", "snippet": "For t > 0 t>0 italic_t > 0 , the differential entropy h ‚Äã ( ùíô t ) h(\\bm{x}_{t}) italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) exists and is > ‚àí ‚àû >-\\infty > - ‚àû ."}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.I2.i2.p1", "title": "If in addition Assumption B.1 holds for ùíô \\bm{x} bold_italic_x , then h ‚Äã ( ùíô ) < ‚àû h(\\bm{x})<\\infty italic_h ( bold_italic_x ) < ‚àû and h ‚Äã ( ùíô t ) < ‚àû h(\\bm{x}_{t})\\ <\\infty italic_h ( bold_italic_x ", "snippet": "If in addition Assumption B.1 holds for ùíô \\bm{x} bold_italic_x , then h ‚Äã ( ùíô ) < ‚àû h(\\bm{x})<\\infty italic_h ( bold_italic_x ) < ‚àû and h ‚Äã ( ùíô t ) < ‚àû h(\\bm{x}_{t})\\ <\\infty italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) < ‚àû ."}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS3.SSSx1.p2", "title": "To prove Lemma B.1 .1, we use a classic yet tedious analysis argument. Since ùíô t \\bm{x}_{t} bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT has a density, we can write h ‚Äã ( ùíô t ) = ‚àí ‚à´ ‚Ñù", "snippet": "To prove Lemma B.1 .1, we use a classic yet tedious analysis argument. Since ùíô t \\bm{x}_{t} bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT has a density, we can write h ‚Äã ( ùíô t ) = ‚àí ‚à´ ‚Ñù D p t ‚Äã ( ùùÉ ) ‚Äã log ‚Å° p t ‚Äã ( ùùÉ ) ‚Äã d ùùÉ . h(\\bm{x}_{t})=-\\int_{\\mathbb{R}^{D}}p_{t}(\\bm{\\xi})\\log p_{t}(\\bm{\\xi})\\mathrm{d}\\bm{\\xi}. italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = - ‚à´ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) roman_log "}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS3.SSSx1.p3", "title": "In order to show that h ‚Äã ( ùíô t ) h(\\bm{x}_{t}) italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) is well-defined, we need to show that ‚à´ ‚Ñù D g + ‚Äã ( ùùÉ ) ‚Äã d ùùÉ < ‚àû \\int_{\\mathb", "snippet": "In order to show that h ‚Äã ( ùíô t ) h(\\bm{x}_{t}) italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) is well-defined, we need to show that ‚à´ ‚Ñù D g + ‚Äã ( ùùÉ ) ‚Äã d ùùÉ < ‚àû \\int_{\\mathbb{R}^{D}}g_{+}(\\bm{\\xi})\\mathrm{d}\\bm{\\xi}<\\infty ‚à´ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_g start_POSTSUBSCRIPT + end_POSTSUBSCRIPT ( bold_italic_Œæ ) roman_d bold_italic_Œæ < ‚àû or ‚à´ ‚Ñù D g ‚àí ‚Äã ( ùùÉ ) ‚Äã d ùùÉ < ‚àû \\int_{\\mathbb{R}^{D}}g_{-}(\\bm{\\xi})\\mathrm{d}\\bm{\\xi}<\\infty ‚à´ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT ita"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.I3.i1.p1", "title": "If C t < 1 C_{t}<1 italic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT < 1 , then p t ‚Äã ( ùùÉ ) < 1 p_{t}(\\bm{\\xi})<1 italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) < 1 , ", "snippet": "If C t < 1 C_{t}<1 italic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT < 1 , then p t ‚Äã ( ùùÉ ) < 1 p_{t}(\\bm{\\xi})<1 italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) < 1 , so the indicator is never 1 1 1 , hence g ‚àí = 0 g_{-}=0 italic_g start_POSTSUBSCRIPT - end_POSTSUBSCRIPT = 0 identically and its integral is also 0 ."}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.I3.i2.p1", "title": "If C t ‚â• 1 C_{t}\\geq 1 italic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ‚â• 1 , then log ‚Å° C t ‚â• 0 \\log C_{t}\\geq 0 roman_log italic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ‚â• 0 , so sinc", "snippet": "If C t ‚â• 1 C_{t}\\geq 1 italic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ‚â• 1 , then log ‚Å° C t ‚â• 0 \\log C_{t}\\geq 0 roman_log italic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ‚â• 0 , so since the logarithm is monotonically increasing, ‚à´ ‚Ñù D g ‚àí ‚Äã ( ùùÉ ) ‚Äã d ùùÉ \\displaystyle\\int_{\\mathbb{R}^{D}}g_{-}(\\bm{\\xi})\\mathrm{d}\\bm{\\xi} ‚à´ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_g start_POSTSUBSCRIPT - end_POSTSUBSCRIPT ( bold_italic_Œæ ) roman_d bold_italic_Œæ = ‚à´ ‚Ñù D ùüè ‚Äã ( p t ‚Äã ( ùùÉ ) ‚â• 1 ) ‚Äã p t ‚Äã ( ùùÉ ) ‚Äã log ‚Å° p t ‚Äã ( ùùÉ ) ‚Äã d "}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS3.SSSx1.p4", "title": "To prove Lemma B.1 .2, suppose that Assumption B.1 holds. We want to show that h ‚Äã ( ùíô ) < ‚àû h(\\bm{x})<\\infty italic_h ( bold_italic_x ) < ‚àû and h ‚Äã ( ùíô t ) < ‚àû h(\\bm{x}_{t})<\\infty italic_h ( bold_it", "snippet": "To prove Lemma B.1 .2, suppose that Assumption B.1 holds. We want to show that h ‚Äã ( ùíô ) < ‚àû h(\\bm{x})<\\infty italic_h ( bold_italic_x ) < ‚àû and h ‚Äã ( ùíô t ) < ‚àû h(\\bm{x}_{t})<\\infty italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) < ‚àû . The mechanism for doing this is the same, and involves the maximum entropy result Theorem B.1 . Namely, since ùíô \\bm{x} bold_italic_x is absolutely bounded, it has a finite covariance which we will denote ùö∫ \\bm{\\Sigma} bold_Œ£ . Then the covariance of ùíô t \\bm{x}_{t} bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is ùö∫ + t 2 ‚Äã"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS3.SSSx2.p1", "title": "Finally, we fill in the integration-by-parts argument alluded to in the proofs of Theorems B.2 and B.3 . The argument is conceptually pretty simple but requires some technical estimates to show that t", "snippet": "Finally, we fill in the integration-by-parts argument alluded to in the proofs of Theorems B.2 and B.3 . The argument is conceptually pretty simple but requires some technical estimates to show that the boundary term in the integration-by-parts vanishes."}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmlemma2.p1", "title": "Let ùê± \\bm{x} bold_italic_x be any random variable such that Assumptions B.1 and B.2 hold, and let ( ùê± t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSU", "snippet": "Let ùê± \\bm{x} bold_italic_x be any random variable such that Assumptions B.1 and B.2 hold, and let ( ùê± t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ‚àà [ 0 , italic_T ] end_POSTSUBSCRIPT be the stochastic process ( B.2.1 ). For t ‚â• 0 t\\geq 0 italic_t ‚â• 0 , let p t p_{t} italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT be the density of ùê± t \\bm{x}_{t} bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . Then for a constant c ‚àà ‚Ñù c\\in\\mathbb{R} italic_c ‚àà blackboard_R it holds ‚à´ ‚Ñù D Œî ‚Äã p t "}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS3.SSSx2.p2", "title": "The basic idea of this proof is in two steps: ‚Ä¢ First, apply Green‚Äôs theorem to do integration by parts over a compact set; ‚Ä¢ Second, send the radius of this compact set to + ‚àû +\\infty + ‚àû , to get in", "snippet": "The basic idea of this proof is in two steps: ‚Ä¢ First, apply Green‚Äôs theorem to do integration by parts over a compact set; ‚Ä¢ Second, send the radius of this compact set to + ‚àû +\\infty + ‚àû , to get integrals over all of ‚Ñù D \\mathbb{R}^{D} blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT . Green‚Äôs theorem says that for any compact set ùí¶ ‚äÜ ‚Ñù D \\mathcal{K}\\subseteq\\mathbb{R}^{D} caligraphic_K ‚äÜ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT , twice continuously differentiable œï : ‚Ñù D ‚Üí ‚Ñù \\phi\\colon\\mathbb{R}^{D}\\to\\mathbb{R} italic_œï : blackboard_R start_POSTSUPERS"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.I4.i1.p1", "title": "First, apply Green‚Äôs theorem to do integration by parts over a compact set;", "snippet": "First, apply Green‚Äôs theorem to do integration by parts over a compact set;"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.I4.i2.p1", "title": "Second, send the radius of this compact set to + ‚àû +\\infty + ‚àû , to get integrals over all of ‚Ñù D \\mathbb{R}^{D} blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT .", "snippet": "Second, send the radius of this compact set to + ‚àû +\\infty + ‚àû , to get integrals over all of ‚Ñù D \\mathbb{R}^{D} blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT ."}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS3.SSSx3.p1", "title": "Here we provide some results used in the proof of Theorem B.3 which are appropriate generalizations of corresponding results in [ Gri11 ] .", "snippet": "Here we provide some results used in the proof of Theorem B.3 which are appropriate generalizations of corresponding results in [ Gri11 ] ."}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmlemma3.p1", "title": "Let ùê± \\bm{x} bold_italic_x be any random variable such that Assumptions B.1 and B.2 hold, and let ( ùê± t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSU", "snippet": "Let ùê± \\bm{x} bold_italic_x be any random variable such that Assumptions B.1 and B.2 hold, and let ( ùê± t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ‚àà [ 0 , italic_T ] end_POSTSUBSCRIPT be the stochastic process ( B.2.1 ). Let s , t ‚àà [ 0 , T ] s,t\\in[0,T] italic_s , italic_t ‚àà [ 0 , italic_T ] be such that 0 ‚â§ s < t ‚â§ T 0\\leq s<t\\leq T 0 ‚â§ italic_s < italic_t ‚â§ italic_T , and let ùê± ¬Ø ‚Äã ( ùõè ) ‚âê ùîº ‚Å° [ ùê± s ‚à£ ùê± t = ùõè ] \\bar{\\bm{x}}(\\bm{\\xi})\\doteq\\operatorname{\\mathbb{E}}[\\bm{x}_{s}\\mid\\bm{x}_{t}=\\bm{\\xi}] o"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS3.SSSx3.p2", "title": "We have ùíô ¬Ø ‚Ä≤ ‚Äã ( ùùÉ ) = ùë∞ + ( 1 ‚àí s t ) ‚Äã t 2 ‚Äã ‚àá 2 log ‚Å° p t ‚Äã ( ùùÉ ) . \\bar{\\bm{x}}^{\\prime}(\\bm{\\xi})=\\bm{I}+\\left(1-\\frac{s}{t}\\right)t^{2}\\nabla^{2}\\log p_{t}(\\bm{\\xi}). over¬Ø start_ARG bold_itali", "snippet": "We have ùíô ¬Ø ‚Ä≤ ‚Äã ( ùùÉ ) = ùë∞ + ( 1 ‚àí s t ) ‚Äã t 2 ‚Äã ‚àá 2 log ‚Å° p t ‚Äã ( ùùÉ ) . \\bar{\\bm{x}}^{\\prime}(\\bm{\\xi})=\\bm{I}+\\left(1-\\frac{s}{t}\\right)t^{2}\\nabla^{2}\\log p_{t}(\\bm{\\xi}). over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( bold_italic_Œæ ) = bold_italic_I + ( 1 - divide start_ARG italic_s end_ARG start_ARG italic_t end_ARG ) italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ‚àá start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) . (B.2.90) Here we expand ‚àá 2 log ‚Å° p t ‚Äã ( ùùÉ ) = p t ‚Äã "}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmlemma4.p1", "title": "Let f : ‚Ñù D ‚Üí ‚Ñù D f\\colon\\mathbb{R}^{D}\\to\\mathbb{R}^{D} italic_f : blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT ‚Üí blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT be", "snippet": "Let f : ‚Ñù D ‚Üí ‚Ñù D f\\colon\\mathbb{R}^{D}\\to\\mathbb{R}^{D} italic_f : blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT ‚Üí blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT be any differentiable function whose Jacobian f ‚Ä≤ ‚Äã ( ùê± ) f^{\\prime}(\\bm{x}) italic_f start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( bold_italic_x ) is symmetric positive definite. Then f f italic_f is injective, and hence invertible as a function ‚Ñù D ‚Üí ‚Ñõ ‚Äã ( f ) \\mathbb{R}^{D}\\to\\mathcal{R}(f) blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT ‚Üí caligraphic_R ( italic_f ) where ‚Ñõ ‚Äã ( f "}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS3.SSSx3.p3", "title": "Suppose that f f italic_f were not injective, i.e., there exists ùíô , ùíô ‚Ä≤ \\bm{x},\\bm{x}^{\\prime} bold_italic_x , bold_italic_x start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT such that f ‚Äã ( ùíô ) = f ‚Äã ( ùíô ", "snippet": "Suppose that f f italic_f were not injective, i.e., there exists ùíô , ùíô ‚Ä≤ \\bm{x},\\bm{x}^{\\prime} bold_italic_x , bold_italic_x start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT such that f ‚Äã ( ùíô ) = f ‚Äã ( ùíô ‚Ä≤ ) f(\\bm{x})=f(\\bm{x}^{\\prime}) italic_f ( bold_italic_x ) = italic_f ( bold_italic_x start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ) while ùíô ‚â† ùíô ‚Ä≤ \\bm{x}\\neq\\bm{x}^{\\prime} bold_italic_x ‚â† bold_italic_x start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT . Define ùíó ‚âê ( ùíô ‚Ä≤ ‚àí ùíô ) / ‚Äñ ùíô ‚Ä≤ ‚àí ùíô ‚Äñ 2 \\bm{v}\\doteq(\\bm{x}^{\\prime}-\\bm{x})/\\|\\bm{x}^{\\prime}-\\bm{x}\\|_{2} bold_italic_v ‚âê ( bold_italic_x start_POSTSU"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS3.SSSx3.p4", "title": "Combining the above two results, we obtain the following crucial result.", "snippet": "Combining the above two results, we obtain the following crucial result."}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmcorollary1.p1", "title": "Let ùê± \\bm{x} bold_italic_x be any random variable such that Assumptions B.1 and B.2 hold, and let ( ùê± t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSU", "snippet": "Let ùê± \\bm{x} bold_italic_x be any random variable such that Assumptions B.1 and B.2 hold, and let ( ùê± t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ‚àà [ 0 , italic_T ] end_POSTSUBSCRIPT be the stochastic process ( B.2.1 ). Let s , t ‚àà [ 0 , T ] s,t\\in[0,T] italic_s , italic_t ‚àà [ 0 , italic_T ] be such that 0 ‚â§ s < t ‚â§ T 0\\leq s<t\\leq T 0 ‚â§ italic_s < italic_t ‚â§ italic_T , and let ùê± ¬Ø ‚Äã ( ùõè ) ‚âê ùîº ‚Å° [ ùê± s ‚à£ ùê± t = ùõè ] \\bar{\\bm{x}}(\\bm{\\xi})\\doteq\\operatorname{\\mathbb{E}}[\\bm{x}_{s}\\mid\\bm{x}_{t}=\\bm{\\xi}] o"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS3.SSSx3.p5", "title": "The only thing left to show is that ùíô ¬Ø \\bar{\\bm{x}} over¬Ø start_ARG bold_italic_x end_ARG is differentiable, but this is immediate from Tweedie‚Äôs formula ( Theorem 3.3 ) which shows that ùíô ¬Ø \\bar{\\bm", "snippet": "The only thing left to show is that ùíô ¬Ø \\bar{\\bm{x}} over¬Ø start_ARG bold_italic_x end_ARG is differentiable, but this is immediate from Tweedie‚Äôs formula ( Theorem 3.3 ) which shows that ùíô ¬Ø \\bar{\\bm{x}} over¬Ø start_ARG bold_italic_x end_ARG is differentiable if and only if ‚àá log ‚Å° p t \\nabla\\log p_{t} ‚àá roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT is differentiable, and this is provided by Equation B.2.3 . ‚àé"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS3.SSSx4.p1", "title": "Finally, we develop a technical estimate which is required for the proof of Theorem B.3 and actually motivates the assumption for the viable t t italic_t .", "snippet": "Finally, we develop a technical estimate which is required for the proof of Theorem B.3 and actually motivates the assumption for the viable t t italic_t ."}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmlemma5.p1", "title": "Let ùê± \\bm{x} bold_italic_x be any random variable such that Assumptions B.1 and B.2 hold, and let ( ùê± t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSU", "snippet": "Let ùê± \\bm{x} bold_italic_x be any random variable such that Assumptions B.1 and B.2 hold, and let ( ùê± t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ‚àà [ 0 , italic_T ] end_POSTSUBSCRIPT be the stochastic process ( B.2.1 ). Let p t p_{t} italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT be the density of ùê± t \\bm{x}_{t} bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . Then, for t > 0 t>0 italic_t > 0 it holds sup ùùÉ ‚àà ‚Ñù D | Œî ‚Äã log ‚Å° p t ‚Äã ( ùùÉ ) | ‚â§ max ‚Å° ( D t 2 , | R t 4 ‚àí D t 2 | ) . \\"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS3.SSSx4.p2", "title": "By chain rule, a simple exercise computes Œî ‚Äã log ‚Å° p t ‚Äã ( ùùÉ ) = Œî ‚Äã p t ‚Äã ( ùùÉ ) p t ‚Äã ( ùùÉ ) ‚àí ‚Äñ ‚àá p t ‚Äã ( ùùÉ ) ‚Äñ 2 2 p t ‚Äã ( ùùÉ ) 2 . \\Delta\\log p_{t}(\\bm{\\xi})=\\frac{\\Delta p_{t}(\\bm{\\xi})}{p_{t}(\\bm", "snippet": "By chain rule, a simple exercise computes Œî ‚Äã log ‚Å° p t ‚Äã ( ùùÉ ) = Œî ‚Äã p t ‚Äã ( ùùÉ ) p t ‚Äã ( ùùÉ ) ‚àí ‚Äñ ‚àá p t ‚Äã ( ùùÉ ) ‚Äñ 2 2 p t ‚Äã ( ùùÉ ) 2 . \\Delta\\log p_{t}(\\bm{\\xi})=\\frac{\\Delta p_{t}(\\bm{\\xi})}{p_{t}(\\bm{\\xi})}-\\frac{\\|\\nabla p_{t}(\\bm{\\xi})\\|_{2}^{2}}{p_{t}(\\bm{\\xi})^{2}}. roman_Œî roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) = divide start_ARG roman_Œî italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) end_ARG start_ARG italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) end_ARG - divide start_ARG ‚à• ‚àá italic_p sta"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS3.SSSx5.p1", "title": "Here we calculate some useful derivatives which will be reused throughout the appendix.", "snippet": "Here we calculate some useful derivatives which will be reused throughout the appendix."}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmproposition1.p1", "title": "Let ùê± \\bm{x} bold_italic_x be any random variable such that Assumptions B.1 and B.2 hold, and let ( ùê± t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSU", "snippet": "Let ùê± \\bm{x} bold_italic_x be any random variable such that Assumptions B.1 and B.2 hold, and let ( ùê± t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ‚àà [ 0 , italic_T ] end_POSTSUBSCRIPT be the stochastic process ( B.2.1 ). For t ‚â• 0 t\\geq 0 italic_t ‚â• 0 , let p t p_{t} italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT be the density of ùê± t \\bm{x}_{t} bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . Then ‚àÇ p t ‚àÇ t ‚Äã ( ùùÉ ) \\displaystyle\\frac{\\partial p_{t}}{\\partial t}(\\bm{\\xi}) divide s"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS3.SSSx5.p2", "title": "We use the convolution representation of p t p_{t} italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , namely ( B.2.3 ). First taking the time derivative, a computation yields that Proposition B", "snippet": "We use the convolution representation of p t p_{t} italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , namely ( B.2.3 ). First taking the time derivative, a computation yields that Proposition B.3 applies, 5 5 5 We use f t ‚Äã ( ùùÉ ) = p ‚Äã ( ùùÉ ) ‚Äã œÜ t ‚Äã ( ùùÉ ‚àí ùíô ) f_{t}(\\bm{\\xi})=p(\\bm{\\xi})\\varphi_{t}(\\bm{\\xi}-\\bm{x}) italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) = italic_p ( bold_italic_Œæ ) italic_œÜ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ - bold_italic_x ) , noting that it is twice continuously differentiable in ùùÉ \\bm{\\xi} bold_italic_Œæ an"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmproposition2.p1", "title": "For t > 0 t>0 italic_t > 0 and ùõè ‚àà ‚Ñù D \\bm{\\xi}\\in\\mathbb{R}^{D} bold_italic_Œæ ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT it holds ‚àÇ ‚àÇ t ‚Äã œÜ t ‚Äã ( ùùÉ ) \\displaystyle\\frac{\\partia", "snippet": "For t > 0 t>0 italic_t > 0 and ùõè ‚àà ‚Ñù D \\bm{\\xi}\\in\\mathbb{R}^{D} bold_italic_Œæ ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT it holds ‚àÇ ‚àÇ t ‚Äã œÜ t ‚Äã ( ùùÉ ) \\displaystyle\\frac{\\partial}{\\partial t}\\varphi_{t}(\\bm{\\xi}) divide start_ARG ‚àÇ end_ARG start_ARG ‚àÇ italic_t end_ARG italic_œÜ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) = œÜ t ‚Äã ( ùùÉ ) ‚ãÖ ‚Äñ ùùÉ ‚Äñ 2 2 ‚àí D ‚Äã t 2 t 3 \\displaystyle=\\varphi_{t}(\\bm{\\xi})\\cdot\\frac{\\|\\bm{\\xi}\\|_{2}^{2}-Dt^{2}}{t^{3}} = italic_œÜ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) ‚ãÖ divide start_ARG ‚à• bold_italic_Œæ ‚à•"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS3.SSSx5.p3", "title": "Direct computation. ‚àé", "snippet": "Direct computation. ‚àé"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS3.SSSx6.p1", "title": "In this appendix, we differentiate under the integral sign many times, and it is important to know when we can do this. There are two kinds of differentiating under the integral sign: 1. Differentiati", "snippet": "In this appendix, we differentiate under the integral sign many times, and it is important to know when we can do this. There are two kinds of differentiating under the integral sign: 1. Differentiating an integral ‚à´ f t ‚Äã ( ùùÉ ) ‚Äã d ùùÉ \\int f_{t}(\\bm{\\xi})\\mathrm{d}\\bm{\\xi} ‚à´ italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) roman_d bold_italic_Œæ with respect to the auxiliary parameter t t italic_t . 2. Differentiating a convolution ( f ‚àó g ) ‚Äã ( ùùÉ ) = ‚à´ f ‚Äã ( ùùÉ ) ‚Äã g ‚Äã ( ùùÉ ‚àí ùíñ ) ‚Äã d u (f*g)(\\bm{\\xi})=\\int f(\\bm{\\xi})g(\\bm{\\xi}-\\bm{u})\\mathrm{d}u ( italic_f ‚àó italic_g ) "}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.I5.i1.p1", "title": "Differentiating an integral ‚à´ f t ‚Äã ( ùùÉ ) ‚Äã d ùùÉ \\int f_{t}(\\bm{\\xi})\\mathrm{d}\\bm{\\xi} ‚à´ italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) roman_d bold_italic_Œæ with respect to", "snippet": "Differentiating an integral ‚à´ f t ‚Äã ( ùùÉ ) ‚Äã d ùùÉ \\int f_{t}(\\bm{\\xi})\\mathrm{d}\\bm{\\xi} ‚à´ italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) roman_d bold_italic_Œæ with respect to the auxiliary parameter t t italic_t ."}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.I5.i2.p1", "title": "Differentiating a convolution ( f ‚àó g ) ‚Äã ( ùùÉ ) = ‚à´ f ‚Äã ( ùùÉ ) ‚Äã g ‚Äã ( ùùÉ ‚àí ùíñ ) ‚Äã d u (f*g)(\\bm{\\xi})=\\int f(\\bm{\\xi})g(\\bm{\\xi}-\\bm{u})\\mathrm{d}u ( italic_f ‚àó italic_g ) ( bold_italic_Œæ ) = ‚à´ italic_f", "snippet": "Differentiating a convolution ( f ‚àó g ) ‚Äã ( ùùÉ ) = ‚à´ f ‚Äã ( ùùÉ ) ‚Äã g ‚Äã ( ùùÉ ‚àí ùíñ ) ‚Äã d u (f*g)(\\bm{\\xi})=\\int f(\\bm{\\xi})g(\\bm{\\xi}-\\bm{u})\\mathrm{d}u ( italic_f ‚àó italic_g ) ( bold_italic_Œæ ) = ‚à´ italic_f ( bold_italic_Œæ ) italic_g ( bold_italic_Œæ - bold_italic_u ) roman_d italic_u with respect to the variable ùùÉ \\bm{\\xi} bold_italic_Œæ ."}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS3.SSSx6.p2", "title": "For the first category, we give a concrete result, stated without proof but attributable to the linked source , which derives the following result as a special case of a more general theorem about the", "snippet": "For the first category, we give a concrete result, stated without proof but attributable to the linked source , which derives the following result as a special case of a more general theorem about the interaction of differential operators and tempered distributions, much beyond the scope of the book. A full formal reference can be found in [ Jon82 ] ."}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmproposition3.p1", "title": "Let f : ( 0 , T ) √ó ‚Ñù D ‚Üí ‚Ñù f\\colon(0,T)\\times\\mathbb{R}^{D}\\to\\mathbb{R} italic_f : ( 0 , italic_T ) √ó blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT ‚Üí blackboard_R be such that: ‚Ä¢ f", "snippet": "Let f : ( 0 , T ) √ó ‚Ñù D ‚Üí ‚Ñù f\\colon(0,T)\\times\\mathbb{R}^{D}\\to\\mathbb{R} italic_f : ( 0 , italic_T ) √ó blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT ‚Üí blackboard_R be such that: ‚Ä¢ f f italic_f is a jointly measurable function of ( t , ùùÉ ) (t,\\bm{\\xi}) ( italic_t , bold_italic_Œæ ) ; ‚Ä¢ For Lebesgue-almost every ùùÉ ‚àà ‚Ñù D \\bm{\\xi}\\in\\mathbb{R}^{D} bold_italic_Œæ ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT , the function t ‚Ü¶ f t ‚Äã ( ùùÉ ) t\\mapsto f_{t}(\\bm{\\xi}) italic_t ‚Ü¶ italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) is absolutely c"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.I6.i1.p1", "title": "f f italic_f is a jointly measurable function of ( t , ùùÉ ) (t,\\bm{\\xi}) ( italic_t , bold_italic_Œæ ) ;", "snippet": "f f italic_f is a jointly measurable function of ( t , ùùÉ ) (t,\\bm{\\xi}) ( italic_t , bold_italic_Œæ ) ;"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.I6.i2.p1", "title": "For Lebesgue-almost every ùùÉ ‚àà ‚Ñù D \\bm{\\xi}\\in\\mathbb{R}^{D} bold_italic_Œæ ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT , the function t ‚Ü¶ f t ‚Äã ( ùùÉ ) t\\mapsto f_{t}(\\bm{\\xi}) ital", "snippet": "For Lebesgue-almost every ùùÉ ‚àà ‚Ñù D \\bm{\\xi}\\in\\mathbb{R}^{D} bold_italic_Œæ ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT , the function t ‚Ü¶ f t ‚Äã ( ùùÉ ) t\\mapsto f_{t}(\\bm{\\xi}) italic_t ‚Ü¶ italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) is absolutely continuous;"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.I6.i3.p1", "title": "‚àÇ f t ‚àÇ t \\frac{\\partial f_{t}}{\\partial t} divide start_ARG ‚àÇ italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG ‚àÇ italic_t end_ARG is locally integrable, i.e., for every [ t m", "snippet": "‚àÇ f t ‚àÇ t \\frac{\\partial f_{t}}{\\partial t} divide start_ARG ‚àÇ italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG ‚àÇ italic_t end_ARG is locally integrable, i.e., for every [ t min , t max ] ‚äÜ ( 0 , T ) [t_{\\min},t_{\\max}]\\subseteq(0,T) [ italic_t start_POSTSUBSCRIPT roman_min end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT ] ‚äÜ ( 0 , italic_T ) it holds ‚à´ t min t max ‚à´ ‚Ñù D | ‚àÇ f t ‚àÇ t ‚Äã ( ùùÉ ) | ‚Äã d ùùÉ < ‚àû . \\int_{t_{\\min}}^{t_{\\max}}\\int_{\\mathbb{R}^{D}}\\left\\lvert\\frac{\\partial f_{t}}{\\partial t}(\\bm{\\xi})\\right\\rvert\\mathrm{d}\\bm{\\xi}<\\infty."}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS3.SSSx6.p3", "title": "For the second category, we give another concrete result, stated without proof but fully formalized in [ BB11 ] .", "snippet": "For the second category, we give another concrete result, stated without proof but fully formalized in [ BB11 ] ."}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmproposition4.p1", "title": "Let f f italic_f be k k italic_k -times continuously differentiable with compact support, and let g g italic_g be locally integrable. Then the convolution f ‚àó g f*g italic_f ‚àó italic_g defined by ( f ", "snippet": "Let f f italic_f be k k italic_k -times continuously differentiable with compact support, and let g g italic_g be locally integrable. Then the convolution f ‚àó g f*g italic_f ‚àó italic_g defined by ( f ‚àó g ) ‚Äã ( ùùÉ ) ‚âê ‚à´ ‚Ñù D f ‚Äã ( ùíñ ) ‚Äã g ‚Äã ( ùùÉ ‚àí ùíñ ) ‚Äã d ùíñ (f*g)(\\bm{\\xi})\\doteq\\int_{\\mathbb{R}^{D}}f(\\bm{u})g(\\bm{\\xi}-\\bm{u})\\mathrm{d}\\bm{u} ( italic_f ‚àó italic_g ) ( bold_italic_Œæ ) ‚âê ‚à´ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_f ( bold_italic_u ) italic_g ( bold_italic_Œæ - bold_italic_u ) roman_d bold_italic_u (B.2.135) is k k ita"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS3.SSSx6.p4", "title": "Although not in the book, a simple integration by parts argument shows that if g g italic_g is also k k italic_k -times differentiable, then we can ‚Äútrade off‚Äù the regularity: ‚àá k ( f ‚àó g ) = f ‚àó ( ‚àá ", "snippet": "Although not in the book, a simple integration by parts argument shows that if g g italic_g is also k k italic_k -times differentiable, then we can ‚Äútrade off‚Äù the regularity: ‚àá k ( f ‚àó g ) = f ‚àó ( ‚àá k g ) . \\nabla^{k}(f*g)=f*(\\nabla^{k}g). ‚àá start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ( italic_f ‚àó italic_g ) = italic_f ‚àó ( ‚àá start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_g ) . (B.2.137)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.p1", "title": "In this section, we prove Theorem 3.6 . Following our conventions throughout this appendix, we write ùíÆ = Supp ‚Å° ( ùíô ) \\mathcal{S}=\\operatorname{Supp}(\\bm{x}) caligraphic_S = roman_Supp ( bold_italic_x", "snippet": "In this section, we prove Theorem 3.6 . Following our conventions throughout this appendix, we write ùíÆ = Supp ‚Å° ( ùíô ) \\mathcal{S}=\\operatorname{Supp}(\\bm{x}) caligraphic_S = roman_Supp ( bold_italic_x ) for the compact support of the random variable ùíô \\bm{x} bold_italic_x ."}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.p2", "title": "As foreshadowed, we will make a regularity assumption on the support set ùíÆ \\mathcal{S} caligraphic_S to prove Theorem 3.6 . One possibility for proceeding under minimal assumptions would be to instant", "snippet": "As foreshadowed, we will make a regularity assumption on the support set ùíÆ \\mathcal{S} caligraphic_S to prove Theorem 3.6 . One possibility for proceeding under minimal assumptions would be to instantiate the results of [ RBK18 , RKB23 ] in our setting, since these results apply to sets ùíÆ \\mathcal{S} caligraphic_S with very low regularity (e.g., Cantor-like sets with fractal structure). However, we have found precisely computing the constants in these results, a necessary endeavor to assert a conclusion like Theorem 3.6 , to be somewhat onerous in our setting. Our approach is therefore to add "}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmassumption3.p1", "title": "The support ùíÆ ‚äÇ ‚Ñù D \\mathcal{S}\\subset\\mathbb{R}^{D} caligraphic_S ‚äÇ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT of the random variable ùíô \\bm{x} bold_italic_x is a finite union of ", "snippet": "The support ùíÆ ‚äÇ ‚Ñù D \\mathcal{S}\\subset\\mathbb{R}^{D} caligraphic_S ‚äÇ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT of the random variable ùíô \\bm{x} bold_italic_x is a finite union of K K italic_K spheres, each with dimension d k d_{k} italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , k ‚àà [ K ] k\\in[K] italic_k ‚àà [ italic_K ] . The probability that ùíô \\bm{x} bold_italic_x is drawn from the k k italic_k -th sphere is given by œÄ k ‚àà [ 0 , 1 ] \\pi_{k}\\in[0,1] italic_œÄ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ‚àà [ 0 , 1 ] , and conditional on being drawn from the k k it"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.p3", "title": "We proceed under the simplifying Assumption B.3 in order to simplify excessive technicality, and to connect to an important running example used throughout the monograph. We believe our results can be", "snippet": "We proceed under the simplifying Assumption B.3 in order to simplify excessive technicality, and to connect to an important running example used throughout the monograph. We believe our results can be generalized to support ùíÆ \\mathcal{S} caligraphic_S from the class of sets with positive reach with additional technical effort, but leave this for the future."}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.SS1.p1", "title": "We briefly sketch the proof, then proceed to establishing three fundamental lemmas, then give the proof. The proof will depend on notions introduced in the sketch below.", "snippet": "We briefly sketch the proof, then proceed to establishing three fundamental lemmas, then give the proof. The proof will depend on notions introduced in the sketch below."}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.SS1.p2", "title": "Obtaining an upper bound on the rate distortion function ( 3.3.3 ) is straightforward: by the rate characterization (i.e., the rate distortion function is the minimum rate of a code for ùíô \\bm{x} bold_", "snippet": "Obtaining an upper bound on the rate distortion function ( 3.3.3 ) is straightforward: by the rate characterization (i.e., the rate distortion function is the minimum rate of a code for ùíô \\bm{x} bold_italic_x with expected squared ‚Ñì 2 \\ell^{2} roman_‚Ñì start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT distortion œµ \\epsilon italic_œµ ), upper bounding ‚Ñõ œµ ‚Äã ( ùíô ) \\mathcal{R}_{\\epsilon}(\\bm{x}) caligraphic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_x ) only requires demonstrating one code for ùíô \\bm{x} bold_italic_x that achieves this target distortion, and any œµ \\epsilon italic_œµ -co"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.SS1.p3", "title": "Now, the important constraint for our current purposes is that the Shannon lower bound requires the random variable ùíô \\bm{x} bold_italic_x to have a density, which rules out many low-dimensional distr", "snippet": "Now, the important constraint for our current purposes is that the Shannon lower bound requires the random variable ùíô \\bm{x} bold_italic_x to have a density, which rules out many low-dimensional distributions of interest. But let us momentarily consider the situation when ùíô \\bm{x} bold_italic_x does admit a density. The assumption that ùíô \\bm{x} bold_italic_x is uniformly distributed on its support is easily formalized in this setting: for any Borel set A ‚äÇ ùíÆ A\\subset\\mathcal{S} italic_A ‚äÇ caligraphic_S , we have ‚Ñô ‚Å° [ ùíô ‚àà A ] = ‚à´ A 1 vol ‚Å° ( ùíÆ ) ‚Äã d ùíô . \\operatorname{\\mathbb{P}}[\\bm{x}\\in A]=\\"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.SS1.p4", "title": "To extend the program above to degenerate distributions satisfying Assumption B.3 , our proof of the lower bound in Theorem 3.6 will leverage an approximation argument of the actual low-dimensional di", "snippet": "To extend the program above to degenerate distributions satisfying Assumption B.3 , our proof of the lower bound in Theorem 3.6 will leverage an approximation argument of the actual low-dimensional distribution ùíô \\bm{x} bold_italic_x by ‚Äúnearby‚Äù distributions which have densities, similarly but not exactly the same as the proof sketch preceding Theorem B.1 . We will then link the parameter introduced in the approximating sequence to the distortion parameter œµ \\epsilon italic_œµ in order to obtain the desired conclusion in Theorem 3.6 ."}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmdefinition1.p1", "title": "Let ùíÆ \\mathcal{S} caligraphic_S be a compact set. For any Œ¥ > 0 \\delta>0 italic_Œ¥ > 0 , define the Œ¥ \\delta italic_Œ¥ -thickening of ùíÆ \\mathcal{S} caligraphic_S , denoted ùíÆ Œ¥ \\mathcal{S}_{\\delta} calig", "snippet": "Let ùíÆ \\mathcal{S} caligraphic_S be a compact set. For any Œ¥ > 0 \\delta>0 italic_Œ¥ > 0 , define the Œ¥ \\delta italic_Œ¥ -thickening of ùíÆ \\mathcal{S} caligraphic_S , denoted ùíÆ Œ¥ \\mathcal{S}_{\\delta} caligraphic_S start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT , by ùíÆ Œ¥ = { ùùÉ ‚àà ‚Ñù D ‚à£ dist ‚Äã ( ùùÉ , ùíÆ ) ‚â§ Œ¥ } . \\mathcal{S}_{\\delta}=\\left\\{\\bm{\\xi}\\in\\mathbb{R}^{D}\\mid\\mathrm{dist}(\\bm{\\xi},\\mathcal{S})\\leq\\delta\\right\\}. caligraphic_S start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT = { bold_italic_Œæ ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT ‚à£ roman_dist ( bold_italic_Œæ , caligrap"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.SS1.p5", "title": "The distance function referenced in Definition B.1 is defined by dist ‚Å° ( ùùÉ , ùíÆ ) = inf ùùÉ ‚Ä≤ ‚àà ùíÆ ‚Äñ ùùÉ ‚àí ùùÉ ‚Ä≤ ‚Äñ 2 . \\operatorname{dist}(\\bm{\\xi},\\mathcal{S})=\\inf_{\\bm{\\xi}^{\\prime}\\in\\mathcal{S}}\\left\\|\\", "snippet": "The distance function referenced in Definition B.1 is defined by dist ‚Å° ( ùùÉ , ùíÆ ) = inf ùùÉ ‚Ä≤ ‚àà ùíÆ ‚Äñ ùùÉ ‚àí ùùÉ ‚Ä≤ ‚Äñ 2 . \\operatorname{dist}(\\bm{\\xi},\\mathcal{S})=\\inf_{\\bm{\\xi}^{\\prime}\\in\\mathcal{S}}\\left\\|\\bm{\\xi}-\\bm{\\xi}^{\\prime}\\right\\|_{2}. roman_dist ( bold_italic_Œæ , caligraphic_S ) = roman_inf start_POSTSUBSCRIPT bold_italic_Œæ start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ‚àà caligraphic_S end_POSTSUBSCRIPT ‚à• bold_italic_Œæ - bold_italic_Œæ start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT . (B.3.9) For a compact set ùíÆ \\mathcal{S} caligraphic_S , Weierstrass‚Äôs th"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmdefinition2.p1", "title": "Let ùíô \\bm{x} bold_italic_x be a random variable such that Supp ‚Å° ( ùíô ) = ùíÆ \\operatorname{Supp}(\\bm{x})=\\mathcal{S} roman_Supp ( bold_italic_x ) = caligraphic_S is a union of K K italic_K hyperspheres,", "snippet": "Let ùíô \\bm{x} bold_italic_x be a random variable such that Supp ‚Å° ( ùíô ) = ùíÆ \\operatorname{Supp}(\\bm{x})=\\mathcal{S} roman_Supp ( bold_italic_x ) = caligraphic_S is a union of K K italic_K hyperspheres, distributed as in Assumption B.3 . Denote the support of each component of the mixture by ùíÆ k \\mathcal{S}_{k} caligraphic_S start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT . Define the thickened random variable ùíô Œ¥ \\bm{x}_{\\delta} bold_italic_x start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT as the mixture of measures where each component measure is uniform on the thickened set ùíÆ k , Œ¥ \\mathcal{S}_"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmlemma6.p1", "title": "Suppose the random variable ùê± \\bm{x} bold_italic_x satisfies Assumption B.3 . Then if 0 < Œ¥ < 1 2 0<\\delta<\\tfrac{1}{2} 0 < italic_Œ¥ < divide start_ARG 1 end_ARG start_ARG 2 end_ARG , the thickened ra", "snippet": "Suppose the random variable ùê± \\bm{x} bold_italic_x satisfies Assumption B.3 . Then if 0 < Œ¥ < 1 2 0<\\delta<\\tfrac{1}{2} 0 < italic_Œ¥ < divide start_ARG 1 end_ARG start_ARG 2 end_ARG , the thickened random variable ùê± Œ¥ \\bm{x}_{\\delta} bold_italic_x start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT ( Definition B.2 ) satisfies for any œµ > 0 \\epsilon>0 italic_œµ > 0 R Œ¥ + œµ ‚Äã ( ùíô Œ¥ ) ‚â§ R œµ ‚Äã ( ùíô ) . R_{\\delta+\\epsilon}(\\bm{x}_{\\delta})\\leq R_{\\epsilon}(\\bm{x}). italic_R start_POSTSUBSCRIPT italic_Œ¥ + italic_œµ end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT ) ‚â§ italic_"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.SS1.p6", "title": "The proof of Lemma B.6 is deferred to Section B.3.2 . Using Lemma B.6 , the above program can be realized, because the random variable ùíô Œ¥ \\bm{x}_{\\delta} bold_italic_x start_POSTSUBSCRIPT italic_Œ¥ en", "snippet": "The proof of Lemma B.6 is deferred to Section B.3.2 . Using Lemma B.6 , the above program can be realized, because the random variable ùíô Œ¥ \\bm{x}_{\\delta} bold_italic_x start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT has a density that is uniform with respect to the Lebesgue measure."}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.SS1.p7", "title": "The upper bound is readily shown. If S S italic_S is any œµ \\epsilon italic_œµ -cover of the support of ùíô \\bm{x} bold_italic_x with cardinality ùí© œµ ‚Äã ( Supp ‚Å° ( ùíô ) ) \\mathcal{N}_{\\epsilon}(\\operatornam", "snippet": "The upper bound is readily shown. If S S italic_S is any œµ \\epsilon italic_œµ -cover of the support of ùíô \\bm{x} bold_italic_x with cardinality ùí© œµ ‚Äã ( Supp ‚Å° ( ùíô ) ) \\mathcal{N}_{\\epsilon}(\\operatorname{Supp}(\\bm{x})) caligraphic_N start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( roman_Supp ( bold_italic_x ) ) , then consider the coding scheme assigning to each ùùÉ ‚àà Supp ‚Å° ( ùíô ) \\bm{\\xi}\\in\\operatorname{Supp}(\\bm{x}) bold_italic_Œæ ‚àà roman_Supp ( bold_italic_x ) the reconstruction ùùÉ ^ = arg ‚Äã min ùùÉ ‚Ä≤ ‚àà S ‚Å° ‚Äñ ùùÉ ‚àí ùùÉ ‚Ä≤ ‚Äñ 2 \\hat{\\bm{\\xi}}=\\operatorname*{arg\\ min}_{\\bm{\\xi}^{\\prime}\\in S}\\,\\|\\bm{\\xi}-"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.SS1.p8", "title": "For the lower bound, let 0 < Œ¥ < 1 2 0<\\delta<\\tfrac{1}{2} 0 < italic_Œ¥ < divide start_ARG 1 end_ARG start_ARG 2 end_ARG , and consider the thickened random variable ùíô Œ¥ \\bm{x}_{\\delta} bold_italic_x ", "snippet": "For the lower bound, let 0 < Œ¥ < 1 2 0<\\delta<\\tfrac{1}{2} 0 < italic_Œ¥ < divide start_ARG 1 end_ARG start_ARG 2 end_ARG , and consider the thickened random variable ùíô Œ¥ \\bm{x}_{\\delta} bold_italic_x start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT . By Lemma B.6 , we have R Œ¥ + œµ ‚Äã ( ùíô Œ¥ ) ‚â§ R œµ ‚Äã ( ùíô ) . R_{\\delta+\\epsilon}(\\bm{x}_{\\delta})\\leq R_{\\epsilon}(\\bm{x}). italic_R start_POSTSUBSCRIPT italic_Œ¥ + italic_œµ end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT ) ‚â§ italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_x ) . (B.3.11) Since ùíô Œ¥ \\b"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.SS1.p9", "title": "‚àé", "snippet": "‚àé"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.SS2.p1", "title": "It suffices to show that any code for ùíô \\bm{x} bold_italic_x with expected squared distortion œµ 2 \\epsilon^{2} italic_œµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT produces a code for ùíô Œ¥ \\bm{x}_{\\del", "snippet": "It suffices to show that any code for ùíô \\bm{x} bold_italic_x with expected squared distortion œµ 2 \\epsilon^{2} italic_œµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT produces a code for ùíô Œ¥ \\bm{x}_{\\delta} bold_italic_x start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT with the same rate and distortion not much larger, for a suitable choice of Œ¥ \\delta italic_Œ¥ . So fix such a code for ùíô \\bm{x} bold_italic_x , achieving rate R R italic_R and expected squared distortion œµ 2 \\epsilon^{2} italic_œµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . We write ùíô ^ \\hat{\\bm{x}} over^ start_ARG bold_italic_x end"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.SS2.p2", "title": "Now let ùíÆ k \\mathcal{S}_{k} caligraphic_S start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT denote the k k italic_k -th hypersphere in the support of ùíô \\bm{x} bold_italic_x . There is an orthonormal basi", "snippet": "Now let ùíÆ k \\mathcal{S}_{k} caligraphic_S start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT denote the k k italic_k -th hypersphere in the support of ùíô \\bm{x} bold_italic_x . There is an orthonormal basis ùëº k ‚àà ‚Ñù D √ó d k \\bm{U}_{k}\\in\\mathbb{R}^{D\\times d_{k}} bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D √ó italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT such that Span ‚Å° ( ùíÆ k ) = Span ‚Å° ( ùëº k ) \\operatorname{Span}(\\mathcal{S}_{k})=\\operatorname{Span}(\\bm{U}_{k}) roman_Span ( caligraphic_S start_POSTSUBSCRIPT"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.SS2.p3", "title": "Now, we define a code for ùíô Œ¥ \\bm{x}_{\\delta} bold_italic_x start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT by ùíô ^ Œ¥ = q ‚Äã ( œÄ ùíÆ ‚Äã ( ùíô Œ¥ ) ) . \\hat{\\bm{x}}_{\\delta}=\\mathrm{q}(\\pi_{\\mathcal{S}}(\\bm{x}_", "snippet": "Now, we define a code for ùíô Œ¥ \\bm{x}_{\\delta} bold_italic_x start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT by ùíô ^ Œ¥ = q ‚Äã ( œÄ ùíÆ ‚Äã ( ùíô Œ¥ ) ) . \\hat{\\bm{x}}_{\\delta}=\\mathrm{q}(\\pi_{\\mathcal{S}}(\\bm{x}_{\\delta})). over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT = roman_q ( italic_œÄ start_POSTSUBSCRIPT caligraphic_S end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT ) ) . (B.3.35) Clearly this is associated to a rate- R R italic_R code for ùíô Œ¥ \\bm{x}_{\\delta} bold_italic_x start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT , because i"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.SS2.p4", "title": "Thus it follows from the above argument that œÄ ùíÆ ‚Äã ( ùíô Œ¥ ) \\pi_{\\mathcal{S}}(\\bm{x}_{\\delta}) italic_œÄ start_POSTSUBSCRIPT caligraphic_S end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_Œ¥ ", "snippet": "Thus it follows from the above argument that œÄ ùíÆ ‚Äã ( ùíô Œ¥ ) \\pi_{\\mathcal{S}}(\\bm{x}_{\\delta}) italic_œÄ start_POSTSUBSCRIPT caligraphic_S end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT ) is uniform. Because the assumption on Œ¥ \\delta italic_Œ¥ implies that the mixture components in the distribution of ùíô Œ¥ \\bm{x}_{\\delta} bold_italic_x start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT do not overlap, the mixing weights œÄ k \\pi_{k} italic_œÄ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT are also preserved in the image œÄ ùíÆ ‚Äã ( ùíô Œ¥ ) \\pi_{\\mathcal{S}}(\\bm{x}_{\\delta}) "}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.SS2.p5", "title": "We have thus shown that the hypothesized rate- R R italic_R , (expected squared) distortion- œµ 2 \\epsilon^{2} italic_œµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT code for ùíô \\bm{x} bold_italic_x produ", "snippet": "We have thus shown that the hypothesized rate- R R italic_R , (expected squared) distortion- œµ 2 \\epsilon^{2} italic_œµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT code for ùíô \\bm{x} bold_italic_x produces a rate- R R italic_R , (expected squared) distortion Œ¥ + œµ \\delta+\\epsilon italic_Œ¥ + italic_œµ code for ùíô Œ¥ \\bm{x}_{\\delta} bold_italic_x start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT . This establishes that R Œ¥ + œµ ‚Äã ( ùíô Œ¥ ) ‚â§ R œµ ‚Äã ( ùíô ) , R_{\\delta+\\epsilon}(\\bm{x}_{\\delta})\\leq R_{\\epsilon}(\\bm{x}), italic_R start_POSTSUBSCRIPT italic_Œ¥ + italic_œµ end_POSTSUBSCRIPT ( bold_italic_x start_P"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.SS2.p6", "title": "‚àé", "snippet": "‚àé"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmassumption1", "title": "Assumption B.1 .", "snippet": "Assumption B.1 . ùíô \\bm{x} bold_italic_x is supported on a compact set ùíÆ ‚äÜ ‚Ñù D \\mathcal{S}\\subseteq\\mathbb{R}^{D} caligraphic_S ‚äÜ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT of radius at most R R italic_R , i.e., R ‚âê sup ùùÉ ‚àà ùíÆ ‚Äñ ùùÉ ‚Äñ 2 R\\doteq\\sup_{\\bm{\\xi}\\in\\mathcal{S}}\\|\\bm{\\xi}\\|_{2} italic_R ‚âê roman_sup start_POSTSUBSCRIPT bold_italic_Œæ ‚àà caligraphic_S end_POSTSUBSCRIPT ‚à• bold_italic_Œæ ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ."}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmtheorem1", "title": "Theorem B.1 .", "snippet": "Theorem B.1 . Let ùê± \\bm{x} bold_italic_x be a random variable on ‚Ñù D \\mathbb{R}^{D} blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT . 1. If ùíô \\bm{x} bold_italic_x is supported on a compact set ùíÆ ‚äÜ ‚Ñù D \\mathcal{S}\\subseteq\\mathbb{R}^{D} caligraphic_S ‚äÜ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT (i.e., Assumption B.1 ) then h ‚Äã ( ùíô ) ‚â§ h ‚Äã ( ùí∞ ‚Å° ( ùíÆ ) ) = log ‚Å° vol ‚Å° ( ùíÆ ) . h(\\bm{x})\\leq h(\\operatorname{\\mathcal{U}}(\\mathcal{S}))=\\log\\operatorname{vol}(\\mathcal{S}). italic_h ( bold_italic_x ) ‚â§ italic_h ( caligraphic_U ( caligraphic_S ) ) = roman_log roman_v"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmassumption2", "title": "Assumption B.2 .", "snippet": "Assumption B.2 . ùíô \\bm{x} bold_italic_x has a twice continuously differentiable density, denoted p p italic_p ."}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmtheorem2", "title": "Theorem B.2 (Diffusion Increases Entropy) .", "snippet": "Theorem B.2 (Diffusion Increases Entropy) . Let ùê± \\bm{x} bold_italic_x be any random variable such that Assumptions B.1 and B.2 hold, and let ( ùê± t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ‚àà [ 0 , italic_T ] end_POSTSUBSCRIPT be the stochastic process ( B.2.1 ). Then h ‚Äã ( ùíô s ) < h ‚Äã ( ùíô t ) , ‚àÄ s , t : 0 ‚â§ s < t ‚â§ T . h(\\bm{x}_{s})<h(\\bm{x}_{t}),\\qquad\\forall s,t\\colon 0\\leq s<t\\leq T. italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) < italic_h ( bold_italic_x start_POSTSUBS"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmtheorem3", "title": "Theorem B.3 .", "snippet": "Theorem B.3 . Let ùê± \\bm{x} bold_italic_x be any random variable such that Assumptions B.1 and B.2 hold, and let ( ùê± t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ‚àà [ 0 , italic_T ] end_POSTSUBSCRIPT be the stochastic process ( B.2.1 ). Then h ( ùîº [ ùíô s ‚à£ ùíô t ] ) < h ( ùíô t ) , ‚àÄ s , t ‚àà [ 0 , T ] : 0 < t ‚â§ R 2 ‚Äã D , 0 ‚â§ s < t ‚ãÖ min { 1 , R 2 / D ‚àí 2 ‚Äã t 2 R 2 / D ‚àí t 2 } . h(\\operatorname{\\mathbb{E}}[\\bm{x}_{s}\\mid\\bm{x}_{t}])<h(\\bm{x}_{t}),\\qquad\\forall s,t\\in[0,T]\\colon\\quad 0<t\\leq\\frac{R}{\\sqrt{2D}},\\"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmlemma1", "title": "Lemma B.1 .", "snippet": "Lemma B.1 . Let ùê± \\bm{x} bold_italic_x be any random variable, and let ( ùê± t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ‚àà [ 0 , italic_T ] end_POSTSUBSCRIPT be the stochastic process ( B.2.1 ). 1. For t > 0 t>0 italic_t > 0 , the differential entropy h ‚Äã ( ùíô t ) h(\\bm{x}_{t}) italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) exists and is > ‚àí ‚àû >-\\infty > - ‚àû . 2. If in addition Assumption B.1 holds for ùíô \\bm{x} bold_italic_x , then h ‚Äã ( ùíô ) < ‚àû h(\\bm{x})<\\infty italic_h ( bold_"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmlemma2", "title": "Lemma B.2 .", "snippet": "Lemma B.2 . Let ùê± \\bm{x} bold_italic_x be any random variable such that Assumptions B.1 and B.2 hold, and let ( ùê± t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ‚àà [ 0 , italic_T ] end_POSTSUBSCRIPT be the stochastic process ( B.2.1 ). For t ‚â• 0 t\\geq 0 italic_t ‚â• 0 , let p t p_{t} italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT be the density of ùê± t \\bm{x}_{t} bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . Then for a constant c ‚àà ‚Ñù c\\in\\mathbb{R} italic_c ‚àà blackboard_R it holds ‚à´ "}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmlemma3", "title": "Lemma B.3 (Generalization of [ Gri11 ] , Lemma A.1) .", "snippet": "Lemma B.3 (Generalization of [ Gri11 ] , Lemma A.1) . Let ùê± \\bm{x} bold_italic_x be any random variable such that Assumptions B.1 and B.2 hold, and let ( ùê± t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ‚àà [ 0 , italic_T ] end_POSTSUBSCRIPT be the stochastic process ( B.2.1 ). Let s , t ‚àà [ 0 , T ] s,t\\in[0,T] italic_s , italic_t ‚àà [ 0 , italic_T ] be such that 0 ‚â§ s < t ‚â§ T 0\\leq s<t\\leq T 0 ‚â§ italic_s < italic_t ‚â§ italic_T , and let ùê± ¬Ø ‚Äã ( ùõè ) ‚âê ùîº ‚Å° [ ùê± s ‚à£ ùê± t = ùõè ] \\bar{\\bm{x}}(\\bm{\\xi})\\doteq\\operato"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmlemma4", "title": "Lemma B.4 (Generalization of [ Gri11 ] Corollary A.2, Part 1) .", "snippet": "Lemma B.4 (Generalization of [ Gri11 ] Corollary A.2, Part 1) . Let f : ‚Ñù D ‚Üí ‚Ñù D f\\colon\\mathbb{R}^{D}\\to\\mathbb{R}^{D} italic_f : blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT ‚Üí blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT be any differentiable function whose Jacobian f ‚Ä≤ ‚Äã ( ùê± ) f^{\\prime}(\\bm{x}) italic_f start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( bold_italic_x ) is symmetric positive definite. Then f f italic_f is injective, and hence invertible as a function ‚Ñù D ‚Üí ‚Ñõ ‚Äã ( f ) \\mathbb{R}^{D}\\to\\mathcal{R}(f) blackboard_R start_POSTSUPERSCRIPT italic_D"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmcorollary1", "title": "Corollary B.1 (Generalization of [ Gri11 ] Corollary A.2, Part 2) .", "snippet": "Corollary B.1 (Generalization of [ Gri11 ] Corollary A.2, Part 2) . Let ùê± \\bm{x} bold_italic_x be any random variable such that Assumptions B.1 and B.2 hold, and let ( ùê± t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ‚àà [ 0 , italic_T ] end_POSTSUBSCRIPT be the stochastic process ( B.2.1 ). Let s , t ‚àà [ 0 , T ] s,t\\in[0,T] italic_s , italic_t ‚àà [ 0 , italic_T ] be such that 0 ‚â§ s < t ‚â§ T 0\\leq s<t\\leq T 0 ‚â§ italic_s < italic_t ‚â§ italic_T , and let ùê± ¬Ø ‚Äã ( ùõè ) ‚âê ùîº ‚Å° [ ùê± s ‚à£ ùê± t = ùõè ] \\bar{\\bm{x}}(\\bm{\\xi})"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmlemma5", "title": "Lemma B.5 .", "snippet": "Lemma B.5 . Let ùê± \\bm{x} bold_italic_x be any random variable such that Assumptions B.1 and B.2 hold, and let ( ùê± t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ‚àà [ 0 , italic_T ] end_POSTSUBSCRIPT be the stochastic process ( B.2.1 ). Let p t p_{t} italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT be the density of ùê± t \\bm{x}_{t} bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . Then, for t > 0 t>0 italic_t > 0 it holds sup ùùÉ ‚àà ‚Ñù D | Œî ‚Äã log ‚Å° p t ‚Äã ( ùùÉ ) | ‚â§ max ‚Å° ( D t 2 , | R t 4 ‚àí D"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmproposition1", "title": "Proposition B.1 .", "snippet": "Proposition B.1 . Let ùê± \\bm{x} bold_italic_x be any random variable such that Assumptions B.1 and B.2 hold, and let ( ùê± t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ‚àà [ 0 , italic_T ] end_POSTSUBSCRIPT be the stochastic process ( B.2.1 ). For t ‚â• 0 t\\geq 0 italic_t ‚â• 0 , let p t p_{t} italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT be the density of ùê± t \\bm{x}_{t} bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . Then ‚àÇ p t ‚àÇ t ‚Äã ( ùùÉ ) \\displaystyle\\frac{\\partial p_{t}}{\\partial t}("}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmproposition2", "title": "Proposition B.2 .", "snippet": "Proposition B.2 . For t > 0 t>0 italic_t > 0 and ùõè ‚àà ‚Ñù D \\bm{\\xi}\\in\\mathbb{R}^{D} bold_italic_Œæ ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT it holds ‚àÇ ‚àÇ t ‚Äã œÜ t ‚Äã ( ùùÉ ) \\displaystyle\\frac{\\partial}{\\partial t}\\varphi_{t}(\\bm{\\xi}) divide start_ARG ‚àÇ end_ARG start_ARG ‚àÇ italic_t end_ARG italic_œÜ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) = œÜ t ‚Äã ( ùùÉ ) ‚ãÖ ‚Äñ ùùÉ ‚Äñ 2 2 ‚àí D ‚Äã t 2 t 3 \\displaystyle=\\varphi_{t}(\\bm{\\xi})\\cdot\\frac{\\|\\bm{\\xi}\\|_{2}^{2}-Dt^{2}}{t^{3}} = italic_œÜ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) ‚ãÖ divide start_ARG"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmproposition3", "title": "Proposition B.3 ( [ Jon82 ] , Section 11.12) .", "snippet": "Proposition B.3 ( [ Jon82 ] , Section 11.12) . Let f : ( 0 , T ) √ó ‚Ñù D ‚Üí ‚Ñù f\\colon(0,T)\\times\\mathbb{R}^{D}\\to\\mathbb{R} italic_f : ( 0 , italic_T ) √ó blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT ‚Üí blackboard_R be such that: ‚Ä¢ f f italic_f is a jointly measurable function of ( t , ùùÉ ) (t,\\bm{\\xi}) ( italic_t , bold_italic_Œæ ) ; ‚Ä¢ For Lebesgue-almost every ùùÉ ‚àà ‚Ñù D \\bm{\\xi}\\in\\mathbb{R}^{D} bold_italic_Œæ ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT , the function t ‚Ü¶ f t ‚Äã ( ùùÉ ) t\\mapsto f_{t}(\\bm{\\xi}) italic_t ‚Ü¶ italic_f start_POSTSUBSCRIPT italic_t end_"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmproposition4", "title": "Proposition B.4 ( [ BB11 ] , Proposition 4.20) .", "snippet": "Proposition B.4 ( [ BB11 ] , Proposition 4.20) . Let f f italic_f be k k italic_k -times continuously differentiable with compact support, and let g g italic_g be locally integrable. Then the convolution f ‚àó g f*g italic_f ‚àó italic_g defined by ( f ‚àó g ) ‚Äã ( ùùÉ ) ‚âê ‚à´ ‚Ñù D f ‚Äã ( ùíñ ) ‚Äã g ‚Äã ( ùùÉ ‚àí ùíñ ) ‚Äã d ùíñ (f*g)(\\bm{\\xi})\\doteq\\int_{\\mathbb{R}^{D}}f(\\bm{u})g(\\bm{\\xi}-\\bm{u})\\mathrm{d}\\bm{u} ( italic_f ‚àó italic_g ) ( bold_italic_Œæ ) ‚âê ‚à´ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_f ( bold_italic_u ) italic_g ( bold_italic_Œæ - bold_ital"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmassumption3", "title": "Assumption B.3 .", "snippet": "Assumption B.3 . The support ùíÆ ‚äÇ ‚Ñù D \\mathcal{S}\\subset\\mathbb{R}^{D} caligraphic_S ‚äÇ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT of the random variable ùíô \\bm{x} bold_italic_x is a finite union of K K italic_K spheres, each with dimension d k d_{k} italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , k ‚àà [ K ] k\\in[K] italic_k ‚àà [ italic_K ] . The probability that ùíô \\bm{x} bold_italic_x is drawn from the k k italic_k -th sphere is given by œÄ k ‚àà [ 0 , 1 ] \\pi_{k}\\in[0,1] italic_œÄ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ‚àà [ 0 , 1 ] , and conditional on being draw"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmdefinition1", "title": "Definition B.1 .", "snippet": "Definition B.1 . Let ùíÆ \\mathcal{S} caligraphic_S be a compact set. For any Œ¥ > 0 \\delta>0 italic_Œ¥ > 0 , define the Œ¥ \\delta italic_Œ¥ -thickening of ùíÆ \\mathcal{S} caligraphic_S , denoted ùíÆ Œ¥ \\mathcal{S}_{\\delta} caligraphic_S start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT , by ùíÆ Œ¥ = { ùùÉ ‚àà ‚Ñù D ‚à£ dist ‚Äã ( ùùÉ , ùíÆ ) ‚â§ Œ¥ } . \\mathcal{S}_{\\delta}=\\left\\{\\bm{\\xi}\\in\\mathbb{R}^{D}\\mid\\mathrm{dist}(\\bm{\\xi},\\mathcal{S})\\leq\\delta\\right\\}. caligraphic_S start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT = { bold_italic_Œæ ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT ‚à£ roman_dist ( bold_it"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmdefinition2", "title": "Definition B.2 .", "snippet": "Definition B.2 . Let ùíô \\bm{x} bold_italic_x be a random variable such that Supp ‚Å° ( ùíô ) = ùíÆ \\operatorname{Supp}(\\bm{x})=\\mathcal{S} roman_Supp ( bold_italic_x ) = caligraphic_S is a union of K K italic_K hyperspheres, distributed as in Assumption B.3 . Denote the support of each component of the mixture by ùíÆ k \\mathcal{S}_{k} caligraphic_S start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT . Define the thickened random variable ùíô Œ¥ \\bm{x}_{\\delta} bold_italic_x start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT as the mixture of measures where each component measure is uniform on the thickened set ùíÆ k"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#Thmlemma6", "title": "Lemma B.6 .", "snippet": "Lemma B.6 . Suppose the random variable ùê± \\bm{x} bold_italic_x satisfies Assumption B.3 . Then if 0 < Œ¥ < 1 2 0<\\delta<\\tfrac{1}{2} 0 < italic_Œ¥ < divide start_ARG 1 end_ARG start_ARG 2 end_ARG , the thickened random variable ùê± Œ¥ \\bm{x}_{\\delta} bold_italic_x start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT ( Definition B.2 ) satisfies for any œµ > 0 \\epsilon>0 italic_œµ > 0 R Œ¥ + œµ ‚Äã ( ùíô Œ¥ ) ‚â§ R œµ ‚Äã ( ùíô ) . R_{\\delta+\\epsilon}(\\bm{x}_{\\delta})\\leq R_{\\epsilon}(\\bm{x}). italic_R start_POSTSUBSCRIPT italic_Œ¥ + italic_œµ end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S1.E1", "title": "h ‚Äã ( ùíô ) ‚Äã ‚Äú=‚Äù ‚Äã lim Œµ ‚Üò 0 h ‚Äã ( ùíô Œµ ) . h(\\bm{x})\\ \\text{``=''}\\ \\lim_{\\varepsilon\\searrow 0}h(\\bm{x}_{\\varepsilon}). italic_h ( bold_italic_x ) ‚Äú=‚Äù roman_lim start_POSTSUBSCRIPT italic_Œµ ‚Üò 0 end_PO", "snippet": "h ‚Äã ( ùíô ) ‚Äã ‚Äú=‚Äù ‚Äã lim Œµ ‚Üò 0 h ‚Äã ( ùíô Œµ ) . h(\\bm{x})\\ \\text{``=''}\\ \\lim_{\\varepsilon\\searrow 0}h(\\bm{x}_{\\varepsilon}). italic_h ( bold_italic_x ) ‚Äú=‚Äù roman_lim start_POSTSUBSCRIPT italic_Œµ ‚Üò 0 end_POSTSUBSCRIPT italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_Œµ end_POSTSUBSCRIPT ) . (B.1.1)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S1.E2", "title": "S Œµ = ‚ãÉ ùùÉ ‚àà ùíÆ B Œµ ‚Äã ( ùùÉ ) S_{\\varepsilon}=\\bigcup_{\\bm{\\xi}\\in\\mathcal{S}}B_{\\varepsilon}(\\bm{\\xi}) italic_S start_POSTSUBSCRIPT italic_Œµ end_POSTSUBSCRIPT = ‚ãÉ start_POSTSUBSCRIPT bold_italic_Œæ ‚àà cali", "snippet": "S Œµ = ‚ãÉ ùùÉ ‚àà ùíÆ B Œµ ‚Äã ( ùùÉ ) S_{\\varepsilon}=\\bigcup_{\\bm{\\xi}\\in\\mathcal{S}}B_{\\varepsilon}(\\bm{\\xi}) italic_S start_POSTSUBSCRIPT italic_Œµ end_POSTSUBSCRIPT = ‚ãÉ start_POSTSUBSCRIPT bold_italic_Œæ ‚àà caligraphic_S end_POSTSUBSCRIPT italic_B start_POSTSUBSCRIPT italic_Œµ end_POSTSUBSCRIPT ( bold_italic_Œæ ) (B.1.2)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S1.E3", "title": "p Œµ ‚Äã ( ùùÉ ) = ùüè ‚Äã ( ùùÉ ‚àà ùíÆ Œµ ) ‚ãÖ 1 vol ‚Å° ( ùíÆ Œµ ) . p_{\\varepsilon}(\\bm{\\xi})=\\mathbf{1}(\\bm{\\xi}\\in\\mathcal{S}_{\\varepsilon})\\cdot\\frac{1}{\\operatorname{vol}(\\mathcal{S}_{\\varepsilon})}. italic_p start", "snippet": "p Œµ ‚Äã ( ùùÉ ) = ùüè ‚Äã ( ùùÉ ‚àà ùíÆ Œµ ) ‚ãÖ 1 vol ‚Å° ( ùíÆ Œµ ) . p_{\\varepsilon}(\\bm{\\xi})=\\mathbf{1}(\\bm{\\xi}\\in\\mathcal{S}_{\\varepsilon})\\cdot\\frac{1}{\\operatorname{vol}(\\mathcal{S}_{\\varepsilon})}. italic_p start_POSTSUBSCRIPT italic_Œµ end_POSTSUBSCRIPT ( bold_italic_Œæ ) = bold_1 ( bold_italic_Œæ ‚àà caligraphic_S start_POSTSUBSCRIPT italic_Œµ end_POSTSUBSCRIPT ) ‚ãÖ divide start_ARG 1 end_ARG start_ARG roman_vol ( caligraphic_S start_POSTSUBSCRIPT italic_Œµ end_POSTSUBSCRIPT ) end_ARG . (B.1.3)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S1.E8", "title": "h ‚Äã ( ùíô ) = lim Œµ ‚Üò 0 h ‚Äã ( ùíô Œµ ) = lim Œµ ‚Üò 0 log ‚Å° ( vol ‚Å° ( ùíÆ Œµ ) ) = ‚àí ‚àû , h(\\bm{x})=\\lim_{\\varepsilon\\searrow 0}h(\\bm{x}_{\\varepsilon})=\\lim_{\\varepsilon\\searrow 0}\\log(\\operatorname{vol}(\\mathcal", "snippet": "h ‚Äã ( ùíô ) = lim Œµ ‚Üò 0 h ‚Äã ( ùíô Œµ ) = lim Œµ ‚Üò 0 log ‚Å° ( vol ‚Å° ( ùíÆ Œµ ) ) = ‚àí ‚àû , h(\\bm{x})=\\lim_{\\varepsilon\\searrow 0}h(\\bm{x}_{\\varepsilon})=\\lim_{\\varepsilon\\searrow 0}\\log(\\operatorname{vol}(\\mathcal{S}_{\\varepsilon}))=-\\infty, italic_h ( bold_italic_x ) = roman_lim start_POSTSUBSCRIPT italic_Œµ ‚Üò 0 end_POSTSUBSCRIPT italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_Œµ end_POSTSUBSCRIPT ) = roman_lim start_POSTSUBSCRIPT italic_Œµ ‚Üò 0 end_POSTSUBSCRIPT roman_log ( roman_vol ( caligraphic_S start_POSTSUBSCRIPT italic_Œµ end_POSTSUBSCRIPT ) ) = - ‚àû , (B.1.8)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S1.E9", "title": "h ‚Äã ( ùíô ) ‚â§ h ‚Äã ( ùí∞ ‚Å° ( ùíÆ ) ) = log ‚Å° vol ‚Å° ( ùíÆ ) . h(\\bm{x})\\leq h(\\operatorname{\\mathcal{U}}(\\mathcal{S}))=\\log\\operatorname{vol}(\\mathcal{S}). italic_h ( bold_italic_x ) ‚â§ italic_h ( caligraphic_U ", "snippet": "h ‚Äã ( ùíô ) ‚â§ h ‚Äã ( ùí∞ ‚Å° ( ùíÆ ) ) = log ‚Å° vol ‚Å° ( ùíÆ ) . h(\\bm{x})\\leq h(\\operatorname{\\mathcal{U}}(\\mathcal{S}))=\\log\\operatorname{vol}(\\mathcal{S}). italic_h ( bold_italic_x ) ‚â§ italic_h ( caligraphic_U ( caligraphic_S ) ) = roman_log roman_vol ( caligraphic_S ) . (B.1.9)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S1.E10", "title": "h ‚Äã ( ùíô ) ‚â§ h ‚Äã ( ùí© ‚Å° ( ùüé , ùö∫ ) ) = 1 2 ‚Äã log ‚Å° ( ( 2 ‚Äã œÄ ‚Äã e ) D ‚Äã det ùö∫ ) . h(\\bm{x})\\leq h(\\operatorname{\\mathcal{N}}(\\bm{0},\\bm{\\Sigma}))=\\frac{1}{2}\\log((2\\pi e)^{D}\\det\\bm{\\Sigma}). italic_h ( b", "snippet": "h ‚Äã ( ùíô ) ‚â§ h ‚Äã ( ùí© ‚Å° ( ùüé , ùö∫ ) ) = 1 2 ‚Äã log ‚Å° ( ( 2 ‚Äã œÄ ‚Äã e ) D ‚Äã det ùö∫ ) . h(\\bm{x})\\leq h(\\operatorname{\\mathcal{N}}(\\bm{0},\\bm{\\Sigma}))=\\frac{1}{2}\\log((2\\pi e)^{D}\\det\\bm{\\Sigma}). italic_h ( bold_italic_x ) ‚â§ italic_h ( caligraphic_N ( bold_0 , bold_Œ£ ) ) = divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_log ( ( 2 italic_œÄ italic_e ) start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT roman_det bold_Œ£ ) . (B.1.10)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S1.E11", "title": "h ‚Äã ( ùíô ) ‚â§ h ‚Äã ( ùí© ‚Å° ( ùüé , a D ‚Äã ùë∞ ) ) = D 2 ‚Äã log ‚Å° 2 ‚Äã œÄ ‚Äã e ‚Äã a D . h(\\bm{x})\\leq h\\left(\\operatorname{\\mathcal{N}}\\left(\\bm{0},\\frac{a}{D}\\bm{I}\\right)\\right)=\\frac{D}{2}\\log\\frac{2\\pi ea}{D}. it", "snippet": "h ‚Äã ( ùíô ) ‚â§ h ‚Äã ( ùí© ‚Å° ( ùüé , a D ‚Äã ùë∞ ) ) = D 2 ‚Äã log ‚Å° 2 ‚Äã œÄ ‚Äã e ‚Äã a D . h(\\bm{x})\\leq h\\left(\\operatorname{\\mathcal{N}}\\left(\\bm{0},\\frac{a}{D}\\bm{I}\\right)\\right)=\\frac{D}{2}\\log\\frac{2\\pi ea}{D}. italic_h ( bold_italic_x ) ‚â§ italic_h ( caligraphic_N ( bold_0 , divide start_ARG italic_a end_ARG start_ARG italic_D end_ARG bold_italic_I ) ) = divide start_ARG italic_D end_ARG start_ARG 2 end_ARG roman_log divide start_ARG 2 italic_œÄ italic_e italic_a end_ARG start_ARG italic_D end_ARG . (B.1.11)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E1", "title": "ùíô t = ùíô + t ‚Äã ùíà , ‚àÄ t ‚àà [ 0 , T ] \\bm{x}_{t}=\\bm{x}+t\\bm{g},\\qquad\\forall t\\in[0,T] bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_x + italic_t bold_italic_g , ‚àÄ italic_t ‚àà", "snippet": "ùíô t = ùíô + t ‚Äã ùíà , ‚àÄ t ‚àà [ 0 , T ] \\bm{x}_{t}=\\bm{x}+t\\bm{g},\\qquad\\forall t\\in[0,T] bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_x + italic_t bold_italic_g , ‚àÄ italic_t ‚àà [ 0 , italic_T ] (B.2.1)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E2", "title": "œÜ t ‚Äã ( ùùÉ ) ‚âê 1 ( 2 ‚Äã œÄ ) D / 2 ‚Äã t D ‚Äã exp ‚Å° ( ‚àí ‚Äñ ùùÉ ‚Äñ 2 2 2 ‚Äã t 2 ) . \\varphi_{t}(\\bm{\\xi})\\doteq\\frac{1}{(2\\pi)^{D/2}t^{D}}\\exp\\left(-\\frac{\\|\\bm{\\xi}\\|_{2}^{2}}{2t^{2}}\\right). italic_œÜ start_POST", "snippet": "œÜ t ‚Äã ( ùùÉ ) ‚âê 1 ( 2 ‚Äã œÄ ) D / 2 ‚Äã t D ‚Äã exp ‚Å° ( ‚àí ‚Äñ ùùÉ ‚Äñ 2 2 2 ‚Äã t 2 ) . \\varphi_{t}(\\bm{\\xi})\\doteq\\frac{1}{(2\\pi)^{D/2}t^{D}}\\exp\\left(-\\frac{\\|\\bm{\\xi}\\|_{2}^{2}}{2t^{2}}\\right). italic_œÜ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) ‚âê divide start_ARG 1 end_ARG start_ARG ( 2 italic_œÄ ) start_POSTSUPERSCRIPT italic_D / 2 end_POSTSUPERSCRIPT italic_t start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_ARG roman_exp ( - divide start_ARG ‚à• bold_italic_Œæ ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG 2 italic_t start_"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E3", "title": "p t ‚Äã ( ùùÉ ) = ùîº ‚Å° [ œÜ t ‚Äã ( ùùÉ ‚àí ùíô ) ] , p_{t}(\\bm{\\xi})=\\operatorname{\\mathbb{E}}[\\varphi_{t}(\\bm{\\xi}-\\bm{x})], italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) = blackboard_", "snippet": "p t ‚Äã ( ùùÉ ) = ùîº ‚Å° [ œÜ t ‚Äã ( ùùÉ ‚àí ùíô ) ] , p_{t}(\\bm{\\xi})=\\operatorname{\\mathbb{E}}[\\varphi_{t}(\\bm{\\xi}-\\bm{x})], italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) = blackboard_E [ italic_œÜ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ - bold_italic_x ) ] , (B.2.3)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E4", "title": "h ‚Äã ( ùíô s ) < h ‚Äã ( ùíô t ) , ‚àÄ s , t : 0 ‚â§ s < t ‚â§ T . h(\\bm{x}_{s})<h(\\bm{x}_{t}),\\qquad\\forall s,t\\colon 0\\leq s<t\\leq T. italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) < i", "snippet": "h ‚Äã ( ùíô s ) < h ‚Äã ( ùíô t ) , ‚àÄ s , t : 0 ‚â§ s < t ‚â§ T . h(\\bm{x}_{s})<h(\\bm{x}_{t}),\\qquad\\forall s,t\\colon 0\\leq s<t\\leq T. italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) < italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , ‚àÄ italic_s , italic_t : 0 ‚â§ italic_s < italic_t ‚â§ italic_T . (B.2.4)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E5", "title": "Œî ‚Äã f t ‚Äã ( ùùÉ ) = tr ‚Å° ( ‚àá 2 f t ‚Äã ( ùùÉ ) ) = ‚àë i = 1 D ‚àÇ 2 f t ‚àÇ Œæ i 2 ‚Äã ( ùùÉ ) . \\Delta f_{t}(\\bm{\\xi})=\\operatorname{tr}(\\nabla^{2}f_{t}(\\bm{\\xi}))=\\sum_{i=1}^{D}\\frac{\\partial^{2}f_{t}}{\\partial\\xi_", "snippet": "Œî ‚Äã f t ‚Äã ( ùùÉ ) = tr ‚Å° ( ‚àá 2 f t ‚Äã ( ùùÉ ) ) = ‚àë i = 1 D ‚àÇ 2 f t ‚àÇ Œæ i 2 ‚Äã ( ùùÉ ) . \\Delta f_{t}(\\bm{\\xi})=\\operatorname{tr}(\\nabla^{2}f_{t}(\\bm{\\xi}))=\\sum_{i=1}^{D}\\frac{\\partial^{2}f_{t}}{\\partial\\xi_{i}^{2}}(\\bm{\\xi}). roman_Œî italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) = roman_tr ( ‚àá start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) ) = ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT divide start_ARG ‚àÇ start_POSTSUPERSCRIPT 2 end_POSTSUPERS"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E6", "title": "‚àÇ p t ‚àÇ t ‚Äã ( ùùÉ ) = t ‚Äã Œî ‚Äã p t ‚Äã ( ùùÉ ) . \\frac{\\partial p_{t}}{\\partial t}(\\bm{\\xi})=t\\Delta p_{t}(\\bm{\\xi}). divide start_ARG ‚àÇ italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_", "snippet": "‚àÇ p t ‚àÇ t ‚Äã ( ùùÉ ) = t ‚Äã Œî ‚Äã p t ‚Äã ( ùùÉ ) . \\frac{\\partial p_{t}}{\\partial t}(\\bm{\\xi})=t\\Delta p_{t}(\\bm{\\xi}). divide start_ARG ‚àÇ italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG ‚àÇ italic_t end_ARG ( bold_italic_Œæ ) = italic_t roman_Œî italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) . (B.2.6)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E14", "title": "h ‚Äã ( ùíô t ) = h ‚Äã ( ùíô s ) + ‚à´ s t d d ‚Äã u ‚Äã h ‚Äã ( ùíô u ) ‚Äã d u > h ‚Äã ( ùíô s ) , h(\\bm{x}_{t})=h(\\bm{x}_{s})+\\int_{s}^{t}\\frac{\\mathrm{d}}{\\mathrm{d}u}h(\\bm{x}_{u})\\mathrm{d}u>h(\\bm{x}_{s}), italic_h ( b", "snippet": "h ‚Äã ( ùíô t ) = h ‚Äã ( ùíô s ) + ‚à´ s t d d ‚Äã u ‚Äã h ‚Äã ( ùíô u ) ‚Äã d u > h ‚Äã ( ùíô s ) , h(\\bm{x}_{t})=h(\\bm{x}_{s})+\\int_{s}^{t}\\frac{\\mathrm{d}}{\\mathrm{d}u}h(\\bm{x}_{u})\\mathrm{d}u>h(\\bm{x}_{s}), italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) + ‚à´ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT divide start_ARG roman_d end_ARG start_ARG roman_d italic_u end_ARG italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT ) roman_d italic_u >"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E15", "title": "ùíô ^ s ‚âê ùîº ‚Å° [ ùíô s ‚à£ ùíô t = ùíô ^ t ] = s t ‚Äã ùíô ^ t + ( 1 ‚àí s t ) ‚Äã ùíô ¬Ø ‚àó ‚Äã ( t , ùíô ^ t ) . \\hat{\\bm{x}}_{s}\\doteq\\operatorname{\\mathbb{E}}[\\bm{x}_{s}\\mid\\bm{x}_{t}=\\hat{\\bm{x}}_{t}]=\\frac{s}{t}\\hat{\\bm{x", "snippet": "ùíô ^ s ‚âê ùîº ‚Å° [ ùíô s ‚à£ ùíô t = ùíô ^ t ] = s t ‚Äã ùíô ^ t + ( 1 ‚àí s t ) ‚Äã ùíô ¬Ø ‚àó ‚Äã ( t , ùíô ^ t ) . \\hat{\\bm{x}}_{s}\\doteq\\operatorname{\\mathbb{E}}[\\bm{x}_{s}\\mid\\bm{x}_{t}=\\hat{\\bm{x}}_{t}]=\\frac{s}{t}\\hat{\\bm{x}}_{t}+\\left(1-\\frac{s}{t}\\right)\\bar{\\bm{x}}^{\\ast}(t,\\hat{\\bm{x}}_{t}). over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ‚âê blackboard_E [ bold_italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] = divide st"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E16", "title": "ùíô ¬Ø ‚àó ‚Äã ( t , ùíô t ) = ùíô t + t 2 ‚Äã ‚àá p t ‚Äã ( ùíô t ) , \\bar{\\bm{x}}^{\\ast}(t,\\bm{x}_{t})=\\bm{x}_{t}+t^{2}\\nabla p_{t}(\\bm{x}_{t}), over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ‚àó end_POSTSU", "snippet": "ùíô ¬Ø ‚àó ‚Äã ( t , ùíô t ) = ùíô t + t 2 ‚Äã ‚àá p t ‚Äã ( ùíô t ) , \\bar{\\bm{x}}^{\\ast}(t,\\bm{x}_{t})=\\bm{x}_{t}+t^{2}\\nabla p_{t}(\\bm{x}_{t}), over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ‚àá italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , (B.2.16)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E17", "title": "ùîº ‚Å° [ ùíô s ‚à£ ùíô t ] = s t ‚Äã ùíô t + ( 1 ‚àí s t ) ‚Äã ( ùíô t + t 2 ‚Äã ‚àá ùíô t log ‚Å° p t ‚Äã ( ùíô t ) ) = ùíô t + ( 1 ‚àí s t ) ‚Äã t 2 ‚Äã ‚àá ùíô t log ‚Å° p t ‚Äã ( ùíô t ) . \\operatorname{\\mathbb{E}}[\\bm{x}_{s}\\mid\\bm{x}_{t}]=\\fra", "snippet": "ùîº ‚Å° [ ùíô s ‚à£ ùíô t ] = s t ‚Äã ùíô t + ( 1 ‚àí s t ) ‚Äã ( ùíô t + t 2 ‚Äã ‚àá ùíô t log ‚Å° p t ‚Äã ( ùíô t ) ) = ùíô t + ( 1 ‚àí s t ) ‚Äã t 2 ‚Äã ‚àá ùíô t log ‚Å° p t ‚Äã ( ùíô t ) . \\operatorname{\\mathbb{E}}[\\bm{x}_{s}\\mid\\bm{x}_{t}]=\\frac{s}{t}\\bm{x}_{t}+\\left(1-\\frac{s}{t}\\right)\\left(\\bm{x}_{t}+t^{2}\\nabla_{\\bm{x}_{t}}\\log p_{t}(\\bm{x}_{t})\\right)=\\bm{x}_{t}+\\left(1-\\frac{s}{t}\\right)t^{2}\\nabla_{\\bm{x}_{t}}\\log p_{t}(\\bm{x}_{t}). blackboard_E [ bold_italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] = divide start_ARG italic_s end_ARG start_ARG italic_t end_"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E18", "title": "h ( ùîº [ ùíô s ‚à£ ùíô t ] ) < h ( ùíô t ) , ‚àÄ s , t ‚àà [ 0 , T ] : 0 < t ‚â§ R 2 ‚Äã D , 0 ‚â§ s < t ‚ãÖ min { 1 , R 2 / D ‚àí 2 ‚Äã t 2 R 2 / D ‚àí t 2 } . h(\\operatorname{\\mathbb{E}}[\\bm{x}_{s}\\mid\\bm{x}_{t}])<h(\\bm{x}_{t", "snippet": "h ( ùîº [ ùíô s ‚à£ ùíô t ] ) < h ( ùíô t ) , ‚àÄ s , t ‚àà [ 0 , T ] : 0 < t ‚â§ R 2 ‚Äã D , 0 ‚â§ s < t ‚ãÖ min { 1 , R 2 / D ‚àí 2 ‚Äã t 2 R 2 / D ‚àí t 2 } . h(\\operatorname{\\mathbb{E}}[\\bm{x}_{s}\\mid\\bm{x}_{t}])<h(\\bm{x}_{t}),\\qquad\\forall s,t\\in[0,T]\\colon\\quad 0<t\\leq\\frac{R}{\\sqrt{2D}},\\quad 0\\leq s<t\\cdot\\min\\left\\{1,\\frac{R^{2}/D-2t^{2}}{R^{2}/D-t^{2}}\\right\\}. italic_h ( blackboard_E [ bold_italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] ) < italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , ‚àÄ italic_s , italic_t "}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E19", "title": "p ¬Ø ‚Äã ( ùùÉ ) ‚âê ( p t ‚àò ùíô ¬Ø ‚àí 1 ) ‚Äã ( ùùÉ ) det ( ùíô ¬Ø ‚Ä≤ ‚Äã ( ùíô ¬Ø ‚àí 1 ‚Äã ( ùùÉ ) ) ) , \\bar{p}(\\bm{\\xi})\\doteq\\frac{(p_{t}\\circ\\bar{\\bm{x}}^{-1})(\\bm{\\xi})}{\\det(\\bar{\\bm{x}}^{\\prime}(\\bar{\\bm{x}}^{-1}(\\bm{\\xi", "snippet": "p ¬Ø ‚Äã ( ùùÉ ) ‚âê ( p t ‚àò ùíô ¬Ø ‚àí 1 ) ‚Äã ( ùùÉ ) det ( ùíô ¬Ø ‚Ä≤ ‚Äã ( ùíô ¬Ø ‚àí 1 ‚Äã ( ùùÉ ) ) ) , \\bar{p}(\\bm{\\xi})\\doteq\\frac{(p_{t}\\circ\\bar{\\bm{x}}^{-1})(\\bm{\\xi})}{\\det(\\bar{\\bm{x}}^{\\prime}(\\bar{\\bm{x}}^{-1}(\\bm{\\xi})))}, over¬Ø start_ARG italic_p end_ARG ( bold_italic_Œæ ) ‚âê divide start_ARG ( italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ‚àò over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ) ( bold_italic_Œæ ) end_ARG start_ARG roman_det ( over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( over¬Ø start_ARG bold_italic_x end_ARG start_P"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E30", "title": "det ( ùë¥ ) 1 / D = ‚àè i = 1 D Œª i ‚Äã ( ùë¥ ) 1 / D ‚â§ ‚àë i = 1 D Œª i ‚Äã ( ùë¥ ) D = tr ‚Å° ( ùë¥ ) D , \\det(\\bm{M})^{1/D}=\\prod_{i=1}^{D}\\lambda_{i}(\\bm{M})^{1/D}\\leq\\frac{\\sum_{i=1}^{D}\\lambda_{i}(\\bm{M})}{D}=\\fra", "snippet": "det ( ùë¥ ) 1 / D = ‚àè i = 1 D Œª i ‚Äã ( ùë¥ ) 1 / D ‚â§ ‚àë i = 1 D Œª i ‚Äã ( ùë¥ ) D = tr ‚Å° ( ùë¥ ) D , \\det(\\bm{M})^{1/D}=\\prod_{i=1}^{D}\\lambda_{i}(\\bm{M})^{1/D}\\leq\\frac{\\sum_{i=1}^{D}\\lambda_{i}(\\bm{M})}{D}=\\frac{\\operatorname{tr}(\\bm{M})}{D}, roman_det ( bold_italic_M ) start_POSTSUPERSCRIPT 1 / italic_D end_POSTSUPERSCRIPT = ‚àè start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT italic_Œª start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_italic_M ) start_POSTSUPERSCRIPT 1 / italic_D end_POSTSUPERSCRIPT ‚â§ divide start_ARG ‚àë start_POSTSUBSCRIPT italic_i"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E35", "title": "| Œî log p t ( ùùÉ ) | ‚â§ max ( D t 2 , | R 2 t 4 ‚àí D t 2 | ) = : U t . \\lvert\\Delta\\log p_{t}(\\bm{\\xi})\\rvert\\leq\\max\\left(\\frac{D}{t^{2}},\\left\\lvert\\frac{R^{2}}{t^{4}}-\\frac{D}{t^{2}}\\right\\rvert\\right", "snippet": "| Œî log p t ( ùùÉ ) | ‚â§ max ( D t 2 , | R 2 t 4 ‚àí D t 2 | ) = : U t . \\lvert\\Delta\\log p_{t}(\\bm{\\xi})\\rvert\\leq\\max\\left(\\frac{D}{t^{2}},\\left\\lvert\\frac{R^{2}}{t^{4}}-\\frac{D}{t^{2}}\\right\\rvert\\right)=:U_{t}. | roman_Œî roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) | ‚â§ roman_max ( divide start_ARG italic_D end_ARG start_ARG italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG , | divide start_ARG italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_t start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT end_ARG - divide start_ARG ita"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E36", "title": "‚àí ( 1 ‚àí s t ) ‚Äã t 2 D ‚Äã U t ‚â§ ( 1 ‚àí s t ) ‚Äã t 2 D ‚Äã Œî ‚Äã log ‚Å° p t ‚Äã ( ùùÉ ) ‚â§ ( 1 ‚àí s t ) ‚Äã t 2 D ‚Äã U t . -\\frac{\\left(1-\\frac{s}{t}\\right)t^{2}}{D}U_{t}\\leq\\frac{\\left(1-\\frac{s}{t}\\right)t^{2}}{D}\\Del", "snippet": "‚àí ( 1 ‚àí s t ) ‚Äã t 2 D ‚Äã U t ‚â§ ( 1 ‚àí s t ) ‚Äã t 2 D ‚Äã Œî ‚Äã log ‚Å° p t ‚Äã ( ùùÉ ) ‚â§ ( 1 ‚àí s t ) ‚Äã t 2 D ‚Äã U t . -\\frac{\\left(1-\\frac{s}{t}\\right)t^{2}}{D}U_{t}\\leq\\frac{\\left(1-\\frac{s}{t}\\right)t^{2}}{D}\\Delta\\log p_{t}(\\bm{\\xi})\\leq\\frac{\\left(1-\\frac{s}{t}\\right)t^{2}}{D}U_{t}. - divide start_ARG ( 1 - divide start_ARG italic_s end_ARG start_ARG italic_t end_ARG ) italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_D end_ARG italic_U start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ‚â§ divide start_ARG ( 1 - divide start_ARG italic_s end_ARG start_ARG italic_t end_ARG ) italic"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E46", "title": "h ‚Äã ( ùíô ¬Ø ‚Äã ( ùíô t ) ) ‚àí h ‚Äã ( ùíô t ) ‚â§ ‚àí M ‚Äã ( s , t , D ) ‚Äã ‚à´ ‚Ñù D ‚Äñ ‚àá p t ‚Äã ( ùùÉ ) ‚Äñ 2 2 p t ‚Äã ( ùùÉ ) ‚Äã d ùùÉ < 0 h(\\bar{\\bm{x}}(\\bm{x}_{t}))-h(\\bm{x}_{t})\\leq-M(s,t,D)\\int_{\\mathbb{R}^{D}}\\frac{\\|\\nabla ", "snippet": "h ‚Äã ( ùíô ¬Ø ‚Äã ( ùíô t ) ) ‚àí h ‚Äã ( ùíô t ) ‚â§ ‚àí M ‚Äã ( s , t , D ) ‚Äã ‚à´ ‚Ñù D ‚Äñ ‚àá p t ‚Äã ( ùùÉ ) ‚Äñ 2 2 p t ‚Äã ( ùùÉ ) ‚Äã d ùùÉ < 0 h(\\bar{\\bm{x}}(\\bm{x}_{t}))-h(\\bm{x}_{t})\\leq-M(s,t,D)\\int_{\\mathbb{R}^{D}}\\frac{\\|\\nabla p_{t}(\\bm{\\xi})\\|_{2}^{2}}{p_{t}(\\bm{\\xi})}\\mathrm{d}\\bm{\\xi}<0 italic_h ( over¬Ø start_ARG bold_italic_x end_ARG ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ) - italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ‚â§ - italic_M ( italic_s , italic_t , italic_D ) ‚à´ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSU"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E47", "title": "h ‚Äã ( ùíô t ) = ‚àí ‚à´ ‚Ñù D p t ‚Äã ( ùùÉ ) ‚Äã log ‚Å° p t ‚Äã ( ùùÉ ) ‚Äã d ùùÉ . h(\\bm{x}_{t})=-\\int_{\\mathbb{R}^{D}}p_{t}(\\bm{\\xi})\\log p_{t}(\\bm{\\xi})\\mathrm{d}\\bm{\\xi}. italic_h ( bold_italic_x start_POSTSUBSCRIPT it", "snippet": "h ‚Äã ( ùíô t ) = ‚àí ‚à´ ‚Ñù D p t ‚Äã ( ùùÉ ) ‚Äã log ‚Å° p t ‚Äã ( ùùÉ ) ‚Äã d ùùÉ . h(\\bm{x}_{t})=-\\int_{\\mathbb{R}^{D}}p_{t}(\\bm{\\xi})\\log p_{t}(\\bm{\\xi})\\mathrm{d}\\bm{\\xi}. italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = - ‚à´ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) roman_d bold_italic_Œæ . (B.2.47)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E48", "title": "g ‚Äã ( ùùÉ ) ‚âê ‚àí p t ‚Äã ( ùùÉ ) ‚Äã log ‚Å° p t ‚Äã ( ùùÉ ) ‚üπ h ‚Äã ( ùíô t ) = ‚à´ ‚Ñù D g ‚Äã ( ùùÉ ) ‚Äã d ùùÉ . g(\\bm{\\xi})\\doteq-p_{t}(\\bm{\\xi})\\log p_{t}(\\bm{\\xi})\\implies h(\\bm{x}_{t})=\\int_{\\mathbb{R}^{D}}g(\\bm{\\xi})\\mathr", "snippet": "g ‚Äã ( ùùÉ ) ‚âê ‚àí p t ‚Äã ( ùùÉ ) ‚Äã log ‚Å° p t ‚Äã ( ùùÉ ) ‚üπ h ‚Äã ( ùíô t ) = ‚à´ ‚Ñù D g ‚Äã ( ùùÉ ) ‚Äã d ùùÉ . g(\\bm{\\xi})\\doteq-p_{t}(\\bm{\\xi})\\log p_{t}(\\bm{\\xi})\\implies h(\\bm{x}_{t})=\\int_{\\mathbb{R}^{D}}g(\\bm{\\xi})\\mathrm{d}\\bm{\\xi}. italic_g ( bold_italic_Œæ ) ‚âê - italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) ‚üπ italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = ‚à´ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_g ( bold"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E49", "title": "g + ‚Äã ( ùùÉ ) ‚âê max ‚Å° ( g ‚Äã ( ùùÉ ) , 0 ) , g ‚àí ‚Äã ( ùùÉ ) ‚âê max ‚Å° ( ‚àí g ‚Äã ( ùùÉ ) , 0 ) ‚üπ g = g + ‚àí g ‚àí and g + , g ‚àí ‚â• 0 . g_{+}(\\bm{\\xi})\\doteq\\max(g(\\bm{\\xi}),0),\\quad g_{-}(\\bm{\\xi})\\doteq\\max(-g(\\bm{\\xi}", "snippet": "g + ‚Äã ( ùùÉ ) ‚âê max ‚Å° ( g ‚Äã ( ùùÉ ) , 0 ) , g ‚àí ‚Äã ( ùùÉ ) ‚âê max ‚Å° ( ‚àí g ‚Äã ( ùùÉ ) , 0 ) ‚üπ g = g + ‚àí g ‚àí and g + , g ‚àí ‚â• 0 . g_{+}(\\bm{\\xi})\\doteq\\max(g(\\bm{\\xi}),0),\\quad g_{-}(\\bm{\\xi})\\doteq\\max(-g(\\bm{\\xi}),0)\\quad\\implies\\quad g=g_{+}-g_{-}\\quad\\text{and}\\quad g_{+},g_{-}\\geq 0. italic_g start_POSTSUBSCRIPT + end_POSTSUBSCRIPT ( bold_italic_Œæ ) ‚âê roman_max ( italic_g ( bold_italic_Œæ ) , 0 ) , italic_g start_POSTSUBSCRIPT - end_POSTSUBSCRIPT ( bold_italic_Œæ ) ‚âê roman_max ( - italic_g ( bold_italic_Œæ ) , 0 ) ‚üπ italic_g = italic_g start_POSTSUBSCRIPT + end_POSTSUBSCRIPT - italic_g start_POSTSUBSCRIPT"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E50", "title": "h ‚Äã ( ùíô t ) = ‚à´ ‚Ñù D g + ‚Äã ( ùùÉ ) ‚Äã d ùùÉ ‚àí ‚à´ ‚Ñù D g ‚àí ‚Äã ( ùùÉ ) ‚Äã d ùùÉ , h(\\bm{x}_{t})=\\int_{\\mathbb{R}^{D}}g_{+}(\\bm{\\xi})\\mathrm{d}\\bm{\\xi}-\\int_{\\mathbb{R}^{D}}g_{-}(\\bm{\\xi})\\mathrm{d}\\bm{\\xi}, italic_h ", "snippet": "h ‚Äã ( ùíô t ) = ‚à´ ‚Ñù D g + ‚Äã ( ùùÉ ) ‚Äã d ùùÉ ‚àí ‚à´ ‚Ñù D g ‚àí ‚Äã ( ùùÉ ) ‚Äã d ùùÉ , h(\\bm{x}_{t})=\\int_{\\mathbb{R}^{D}}g_{+}(\\bm{\\xi})\\mathrm{d}\\bm{\\xi}-\\int_{\\mathbb{R}^{D}}g_{-}(\\bm{\\xi})\\mathrm{d}\\bm{\\xi}, italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = ‚à´ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_g start_POSTSUBSCRIPT + end_POSTSUBSCRIPT ( bold_italic_Œæ ) roman_d bold_italic_Œæ - ‚à´ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_g start_POSTSUBSCRIPT - end"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E51", "title": "g ‚Äã ( ùùÉ ) ‚â§ 0 ‚áî p t ‚Äã ( ùùÉ ) ‚Äã log ‚Å° p t ‚Äã ( ùùÉ ) ‚â• 0 ‚áî log ‚Å° p t ‚Äã ( ùùÉ ) ‚â• 0 ‚áî p t ‚Äã ( ùùÉ ) ‚â• 1 . g(\\bm{\\xi})\\leq 0\\iff p_{t}(\\bm{\\xi})\\log p_{t}(\\bm{\\xi})\\geq 0\\iff\\log p_{t}(\\bm{\\xi})\\geq 0\\iff p_{t}(", "snippet": "g ‚Äã ( ùùÉ ) ‚â§ 0 ‚áî p t ‚Äã ( ùùÉ ) ‚Äã log ‚Å° p t ‚Äã ( ùùÉ ) ‚â• 0 ‚áî log ‚Å° p t ‚Äã ( ùùÉ ) ‚â• 0 ‚áî p t ‚Äã ( ùùÉ ) ‚â• 1 . g(\\bm{\\xi})\\leq 0\\iff p_{t}(\\bm{\\xi})\\log p_{t}(\\bm{\\xi})\\geq 0\\iff\\log p_{t}(\\bm{\\xi})\\geq 0\\iff p_{t}(\\bm{\\xi})\\geq 1. italic_g ( bold_italic_Œæ ) ‚â§ 0 ‚áî italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) ‚â• 0 ‚áî roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) ‚â• 0 ‚áî italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) ‚â• 1 . (B.2.51)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E52", "title": "g ‚àí ‚Äã ( ùùÉ ) = ùüè ‚Äã ( p t ‚Äã ( ùùÉ ) ‚â• 1 ) ‚ãÖ ( ‚àí g ‚Äã ( ùùÉ ) ) = ùüè ‚Äã ( p t ‚Äã ( ùùÉ ) ‚â• 1 ) ‚Äã p t ‚Äã ( ùùÉ ) ‚Äã log ‚Å° p t ‚Äã ( ùùÉ ) . g_{-}(\\bm{\\xi})=\\mathbf{1}(p_{t}(\\bm{\\xi})\\geq 1)\\cdot(-g(\\bm{\\xi}))=\\mathbf{1}(p_", "snippet": "g ‚àí ‚Äã ( ùùÉ ) = ùüè ‚Äã ( p t ‚Äã ( ùùÉ ) ‚â• 1 ) ‚ãÖ ( ‚àí g ‚Äã ( ùùÉ ) ) = ùüè ‚Äã ( p t ‚Äã ( ùùÉ ) ‚â• 1 ) ‚Äã p t ‚Äã ( ùùÉ ) ‚Äã log ‚Å° p t ‚Äã ( ùùÉ ) . g_{-}(\\bm{\\xi})=\\mathbf{1}(p_{t}(\\bm{\\xi})\\geq 1)\\cdot(-g(\\bm{\\xi}))=\\mathbf{1}(p_{t}(\\bm{\\xi})\\geq 1)p_{t}(\\bm{\\xi})\\log p_{t}(\\bm{\\xi}). italic_g start_POSTSUBSCRIPT - end_POSTSUBSCRIPT ( bold_italic_Œæ ) = bold_1 ( italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) ‚â• 1 ) ‚ãÖ ( - italic_g ( bold_italic_Œæ ) ) = bold_1 ( italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) ‚â• 1 ) italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( b"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E53", "title": "max ùùÉ ‚àà ‚Ñù D œÜ t ( ùùÉ ‚àí ùíô ) = œÜ t ( ùüé ) = 1 ( 2 ‚Äã œÄ ) D / 2 ‚Äã t D = : C t . \\max_{\\bm{\\xi}\\in\\mathbb{R}^{D}}\\varphi_{t}(\\bm{\\xi}-\\bm{x})=\\varphi_{t}(\\bm{0})=\\frac{1}{(2\\pi)^{D/2}t^{D}}=:C_{t}. roman_max", "snippet": "max ùùÉ ‚àà ‚Ñù D œÜ t ( ùùÉ ‚àí ùíô ) = œÜ t ( ùüé ) = 1 ( 2 ‚Äã œÄ ) D / 2 ‚Äã t D = : C t . \\max_{\\bm{\\xi}\\in\\mathbb{R}^{D}}\\varphi_{t}(\\bm{\\xi}-\\bm{x})=\\varphi_{t}(\\bm{0})=\\frac{1}{(2\\pi)^{D/2}t^{D}}=:C_{t}. roman_max start_POSTSUBSCRIPT bold_italic_Œæ ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_œÜ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ - bold_italic_x ) = italic_œÜ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_0 ) = divide start_ARG 1 end_ARG start_ARG ( 2 italic_œÄ ) start_POSTSUPERSCRIPT italic_D / 2 end_POSTSUPERSCRIPT italic_t star"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E54", "title": "p t ‚Äã ( ùùÉ ) = ùîº ‚Å° œÜ t ‚Äã ( ùùÉ ‚àí ùíô ) ‚â§ ùîº ‚Å° C t = C t . p_{t}(\\bm{\\xi})=\\operatorname{\\mathbb{E}}\\varphi_{t}(\\bm{\\xi}-\\bm{x})\\leq\\operatorname{\\mathbb{E}}C_{t}=C_{t}. italic_p start_POSTSUBSCRIPT italic_t", "snippet": "p t ‚Äã ( ùùÉ ) = ùîº ‚Å° œÜ t ‚Äã ( ùùÉ ‚àí ùíô ) ‚â§ ùîº ‚Å° C t = C t . p_{t}(\\bm{\\xi})=\\operatorname{\\mathbb{E}}\\varphi_{t}(\\bm{\\xi}-\\bm{x})\\leq\\operatorname{\\mathbb{E}}C_{t}=C_{t}. italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) = blackboard_E italic_œÜ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ - bold_italic_x ) ‚â§ blackboard_E italic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . (B.2.54)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E59", "title": "‚à´ ‚Ñù D Œî ‚Äã p t ‚Äã ( ùùÉ ) ‚Äã [ c + log ‚Å° p t ‚Äã ( ùùÉ ) ] ‚Äã d ùùÉ = ‚àí ‚à´ ‚Ñù D ‚ü® ‚àá log ‚Å° p t ‚Äã ( ùùÉ ) , ‚àá p t ‚Äã ( ùùÉ ) ‚ü© ‚Äã d ùùÉ . \\int_{\\mathbb{R}^{D}}\\Delta p_{t}(\\bm{\\xi})[c+\\log p_{t}(\\bm{\\xi})]\\mathrm{d}\\bm{\\xi}=", "snippet": "‚à´ ‚Ñù D Œî ‚Äã p t ‚Äã ( ùùÉ ) ‚Äã [ c + log ‚Å° p t ‚Äã ( ùùÉ ) ] ‚Äã d ùùÉ = ‚àí ‚à´ ‚Ñù D ‚ü® ‚àá log ‚Å° p t ‚Äã ( ùùÉ ) , ‚àá p t ‚Äã ( ùùÉ ) ‚ü© ‚Äã d ùùÉ . \\int_{\\mathbb{R}^{D}}\\Delta p_{t}(\\bm{\\xi})[c+\\log p_{t}(\\bm{\\xi})]\\mathrm{d}\\bm{\\xi}=-\\int_{\\mathbb{R}^{D}}\\langle\\nabla\\log p_{t}(\\bm{\\xi}),\\nabla p_{t}(\\bm{\\xi})\\rangle\\mathrm{d}\\bm{\\xi}. ‚à´ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT roman_Œî italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) [ italic_c + roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) ] roman_d bo"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E60", "title": "‚à´ ùí¶ { œà ‚Äã ( ùùÉ ) ‚Äã Œî ‚Äã œï ‚Äã ( ùùÉ ) + ‚ü® ‚àá œà ‚Äã ( ùùÉ ) , ‚àá œï ‚Äã ( ùùÉ ) ‚ü© } ‚Äã d ùùÉ = ‚à´ ‚àÇ ùí¶ œà ‚Äã ( ùùÉ ) ‚Äã ‚ü® ‚àá œï ‚Äã ( ùùÉ ) , ùíè ‚Äã ( ùùÉ ) ‚ü© ‚Äã d œÉ ‚Äã ( ùùÉ ) \\int_{\\mathcal{K}}\\left\\{\\psi(\\bm{\\xi})\\Delta\\phi(\\bm{\\xi})+\\langl", "snippet": "‚à´ ùí¶ { œà ‚Äã ( ùùÉ ) ‚Äã Œî ‚Äã œï ‚Äã ( ùùÉ ) + ‚ü® ‚àá œà ‚Äã ( ùùÉ ) , ‚àá œï ‚Äã ( ùùÉ ) ‚ü© } ‚Äã d ùùÉ = ‚à´ ‚àÇ ùí¶ œà ‚Äã ( ùùÉ ) ‚Äã ‚ü® ‚àá œï ‚Äã ( ùùÉ ) , ùíè ‚Äã ( ùùÉ ) ‚ü© ‚Äã d œÉ ‚Äã ( ùùÉ ) \\int_{\\mathcal{K}}\\left\\{\\psi(\\bm{\\xi})\\Delta\\phi(\\bm{\\xi})+\\langle\\nabla\\psi(\\bm{\\xi}),\\nabla\\phi(\\bm{\\xi})\\rangle\\right\\}\\mathrm{d}\\bm{\\xi}=\\int_{\\partial\\mathcal{K}}\\psi(\\bm{\\xi})\\langle\\nabla\\phi(\\bm{\\xi}),\\bm{n}(\\bm{\\xi})\\rangle\\mathrm{d}\\sigma(\\bm{\\xi}) ‚à´ start_POSTSUBSCRIPT caligraphic_K end_POSTSUBSCRIPT { italic_œà ( bold_italic_Œæ ) roman_Œî italic_œï ( bold_italic_Œæ ) + ‚ü® ‚àá italic_œà ( bold_italic_Œæ ) , ‚àá italic_œï ( bold_italic_Œæ ) ‚ü© } roman_d bold_italic_"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E73", "title": "‚àí 2 ‚Äã r ‚Äã ‚Äñ ùíô ‚Äñ 2 ‚àí ‚Äñ ùíô ‚Äñ 2 2 ‚â§ 2 ‚Äã ‚ü® ùùÉ , ùíô ‚ü© ‚àí ‚Äñ ùíô ‚Äñ 2 2 ‚â§ 2 ‚Äã r ‚Äã ‚Äñ ùíô ‚Äñ 2 ‚àí ‚Äñ ùíô ‚Äñ 2 2 . -2r\\|\\bm{x}\\|_{2}-\\|\\bm{x}\\|_{2}^{2}\\leq 2\\langle\\bm{\\xi},\\bm{x}\\rangle-\\|\\bm{x}\\|_{2}^{2}\\leq 2r\\|\\bm{x}\\|_{2", "snippet": "‚àí 2 ‚Äã r ‚Äã ‚Äñ ùíô ‚Äñ 2 ‚àí ‚Äñ ùíô ‚Äñ 2 2 ‚â§ 2 ‚Äã ‚ü® ùùÉ , ùíô ‚ü© ‚àí ‚Äñ ùíô ‚Äñ 2 2 ‚â§ 2 ‚Äã r ‚Äã ‚Äñ ùíô ‚Äñ 2 ‚àí ‚Äñ ùíô ‚Äñ 2 2 . -2r\\|\\bm{x}\\|_{2}-\\|\\bm{x}\\|_{2}^{2}\\leq 2\\langle\\bm{\\xi},\\bm{x}\\rangle-\\|\\bm{x}\\|_{2}^{2}\\leq 2r\\|\\bm{x}\\|_{2}-\\|\\bm{x}\\|_{2}^{2}. - 2 italic_r ‚à• bold_italic_x ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - ‚à• bold_italic_x ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ‚â§ 2 ‚ü® bold_italic_Œæ , bold_italic_x ‚ü© - ‚à• bold_italic_x ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ‚â§ 2 italic_r ‚à• bold_italic_x ‚à• start_POSTSUBSCRIPT 2 end_POST"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E74", "title": "‚àí 2 ‚Äã R ‚Äã ( r + R ) ‚â§ 2 ‚Äã ‚ü® ùùÉ , ùíô ‚ü© ‚àí ‚Äñ ùíô ‚Äñ 2 2 ‚â§ 2 ‚Äã R ‚Äã r . -2R(r+R)\\leq 2\\langle\\bm{\\xi},\\bm{x}\\rangle-\\|\\bm{x}\\|_{2}^{2}\\leq 2Rr. - 2 italic_R ( italic_r + italic_R ) ‚â§ 2 ‚ü® bold_italic_Œæ , bold_it", "snippet": "‚àí 2 ‚Äã R ‚Äã ( r + R ) ‚â§ 2 ‚Äã ‚ü® ùùÉ , ùíô ‚ü© ‚àí ‚Äñ ùíô ‚Äñ 2 2 ‚â§ 2 ‚Äã R ‚Äã r . -2R(r+R)\\leq 2\\langle\\bm{\\xi},\\bm{x}\\rangle-\\|\\bm{x}\\|_{2}^{2}\\leq 2Rr. - 2 italic_R ( italic_r + italic_R ) ‚â§ 2 ‚ü® bold_italic_Œæ , bold_italic_x ‚ü© - ‚à• bold_italic_x ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ‚â§ 2 italic_R italic_r . (B.2.74)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E75", "title": "C t ‚Äã e ‚àí [ r 2 + 2 ‚Äã R ‚Äã ( r + R ) ] / ( 2 ‚Äã t 2 ) ‚â§ p t ‚Äã ( ùùÉ ) ‚â§ C t ‚Äã e [ ‚àí r 2 + 2 ‚Äã R ‚Äã r ] / ( 2 ‚Äã t 2 ) . C_{t}e^{-[r^{2}+2R(r+R)]/(2t^{2})}\\leq p_{t}(\\bm{\\xi})\\leq C_{t}e^{[-r^{2}+2Rr]/(2t^{2", "snippet": "C t ‚Äã e ‚àí [ r 2 + 2 ‚Äã R ‚Äã ( r + R ) ] / ( 2 ‚Äã t 2 ) ‚â§ p t ‚Äã ( ùùÉ ) ‚â§ C t ‚Äã e [ ‚àí r 2 + 2 ‚Äã R ‚Äã r ] / ( 2 ‚Äã t 2 ) . C_{t}e^{-[r^{2}+2R(r+R)]/(2t^{2})}\\leq p_{t}(\\bm{\\xi})\\leq C_{t}e^{[-r^{2}+2Rr]/(2t^{2})}. italic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_e start_POSTSUPERSCRIPT - [ italic_r start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + 2 italic_R ( italic_r + italic_R ) ] / ( 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_POSTSUPERSCRIPT ‚â§ italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) ‚â§ italic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRI"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E84", "title": "‚àí r ‚Äã ( R + r ) t 2 ‚ãÖ C t ‚Äã e [ ‚àí r 2 + 2 ‚Äã R ‚Äã r ] / ( 2 ‚Äã t 2 ) ‚â§ ‚ü® ‚àá p t ‚Äã ( ùùÉ ) , ùùÉ ‚ü© ‚â§ ‚àí r ‚Äã ( r ‚àí R ) t 2 ‚ãÖ C t ‚Äã e ‚àí [ r 2 + 2 ‚Äã R ‚Äã ( r + R ) ] / ( 2 ‚Äã t 2 ) . -\\frac{r(R+r)}{t^{2}}\\cdot C_{t}", "snippet": "‚àí r ‚Äã ( R + r ) t 2 ‚ãÖ C t ‚Äã e [ ‚àí r 2 + 2 ‚Äã R ‚Äã r ] / ( 2 ‚Äã t 2 ) ‚â§ ‚ü® ‚àá p t ‚Äã ( ùùÉ ) , ùùÉ ‚ü© ‚â§ ‚àí r ‚Äã ( r ‚àí R ) t 2 ‚ãÖ C t ‚Äã e ‚àí [ r 2 + 2 ‚Äã R ‚Äã ( r + R ) ] / ( 2 ‚Äã t 2 ) . -\\frac{r(R+r)}{t^{2}}\\cdot C_{t}e^{[-r^{2}+2Rr]/(2t^{2})}\\leq\\langle\\nabla p_{t}(\\bm{\\xi}),\\bm{\\xi}\\rangle\\leq-\\frac{r(r-R)}{t^{2}}\\cdot C_{t}e^{-[r^{2}+2R(r+R)]/(2t^{2})}. - divide start_ARG italic_r ( italic_R + italic_r ) end_ARG start_ARG italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ‚ãÖ italic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_e start_POSTSUPERSCRIPT [ - italic_r start_POSTSUPERSCRIPT 2 en"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E85", "title": "[ c + log ‚Å° p t ‚Äã ( ùùÉ ) ] ‚Äã ‚ü® ‚àá p t ‚Äã ( ùùÉ ) , ùùÉ ‚ü© = poly ‚Äã ( r , R , t ‚àí 1 , c ) ‚Äã e ‚àí Œò r ‚Äã ( r 2 ) [c+\\log p_{t}(\\bm{\\xi})]\\langle\\nabla p_{t}(\\bm{\\xi}),\\bm{\\xi}\\rangle=\\mathrm{poly}(r,R,t^{-1},c)e^", "snippet": "[ c + log ‚Å° p t ‚Äã ( ùùÉ ) ] ‚Äã ‚ü® ‚àá p t ‚Äã ( ùùÉ ) , ùùÉ ‚ü© = poly ‚Äã ( r , R , t ‚àí 1 , c ) ‚Äã e ‚àí Œò r ‚Äã ( r 2 ) [c+\\log p_{t}(\\bm{\\xi})]\\langle\\nabla p_{t}(\\bm{\\xi}),\\bm{\\xi}\\rangle=\\mathrm{poly}(r,R,t^{-1},c)e^{-\\Theta_{r}(r^{2})} [ italic_c + roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) ] ‚ü® ‚àá italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) , bold_italic_Œæ ‚ü© = roman_poly ( italic_r , italic_R , italic_t start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT , italic_c ) italic_e start_POSTSUPERSCRIPT - roman_Œò start_POSTSUBSCRIPT italic_r end_POSTS"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E86", "title": "1 r ‚Äã ‚à´ ‚àÇ B r ‚Äã ( ùüé ) [ c + log ‚Å° p t ‚Äã ( ùùÉ ) ] ‚Äã ‚ü® ‚àá p t ‚Äã ( ùùÉ ) , ùùÉ ‚ü© ‚Äã d ùùÉ = poly ‚Äã ( r , R , t ‚àí 1 , c ) ‚Äã e ‚àí Œò r ‚Äã ( r 2 ) \\frac{1}{r}\\int_{\\partial B_{r}(\\bm{0})}[c+\\log p_{t}(\\bm{\\xi})]\\langle", "snippet": "1 r ‚Äã ‚à´ ‚àÇ B r ‚Äã ( ùüé ) [ c + log ‚Å° p t ‚Äã ( ùùÉ ) ] ‚Äã ‚ü® ‚àá p t ‚Äã ( ùùÉ ) , ùùÉ ‚ü© ‚Äã d ùùÉ = poly ‚Äã ( r , R , t ‚àí 1 , c ) ‚Äã e ‚àí Œò r ‚Äã ( r 2 ) \\frac{1}{r}\\int_{\\partial B_{r}(\\bm{0})}[c+\\log p_{t}(\\bm{\\xi})]\\langle\\nabla p_{t}(\\bm{\\xi}),\\bm{\\xi}\\rangle\\mathrm{d}\\bm{\\xi}=\\mathrm{poly}(r,R,t^{-1},c)e^{-\\Theta_{r}(r^{2})} divide start_ARG 1 end_ARG start_ARG italic_r end_ARG ‚à´ start_POSTSUBSCRIPT ‚àÇ italic_B start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ( bold_0 ) end_POSTSUBSCRIPT [ italic_c + roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) ] ‚ü® ‚àá italic_p start_POSTSUBSCRIP"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E87", "title": "lim r ‚Üí ‚àû 1 r ‚Äã ‚à´ ‚àÇ B r ‚Äã ( ùüé ) [ c + log ‚Å° p t ‚Äã ( ùùÉ ) ] ‚Äã ‚ü® ‚àá p t ‚Äã ( ùùÉ ) , ùùÉ ‚ü© ‚Äã d ùùÉ = 0 . \\lim_{r\\to\\infty}\\frac{1}{r}\\int_{\\partial B_{r}(\\bm{0})}[c+\\log p_{t}(\\bm{\\xi})]\\langle\\nabla p_{t}(\\bm{\\", "snippet": "lim r ‚Üí ‚àû 1 r ‚Äã ‚à´ ‚àÇ B r ‚Äã ( ùüé ) [ c + log ‚Å° p t ‚Äã ( ùùÉ ) ] ‚Äã ‚ü® ‚àá p t ‚Äã ( ùùÉ ) , ùùÉ ‚ü© ‚Äã d ùùÉ = 0 . \\lim_{r\\to\\infty}\\frac{1}{r}\\int_{\\partial B_{r}(\\bm{0})}[c+\\log p_{t}(\\bm{\\xi})]\\langle\\nabla p_{t}(\\bm{\\xi}),\\bm{\\xi}\\rangle\\mathrm{d}\\bm{\\xi}=0. roman_lim start_POSTSUBSCRIPT italic_r ‚Üí ‚àû end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG italic_r end_ARG ‚à´ start_POSTSUBSCRIPT ‚àÇ italic_B start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ( bold_0 ) end_POSTSUBSCRIPT [ italic_c + roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) ] ‚ü® ‚àá italic_p start_POSTSUBSCRIPT it"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E90", "title": "ùíô ¬Ø ‚Ä≤ ‚Äã ( ùùÉ ) = ùë∞ + ( 1 ‚àí s t ) ‚Äã t 2 ‚Äã ‚àá 2 log ‚Å° p t ‚Äã ( ùùÉ ) . \\bar{\\bm{x}}^{\\prime}(\\bm{\\xi})=\\bm{I}+\\left(1-\\frac{s}{t}\\right)t^{2}\\nabla^{2}\\log p_{t}(\\bm{\\xi}). over¬Ø start_ARG bold_italic_x end_", "snippet": "ùíô ¬Ø ‚Ä≤ ‚Äã ( ùùÉ ) = ùë∞ + ( 1 ‚àí s t ) ‚Äã t 2 ‚Äã ‚àá 2 log ‚Å° p t ‚Äã ( ùùÉ ) . \\bar{\\bm{x}}^{\\prime}(\\bm{\\xi})=\\bm{I}+\\left(1-\\frac{s}{t}\\right)t^{2}\\nabla^{2}\\log p_{t}(\\bm{\\xi}). over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( bold_italic_Œæ ) = bold_italic_I + ( 1 - divide start_ARG italic_s end_ARG start_ARG italic_t end_ARG ) italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ‚àá start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) . (B.2.90)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E91", "title": "‚àá 2 log ‚Å° p t ‚Äã ( ùùÉ ) = p t ‚Äã ( ùùÉ ) ‚Äã ‚àá 2 p t ‚Äã ( ùùÉ ) ‚àí ( ‚àá p t ‚Äã ( ùùÉ ) ) ‚Äã ( ‚àá p t ‚Äã ( ùùÉ ) ) ‚ä§ p t ‚Äã ( ùùÉ ) 2 . \\nabla^{2}\\log p_{t}(\\bm{\\xi})=\\frac{p_{t}(\\bm{\\xi})\\nabla^{2}p_{t}(\\bm{\\xi})-(\\nabla p_", "snippet": "‚àá 2 log ‚Å° p t ‚Äã ( ùùÉ ) = p t ‚Äã ( ùùÉ ) ‚Äã ‚àá 2 p t ‚Äã ( ùùÉ ) ‚àí ( ‚àá p t ‚Äã ( ùùÉ ) ) ‚Äã ( ‚àá p t ‚Äã ( ùùÉ ) ) ‚ä§ p t ‚Äã ( ùùÉ ) 2 . \\nabla^{2}\\log p_{t}(\\bm{\\xi})=\\frac{p_{t}(\\bm{\\xi})\\nabla^{2}p_{t}(\\bm{\\xi})-(\\nabla p_{t}(\\bm{\\xi}))(\\nabla p_{t}(\\bm{\\xi}))^{\\top}}{p_{t}(\\bm{\\xi})^{2}}. ‚àá start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) = divide start_ARG italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) ‚àá start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E108", "title": "g ‚Ä≤ ‚Äã ( t ‚àó ) ‚âê ùíó ‚ä§ ‚Äã [ f ‚Ä≤ ‚Äã ( ùíô + t ‚àó ‚Äã ùíó ) ] ‚Äã ùíó > 0 g^{\\prime}(t^{\\ast})\\doteq\\bm{v}^{\\top}\\left[f^{\\prime}(\\bm{x}+t^{\\ast}\\bm{v})\\right]\\bm{v}>0 italic_g start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRI", "snippet": "g ‚Ä≤ ‚Äã ( t ‚àó ) ‚âê ùíó ‚ä§ ‚Äã [ f ‚Ä≤ ‚Äã ( ùíô + t ‚àó ‚Äã ùíó ) ] ‚Äã ùíó > 0 g^{\\prime}(t^{\\ast})\\doteq\\bm{v}^{\\top}\\left[f^{\\prime}(\\bm{x}+t^{\\ast}\\bm{v})\\right]\\bm{v}>0 italic_g start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( italic_t start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT ) ‚âê bold_italic_v start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT [ italic_f start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( bold_italic_x + italic_t start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT bold_italic_v ) ] bold_italic_v > 0 (B.2.108)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E109", "title": "sup ùùÉ ‚àà ‚Ñù D | Œî ‚Äã log ‚Å° p t ‚Äã ( ùùÉ ) | ‚â§ max ‚Å° ( D t 2 , | R t 4 ‚àí D t 2 | ) . \\sup_{\\bm{\\xi}\\in\\mathbb{R}^{D}}\\lvert\\Delta\\log p_{t}(\\bm{\\xi})\\rvert\\leq\\max\\left(\\frac{D}{t^{2}},\\left\\lvert\\frac{R}{t^", "snippet": "sup ùùÉ ‚àà ‚Ñù D | Œî ‚Äã log ‚Å° p t ‚Äã ( ùùÉ ) | ‚â§ max ‚Å° ( D t 2 , | R t 4 ‚àí D t 2 | ) . \\sup_{\\bm{\\xi}\\in\\mathbb{R}^{D}}\\lvert\\Delta\\log p_{t}(\\bm{\\xi})\\rvert\\leq\\max\\left(\\frac{D}{t^{2}},\\left\\lvert\\frac{R}{t^{4}}-\\frac{D}{t^{2}}\\right\\rvert\\right). roman_sup start_POSTSUBSCRIPT bold_italic_Œæ ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT | roman_Œî roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) | ‚â§ roman_max ( divide start_ARG italic_D end_ARG start_ARG italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG , | divide start"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E110", "title": "Œî ‚Äã log ‚Å° p t ‚Äã ( ùùÉ ) = Œî ‚Äã p t ‚Äã ( ùùÉ ) p t ‚Äã ( ùùÉ ) ‚àí ‚Äñ ‚àá p t ‚Äã ( ùùÉ ) ‚Äñ 2 2 p t ‚Äã ( ùùÉ ) 2 . \\Delta\\log p_{t}(\\bm{\\xi})=\\frac{\\Delta p_{t}(\\bm{\\xi})}{p_{t}(\\bm{\\xi})}-\\frac{\\|\\nabla p_{t}(\\bm{\\xi})\\|_{", "snippet": "Œî ‚Äã log ‚Å° p t ‚Äã ( ùùÉ ) = Œî ‚Äã p t ‚Äã ( ùùÉ ) p t ‚Äã ( ùùÉ ) ‚àí ‚Äñ ‚àá p t ‚Äã ( ùùÉ ) ‚Äñ 2 2 p t ‚Äã ( ùùÉ ) 2 . \\Delta\\log p_{t}(\\bm{\\xi})=\\frac{\\Delta p_{t}(\\bm{\\xi})}{p_{t}(\\bm{\\xi})}-\\frac{\\|\\nabla p_{t}(\\bm{\\xi})\\|_{2}^{2}}{p_{t}(\\bm{\\xi})^{2}}. roman_Œî roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) = divide start_ARG roman_Œî italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) end_ARG start_ARG italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) end_ARG - divide start_ARG ‚à• ‚àá italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIP"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E113", "title": "q ùùÉ ‚Äã ( ùíñ ) = œÜ t ‚Äã ( ùùÉ ‚àí ùíñ ) ‚Äã p ‚Äã ( ùíñ ) ‚à´ ‚Ñù D œÜ t ‚Äã ( ùùÉ ‚àí ùíó ) ‚Äã p ‚Äã ( ùíó ) ‚Äã d ùíó = œÜ t ‚Äã ( ùùÉ ‚àí ùíñ ) ‚Äã p ‚Äã ( ùíñ ) ùîº ‚Å° [ œÜ t ‚Äã ( ùùÉ ‚àí ùíô ) ] = œÜ t ‚Äã ( ùùÉ ‚àí ùíñ ) ‚Äã p ‚Äã ( ùíñ ) p t ‚Äã ( ùùÉ ) q_{\\bm{\\xi}}(\\bm{u})=\\", "snippet": "q ùùÉ ‚Äã ( ùíñ ) = œÜ t ‚Äã ( ùùÉ ‚àí ùíñ ) ‚Äã p ‚Äã ( ùíñ ) ‚à´ ‚Ñù D œÜ t ‚Äã ( ùùÉ ‚àí ùíó ) ‚Äã p ‚Äã ( ùíó ) ‚Äã d ùíó = œÜ t ‚Äã ( ùùÉ ‚àí ùíñ ) ‚Äã p ‚Äã ( ùíñ ) ùîº ‚Å° [ œÜ t ‚Äã ( ùùÉ ‚àí ùíô ) ] = œÜ t ‚Äã ( ùùÉ ‚àí ùíñ ) ‚Äã p ‚Äã ( ùíñ ) p t ‚Äã ( ùùÉ ) q_{\\bm{\\xi}}(\\bm{u})=\\frac{\\varphi_{t}(\\bm{\\xi}-\\bm{u})p(\\bm{u})}{\\int_{\\mathbb{R}^{D}}\\varphi_{t}(\\bm{\\xi}-\\bm{v})p(\\bm{v})\\mathrm{d}\\bm{v}}=\\frac{\\varphi_{t}(\\bm{\\xi}-\\bm{u})p(\\bm{u})}{\\operatorname{\\mathbb{E}}[\\varphi_{t}(\\bm{\\xi}-\\bm{x})]}=\\frac{\\varphi_{t}(\\bm{\\xi}-\\bm{u})p(\\bm{u})}{p_{t}(\\bm{\\xi})} italic_q start_POSTSUBSCRIPT bold_italic_Œæ end_POSTSUBSCRIPT ( bold_italic_u ) = divide start_ARG italic_œÜ start_POS"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E114", "title": "Œî ‚Äã p t ‚Äã ( ùùÉ ) p t ‚Äã ( ùùÉ ) = ‚à´ ‚Ñù D { ‚Äñ ùùÉ ‚àí ùíñ ‚Äñ 2 2 ‚àí D ‚Äã t 2 t 4 } ‚Äã q ùùÉ ‚Äã ( ùíñ ) ‚Äã d ùíñ = 1 t 4 ‚Äã ùîº ‚Å° [ ‚Äñ ùùÉ ‚àí ùíö ùùÉ ‚Äñ 2 2 ] ‚àí D t 2 . \\frac{\\Delta p_{t}(\\bm{\\xi})}{p_{t}(\\bm{\\xi})}=\\int_{\\mathbb{R}^{D}}", "snippet": "Œî ‚Äã p t ‚Äã ( ùùÉ ) p t ‚Äã ( ùùÉ ) = ‚à´ ‚Ñù D { ‚Äñ ùùÉ ‚àí ùíñ ‚Äñ 2 2 ‚àí D ‚Äã t 2 t 4 } ‚Äã q ùùÉ ‚Äã ( ùíñ ) ‚Äã d ùíñ = 1 t 4 ‚Äã ùîº ‚Å° [ ‚Äñ ùùÉ ‚àí ùíö ùùÉ ‚Äñ 2 2 ] ‚àí D t 2 . \\frac{\\Delta p_{t}(\\bm{\\xi})}{p_{t}(\\bm{\\xi})}=\\int_{\\mathbb{R}^{D}}\\left\\{\\frac{\\|\\bm{\\xi}-\\bm{u}\\|_{2}^{2}-Dt^{2}}{t^{4}}\\right\\}q_{\\bm{\\xi}}(\\bm{u})\\mathrm{d}\\bm{u}=\\frac{1}{t^{4}}\\operatorname{\\mathbb{E}}[\\|\\bm{\\xi}-\\bm{y}_{\\bm{\\xi}}\\|_{2}^{2}]-\\frac{D}{t^{2}}. divide start_ARG roman_Œî italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) end_ARG start_ARG italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) end_ARG = ‚à´"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E115", "title": "‚àá p t ‚Äã ( ùùÉ ) p t ‚Äã ( ùùÉ ) = ‚àí ùùÉ ‚àí ùîº ‚Å° [ ùíö ùùÉ ] t 2 . \\frac{\\nabla p_{t}(\\bm{\\xi})}{p_{t}(\\bm{\\xi})}=-\\frac{\\bm{\\xi}-\\operatorname{\\mathbb{E}}[\\bm{y}_{\\bm{\\xi}}]}{t^{2}}. divide start_ARG ‚àá italic_p sta", "snippet": "‚àá p t ‚Äã ( ùùÉ ) p t ‚Äã ( ùùÉ ) = ‚àí ùùÉ ‚àí ùîº ‚Å° [ ùíö ùùÉ ] t 2 . \\frac{\\nabla p_{t}(\\bm{\\xi})}{p_{t}(\\bm{\\xi})}=-\\frac{\\bm{\\xi}-\\operatorname{\\mathbb{E}}[\\bm{y}_{\\bm{\\xi}}]}{t^{2}}. divide start_ARG ‚àá italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) end_ARG start_ARG italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) end_ARG = - divide start_ARG bold_italic_Œæ - blackboard_E [ bold_italic_y start_POSTSUBSCRIPT bold_italic_Œæ end_POSTSUBSCRIPT ] end_ARG start_ARG italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG . (B.2.115)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E116", "title": "Œî ‚Äã p t ‚Äã ( ùùÉ ) p t ‚Äã ( ùùÉ ) = ùîº ‚Å° [ ‚Äñ ùíõ ùùÉ ‚Äñ 2 2 ] t 4 ‚àí D t 2 , ‚àá p t ‚Äã ( ùùÉ ) p t ‚Äã ( ùùÉ ) = ùîº ‚Å° [ ùíõ ùùÉ ] t 2 . \\frac{\\Delta p_{t}(\\bm{\\xi})}{p_{t}(\\bm{\\xi})}=\\frac{\\operatorname{\\mathbb{E}}[\\|\\bm{z}_{\\", "snippet": "Œî ‚Äã p t ‚Äã ( ùùÉ ) p t ‚Äã ( ùùÉ ) = ùîº ‚Å° [ ‚Äñ ùíõ ùùÉ ‚Äñ 2 2 ] t 4 ‚àí D t 2 , ‚àá p t ‚Äã ( ùùÉ ) p t ‚Äã ( ùùÉ ) = ùîº ‚Å° [ ùíõ ùùÉ ] t 2 . \\frac{\\Delta p_{t}(\\bm{\\xi})}{p_{t}(\\bm{\\xi})}=\\frac{\\operatorname{\\mathbb{E}}[\\|\\bm{z}_{\\bm{\\xi}}\\|_{2}^{2}]}{t^{4}}-\\frac{D}{t^{2}},\\qquad\\frac{\\nabla p_{t}(\\bm{\\xi})}{p_{t}(\\bm{\\xi})}=\\frac{\\operatorname{\\mathbb{E}}[\\bm{z}_{\\bm{\\xi}}]}{t^{2}}. divide start_ARG roman_Œî italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) end_ARG start_ARG italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) end_ARG = divide start_ARG blackboard_E [ ‚à• bold_ita"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E121", "title": "tr ‚Å° ( Cov ‚Å° ( ùíö ùùÉ ) ) = ùîº ‚Å° [ ‚Äñ ùíö ùùÉ ‚Äñ 2 2 ] ‚àí ‚Äñ ùîº ‚Å° [ ùíö ùùÉ ] ‚Äñ 2 2 ‚â§ ùîº ‚Å° [ ‚Äñ ùíö ùùÉ ‚Äñ 2 2 ] ‚â§ R 2 . \\operatorname{tr}(\\operatorname{Cov}(\\bm{y}_{\\bm{\\xi}}))=\\operatorname{\\mathbb{E}}[\\|\\bm{y}_{\\bm{\\xi}}\\", "snippet": "tr ‚Å° ( Cov ‚Å° ( ùíö ùùÉ ) ) = ùîº ‚Å° [ ‚Äñ ùíö ùùÉ ‚Äñ 2 2 ] ‚àí ‚Äñ ùîº ‚Å° [ ùíö ùùÉ ] ‚Äñ 2 2 ‚â§ ùîº ‚Å° [ ‚Äñ ùíö ùùÉ ‚Äñ 2 2 ] ‚â§ R 2 . \\operatorname{tr}(\\operatorname{Cov}(\\bm{y}_{\\bm{\\xi}}))=\\operatorname{\\mathbb{E}}[\\|\\bm{y}_{\\bm{\\xi}}\\|_{2}^{2}]-\\|\\operatorname{\\mathbb{E}}[\\bm{y}_{\\bm{\\xi}}]\\|_{2}^{2}\\leq\\operatorname{\\mathbb{E}}[\\|\\bm{y}_{\\bm{\\xi}}\\|_{2}^{2}]\\leq R^{2}. roman_tr ( roman_Cov ( bold_italic_y start_POSTSUBSCRIPT bold_italic_Œæ end_POSTSUBSCRIPT ) ) = blackboard_E [ ‚à• bold_italic_y start_POSTSUBSCRIPT bold_italic_Œæ end_POSTSUBSCRIPT ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRI"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E122", "title": "‚àí D t 2 ‚â§ Œî ‚Äã log ‚Å° p t ‚Äã ( ùùÉ ) ‚â§ R 2 t 4 ‚àí D t 2 , -\\frac{D}{t^{2}}\\leq\\Delta\\log p_{t}(\\bm{\\xi})\\leq\\frac{R^{2}}{t^{4}}-\\frac{D}{t^{2}}, - divide start_ARG italic_D end_ARG start_ARG italic_t start_", "snippet": "‚àí D t 2 ‚â§ Œî ‚Äã log ‚Å° p t ‚Äã ( ùùÉ ) ‚â§ R 2 t 4 ‚àí D t 2 , -\\frac{D}{t^{2}}\\leq\\Delta\\log p_{t}(\\bm{\\xi})\\leq\\frac{R^{2}}{t^{4}}-\\frac{D}{t^{2}}, - divide start_ARG italic_D end_ARG start_ARG italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ‚â§ roman_Œî roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) ‚â§ divide start_ARG italic_R start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_t start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT end_ARG - divide start_ARG italic_D end_ARG start_ARG italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG , "}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E127", "title": "‚àÇ p t ‚àÇ t ‚Äã ( ùùÉ ) = ‚àÇ ‚àÇ t ‚Äã ùîº ‚Å° [ œÜ t ‚Äã ( ùùÉ ‚àí ùíô ) ] = ùîº ‚Å° [ ‚àÇ ‚àÇ t ‚Äã œÜ t ‚Äã ( ùùÉ ‚àí ùíô ) ] = ‚àÇ œÜ t ‚àÇ t ‚àó p . \\frac{\\partial p_{t}}{\\partial t}(\\bm{\\xi})=\\frac{\\partial}{\\partial t}\\operatorname{\\mathbb{E}}", "snippet": "‚àÇ p t ‚àÇ t ‚Äã ( ùùÉ ) = ‚àÇ ‚àÇ t ‚Äã ùîº ‚Å° [ œÜ t ‚Äã ( ùùÉ ‚àí ùíô ) ] = ùîº ‚Å° [ ‚àÇ ‚àÇ t ‚Äã œÜ t ‚Äã ( ùùÉ ‚àí ùíô ) ] = ‚àÇ œÜ t ‚àÇ t ‚àó p . \\frac{\\partial p_{t}}{\\partial t}(\\bm{\\xi})=\\frac{\\partial}{\\partial t}\\operatorname{\\mathbb{E}}[\\varphi_{t}(\\bm{\\xi}-\\bm{x})]=\\operatorname{\\mathbb{E}}\\left[\\frac{\\partial}{\\partial t}\\varphi_{t}(\\bm{\\xi}-\\bm{x})\\right]=\\frac{\\partial\\varphi_{t}}{\\partial t}*p. divide start_ARG ‚àÇ italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG ‚àÇ italic_t end_ARG ( bold_italic_Œæ ) = divide start_ARG ‚àÇ end_ARG start_ARG ‚àÇ italic_t end_ARG blackboard_E [ italic_œÜ start_POSTSUBSCRIPT i"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E128", "title": "p t = œÜ t ‚àó p ‚üπ ‚àá p t = ‚àá œÜ t ‚àó p ‚üπ ‚àá 2 p t = ‚àá 2 œÜ t ‚àó p ‚üπ Œî ‚Äã p t = Œî ‚Äã œÜ t ‚àó p . p_{t}=\\varphi_{t}*p\\implies\\nabla p_{t}=\\nabla\\varphi_{t}*p\\implies\\nabla^{2}p_{t}=\\nabla^{2}\\varphi_{t}*p\\implies\\D", "snippet": "p t = œÜ t ‚àó p ‚üπ ‚àá p t = ‚àá œÜ t ‚àó p ‚üπ ‚àá 2 p t = ‚àá 2 œÜ t ‚àó p ‚üπ Œî ‚Äã p t = Œî ‚Äã œÜ t ‚àó p . p_{t}=\\varphi_{t}*p\\implies\\nabla p_{t}=\\nabla\\varphi_{t}*p\\implies\\nabla^{2}p_{t}=\\nabla^{2}\\varphi_{t}*p\\implies\\Delta p_{t}=\\Delta\\varphi_{t}*p. italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_œÜ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ‚àó italic_p ‚üπ ‚àá italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = ‚àá italic_œÜ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ‚àó italic_p ‚üπ ‚àá start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = ‚àá star"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E133", "title": "‚à´ t min t max ‚à´ ‚Ñù D | ‚àÇ f t ‚àÇ t ‚Äã ( ùùÉ ) | ‚Äã d ùùÉ < ‚àû . \\int_{t_{\\min}}^{t_{\\max}}\\int_{\\mathbb{R}^{D}}\\left\\lvert\\frac{\\partial f_{t}}{\\partial t}(\\bm{\\xi})\\right\\rvert\\mathrm{d}\\bm{\\xi}<\\infty. ‚à´ star", "snippet": "‚à´ t min t max ‚à´ ‚Ñù D | ‚àÇ f t ‚àÇ t ‚Äã ( ùùÉ ) | ‚Äã d ùùÉ < ‚àû . \\int_{t_{\\min}}^{t_{\\max}}\\int_{\\mathbb{R}^{D}}\\left\\lvert\\frac{\\partial f_{t}}{\\partial t}(\\bm{\\xi})\\right\\rvert\\mathrm{d}\\bm{\\xi}<\\infty. ‚à´ start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_min end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ‚à´ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT | divide start_ARG ‚àÇ italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG ‚àÇ italic_t "}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E134", "title": "d d ‚Äã t ‚Äã ‚à´ ‚Ñù D f t ‚Äã ( ùùÉ ) ‚Äã d ùùÉ = ‚à´ ‚Ñù D ‚àÇ ‚àÇ t ‚Äã f t ‚Äã ( ùùÉ ) ‚Äã d ùùÉ , \\frac{\\mathrm{d}}{\\mathrm{d}t}\\int_{\\mathbb{R}^{D}}f_{t}(\\bm{\\xi})\\mathrm{d}\\bm{\\xi}=\\int_{\\mathbb{R}^{D}}\\frac{\\partial}{\\partial", "snippet": "d d ‚Äã t ‚Äã ‚à´ ‚Ñù D f t ‚Äã ( ùùÉ ) ‚Äã d ùùÉ = ‚à´ ‚Ñù D ‚àÇ ‚àÇ t ‚Äã f t ‚Äã ( ùùÉ ) ‚Äã d ùùÉ , \\frac{\\mathrm{d}}{\\mathrm{d}t}\\int_{\\mathbb{R}^{D}}f_{t}(\\bm{\\xi})\\mathrm{d}\\bm{\\xi}=\\int_{\\mathbb{R}^{D}}\\frac{\\partial}{\\partial t}f_{t}(\\bm{\\xi})\\mathrm{d}\\bm{\\xi}, divide start_ARG roman_d end_ARG start_ARG roman_d italic_t end_ARG ‚à´ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) roman_d bold_italic_Œæ = ‚à´ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_PO"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E135", "title": "( f ‚àó g ) ‚Äã ( ùùÉ ) ‚âê ‚à´ ‚Ñù D f ‚Äã ( ùíñ ) ‚Äã g ‚Äã ( ùùÉ ‚àí ùíñ ) ‚Äã d ùíñ (f*g)(\\bm{\\xi})\\doteq\\int_{\\mathbb{R}^{D}}f(\\bm{u})g(\\bm{\\xi}-\\bm{u})\\mathrm{d}\\bm{u} ( italic_f ‚àó italic_g ) ( bold_italic_Œæ ) ‚âê ‚à´ start_POST", "snippet": "( f ‚àó g ) ‚Äã ( ùùÉ ) ‚âê ‚à´ ‚Ñù D f ‚Äã ( ùíñ ) ‚Äã g ‚Äã ( ùùÉ ‚àí ùíñ ) ‚Äã d ùíñ (f*g)(\\bm{\\xi})\\doteq\\int_{\\mathbb{R}^{D}}f(\\bm{u})g(\\bm{\\xi}-\\bm{u})\\mathrm{d}\\bm{u} ( italic_f ‚àó italic_g ) ( bold_italic_Œæ ) ‚âê ‚à´ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_f ( bold_italic_u ) italic_g ( bold_italic_Œæ - bold_italic_u ) roman_d bold_italic_u (B.2.135)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E136", "title": "‚àá k ( f ‚àó g ) = ( ‚àá k f ) ‚àó g . \\nabla^{k}(f*g)=(\\nabla^{k}f)*g. ‚àá start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ( italic_f ‚àó italic_g ) = ( ‚àá start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT i", "snippet": "‚àá k ( f ‚àó g ) = ( ‚àá k f ) ‚àó g . \\nabla^{k}(f*g)=(\\nabla^{k}f)*g. ‚àá start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ( italic_f ‚àó italic_g ) = ( ‚àá start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_f ) ‚àó italic_g . (B.2.136)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.E137", "title": "‚àá k ( f ‚àó g ) = f ‚àó ( ‚àá k g ) . \\nabla^{k}(f*g)=f*(\\nabla^{k}g). ‚àá start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ( italic_f ‚àó italic_g ) = italic_f ‚àó ( ‚àá start_POSTSUPERSCRIPT italic_k end_POSTSU", "snippet": "‚àá k ( f ‚àó g ) = f ‚àó ( ‚àá k g ) . \\nabla^{k}(f*g)=f*(\\nabla^{k}g). ‚àá start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ( italic_f ‚àó italic_g ) = italic_f ‚àó ( ‚àá start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT italic_g ) . (B.2.137)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E1", "title": "‚Ñõ œµ ‚Äã ( ùíô ) ‚â• h ‚Äã ( ùíô ) ‚àí log ‚Å° vol ‚Å° ( B œµ ) + log ‚Å° ( 2 D ‚Äã Œì ‚Äã ( D / 2 ) ‚Äã ( D 2 ‚Äã e ) D / 2 ) , \\mathcal{R}_{\\epsilon}(\\bm{x})\\geq h(\\bm{x})-\\log\\operatorname{vol}(B_{\\epsilon})+\\log\\left(\\frac{2}", "snippet": "‚Ñõ œµ ‚Äã ( ùíô ) ‚â• h ‚Äã ( ùíô ) ‚àí log ‚Å° vol ‚Å° ( B œµ ) + log ‚Å° ( 2 D ‚Äã Œì ‚Äã ( D / 2 ) ‚Äã ( D 2 ‚Äã e ) D / 2 ) , \\mathcal{R}_{\\epsilon}(\\bm{x})\\geq h(\\bm{x})-\\log\\operatorname{vol}(B_{\\epsilon})+\\log\\left(\\frac{2}{D\\Gamma(D/2)}\\left(\\frac{D}{2e}\\right)^{D/2}\\right), caligraphic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_x ) ‚â• italic_h ( bold_italic_x ) - roman_log roman_vol ( italic_B start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ) + roman_log ( divide start_ARG 2 end_ARG start_ARG italic_D roman_Œì ( italic_D / 2 ) end_ARG ( divide start_ARG italic_D end_ARG start_ARG 2 italic_e end_AR"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E2", "title": "Œì ‚Äã ( x ) ‚â§ 2 ‚Äã œÄ ‚Äã x x ‚àí 1 / 2 ‚Äã e ‚àí x ‚Äã e 1 / ( 12 ‚Äã x ) . \\Gamma(x)\\leq\\sqrt{2\\pi}x^{x-1/2}e^{-x}e^{1/(12x)}. roman_Œì ( italic_x ) ‚â§ square-root start_ARG 2 italic_œÄ end_ARG italic_x start_POSTSUPE", "snippet": "Œì ‚Äã ( x ) ‚â§ 2 ‚Äã œÄ ‚Äã x x ‚àí 1 / 2 ‚Äã e ‚àí x ‚Äã e 1 / ( 12 ‚Äã x ) . \\Gamma(x)\\leq\\sqrt{2\\pi}x^{x-1/2}e^{-x}e^{1/(12x)}. roman_Œì ( italic_x ) ‚â§ square-root start_ARG 2 italic_œÄ end_ARG italic_x start_POSTSUPERSCRIPT italic_x - 1 / 2 end_POSTSUPERSCRIPT italic_e start_POSTSUPERSCRIPT - italic_x end_POSTSUPERSCRIPT italic_e start_POSTSUPERSCRIPT 1 / ( 12 italic_x ) end_POSTSUPERSCRIPT . (B.3.2)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E5", "title": "‚Ñõ œµ ‚Äã ( ùíô ) ‚â• h ‚Äã ( ùíô ) ‚àí log 2 ‚Å° vol ‚Å° ( B œµ ) ‚àí O ‚Äã ( log ‚Å° D ) . \\mathcal{R}_{\\epsilon}(\\bm{x})\\geq h(\\bm{x})-\\log_{2}\\operatorname{vol}(B_{\\epsilon})-O(\\log D). caligraphic_R start_POSTSUBSCRIPT i", "snippet": "‚Ñõ œµ ‚Äã ( ùíô ) ‚â• h ‚Äã ( ùíô ) ‚àí log 2 ‚Å° vol ‚Å° ( B œµ ) ‚àí O ‚Äã ( log ‚Å° D ) . \\mathcal{R}_{\\epsilon}(\\bm{x})\\geq h(\\bm{x})-\\log_{2}\\operatorname{vol}(B_{\\epsilon})-O(\\log D). caligraphic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_x ) ‚â• italic_h ( bold_italic_x ) - roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT roman_vol ( italic_B start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ) - italic_O ( roman_log italic_D ) . (B.3.5)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E6", "title": "‚Ñô ‚Å° [ ùíô ‚àà A ] = ‚à´ A 1 vol ‚Å° ( ùíÆ ) ‚Äã d ùíô . \\operatorname{\\mathbb{P}}[\\bm{x}\\in A]=\\int_{A}\\frac{1}{\\operatorname{vol}(\\mathcal{S})}\\mathrm{d}\\bm{x}. blackboard_P [ bold_italic_x ‚àà italic_A ] = ‚à´ start_", "snippet": "‚Ñô ‚Å° [ ùíô ‚àà A ] = ‚à´ A 1 vol ‚Å° ( ùíÆ ) ‚Äã d ùíô . \\operatorname{\\mathbb{P}}[\\bm{x}\\in A]=\\int_{A}\\frac{1}{\\operatorname{vol}(\\mathcal{S})}\\mathrm{d}\\bm{x}. blackboard_P [ bold_italic_x ‚àà italic_A ] = ‚à´ start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG roman_vol ( caligraphic_S ) end_ARG roman_d bold_italic_x . (B.3.6)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E7", "title": "h ‚Äã ( ùíô ) = log 2 ‚Å° vol ‚Å° ( ùíÆ ) . h(\\bm{x})=\\log_{2}\\operatorname{vol}(\\mathcal{S}). italic_h ( bold_italic_x ) = roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT roman_vol ( caligraphic_S ) . (B.3.7", "snippet": "h ‚Äã ( ùíô ) = log 2 ‚Å° vol ‚Å° ( ùíÆ ) . h(\\bm{x})=\\log_{2}\\operatorname{vol}(\\mathcal{S}). italic_h ( bold_italic_x ) = roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT roman_vol ( caligraphic_S ) . (B.3.7)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E8", "title": "ùíÆ Œ¥ = { ùùÉ ‚àà ‚Ñù D ‚à£ dist ‚Äã ( ùùÉ , ùíÆ ) ‚â§ Œ¥ } . \\mathcal{S}_{\\delta}=\\left\\{\\bm{\\xi}\\in\\mathbb{R}^{D}\\mid\\mathrm{dist}(\\bm{\\xi},\\mathcal{S})\\leq\\delta\\right\\}. caligraphic_S start_POSTSUBSCRIPT italic_Œ¥ en", "snippet": "ùíÆ Œ¥ = { ùùÉ ‚àà ‚Ñù D ‚à£ dist ‚Äã ( ùùÉ , ùíÆ ) ‚â§ Œ¥ } . \\mathcal{S}_{\\delta}=\\left\\{\\bm{\\xi}\\in\\mathbb{R}^{D}\\mid\\mathrm{dist}(\\bm{\\xi},\\mathcal{S})\\leq\\delta\\right\\}. caligraphic_S start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT = { bold_italic_Œæ ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT ‚à£ roman_dist ( bold_italic_Œæ , caligraphic_S ) ‚â§ italic_Œ¥ } . (B.3.8)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E9", "title": "dist ‚Å° ( ùùÉ , ùíÆ ) = inf ùùÉ ‚Ä≤ ‚àà ùíÆ ‚Äñ ùùÉ ‚àí ùùÉ ‚Ä≤ ‚Äñ 2 . \\operatorname{dist}(\\bm{\\xi},\\mathcal{S})=\\inf_{\\bm{\\xi}^{\\prime}\\in\\mathcal{S}}\\left\\|\\bm{\\xi}-\\bm{\\xi}^{\\prime}\\right\\|_{2}. roman_dist ( bold_italic_Œæ", "snippet": "dist ‚Å° ( ùùÉ , ùíÆ ) = inf ùùÉ ‚Ä≤ ‚àà ùíÆ ‚Äñ ùùÉ ‚àí ùùÉ ‚Ä≤ ‚Äñ 2 . \\operatorname{dist}(\\bm{\\xi},\\mathcal{S})=\\inf_{\\bm{\\xi}^{\\prime}\\in\\mathcal{S}}\\left\\|\\bm{\\xi}-\\bm{\\xi}^{\\prime}\\right\\|_{2}. roman_dist ( bold_italic_Œæ , caligraphic_S ) = roman_inf start_POSTSUBSCRIPT bold_italic_Œæ start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ‚àà caligraphic_S end_POSTSUBSCRIPT ‚à• bold_italic_Œæ - bold_italic_Œæ start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT . (B.3.9)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E10", "title": "R Œ¥ + œµ ‚Äã ( ùíô Œ¥ ) ‚â§ R œµ ‚Äã ( ùíô ) . R_{\\delta+\\epsilon}(\\bm{x}_{\\delta})\\leq R_{\\epsilon}(\\bm{x}). italic_R start_POSTSUBSCRIPT italic_Œ¥ + italic_œµ end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT ", "snippet": "R Œ¥ + œµ ‚Äã ( ùíô Œ¥ ) ‚â§ R œµ ‚Äã ( ùíô ) . R_{\\delta+\\epsilon}(\\bm{x}_{\\delta})\\leq R_{\\epsilon}(\\bm{x}). italic_R start_POSTSUBSCRIPT italic_Œ¥ + italic_œµ end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT ) ‚â§ italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_x ) . (B.3.10)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E11", "title": "R Œ¥ + œµ ‚Äã ( ùíô Œ¥ ) ‚â§ R œµ ‚Äã ( ùíô ) . R_{\\delta+\\epsilon}(\\bm{x}_{\\delta})\\leq R_{\\epsilon}(\\bm{x}). italic_R start_POSTSUBSCRIPT italic_Œ¥ + italic_œµ end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT ", "snippet": "R Œ¥ + œµ ‚Äã ( ùíô Œ¥ ) ‚â§ R œµ ‚Äã ( ùíô ) . R_{\\delta+\\epsilon}(\\bm{x}_{\\delta})\\leq R_{\\epsilon}(\\bm{x}). italic_R start_POSTSUBSCRIPT italic_Œ¥ + italic_œµ end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT ) ‚â§ italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_x ) . (B.3.11)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E12", "title": "log 2 ‚Å° vol ‚Å° ( Supp ‚Å° ( ùíô Œ¥ ) ) ‚àí log 2 ‚Å° vol ‚Å° ( B Œ¥ + œµ ) ‚àí O ‚Äã ( log ‚Å° D ) ‚â§ R œµ ‚Äã ( ùíô ) . \\log_{2}\\operatorname{vol}(\\operatorname{Supp}(\\bm{x}_{\\delta}))-\\log_{2}\\operatorname{vol}(B_{\\delta+\\ep", "snippet": "log 2 ‚Å° vol ‚Å° ( Supp ‚Å° ( ùíô Œ¥ ) ) ‚àí log 2 ‚Å° vol ‚Å° ( B Œ¥ + œµ ) ‚àí O ‚Äã ( log ‚Å° D ) ‚â§ R œµ ‚Äã ( ùíô ) . \\log_{2}\\operatorname{vol}(\\operatorname{Supp}(\\bm{x}_{\\delta}))-\\log_{2}\\operatorname{vol}(B_{\\delta+\\epsilon})-O(\\log D)\\leq R_{\\epsilon}(\\bm{x}). roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT roman_vol ( roman_Supp ( bold_italic_x start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT ) ) - roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT roman_vol ( italic_B start_POSTSUBSCRIPT italic_Œ¥ + italic_œµ end_POSTSUBSCRIPT ) - italic_O ( roman_log italic_D ) ‚â§ italic_R start_POSTSUBSCRIPT italic_œµ end_POSTS"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E13", "title": "vol ‚Å° ( Supp ‚Å° ( ùíô Œ¥ ) ) vol ‚Å° ( B Œ¥ + œµ ) \\frac{\\operatorname{vol}(\\operatorname{Supp}(\\bm{x}_{\\delta}))}{\\operatorname{vol}(B_{\\delta+\\epsilon})} divide start_ARG roman_vol ( roman_Supp ( bold_itali", "snippet": "vol ‚Å° ( Supp ‚Å° ( ùíô Œ¥ ) ) vol ‚Å° ( B Œ¥ + œµ ) \\frac{\\operatorname{vol}(\\operatorname{Supp}(\\bm{x}_{\\delta}))}{\\operatorname{vol}(B_{\\delta+\\epsilon})} divide start_ARG roman_vol ( roman_Supp ( bold_italic_x start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT ) ) end_ARG start_ARG roman_vol ( italic_B start_POSTSUBSCRIPT italic_Œ¥ + italic_œµ end_POSTSUBSCRIPT ) end_ARG (B.3.13)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E14", "title": "vol ‚Å° ( Supp ‚Å° ( ùíô Œ¥ ) ) ‚â• ùí© 2 ‚Äã Œ¥ ‚Äã ( Supp ‚Å° ( ùíô ) ) ‚Äã vol ‚Å° ( B Œ¥ ) . \\operatorname{vol}(\\operatorname{Supp}(\\bm{x}_{\\delta}))\\geq\\mathcal{N}_{2\\delta}(\\operatorname{Supp}(\\bm{x}))\\operatorname{vol}", "snippet": "vol ‚Å° ( Supp ‚Å° ( ùíô Œ¥ ) ) ‚â• ùí© 2 ‚Äã Œ¥ ‚Äã ( Supp ‚Å° ( ùíô ) ) ‚Äã vol ‚Å° ( B Œ¥ ) . \\operatorname{vol}(\\operatorname{Supp}(\\bm{x}_{\\delta}))\\geq\\mathcal{N}_{2\\delta}(\\operatorname{Supp}(\\bm{x}))\\operatorname{vol}(B_{\\delta}). roman_vol ( roman_Supp ( bold_italic_x start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT ) ) ‚â• caligraphic_N start_POSTSUBSCRIPT 2 italic_Œ¥ end_POSTSUBSCRIPT ( roman_Supp ( bold_italic_x ) ) roman_vol ( italic_B start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT ) . (B.3.14)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E17", "title": "log 2 ‚Å° ùí© œµ ‚Äã ( Supp ‚Å° ( ùíô ) ) ‚àí O ‚Äã ( D ) ‚â§ R œµ ‚Äã ( ùíô ) , \\log_{2}\\mathcal{N}_{\\epsilon}(\\operatorname{Supp}(\\bm{x}))-O(D)\\leq R_{\\epsilon}(\\bm{x}), roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ", "snippet": "log 2 ‚Å° ùí© œµ ‚Äã ( Supp ‚Å° ( ùíô ) ) ‚àí O ‚Äã ( D ) ‚â§ R œµ ‚Äã ( ùíô ) , \\log_{2}\\mathcal{N}_{\\epsilon}(\\operatorname{Supp}(\\bm{x}))-O(D)\\leq R_{\\epsilon}(\\bm{x}), roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT caligraphic_N start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( roman_Supp ( bold_italic_x ) ) - italic_O ( italic_D ) ‚â§ italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_x ) , (B.3.17)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E23", "title": "inf ùùÉ ‚Ä≤ ‚àà ùíÆ k ‚Äñ ùùÉ ‚à• ‚àí ùùÉ ‚Ä≤ ‚Äñ 2 2 = ‚Äñ ùùÉ ‚à• ‚àí ùùÉ ‚à• ‚Äñ ùùÉ ‚à• ‚Äñ 2 ‚Äñ 2 2 . \\inf_{\\bm{\\xi}^{\\prime}\\in\\mathcal{S}_{k}}\\,\\left\\|\\bm{\\xi}^{\\|}-\\bm{\\xi}^{\\prime}\\right\\|_{2}^{2}=\\left\\|\\bm{\\xi}^{\\|}-\\frac{\\bm{\\xi}^{", "snippet": "inf ùùÉ ‚Ä≤ ‚àà ùíÆ k ‚Äñ ùùÉ ‚à• ‚àí ùùÉ ‚Ä≤ ‚Äñ 2 2 = ‚Äñ ùùÉ ‚à• ‚àí ùùÉ ‚à• ‚Äñ ùùÉ ‚à• ‚Äñ 2 ‚Äñ 2 2 . \\inf_{\\bm{\\xi}^{\\prime}\\in\\mathcal{S}_{k}}\\,\\left\\|\\bm{\\xi}^{\\|}-\\bm{\\xi}^{\\prime}\\right\\|_{2}^{2}=\\left\\|\\bm{\\xi}^{\\|}-\\frac{\\bm{\\xi}^{\\|}}{\\|\\bm{\\xi}^{\\|}\\|_{2}}\\right\\|_{2}^{2}. roman_inf start_POSTSUBSCRIPT bold_italic_Œæ start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ‚àà caligraphic_S start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT ‚à• bold_italic_Œæ start_POSTSUPERSCRIPT ‚à• end_POSTSUPERSCRIPT - bold_italic_Œæ start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 "}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E24", "title": "œÄ S k ‚Äã ( ùùÉ ) = ùëº k ‚Äã ùëº k ‚ä§ ‚Äã ùùÉ ‚Äñ ùëº k ‚ä§ ‚Äã ùùÉ ‚Äñ 2 \\pi_{S_{k}}(\\bm{\\xi})=\\frac{\\bm{U}_{k}\\bm{U}_{k}^{\\top}\\bm{\\xi}}{\\|\\bm{U}_{k}^{\\top}\\bm{\\xi}\\|_{2}} italic_œÄ start_POSTSUBSCRIPT italic_S start_POSTSUBS", "snippet": "œÄ S k ‚Äã ( ùùÉ ) = ùëº k ‚Äã ùëº k ‚ä§ ‚Äã ùùÉ ‚Äñ ùëº k ‚ä§ ‚Äã ùùÉ ‚Äñ 2 \\pi_{S_{k}}(\\bm{\\xi})=\\frac{\\bm{U}_{k}\\bm{U}_{k}^{\\top}\\bm{\\xi}}{\\|\\bm{U}_{k}^{\\top}\\bm{\\xi}\\|_{2}} italic_œÄ start_POSTSUBSCRIPT italic_S start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_Œæ ) = divide start_ARG bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_Œæ end_ARG start_ARG ‚à• bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_it"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E29", "title": "‚Äñ ùùÉ k ‚üÇ ‚Äñ 2 2 + ( ‚Äñ ùùÉ k ‚à• ‚Äñ 2 ‚àí 1 ) 2 ‚â§ Œ¥ 2 . \\left\\|\\bm{\\xi}^{\\perp}_{k}\\right\\|_{2}^{2}+\\left(\\left\\|\\bm{\\xi}^{\\|}_{k}\\right\\|_{2}-1\\right)^{2}\\leq\\delta^{2}. ‚à• bold_italic_Œæ start_POSTSUPERSCRIPT ‚üÇ", "snippet": "‚Äñ ùùÉ k ‚üÇ ‚Äñ 2 2 + ( ‚Äñ ùùÉ k ‚à• ‚Äñ 2 ‚àí 1 ) 2 ‚â§ Œ¥ 2 . \\left\\|\\bm{\\xi}^{\\perp}_{k}\\right\\|_{2}^{2}+\\left(\\left\\|\\bm{\\xi}^{\\|}_{k}\\right\\|_{2}-1\\right)^{2}\\leq\\delta^{2}. ‚à• bold_italic_Œæ start_POSTSUPERSCRIPT ‚üÇ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + ( ‚à• bold_italic_Œæ start_POSTSUPERSCRIPT ‚à• end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - 1 ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ‚â§ italic_Œ¥ start_POSTSUPERSCRIPT 2 end_POST"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E33", "title": "‚Äñ ùùÉ j ‚üÇ ‚Äñ 2 2 + ( ‚Äñ ùùÉ j ‚à• ‚Äñ 2 ‚àí 1 ) 2 ‚â• ( 1 ‚àí Œ¥ ) 2 . \\left\\|\\bm{\\xi}^{\\perp}_{j}\\right\\|_{2}^{2}+\\left(\\left\\|\\bm{\\xi}^{\\|}_{j}\\right\\|_{2}-1\\right)^{2}\\geq(1-\\delta)^{2}. ‚à• bold_italic_Œæ start_POSTS", "snippet": "‚Äñ ùùÉ j ‚üÇ ‚Äñ 2 2 + ( ‚Äñ ùùÉ j ‚à• ‚Äñ 2 ‚àí 1 ) 2 ‚â• ( 1 ‚àí Œ¥ ) 2 . \\left\\|\\bm{\\xi}^{\\perp}_{j}\\right\\|_{2}^{2}+\\left(\\left\\|\\bm{\\xi}^{\\|}_{j}\\right\\|_{2}-1\\right)^{2}\\geq(1-\\delta)^{2}. ‚à• bold_italic_Œæ start_POSTSUPERSCRIPT ‚üÇ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + ( ‚à• bold_italic_Œæ start_POSTSUPERSCRIPT ‚à• end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT - 1 ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ‚â• ( 1 - italic_Œ¥ ) start_POSTSU"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E34", "title": "œÄ ùíÆ ‚Äã ( ùùÉ ) = œÄ ùíÆ k ‚ãÜ ‚Äã ( ùùÉ ) , where ‚Äã k ‚ãÜ = arg ‚Äã min k ‚àà [ K ] ‚Å° dist ‚Å° ( ùùÉ , ùíÆ k ) . \\pi_{\\mathcal{S}}(\\bm{\\xi})=\\pi_{\\mathcal{S}_{k_{\\star}}}(\\bm{\\xi}),\\enspace\\text{where}\\enspace k_{\\star}=\\ope", "snippet": "œÄ ùíÆ ‚Äã ( ùùÉ ) = œÄ ùíÆ k ‚ãÜ ‚Äã ( ùùÉ ) , where ‚Äã k ‚ãÜ = arg ‚Äã min k ‚àà [ K ] ‚Å° dist ‚Å° ( ùùÉ , ùíÆ k ) . \\pi_{\\mathcal{S}}(\\bm{\\xi})=\\pi_{\\mathcal{S}_{k_{\\star}}}(\\bm{\\xi}),\\enspace\\text{where}\\enspace k_{\\star}=\\operatorname*{arg\\ min}_{k\\in[K]}\\,\\operatorname{dist}(\\bm{\\xi},\\mathcal{S}_{k}). italic_œÄ start_POSTSUBSCRIPT caligraphic_S end_POSTSUBSCRIPT ( bold_italic_Œæ ) = italic_œÄ start_POSTSUBSCRIPT caligraphic_S start_POSTSUBSCRIPT italic_k start_POSTSUBSCRIPT ‚ãÜ end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_Œæ ) , where italic_k start_POSTSUBSCRIPT ‚ãÜ end_POSTSUBSCRIPT = start_OPERATOR "}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E35", "title": "ùíô ^ Œ¥ = q ‚Äã ( œÄ ùíÆ ‚Äã ( ùíô Œ¥ ) ) . \\hat{\\bm{x}}_{\\delta}=\\mathrm{q}(\\pi_{\\mathcal{S}}(\\bm{x}_{\\delta})). over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT = roman_q ( i", "snippet": "ùíô ^ Œ¥ = q ‚Äã ( œÄ ùíÆ ‚Äã ( ùíô Œ¥ ) ) . \\hat{\\bm{x}}_{\\delta}=\\mathrm{q}(\\pi_{\\mathcal{S}}(\\bm{x}_{\\delta})). over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT = roman_q ( italic_œÄ start_POSTSUBSCRIPT caligraphic_S end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT ) ) . (B.3.35)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E38", "title": "‚Äñ ùíô Œ¥ ‚àí œÄ ùíÆ ‚Äã ( ùíô Œ¥ ) ‚Äñ 2 2 ‚â§ Œ¥ 2 , \\left\\|\\bm{x}_{\\delta}-\\pi_{\\mathcal{S}}(\\bm{x}_{\\delta})\\right\\|_{2}^{2}\\leq\\delta^{2}, ‚à• bold_italic_x start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT - italic_œÄ s", "snippet": "‚Äñ ùíô Œ¥ ‚àí œÄ ùíÆ ‚Äã ( ùíô Œ¥ ) ‚Äñ 2 2 ‚â§ Œ¥ 2 , \\left\\|\\bm{x}_{\\delta}-\\pi_{\\mathcal{S}}(\\bm{x}_{\\delta})\\right\\|_{2}^{2}\\leq\\delta^{2}, ‚à• bold_italic_x start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT - italic_œÄ start_POSTSUBSCRIPT caligraphic_S end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT ) ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ‚â§ italic_Œ¥ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , (B.3.38)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E39", "title": "ùíÆ k , Œ¥ = { ùùÉ ‚à• + ùùÉ ‚üÇ ‚à£ ùùÉ ‚à• ‚àà Span ( ùëº k ) , ùùÉ ‚üÇ ‚àà Span ( ùëº k ) ‚üÇ , ‚à• ùùÉ ‚üÇ ‚à• 2 2 + ( ‚à• ùùÉ ‚à• ‚à• 2 ‚àí 1 ) 2 ‚â§ Œ¥ } . \\mathcal{S}_{k,\\delta}=\\left\\{\\bm{\\xi}^{\\|}+\\bm{\\xi}^{\\perp}\\mid\\bm{\\xi}^{\\|}\\in\\operatorn", "snippet": "ùíÆ k , Œ¥ = { ùùÉ ‚à• + ùùÉ ‚üÇ ‚à£ ùùÉ ‚à• ‚àà Span ( ùëº k ) , ùùÉ ‚üÇ ‚àà Span ( ùëº k ) ‚üÇ , ‚à• ùùÉ ‚üÇ ‚à• 2 2 + ( ‚à• ùùÉ ‚à• ‚à• 2 ‚àí 1 ) 2 ‚â§ Œ¥ } . \\mathcal{S}_{k,\\delta}=\\left\\{\\bm{\\xi}^{\\|}+\\bm{\\xi}^{\\perp}\\mid\\bm{\\xi}^{\\|}\\in\\operatorname{Span}(\\bm{U}_{k}),\\bm{\\xi}^{\\perp}\\in\\operatorname{Span}(\\bm{U}_{k})^{\\perp},\\left\\|\\bm{\\xi}^{\\perp}\\right\\|_{2}^{2}+\\left(\\left\\|\\bm{\\xi}^{\\|}\\right\\|_{2}-1\\right)^{2}\\leq\\delta\\right\\}. caligraphic_S start_POSTSUBSCRIPT italic_k , italic_Œ¥ end_POSTSUBSCRIPT = { bold_italic_Œæ start_POSTSUPERSCRIPT ‚à• end_POSTSUPERSCRIPT + bold_italic_Œæ start_POSTSUPERSCRIPT ‚üÇ end_POSTSUPERSCRIPT ‚à£ bold_italic_"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E40", "title": "œÄ ùíÆ k ‚àí 1 ( ùùÉ ) = { r ùùÉ + ùùÉ ‚üÇ ‚à£ r > 0 , ùùÉ ‚üÇ ‚àà Span ( ùëº k ) ‚üÇ , ‚à• ùùÉ ‚üÇ ‚à• 2 2 + ( r ‚àí 1 ) 2 ‚â§ Œ¥ } . \\pi_{\\mathcal{S}_{k}}^{-1}(\\bm{\\xi})=\\left\\{r\\bm{\\xi}+\\bm{\\xi}^{\\perp}\\mid r>0,\\bm{\\xi}^{\\perp}\\in\\oper", "snippet": "œÄ ùíÆ k ‚àí 1 ( ùùÉ ) = { r ùùÉ + ùùÉ ‚üÇ ‚à£ r > 0 , ùùÉ ‚üÇ ‚àà Span ( ùëº k ) ‚üÇ , ‚à• ùùÉ ‚üÇ ‚à• 2 2 + ( r ‚àí 1 ) 2 ‚â§ Œ¥ } . \\pi_{\\mathcal{S}_{k}}^{-1}(\\bm{\\xi})=\\left\\{r\\bm{\\xi}+\\bm{\\xi}^{\\perp}\\mid r>0,\\bm{\\xi}^{\\perp}\\in\\operatorname{Span}(\\bm{U}_{k})^{\\perp},\\left\\|\\bm{\\xi}^{\\perp}\\right\\|_{2}^{2}+\\left(r-1\\right)^{2}\\leq\\delta\\right\\}. italic_œÄ start_POSTSUBSCRIPT caligraphic_S start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( bold_italic_Œæ ) = { italic_r bold_italic_Œæ + bold_italic_Œæ start_POSTSUPERSCRIPT ‚üÇ end_POSTSUPERSCRIPT ‚à£ italic_r > 0 , bold_ital"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E41", "title": "vol ‚Å° ( ùíÆ k , Œ¥ ) = ‚à¨ Span ( ùëº k ) √ó Span ( ùëº k ) ‚üÇ ùüè ‚Äñ ùùÉ ‚üÇ ‚Äñ 2 2 + ( ‚Äñ ùùÉ ‚à• ‚Äñ 2 ‚àí 1 ) 2 ‚â§ Œ¥ ‚Äã d ùùÉ ‚à• ‚Äã d ùùÉ ‚üÇ . \\operatorname{vol}(\\mathcal{S}_{k,\\delta})=\\iint_{\\operatorname{Span}(\\bm{U}_{k})\\times\\op", "snippet": "vol ‚Å° ( ùíÆ k , Œ¥ ) = ‚à¨ Span ( ùëº k ) √ó Span ( ùëº k ) ‚üÇ ùüè ‚Äñ ùùÉ ‚üÇ ‚Äñ 2 2 + ( ‚Äñ ùùÉ ‚à• ‚Äñ 2 ‚àí 1 ) 2 ‚â§ Œ¥ ‚Äã d ùùÉ ‚à• ‚Äã d ùùÉ ‚üÇ . \\operatorname{vol}(\\mathcal{S}_{k,\\delta})=\\iint_{\\operatorname{Span}(\\bm{U}_{k})\\times\\operatorname{Span}(\\bm{U}_{k})^{\\perp}}\\mathbf{1}_{\\left\\|\\bm{\\xi}^{\\perp}\\right\\|_{2}^{2}+\\left(\\left\\|\\bm{\\xi}^{\\|}\\right\\|_{2}-1\\right)^{2}\\leq\\delta}\\mathrm{d}\\bm{\\xi}^{\\|}\\mathrm{d}\\bm{\\xi}^{\\perp}. roman_vol ( caligraphic_S start_POSTSUBSCRIPT italic_k , italic_Œ¥ end_POSTSUBSCRIPT ) = ‚à¨ start_POSTSUBSCRIPT roman_Span ( bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) √ó roman_Span"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E42", "title": "vol ‚Å° ( ùíÆ k , Œ¥ ) = ‚à´ [ 0 , ‚àû ) ‚à´ ùïä d k ‚àí 1 ‚à´ Span ( ùëº k ) ‚üÇ r d k ‚àí 1 ‚Äã ùüè ‚Äñ ùùÉ ‚üÇ ‚Äñ 2 2 + ( r ‚àí 1 ) 2 ‚â§ Œ¥ ‚Äã d r ‚Äã d ùúΩ d k ‚Äã d ùùÉ ‚üÇ . \\operatorname{vol}(\\mathcal{S}_{k,\\delta})=\\int_{[0,\\infty)}\\int_{\\ma", "snippet": "vol ‚Å° ( ùíÆ k , Œ¥ ) = ‚à´ [ 0 , ‚àû ) ‚à´ ùïä d k ‚àí 1 ‚à´ Span ( ùëº k ) ‚üÇ r d k ‚àí 1 ‚Äã ùüè ‚Äñ ùùÉ ‚üÇ ‚Äñ 2 2 + ( r ‚àí 1 ) 2 ‚â§ Œ¥ ‚Äã d r ‚Äã d ùúΩ d k ‚Äã d ùùÉ ‚üÇ . \\operatorname{vol}(\\mathcal{S}_{k,\\delta})=\\int_{[0,\\infty)}\\int_{\\mathbb{S}^{d_{k}-1}}\\int_{\\operatorname{Span}(\\bm{U}_{k})^{\\perp}}r^{d_{k}-1}\\mathbf{1}_{\\left\\|\\bm{\\xi}^{\\perp}\\right\\|_{2}^{2}+\\left(r-1\\right)^{2}\\leq\\delta}\\mathrm{d}r\\mathrm{d}\\bm{\\theta}^{d_{k}}\\mathrm{d}\\bm{\\xi}^{\\perp}. roman_vol ( caligraphic_S start_POSTSUBSCRIPT italic_k , italic_Œ¥ end_POSTSUBSCRIPT ) = ‚à´ start_POSTSUBSCRIPT [ 0 , ‚àû ) end_POSTSUBSCRIPT ‚à´ start_POSTSUBSCRIPT blackboard_S s"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E43", "title": "ùîº ‚Äã [ ‚Äñ œÄ ùíÆ ‚Äã ( ùíô Œ¥ ) ‚àí q ‚Äã ( œÄ ùíÆ ‚Äã ( ùíô Œ¥ ) ) ‚Äñ 2 2 ] = ùîº ‚Äã [ ‚Äñ ùíô ‚àí q ‚Äã ( ùíô ) ‚Äñ 2 2 ] ‚â§ œµ 2 , \\mathbb{E}\\left[\\left\\|\\pi_{\\mathcal{S}}(\\bm{x}_{\\delta})-\\mathrm{q}(\\pi_{\\mathcal{S}}(\\bm{x}_{\\delta}))\\r", "snippet": "ùîº ‚Äã [ ‚Äñ œÄ ùíÆ ‚Äã ( ùíô Œ¥ ) ‚àí q ‚Äã ( œÄ ùíÆ ‚Äã ( ùíô Œ¥ ) ) ‚Äñ 2 2 ] = ùîº ‚Äã [ ‚Äñ ùíô ‚àí q ‚Äã ( ùíô ) ‚Äñ 2 2 ] ‚â§ œµ 2 , \\mathbb{E}\\left[\\left\\|\\pi_{\\mathcal{S}}(\\bm{x}_{\\delta})-\\mathrm{q}(\\pi_{\\mathcal{S}}(\\bm{x}_{\\delta}))\\right\\|_{2}^{2}\\right]=\\mathbb{E}\\left[\\left\\|\\bm{x}-\\mathrm{q}(\\bm{x})\\right\\|_{2}^{2}\\right]\\leq\\epsilon^{2}, blackboard_E [ ‚à• italic_œÄ start_POSTSUBSCRIPT caligraphic_S end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT ) - roman_q ( italic_œÄ start_POSTSUBSCRIPT caligraphic_S end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT ) ) ‚à• star"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.E44", "title": "R Œ¥ + œµ ‚Äã ( ùíô Œ¥ ) ‚â§ R œµ ‚Äã ( ùíô ) , R_{\\delta+\\epsilon}(\\bm{x}_{\\delta})\\leq R_{\\epsilon}(\\bm{x}), italic_R start_POSTSUBSCRIPT italic_Œ¥ + italic_œµ end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT ", "snippet": "R Œ¥ + œµ ‚Äã ( ùíô Œ¥ ) ‚â§ R œµ ‚Äã ( ùíô ) , R_{\\delta+\\epsilon}(\\bm{x}_{\\delta})\\leq R_{\\epsilon}(\\bm{x}), italic_R start_POSTSUBSCRIPT italic_Œ¥ + italic_œµ end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT ) ‚â§ italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_x ) , (B.3.44)"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx126", "title": "h ‚Äã ( ùíô Œµ ) \\displaystyle h(\\bm{x}_{\\varepsilon}) italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_Œµ end_POSTSUBSCRIPT ) = ‚àí ‚à´ ‚Ñù D p Œµ ‚Äã ( x ) ‚Äã log ‚Å° p Œµ ‚Äã ( x ) ‚Äã d x \\displaystyle=-\\int_{\\mathbb", "snippet": "h ‚Äã ( ùíô Œµ ) \\displaystyle h(\\bm{x}_{\\varepsilon}) italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_Œµ end_POSTSUBSCRIPT ) = ‚àí ‚à´ ‚Ñù D p Œµ ‚Äã ( x ) ‚Äã log ‚Å° p Œµ ‚Äã ( x ) ‚Äã d x \\displaystyle=-\\int_{\\mathbb{R}^{D}}p_{\\varepsilon}(x)\\log p_{\\varepsilon}(x)\\mathrm{d}x = - ‚à´ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT italic_Œµ end_POSTSUBSCRIPT ( italic_x ) roman_log italic_p start_POSTSUBSCRIPT italic_Œµ end_POSTSUBSCRIPT ( italic_x ) roman_d italic_x (B.1.4) = ‚àí ‚à´ ùíÆ Œµ 1 vol ‚Å° ( ùíÆ Œµ ) ‚Äã log ‚Å° ( 1 vol ‚Å° ( ùíÆ Œµ ) ) ‚Äã d "}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx127", "title": "d d ‚Äã t ‚Äã h ‚Äã ( ùíô t ) \\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}t}h(\\bm{x}_{t}) divide start_ARG roman_d end_ARG start_ARG roman_d italic_t end_ARG italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t", "snippet": "d d ‚Äã t ‚Äã h ‚Äã ( ùíô t ) \\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}t}h(\\bm{x}_{t}) divide start_ARG roman_d end_ARG start_ARG roman_d italic_t end_ARG italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = ‚àí d d ‚Äã t ‚Äã ‚à´ ‚Ñù D p t ‚Äã ( ùùÉ ) ‚Äã log ‚Å° p t ‚Äã ( ùùÉ ) ‚Äã d ùùÉ \\displaystyle=-\\frac{\\mathrm{d}}{\\mathrm{d}t}\\int_{\\mathbb{R}^{D}}p_{t}(\\bm{\\xi})\\log p_{t}(\\bm{\\xi})\\mathrm{d}\\bm{\\xi} = - divide start_ARG roman_d end_ARG start_ARG roman_d italic_t end_ARG ‚à´ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_p start_POSTSUBSCR"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx128", "title": "d d ‚Äã t ‚Äã h ‚Äã ( ùíô t ) \\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}t}h(\\bm{x}_{t}) divide start_ARG roman_d end_ARG start_ARG roman_d italic_t end_ARG italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t", "snippet": "d d ‚Äã t ‚Äã h ‚Äã ( ùíô t ) \\displaystyle\\frac{\\mathrm{d}}{\\mathrm{d}t}h(\\bm{x}_{t}) divide start_ARG roman_d end_ARG start_ARG roman_d italic_t end_ARG italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = t ‚Äã ‚à´ ‚Ñù D ‚ü® ‚àá log ‚Å° p t ‚Äã ( ùùÉ ) , ‚àá p t ‚Äã ( ùùÉ ) ‚ü© ‚Äã d ùùÉ \\displaystyle=t\\int_{\\mathbb{R}^{D}}\\langle\\nabla\\log p_{t}(\\bm{\\xi}),\\nabla p_{t}(\\bm{\\xi})\\rangle\\mathrm{d}\\bm{\\xi} = italic_t ‚à´ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ‚ü® ‚àá roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) "}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx129", "title": "h ‚Äã ( ùíô ¬Ø ‚Äã ( ùíô t ) ) \\displaystyle h(\\bar{\\bm{x}}(\\bm{x}_{t})) italic_h ( over¬Ø start_ARG bold_italic_x end_ARG ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ) = ‚àí ‚à´ ùí≥ ( p t ‚àò ùíô ¬Ø ", "snippet": "h ‚Äã ( ùíô ¬Ø ‚Äã ( ùíô t ) ) \\displaystyle h(\\bar{\\bm{x}}(\\bm{x}_{t})) italic_h ( over¬Ø start_ARG bold_italic_x end_ARG ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ) = ‚àí ‚à´ ùí≥ ( p t ‚àò ùíô ¬Ø ‚àí 1 ) ‚Äã ( ùùÉ ) det ( ùíô ¬Ø ‚Ä≤ ‚Äã ( ùíô ¬Ø ‚àí 1 ‚Äã ( ùùÉ ) ) ) ‚Äã log ‚Å° ( p t ‚àò ùíô ¬Ø ‚àí 1 ) ‚Äã ( ùùÉ ) det ( ùíô ¬Ø ‚Ä≤ ‚Äã ( ùíô ¬Ø ‚àí 1 ‚Äã ( ùùÉ ) ) ) ‚Äã d ‚Äã ùùÉ \\displaystyle=-\\int_{\\mathcal{X}}\\frac{(p_{t}\\circ\\bar{\\bm{x}}^{-1})(\\bm{\\xi})}{\\det(\\bar{\\bm{x}}^{\\prime}(\\bar{\\bm{x}}^{-1}(\\bm{\\xi})))}\\log\\frac{(p_{t}\\circ\\bar{\\bm{x}}^{-1})(\\bm{\\xi})}{\\det(\\bar{\\bm{x}}^{\\prime}(\\bar{\\bm{x}}^{-1}(\\bm{\\xi})))}\\mathrm{d}\\bm{\\xi} = - ‚à´ st"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx130", "title": "h ‚Äã ( ùíô ¬Ø ‚Äã ( ùíô t ) ) ‚àí h ‚Äã ( ùíô t ) \\displaystyle h(\\bar{\\bm{x}}(\\bm{x}_{t}))-h(\\bm{x}_{t}) italic_h ( over¬Ø start_ARG bold_italic_x end_ARG ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCR", "snippet": "h ‚Äã ( ùíô ¬Ø ‚Äã ( ùíô t ) ) ‚àí h ‚Äã ( ùíô t ) \\displaystyle h(\\bar{\\bm{x}}(\\bm{x}_{t}))-h(\\bm{x}_{t}) italic_h ( over¬Ø start_ARG bold_italic_x end_ARG ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ) - italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = ‚àí ‚à´ ùí≥ ( p t ‚àò ùíô ¬Ø ‚àí 1 ) ‚Äã ( ùùÉ ) det ( ùíô ¬Ø ‚Ä≤ ‚Äã ( ùíô ¬Ø ‚àí 1 ‚Äã ( ùùÉ ) ) ) ‚Äã log ‚Å° ( 1 det ( ùíô ¬Ø ‚Ä≤ ‚Äã ( ùíô ¬Ø ‚àí 1 ‚Äã ( ùùÉ ) ) ) ) ‚Äã d ùùÉ \\displaystyle=-\\int_{\\mathcal{X}}\\frac{(p_{t}\\circ\\bar{\\bm{x}}^{-1})(\\bm{\\xi})}{\\det(\\bar{\\bm{x}}^{\\prime}(\\bar{\\bm{x}}^{-1}(\\bm{\\xi})))}\\log\\left(\\frac{1}{\\det(\\bar{\\bm{x}}^{\\prime}(\\bar{\\b"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx131", "title": "‚à´ ‚Ñù D p t ‚Äã ( ùùÉ ) ‚Äã det ( ùë∞ + ( 1 ‚àí s t ) ‚Äã t 2 ‚Äã ‚àá 2 log ‚Å° p t ‚Äã ( ùùÉ ) ) ‚Äã d ‚Äã ùùÉ \\displaystyle\\int_{\\mathbb{R}^{D}}p_{t}(\\bm{\\xi})\\det\\left(\\bm{I}+\\left(1-\\frac{s}{t}\\right)t^{2}\\nabla^{2}\\log p_{t}(", "snippet": "‚à´ ‚Ñù D p t ‚Äã ( ùùÉ ) ‚Äã det ( ùë∞ + ( 1 ‚àí s t ) ‚Äã t 2 ‚Äã ‚àá 2 log ‚Å° p t ‚Äã ( ùùÉ ) ) ‚Äã d ‚Äã ùùÉ \\displaystyle\\int_{\\mathbb{R}^{D}}p_{t}(\\bm{\\xi})\\det\\left(\\bm{I}+\\left(1-\\frac{s}{t}\\right)t^{2}\\nabla^{2}\\log p_{t}(\\bm{\\xi})\\right)\\mathrm{d}\\bm{\\xi} ‚à´ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) roman_det ( bold_italic_I + ( 1 - divide start_ARG italic_s end_ARG start_ARG italic_t end_ARG ) italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ‚àá start_POSTSUPERSCRIPT 2 end_POSTSUP"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx132", "title": "( 1 + x ) d \\displaystyle(1+x)^{d} ( 1 + italic_x ) start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ‚â§ ( 1 ‚àí ( 1 ‚àí s t ) ‚Äã t 2 ‚Äã U t D ) D + [ ( 1 + ( 1 ‚àí s t ) ‚Äã t 2 ‚Äã U t D ) D ‚àí ( 1 ‚àí ( 1 ‚àí s t )", "snippet": "( 1 + x ) d \\displaystyle(1+x)^{d} ( 1 + italic_x ) start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ‚â§ ( 1 ‚àí ( 1 ‚àí s t ) ‚Äã t 2 ‚Äã U t D ) D + [ ( 1 + ( 1 ‚àí s t ) ‚Äã t 2 ‚Äã U t D ) D ‚àí ( 1 ‚àí ( 1 ‚àí s t ) ‚Äã t 2 ‚Äã U t D ) D ] ‚èü M ‚Äã ( s , t , D ) ‚Äã x \\displaystyle\\leq\\left(1-\\frac{\\left(1-\\frac{s}{t}\\right)t^{2}U_{t}}{D}\\right)^{D}+\\underbrace{\\left[\\left(1+\\frac{\\left(1-\\frac{s}{t}\\right)t^{2}U_{t}}{D}\\right)^{D}-\\left(1-\\frac{\\left(1-\\frac{s}{t}\\right)t^{2}U_{t}}{D}\\right)^{D}\\right]}_{M(s,t,D)}x ‚â§ ( 1 - divide start_ARG ( 1 - divide start_ARG italic_s end_ARG start_ARG italic_t end_ARG ) italic_t"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx133", "title": "‚àí ( 1 ‚àí s t ) ‚Äã t 2 D ‚Äã U t \\displaystyle-\\frac{\\left(1-\\frac{s}{t}\\right)t^{2}}{D}U_{t} - divide start_ARG ( 1 - divide start_ARG italic_s end_ARG start_ARG italic_t end_ARG ) italic_t start_POSTSUPE", "snippet": "‚àí ( 1 ‚àí s t ) ‚Äã t 2 D ‚Äã U t \\displaystyle-\\frac{\\left(1-\\frac{s}{t}\\right)t^{2}}{D}U_{t} - divide start_ARG ( 1 - divide start_ARG italic_s end_ARG start_ARG italic_t end_ARG ) italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_D end_ARG italic_U start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = ‚àí ( 1 ‚àí s t ) ‚Äã t 2 D ‚Äã max ‚Å° ( D t 2 , | R 2 t 4 ‚àí D t 2 | ) \\displaystyle=-\\frac{\\left(1-\\frac{s}{t}\\right)t^{2}}{D}\\max\\left(\\frac{D}{t^{2}},\\left\\lvert\\frac{R^{2}}{t^{4}}-\\frac{D}{t^{2}}\\right\\rvert\\right) = - divide start_ARG ( 1 - divide start_ARG italic_s end_ARG start_"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx134", "title": "‚à´ ‚Ñù D p t ‚Äã ( ùùÉ ) ‚Äã ( 1 + ( 1 ‚àí s t ) ‚Äã t 2 D ‚Äã Œî ‚Äã log ‚Å° p t ‚Äã ( ùùÉ ) ) D ‚Äã d ùùÉ \\displaystyle\\int_{\\mathbb{R}^{D}}p_{t}(\\bm{\\xi})\\left(1+\\frac{\\left(1-\\frac{s}{t}\\right)t^{2}}{D}\\Delta\\log p_{t}(\\bm{\\", "snippet": "‚à´ ‚Ñù D p t ‚Äã ( ùùÉ ) ‚Äã ( 1 + ( 1 ‚àí s t ) ‚Äã t 2 D ‚Äã Œî ‚Äã log ‚Å° p t ‚Äã ( ùùÉ ) ) D ‚Äã d ùùÉ \\displaystyle\\int_{\\mathbb{R}^{D}}p_{t}(\\bm{\\xi})\\left(1+\\frac{\\left(1-\\frac{s}{t}\\right)t^{2}}{D}\\Delta\\log p_{t}(\\bm{\\xi})\\right)^{D}\\mathrm{d}\\bm{\\xi} ‚à´ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) ( 1 + divide start_ARG ( 1 - divide start_ARG italic_s end_ARG start_ARG italic_t end_ARG ) italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_D end_ARG roman_Œî "}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx135", "title": "‚à´ ‚Ñù D g ‚àí ‚Äã ( ùùÉ ) ‚Äã d ùùÉ \\displaystyle\\int_{\\mathbb{R}^{D}}g_{-}(\\bm{\\xi})\\mathrm{d}\\bm{\\xi} ‚à´ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ital", "snippet": "‚à´ ‚Ñù D g ‚àí ‚Äã ( ùùÉ ) ‚Äã d ùùÉ \\displaystyle\\int_{\\mathbb{R}^{D}}g_{-}(\\bm{\\xi})\\mathrm{d}\\bm{\\xi} ‚à´ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_g start_POSTSUBSCRIPT - end_POSTSUBSCRIPT ( bold_italic_Œæ ) roman_d bold_italic_Œæ = ‚à´ ‚Ñù D ùüè ‚Äã ( p t ‚Äã ( ùùÉ ) ‚â• 1 ) ‚Äã p t ‚Äã ( ùùÉ ) ‚Äã log ‚Å° p t ‚Äã ( ùùÉ ) ‚Äã d ùùÉ \\displaystyle=\\int_{\\mathbb{R}^{D}}\\mathbf{1}(p_{t}(\\bm{\\xi})\\geq 1)p_{t}(\\bm{\\xi})\\log p_{t}(\\bm{\\xi})\\mathrm{d}\\bm{\\xi} = ‚à´ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT bold_1 ( italic"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx136", "title": "‚à´ B r ‚Äã ( ùüé ) { Œî ‚Äã p t ‚Äã ( ùùÉ ) ‚Äã [ c + log ‚Å° p t ‚Äã ( ùùÉ ) ] + ‚ü® ‚àá log ‚Å° p t ‚Äã ( ùùÉ ) , ‚àá p t ‚Äã ( ùùÉ ) ‚ü© } ‚Äã d ùùÉ \\displaystyle\\int_{B_{r}(\\bm{0})}\\left\\{\\Delta p_{t}(\\bm{\\xi})[c+\\log p_{t}(\\bm{\\xi})]+\\la", "snippet": "‚à´ B r ‚Äã ( ùüé ) { Œî ‚Äã p t ‚Äã ( ùùÉ ) ‚Äã [ c + log ‚Å° p t ‚Äã ( ùùÉ ) ] + ‚ü® ‚àá log ‚Å° p t ‚Äã ( ùùÉ ) , ‚àá p t ‚Äã ( ùùÉ ) ‚ü© } ‚Äã d ùùÉ \\displaystyle\\int_{B_{r}(\\bm{0})}\\left\\{\\Delta p_{t}(\\bm{\\xi})[c+\\log p_{t}(\\bm{\\xi})]+\\langle\\nabla\\log p_{t}(\\bm{\\xi}),\\nabla p_{t}(\\bm{\\xi})\\rangle\\right\\}\\mathrm{d}\\bm{\\xi} ‚à´ start_POSTSUBSCRIPT italic_B start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ( bold_0 ) end_POSTSUBSCRIPT { roman_Œî italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) [ italic_c + roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) ] + ‚ü® ‚àá roman_log italic_"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx137", "title": "‚à´ ‚Ñù D { Œî ‚Äã p t ‚Äã ( ùùÉ ) ‚Äã [ c + log ‚Å° p t ‚Äã ( ùùÉ ) ] + ‚ü® ‚àá log ‚Å° p t ‚Äã ( ùùÉ ) , ‚àá p t ‚Äã ( ùùÉ ) ‚ü© } ‚Äã d ùùÉ \\displaystyle\\int_{\\mathbb{R}^{D}}\\left\\{\\Delta p_{t}(\\bm{\\xi})[c+\\log p_{t}(\\bm{\\xi})]+\\langle\\na", "snippet": "‚à´ ‚Ñù D { Œî ‚Äã p t ‚Äã ( ùùÉ ) ‚Äã [ c + log ‚Å° p t ‚Äã ( ùùÉ ) ] + ‚ü® ‚àá log ‚Å° p t ‚Äã ( ùùÉ ) , ‚àá p t ‚Äã ( ùùÉ ) ‚ü© } ‚Äã d ùùÉ \\displaystyle\\int_{\\mathbb{R}^{D}}\\left\\{\\Delta p_{t}(\\bm{\\xi})[c+\\log p_{t}(\\bm{\\xi})]+\\langle\\nabla\\log p_{t}(\\bm{\\xi}),\\nabla p_{t}(\\bm{\\xi})\\rangle\\right\\}\\mathrm{d}\\bm{\\xi} ‚à´ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT { roman_Œî italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) [ italic_c + roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) ] + ‚ü® ‚àá roman_log italic_p start_PO"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx138", "title": "p t ‚Äã ( ùùÉ ) \\displaystyle p_{t}(\\bm{\\xi}) italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) = ùîº ‚Å° [ œÜ t ‚Äã ( ùùÉ ‚àí ùíô ) ] \\displaystyle=\\operatorname{\\mathbb{E}}[\\varphi_{t}(\\bm{\\x", "snippet": "p t ‚Äã ( ùùÉ ) \\displaystyle p_{t}(\\bm{\\xi}) italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) = ùîº ‚Å° [ œÜ t ‚Äã ( ùùÉ ‚àí ùíô ) ] \\displaystyle=\\operatorname{\\mathbb{E}}[\\varphi_{t}(\\bm{\\xi}-\\bm{x})] = blackboard_E [ italic_œÜ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ - bold_italic_x ) ] (B.2.68) = ùîº ‚Å° [ 1 ( 2 ‚Äã œÄ ) D / 2 ‚Äã t D ‚èü ‚âê C t ‚Äã e ‚àí ‚Äñ ùùÉ ‚àí ùíô ‚Äñ 2 2 / ( 2 ‚Äã t 2 ) ] \\displaystyle=\\operatorname{\\mathbb{E}}\\left[\\underbrace{\\frac{1}{(2\\pi)^{D/2}t^{D}}}_{\\doteq C_{t}}e^{-\\|\\bm{\\xi}-\\bm{x}\\|_{2}^{2}/(2t^{2})}\\right] = blackboard_E [ under‚èü start_ARG divide star"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx139", "title": "‚ü® ‚àá p t ‚Äã ( ùùÉ ) , ùùÉ ‚ü© \\displaystyle\\langle\\nabla p_{t}(\\bm{\\xi}),\\bm{\\xi}\\rangle ‚ü® ‚àá italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) , bold_italic_Œæ ‚ü© = ‚ü® ‚àí 1 t 2 ‚Äã ùîº ‚Å° [ ( ùùÉ", "snippet": "‚ü® ‚àá p t ‚Äã ( ùùÉ ) , ùùÉ ‚ü© \\displaystyle\\langle\\nabla p_{t}(\\bm{\\xi}),\\bm{\\xi}\\rangle ‚ü® ‚àá italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) , bold_italic_Œæ ‚ü© = ‚ü® ‚àí 1 t 2 ‚Äã ùîº ‚Å° [ ( ùùÉ ‚àí ùíô ) ‚Äã œÜ t ‚Äã ( ùùÉ ‚àí ùíô ) ] , ùùÉ ‚ü© \\displaystyle=\\left\\langle-\\frac{1}{t^{2}}\\operatorname{\\mathbb{E}}\\left[\\left(\\bm{\\xi}-\\bm{x}\\right)\\varphi_{t}(\\bm{\\xi}-\\bm{x})\\right],\\bm{\\xi}\\right\\rangle = ‚ü® - divide start_ARG 1 end_ARG start_ARG italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG blackboard_E [ ( bold_italic_Œæ - bold_italic_x ) italic_œÜ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx140", "title": "1 t 2 ‚Äã ùîº ‚Å° [ ( ‚àí R ‚Äã r ‚àí r 2 ) ‚Äã œÜ t ‚Äã ( ùùÉ ‚àí ùíô ) ] ‚â§ ‚ü® ‚àá p t ‚Äã ( ùùÉ ) , ùùÉ ‚ü© ‚â§ 1 t 2 ‚Äã ùîº ‚Å° [ ( R ‚Äã r ‚àí r 2 ) ‚Äã œÜ t ‚Äã ( ùùÉ ‚àí ùíô ) ] \\displaystyle\\frac{1}{t^{2}}\\operatorname{\\mathbb{E}}\\left[\\left(-Rr-r^{", "snippet": "1 t 2 ‚Äã ùîº ‚Å° [ ( ‚àí R ‚Äã r ‚àí r 2 ) ‚Äã œÜ t ‚Äã ( ùùÉ ‚àí ùíô ) ] ‚â§ ‚ü® ‚àá p t ‚Äã ( ùùÉ ) , ùùÉ ‚ü© ‚â§ 1 t 2 ‚Äã ùîº ‚Å° [ ( R ‚Äã r ‚àí r 2 ) ‚Äã œÜ t ‚Äã ( ùùÉ ‚àí ùíô ) ] \\displaystyle\\frac{1}{t^{2}}\\operatorname{\\mathbb{E}}\\left[\\left(-Rr-r^{2}\\right)\\varphi_{t}(\\bm{\\xi}-\\bm{x})\\right]\\leq\\langle\\nabla p_{t}(\\bm{\\xi}),\\bm{\\xi}\\rangle\\leq\\frac{1}{t^{2}}\\operatorname{\\mathbb{E}}\\left[\\left(Rr-r^{2}\\right)\\varphi_{t}(\\bm{\\xi}-\\bm{x})\\right] divide start_ARG 1 end_ARG start_ARG italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG blackboard_E [ ( - italic_R italic_r - italic_r start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) italic_œÜ"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx141", "title": "‚à´ ‚Ñù D { Œî ‚Äã p t ‚Äã ( ùùÉ ) ‚Äã [ c + log ‚Å° p t ‚Äã ( ùùÉ ) ] + ‚ü® ‚àá log ‚Å° p t ‚Äã ( ùùÉ ) , ‚àá p t ‚Äã ( ùùÉ ) ‚ü© } ‚Äã d ùùÉ = 0 \\displaystyle\\int_{\\mathbb{R}^{D}}\\left\\{\\Delta p_{t}(\\bm{\\xi})[c+\\log p_{t}(\\bm{\\xi})]+\\langl", "snippet": "‚à´ ‚Ñù D { Œî ‚Äã p t ‚Äã ( ùùÉ ) ‚Äã [ c + log ‚Å° p t ‚Äã ( ùùÉ ) ] + ‚ü® ‚àá log ‚Å° p t ‚Äã ( ùùÉ ) , ‚àá p t ‚Äã ( ùùÉ ) ‚ü© } ‚Äã d ùùÉ = 0 \\displaystyle\\int_{\\mathbb{R}^{D}}\\left\\{\\Delta p_{t}(\\bm{\\xi})[c+\\log p_{t}(\\bm{\\xi})]+\\langle\\nabla\\log p_{t}(\\bm{\\xi}),\\nabla p_{t}(\\bm{\\xi})\\rangle\\right\\}\\mathrm{d}\\bm{\\xi}=0 ‚à´ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT { roman_Œî italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) [ italic_c + roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) ] + ‚ü® ‚àá roman_log italic_p st"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx142", "title": "ùíô ¬Ø ‚Ä≤ ‚Äã ( ùùÉ ) \\displaystyle\\bar{\\bm{x}}^{\\prime}(\\bm{\\xi}) over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( bold_italic_Œæ ) = ùë∞ + ( 1 ‚àí s t ) ‚Äã t 2 ‚Äã p t ‚Äã ( ùùÉ ) ‚Äã ‚àá", "snippet": "ùíô ¬Ø ‚Ä≤ ‚Äã ( ùùÉ ) \\displaystyle\\bar{\\bm{x}}^{\\prime}(\\bm{\\xi}) over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( bold_italic_Œæ ) = ùë∞ + ( 1 ‚àí s t ) ‚Äã t 2 ‚Äã p t ‚Äã ( ùùÉ ) ‚Äã ‚àá 2 p t ‚Äã ( ùùÉ ) ‚àí ( ‚àá p t ‚Äã ( ùùÉ ) ) ‚Äã ( ‚àá p t ‚Äã ( ùùÉ ) ) ‚ä§ p t ‚Äã ( ùùÉ ) 2 \\displaystyle=\\bm{I}+\\left(1-\\frac{s}{t}\\right)t^{2}\\frac{p_{t}(\\bm{\\xi})\\nabla^{2}p_{t}(\\bm{\\xi})-(\\nabla p_{t}(\\bm{\\xi}))(\\nabla p_{t}(\\bm{\\xi}))^{\\top}}{p_{t}(\\bm{\\xi})^{2}} = bold_italic_I + ( 1 - divide start_ARG italic_s end_ARG start_ARG italic_t end_ARG ) italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT divide start"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx143", "title": "ùíó ‚ä§ ‚Äã [ ùíô ¬Ø ‚Ä≤ ‚Äã ( ùùÉ ) ] ‚Äã ùíó \\displaystyle\\bm{v}^{\\top}[\\bar{\\bm{x}}^{\\prime}(\\bm{\\xi})]\\bm{v} bold_italic_v start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT [ over¬Ø start_ARG bold_italic_x end_ARG start_PO", "snippet": "ùíó ‚ä§ ‚Äã [ ùíô ¬Ø ‚Ä≤ ‚Äã ( ùùÉ ) ] ‚Äã ùíó \\displaystyle\\bm{v}^{\\top}[\\bar{\\bm{x}}^{\\prime}(\\bm{\\xi})]\\bm{v} bold_italic_v start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT [ over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ( bold_italic_Œæ ) ] bold_italic_v (B.2.94) = p t ( ùùÉ ) ‚àí 2 ùíó ‚ä§ { p t ( ùùÉ ) 2 ùë∞ + ( 1 ‚àí s t ) t 2 ùîº [ œÜ t ( ùùÉ ‚àí ùíô ) ] ùîº [ œÜ t ( ùùÉ ‚àí ùíô ) ‚ãÖ ( ùùÉ ‚àí ùíô ) ‚Äã ( ùùÉ ‚àí ùíô ) ‚ä§ ‚àí t 2 ‚Äã ùë∞ t 4 ] \\displaystyle=p_{t}(\\bm{\\xi})^{-2}\\bm{v}^{\\top}\\Bigg{\\{}p_{t}(\\bm{\\xi})^{2}\\bm{I}+\\left(1-\\frac{s}{t}\\right)t^{2}\\operatorname{\\mathbb{E}}[\\varphi_{t}(\\bm{\\xi}-\\bm{x})]\\operatorname{\\math"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx144", "title": "Œî ‚Äã p t ‚Äã ( ùùÉ ) p t ‚Äã ( ùùÉ ) \\displaystyle\\frac{\\Delta p_{t}(\\bm{\\xi})}{p_{t}(\\bm{\\xi})} divide start_ARG roman_Œî italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) end_ARG start", "snippet": "Œî ‚Äã p t ‚Äã ( ùùÉ ) p t ‚Äã ( ùùÉ ) \\displaystyle\\frac{\\Delta p_{t}(\\bm{\\xi})}{p_{t}(\\bm{\\xi})} divide start_ARG roman_Œî italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) end_ARG start_ARG italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) end_ARG = ùîº ‚Å° [ ‚Äñ ùùÉ ‚àí ùíô ‚Äñ 2 2 ‚àí D ‚Äã t 2 t 4 ‚ãÖ œÜ t ‚Äã ( ùùÉ ‚àí ùíô ) ] ùîº ‚Å° [ œÜ t ‚Äã ( ùùÉ ‚àí ùíô ) ] \\displaystyle=\\frac{\\operatorname{\\mathbb{E}}\\left[\\frac{\\|\\bm{\\xi}-\\bm{x}\\|_{2}^{2}-Dt^{2}}{t^{4}}\\cdot\\varphi_{t}(\\bm{\\xi}-\\bm{x})\\right]}{\\operatorname{\\mathbb{E}}[\\varphi_{t}(\\bm{\\xi}-\\bm{x})]} = divide start_ARG blackboard_E [ d"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx145", "title": "Œî ‚Äã log ‚Å° p t ‚Äã ( ùùÉ ) \\displaystyle\\Delta\\log p_{t}(\\bm{\\xi}) roman_Œî roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) = ùîº ‚Å° [ ‚Äñ ùíõ ùùÉ ‚Äñ 2 2 ] t 4 ‚àí D t 2 ‚àí ‚Äñ ùîº ‚Å° [ ùíõ ", "snippet": "Œî ‚Äã log ‚Å° p t ‚Äã ( ùùÉ ) \\displaystyle\\Delta\\log p_{t}(\\bm{\\xi}) roman_Œî roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) = ùîº ‚Å° [ ‚Äñ ùíõ ùùÉ ‚Äñ 2 2 ] t 4 ‚àí D t 2 ‚àí ‚Äñ ùîº ‚Å° [ ùíõ ùùÉ ] ‚Äñ 2 2 t 4 \\displaystyle=\\frac{\\operatorname{\\mathbb{E}}[\\|\\bm{z}_{\\bm{\\xi}}\\|_{2}^{2}]}{t^{4}}-\\frac{D}{t^{2}}-\\frac{\\|\\operatorname{\\mathbb{E}}[\\bm{z}_{\\bm{\\xi}}]\\|_{2}^{2}}{t^{4}} = divide start_ARG blackboard_E [ ‚à• bold_italic_z start_POSTSUBSCRIPT bold_italic_Œæ end_POSTSUBSCRIPT ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] end_ARG start_ARG italic"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx146", "title": "‚àÇ p t ‚àÇ t ‚Äã ( ùùÉ ) \\displaystyle\\frac{\\partial p_{t}}{\\partial t}(\\bm{\\xi}) divide start_ARG ‚àÇ italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG ‚àÇ italic_t end_ARG ( bold_italic", "snippet": "‚àÇ p t ‚àÇ t ‚Äã ( ùùÉ ) \\displaystyle\\frac{\\partial p_{t}}{\\partial t}(\\bm{\\xi}) divide start_ARG ‚àÇ italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG ‚àÇ italic_t end_ARG ( bold_italic_Œæ ) = ùîº ‚Å° [ œÜ t ‚Äã ( ùùÉ ‚àí ùíô ) ‚ãÖ ‚Äñ ùùÉ ‚àí ùíô ‚Äñ 2 2 ‚àí D ‚Äã t 2 t 3 ] \\displaystyle=\\operatorname{\\mathbb{E}}\\left[\\varphi_{t}(\\bm{\\xi}-\\bm{x})\\cdot\\frac{\\|\\bm{\\xi}-\\bm{x}\\|_{2}^{2}-Dt^{2}}{t^{3}}\\right] = blackboard_E [ italic_œÜ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ - bold_italic_x ) ‚ãÖ divide start_ARG ‚à• bold_italic_Œæ - bold_italic_x ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx147", "title": "‚àÇ ‚àÇ t ‚Äã œÜ t ‚Äã ( ùùÉ ) \\displaystyle\\frac{\\partial}{\\partial t}\\varphi_{t}(\\bm{\\xi}) divide start_ARG ‚àÇ end_ARG start_ARG ‚àÇ italic_t end_ARG italic_œÜ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold", "snippet": "‚àÇ ‚àÇ t ‚Äã œÜ t ‚Äã ( ùùÉ ) \\displaystyle\\frac{\\partial}{\\partial t}\\varphi_{t}(\\bm{\\xi}) divide start_ARG ‚àÇ end_ARG start_ARG ‚àÇ italic_t end_ARG italic_œÜ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) = œÜ t ‚Äã ( ùùÉ ) ‚ãÖ ‚Äñ ùùÉ ‚Äñ 2 2 ‚àí D ‚Äã t 2 t 3 \\displaystyle=\\varphi_{t}(\\bm{\\xi})\\cdot\\frac{\\|\\bm{\\xi}\\|_{2}^{2}-Dt^{2}}{t^{3}} = italic_œÜ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) ‚ãÖ divide start_ARG ‚à• bold_italic_Œæ ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT - italic_D italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_A"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx148", "title": "log ‚Å° ( 2 D ‚Äã Œì ‚Äã ( D / 2 ) ‚Äã ( D 2 ‚Äã e ) D / 2 ) \\displaystyle\\log\\left(\\frac{2}{D\\Gamma(D/2)}\\left(\\frac{D}{2e}\\right)^{D/2}\\right) roman_log ( divide start_ARG 2 end_ARG start_ARG italic_D roman_Œì ", "snippet": "log ‚Å° ( 2 D ‚Äã Œì ‚Äã ( D / 2 ) ‚Äã ( D 2 ‚Äã e ) D / 2 ) \\displaystyle\\log\\left(\\frac{2}{D\\Gamma(D/2)}\\left(\\frac{D}{2e}\\right)^{D/2}\\right) roman_log ( divide start_ARG 2 end_ARG start_ARG italic_D roman_Œì ( italic_D / 2 ) end_ARG ( divide start_ARG italic_D end_ARG start_ARG 2 italic_e end_ARG ) start_POSTSUPERSCRIPT italic_D / 2 end_POSTSUPERSCRIPT ) ‚â• ‚àí 1 6 ‚Äã D + log ‚Å° ( 2 D ‚Äã ( D 2 ‚Äã e ) D / 2 ‚ãÖ D 4 ‚Äã œÄ ‚Äã ( D 2 ‚Äã e ) ‚àí D / 2 ) \\displaystyle\\geq-\\frac{1}{6D}+\\log\\left(\\frac{2}{D}\\left(\\frac{D}{2e}\\right)^{D/2}\\cdot\\sqrt{\\frac{D}{4\\pi}}\\left(\\frac{D}{2e}\\right)^{-D/2}\\right) ‚â• - divide start_ARG 1"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx149", "title": "vol ‚Å° ( Supp ‚Å° ( ùíô Œ¥ ) ) vol ‚Å° ( B Œ¥ + œµ ) \\displaystyle\\frac{\\operatorname{vol}(\\operatorname{Supp}(\\bm{x}_{\\delta}))}{\\operatorname{vol}(B_{\\delta+\\epsilon})} divide start_ARG roman_vol ( roman_Supp", "snippet": "vol ‚Å° ( Supp ‚Å° ( ùíô Œ¥ ) ) vol ‚Å° ( B Œ¥ + œµ ) \\displaystyle\\frac{\\operatorname{vol}(\\operatorname{Supp}(\\bm{x}_{\\delta}))}{\\operatorname{vol}(B_{\\delta+\\epsilon})} divide start_ARG roman_vol ( roman_Supp ( bold_italic_x start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT ) ) end_ARG start_ARG roman_vol ( italic_B start_POSTSUBSCRIPT italic_Œ¥ + italic_œµ end_POSTSUBSCRIPT ) end_ARG ‚â• ùí© 2 ‚Äã Œ¥ ‚Äã ( Supp ‚Å° ( ùíô ) ) ‚Äã vol ‚Å° ( B Œ¥ ) vol ‚Å° ( B Œ¥ + œµ ) \\displaystyle\\geq\\mathcal{N}_{2\\delta}(\\operatorname{Supp}(\\bm{x}))\\frac{\\operatorname{vol}(B_{\\delta})}{\\operatorname{vol}(B_{\\delta+\\epsilon})} ‚â• caligraphic_N "}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx150", "title": "ùíÆ Œ¥ \\displaystyle\\mathcal{S}_{\\delta} caligraphic_S start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT = { ùùÉ ‚àà ‚Ñù D ‚à£ ‚àÉ k ‚àà [ K ] : dist ‚Å° ( ùùÉ , ùíÆ k ) ‚â§ Œ¥ } \\displaystyle=\\{\\bm{\\xi}\\in\\mathbb{R}^{D}\\mid\\ex", "snippet": "ùíÆ Œ¥ \\displaystyle\\mathcal{S}_{\\delta} caligraphic_S start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT = { ùùÉ ‚àà ‚Ñù D ‚à£ ‚àÉ k ‚àà [ K ] : dist ‚Å° ( ùùÉ , ùíÆ k ) ‚â§ Œ¥ } \\displaystyle=\\{\\bm{\\xi}\\in\\mathbb{R}^{D}\\mid\\exists k\\in[K]\\>:\\>\\operatorname{dist}(\\bm{\\xi},\\mathcal{S}_{k})\\leq\\delta\\} = { bold_italic_Œæ ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT ‚à£ ‚àÉ italic_k ‚àà [ italic_K ] : roman_dist ( bold_italic_Œæ , caligraphic_S start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ‚â§ italic_Œ¥ } (B.3.18) = ‚ãÉ k ‚àà [ K ] { ùùÉ ‚àà ‚Ñù D ‚à£ dist ‚Å° ( ùùÉ , ùíÆ k ) ‚â§ Œ¥ } . \\displaystyle=\\bigcup_{k\\in[K]}\\{\\bm{\\xi}\\in"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx151", "title": "‚Äñ ùùÉ ‚àí ùùÉ ‚Ä≤ ‚Äñ 2 2 = ‚Äñ ùùÉ ‚à• + ùùÉ ‚üÇ ‚àí ùùÉ ‚Ä≤ ‚Äñ 2 2 \\displaystyle\\left\\|\\bm{\\xi}-\\bm{\\xi}^{\\prime}\\right\\|_{2}^{2}=\\left\\|\\bm{\\xi}^{\\|}+\\bm{\\xi}^{\\perp}-\\bm{\\xi}^{\\prime}\\right\\|_{2}^{2} ‚à• bold_italic_Œæ - bold_", "snippet": "‚Äñ ùùÉ ‚àí ùùÉ ‚Ä≤ ‚Äñ 2 2 = ‚Äñ ùùÉ ‚à• + ùùÉ ‚üÇ ‚àí ùùÉ ‚Ä≤ ‚Äñ 2 2 \\displaystyle\\left\\|\\bm{\\xi}-\\bm{\\xi}^{\\prime}\\right\\|_{2}^{2}=\\left\\|\\bm{\\xi}^{\\|}+\\bm{\\xi}^{\\perp}-\\bm{\\xi}^{\\prime}\\right\\|_{2}^{2} ‚à• bold_italic_Œæ - bold_italic_Œæ start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = ‚à• bold_italic_Œæ start_POSTSUPERSCRIPT ‚à• end_POSTSUPERSCRIPT + bold_italic_Œæ start_POSTSUPERSCRIPT ‚üÇ end_POSTSUPERSCRIPT - bold_italic_Œæ start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPER"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx152", "title": "ùíÆ Œ¥ \\displaystyle\\mathcal{S}_{\\delta} caligraphic_S start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT = ‚ãÉ k ‚àà [ K ] { ùùÉ ‚àà ‚Ñù D ‚à£ ‚Äñ ùùÉ ‚àí ùëº k ‚Äã ùëº k ‚ä§ ‚Äã ùùÉ ‚Äñ ùëº k ‚ä§ ‚Äã ùùÉ ‚Äñ 2 ‚Äñ 2 ‚â§ Œ¥ } . \\displaystyle=\\bigcup_{k\\", "snippet": "ùíÆ Œ¥ \\displaystyle\\mathcal{S}_{\\delta} caligraphic_S start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT = ‚ãÉ k ‚àà [ K ] { ùùÉ ‚àà ‚Ñù D ‚à£ ‚Äñ ùùÉ ‚àí ùëº k ‚Äã ùëº k ‚ä§ ‚Äã ùùÉ ‚Äñ ùëº k ‚ä§ ‚Äã ùùÉ ‚Äñ 2 ‚Äñ 2 ‚â§ Œ¥ } . \\displaystyle=\\bigcup_{k\\in[K]}\\left\\{\\bm{\\xi}\\in\\mathbb{R}^{D}\\mid\\left\\|\\bm{\\xi}-\\frac{\\bm{U}_{k}\\bm{U}_{k}^{\\top}\\bm{\\xi}}{\\|\\bm{U}_{k}^{\\top}\\bm{\\xi}\\|_{2}}\\right\\|_{2}\\leq\\delta\\right\\}. = ‚ãÉ start_POSTSUBSCRIPT italic_k ‚àà [ italic_K ] end_POSTSUBSCRIPT { bold_italic_Œæ ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT ‚à£ ‚à• bold_italic_Œæ - divide start_ARG bold_italic_U start_POSTSUBSCRIPT italic_k end_P"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx153", "title": "‚Äñ ùùÉ ‚àí ùëº k ‚Äã ùëº k ‚ä§ ‚Äã ùùÉ ‚Äñ ùëº k ‚ä§ ‚Äã ùùÉ ‚Äñ 2 ‚Äñ 2 2 \\displaystyle\\left\\|\\bm{\\xi}-\\frac{\\bm{U}_{k}\\bm{U}_{k}^{\\top}\\bm{\\xi}}{\\|\\bm{U}_{k}^{\\top}\\bm{\\xi}\\|_{2}}\\right\\|_{2}^{2} ‚à• bold_italic_Œæ - divide start_AR", "snippet": "‚Äñ ùùÉ ‚àí ùëº k ‚Äã ùëº k ‚ä§ ‚Äã ùùÉ ‚Äñ ùëº k ‚ä§ ‚Äã ùùÉ ‚Äñ 2 ‚Äñ 2 2 \\displaystyle\\left\\|\\bm{\\xi}-\\frac{\\bm{U}_{k}\\bm{U}_{k}^{\\top}\\bm{\\xi}}{\\|\\bm{U}_{k}^{\\top}\\bm{\\xi}\\|_{2}}\\right\\|_{2}^{2} ‚à• bold_italic_Œæ - divide start_ARG bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_Œæ end_ARG start_ARG ‚à• bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_Œæ ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG ‚à• start_POSTSUBSCRIPT 2 end_P"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx154", "title": "‚Äñ ùùÉ j ‚à• ‚Äñ 2 = ‚Äñ ùëº j ‚Äã ùëº j ‚ä§ ‚Äã ùùÉ ‚Äñ 2 \\displaystyle\\left\\|\\bm{\\xi}_{j}^{\\|}\\right\\|_{2}=\\left\\|\\bm{U}_{j}\\bm{U}_{j}^{\\top}\\bm{\\xi}\\right\\|_{2} ‚à• bold_italic_Œæ start_POSTSUBSCRIPT italic_j end_POSTSUBSCR", "snippet": "‚Äñ ùùÉ j ‚à• ‚Äñ 2 = ‚Äñ ùëº j ‚Äã ùëº j ‚ä§ ‚Äã ùùÉ ‚Äñ 2 \\displaystyle\\left\\|\\bm{\\xi}_{j}^{\\|}\\right\\|_{2}=\\left\\|\\bm{U}_{j}\\bm{U}_{j}^{\\top}\\bm{\\xi}\\right\\|_{2} ‚à• bold_italic_Œæ start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚à• end_POSTSUPERSCRIPT ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = ‚à• bold_italic_U start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_Œæ ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = ‚Äñ ùëº j ‚Äã ùëº j ‚ä§ ‚Äã ( ùëº k ‚Äã ùëº k ‚ä§ ‚Äã ùùÉ + ( ùë∞ ‚àí ùëº k ‚Äã ùëº k ‚ä§ ) ‚Äã ùùÉ ) ‚Äñ 2 \\displaystyle=\\l"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S3.EGx155", "title": "ùîº ‚Äã [ ‚Äñ ùíô Œ¥ ‚àí ùíô ^ Œ¥ ‚Äñ 2 2 ] \\displaystyle\\mathbb{E}\\left[\\left\\|\\bm{x}_{\\delta}-\\hat{\\bm{x}}_{\\delta}\\right\\|_{2}^{2}\\right] blackboard_E [ ‚à• bold_italic_x start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRI", "snippet": "ùîº ‚Äã [ ‚Äñ ùíô Œ¥ ‚àí ùíô ^ Œ¥ ‚Äñ 2 2 ] \\displaystyle\\mathbb{E}\\left[\\left\\|\\bm{x}_{\\delta}-\\hat{\\bm{x}}_{\\delta}\\right\\|_{2}^{2}\\right] blackboard_E [ ‚à• bold_italic_x start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT - over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_Œ¥ end_POSTSUBSCRIPT ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] = ùîº ‚Äã [ ‚Äñ ùíô Œ¥ ‚àí q ‚Äã ( œÄ ùíÆ ‚Äã ( ùíô Œ¥ ) ) ‚Äñ 2 2 ] \\displaystyle=\\mathbb{E}\\left[\\left\\|\\bm{x}_{\\delta}-\\mathrm{q}(\\pi_{\\mathcal{S}}(\\bm{x}_{\\delta}))\\right\\|_{2}^{2}\\right] = blackboard_E [ ‚à• bold_italic_x start_POSTSUBSCRIPT "}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS3.SSSx1", "title": "Finitneness of the Differential Entropy", "snippet": "Finitneness of the Differential Entropy We first show that the entropy exists along the stochastic process and is finite. Lemma B.1 . Let ùê± \\bm{x} bold_italic_x be any random variable, and let ( ùê± t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ‚àà [ 0 , italic_T ] end_POSTSUBSCRIPT be the stochastic process ( B.2.1 ). 1. For t > 0 t>0 italic_t > 0 , the differential entropy h ‚Äã ( ùíô t ) h(\\bm{x}_{t}) italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) exists and is > ‚àí ‚àû >-\\infty > - ‚àû "}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS3.SSSx2", "title": "Integration by Parts in De Brujin Identity", "snippet": "Integration by Parts in De Brujin Identity Finally, we fill in the integration-by-parts argument alluded to in the proofs of Theorems B.2 and B.3 . The argument is conceptually pretty simple but requires some technical estimates to show that the boundary term in the integration-by-parts vanishes. Lemma B.2 . Let ùê± \\bm{x} bold_italic_x be any random variable such that Assumptions B.1 and B.2 hold, and let ( ùê± t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ‚àà [ 0 , italic_T ] end_POSTSUBSCRIPT be the stochas"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS3.SSSx3", "title": "Local Invertibility of the Denoiser ùíô ¬Ø \\bar{\\bm{x}} over¬Ø start_ARG bold_italic_x end_ARG", "snippet": "Local Invertibility of the Denoiser ùíô ¬Ø \\bar{\\bm{x}} over¬Ø start_ARG bold_italic_x end_ARG Here we provide some results used in the proof of Theorem B.3 which are appropriate generalizations of corresponding results in [ Gri11 ] . Lemma B.3 (Generalization of [ Gri11 ] , Lemma A.1) . Let ùê± \\bm{x} bold_italic_x be any random variable such that Assumptions B.1 and B.2 hold, and let ( ùê± t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ‚àà [ 0 , italic_T ] end_POSTSUBSCRIPT be the stochastic process ( B.2.1 ). Le"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS3.SSSx4", "title": "Controlling the Laplacian Œî ‚Äã log ‚Å° p t \\Delta\\log p_{t} roman_Œî roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT", "snippet": "Controlling the Laplacian Œî ‚Äã log ‚Å° p t \\Delta\\log p_{t} roman_Œî roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT Finally, we develop a technical estimate which is required for the proof of Theorem B.3 and actually motivates the assumption for the viable t t italic_t . Lemma B.5 . Let ùê± \\bm{x} bold_italic_x be any random variable such that Assumptions B.1 and B.2 hold, and let ( ùê± t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ‚àà [ 0 , italic_T ] end_POSTSUBSCRIPT be the stochastic process"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS3.SSSx5", "title": "Derivative Computations", "snippet": "Derivative Computations Here we calculate some useful derivatives which will be reused throughout the appendix. Proposition B.1 . Let ùê± \\bm{x} bold_italic_x be any random variable such that Assumptions B.1 and B.2 hold, and let ( ùê± t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ‚àà [ 0 , italic_T ] end_POSTSUBSCRIPT be the stochastic process ( B.2.1 ). For t ‚â• 0 t\\geq 0 italic_t ‚â• 0 , let p t p_{t} italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT be the density of ùê± t \\bm{x}_{t} bold_italic_x start_P"}, {"page": "Appendix B Entropy, Diffusion, Denoising, and Lossy Coding", "href": "A2.html#S2.SS3.SSSx6", "title": "Differentiating Under the Integral Sign", "snippet": "Differentiating Under the Integral Sign In this appendix, we differentiate under the integral sign many times, and it is important to know when we can do this. There are two kinds of differentiating under the integral sign: 1. Differentiating an integral ‚à´ f t ‚Äã ( ùùÉ ) ‚Äã d ùùÉ \\int f_{t}(\\bm{\\xi})\\mathrm{d}\\bm{\\xi} ‚à´ italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_Œæ ) roman_d bold_italic_Œæ with respect to the auxiliary parameter t t italic_t . 2. Differentiating a convolution ( f ‚àó g ) ‚Äã ( ùùÉ ) = ‚à´ f ‚Äã ( ùùÉ ) ‚Äã g ‚Äã ( ùùÉ ‚àí ùíñ ) ‚Äã d u (f*g)(\\bm{\\xi})=\\int f(\\bm{\\xi})g(\\bm{\\xi}-\\bm"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#top", "title": "Chapter 1 Introduction", "snippet": ""}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S1", "title": "1.1 Intelligence, Cybernetics, and Artificial Intelligence", "snippet": "1.1 Intelligence, Cybernetics, and Artificial Intelligence The world in which we are living is neither fully random nor completely unpredictable. 1 1 1 Note there is no need for an intelligent being to learn or memorize anything if the world is fully random. Instead, it follows certain orders, patterns, and laws that make it largely predictable. 2 2 2 Some deterministic and some probabilistic. The very emergence and existence of life depend on a predictable living environment. Only by learning and memorizing what is predictable in the environment can life survive and thrive since good decision"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2", "title": "1.2 What to Learn?", "snippet": "1.2 What to Learn? 1.2.1 Predictability Data that carry useful information manifest in many different forms. In the most natural form, they can be modeled as sequences that are predictable and computable. The notion and properties of a predictable and computable sequence were at the heart of the theory of computing and very much led to the invention of computers [ Tur36 ] . The role of predictable sequences in (inductive) inference was studied by Ray Solomonoff, Andrey Kolmogorov, and many others in the 1960s [ Kol98 ] as a generalization to Claude Shannon‚Äôs classic Information Theory [ Sha48 "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3", "title": "1.3 How to Learn?", "snippet": "1.3 How to Learn? 1.3.1 Analytical Approaches Note even if a predictive function is tractable to compute, it does not imply it is tractable or scalable to learn this function from a number of sampled segments. Of course, one classical approach to ensure the problems are tractable or amenable to efficient solutions is to make explicit assumptions about the family of low-dimensional structures we are dealing with. Historically, due to limited computation and data, simple and idealistic analytical models were always the first to be studied as they often offer efficient closed-form or numerical so"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4", "title": "1.4 A Unifying Approach", "snippet": "1.4 A Unifying Approach So far, we have given a brief account of the main objective and history of machine intelligence and many important ideas and approaches associated with it. In recent years, after the empirical success of deep neural networks, tremendous efforts have been made to develop theoretical frameworks that can help us understand all the empirically designed deep neural networks, either certain seemingly necessary components (e.g., dropout, normalization, attention, etc.) or their overall behaviors (e.g., double descent, neural collapse, etc.). Partly motivated by this, this book"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S5", "title": "1.5 Bridging Theory and Practice for Machine Intelligence", "snippet": "1.5 Bridging Theory and Practice for Machine Intelligence So far, we have introduced three related frameworks for learning a compact and structured representation ùíÅ \\bm{Z} bold_italic_Z for a given data distribution ùëø \\bm{X} bold_italic_X : ‚Ä¢ The open-ended encoding ( 1.4.10 ); ‚Ä¢ The bi-directional autoencoding ( 1.4.14 ); ‚Ä¢ The closed-loop transcription ( 1.4.16 ). In this book, we will systematically study all three frameworks, one after another: open-ended ‚üπ bi-directional ‚üπ closed-loop , \\mbox{{open-ended}}\\;\\Longrightarrow\\;\\mbox{{bi-directional}}\\;\\Longrightarrow\\;\\mbox{{closed-loop}}, o"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS1", "title": "1.2.1 Predictability", "snippet": "1.2.1 Predictability Data that carry useful information manifest in many different forms. In the most natural form, they can be modeled as sequences that are predictable and computable. The notion and properties of a predictable and computable sequence were at the heart of the theory of computing and very much led to the invention of computers [ Tur36 ] . The role of predictable sequences in (inductive) inference was studied by Ray Solomonoff, Andrey Kolmogorov, and many others in the 1960s [ Kol98 ] as a generalization to Claude Shannon‚Äôs classic Information Theory [ Sha48 ] . To understand t"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS2", "title": "1.2.2 Low Dimensionality", "snippet": "1.2.2 Low Dimensionality Learn to Predict. Now suppose you have observed or are given many sequence segments: { S 1 , S 2 , ‚Ä¶ , S i , ‚Ä¶ , S N } \\{S_{1},S_{2},\\ldots,S_{i},\\ldots,S_{N}\\} { italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ‚Ä¶ , italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , ‚Ä¶ , italic_S start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT } (1.2.19) all from some predictable sequence { x n } n = 1 ‚àû \\{x_{n}\\}_{n=1}^{\\infty} { italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_n = 1 end_POST"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1", "title": "1.3.1 Analytical Approaches", "snippet": "1.3.1 Analytical Approaches Note even if a predictive function is tractable to compute, it does not imply it is tractable or scalable to learn this function from a number of sampled segments. Of course, one classical approach to ensure the problems are tractable or amenable to efficient solutions is to make explicit assumptions about the family of low-dimensional structures we are dealing with. Historically, due to limited computation and data, simple and idealistic analytical models were always the first to be studied as they often offer efficient closed-form or numerical solutions. In additi"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2", "title": "1.3.2 Empirical Approaches", "snippet": "1.3.2 Empirical Approaches In practice, for many important real-world data such as images, sounds, and texts, it is difficult to model them with idealistic linear or mixed linear models. For example, there has been a long and rich history in the fields of image processing and computer vision that tries to model the distribution of natural images analytically. David Mumford, a Fields Medalist, spent considerable effort in the 1990s trying to understand and model the statistics of natural images [ Mum96 ] . He and his students, including Song-Chun Zhu, drew inspiration and techniques from statis"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.SS1", "title": "1.4.1 Learning Parsimonious Representations", "snippet": "1.4.1 Learning Parsimonious Representations One necessary condition for any learning task to be possible is that the sequences of interest must be computable , at least in the sense of Alan Turing [ Tur36 ] . That is, a sequence can be computed via a program on a typical computer. 40 40 40 There are indeed well-defined sequences that are not computable. In addition to being computable, we require computation be tractable . 41 41 41 We do not need to consider predicting things whose computational complexity is intractable, say grows exponentially in the length or dimension of the sequence. That"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.SS2", "title": "1.4.2 Learning Informative Representations", "snippet": "1.4.2 Learning Informative Representations Note that if the goal was simply to compress the given data just for the sake of compression, then in theory the optimal codes that approach the Kolmogorov complexity would become nearly random or structureless [ Cha66 ] . 50 50 50 Because any codes with structures can be further compressed. However, our true purpose of learning the predictive function f f italic_f is to use it repeatedly with ease in future predictions. Hence, while compression allows us to identify the low-dimensional distribution in the data, we would like to encode the distributio"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.SS3", "title": "1.4.3 Learning Consistent Representations", "snippet": "1.4.3 Learning Consistent Representations To summarize our discussions so far, let us denote the data as: ùëø = { S 1 , S 2 , ‚Ä¶ , S i , ‚Ä¶ , S N } ‚äÇ ‚Ñù D , \\bm{X}=\\{S_{1},S_{2},\\ldots,S_{i},\\ldots,S_{N}\\}\\subset\\mathbb{R}^{D}, bold_italic_X = { italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ‚Ä¶ , italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , ‚Ä¶ , italic_S start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT } ‚äÇ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT , (1.4.9) and let ùíÅ = ‚Ñ∞ ‚Äã ( ùëø ) \\bm{Z}=\\mathcal{E}(\\bm{X}) bold_ita"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.SS4", "title": "1.4.4 Learning Self-Consistent Representations", "snippet": "1.4.4 Learning Self-Consistent Representations Note that in the above autoencoding objective, one needs to evaluate how close or consistent the decoded data ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG is to the original ùëø \\bm{X} bold_italic_X . This often requires some external supervision or knowledge on what similarity measure to use. Computing similarity between ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG and ùëø \\bm{X} bold_italic_X can be very expensive, if not entirely impossible or intractable. 55 55 55 Say one wants to minimize certain distributional distance betwee"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S1.SS0.SSS0.Px1", "title": "Emergence and evolution of intelligence.", "snippet": "Emergence and evolution of intelligence. A necessary condition for the emergence of life on earth about 4 billion years ago is that the earth‚Äôs environment is largely predictable. In the environment, life has developed mechanisms that allow it to learn what is predictable about the environment, encode the information in a certain way, and use it for survival. Generally speaking, we call the ability to learn intelligence . To a large extent, the evolution of life is the mechanism of intelligence at work. In nature, intelligence is mainly developed through two types of learning mechanisms: phylo"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S1.SS0.SSS0.Px2", "title": "Evolution of human intelligence.", "snippet": "Evolution of human intelligence. Since the emergence of homo sapiens about 315 thousand years ago, a new and higher form of intelligence emerged which evolves more efficiently and economically. Languages, first spoken 5 5 5 It was believed that Sanskrit was the first spoken language, dated as back as 5000 BC. and then written 6 6 6 Sumerian language is believed to be one of the oldest written language in existence, first attested about 3100 BC in southern Mesopotamia. , were developed a few thousand years ago. See Figure 1.3 . This allows individuals to communicate and share useful information"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S1.SS0.SSS0.Px3", "title": "Origin of machine intelligence ‚Äì Cybernetics.", "snippet": "Origin of machine intelligence ‚Äì Cybernetics. In 1940s, partly due to the war effort, intelligence in nature had inspired scientists in the 1940s to emulate animal intelligence by machines, which led to the ‚ÄúCybernetics‚Äù movement advocated by Norbert Wiener. Wiener studied zoology at Harvard as an undergraduate but later became a mathematician and control theorist. Wiener had a life long passion in understanding and developing autonomous systems that could emulate intelligent behaviors of animals. Today, the Cybernetics program is often narrowly interpreted by people as mainly about feedback c"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S1.SS0.SSS0.Px4", "title": "Origin of Artificial Intelligence.", "snippet": "Origin of Artificial Intelligence. From the subtitle of Wiener‚Äôs Cybernetics book: ‚ÄúControl and Communication in the Animal and the Machine‚Äù , one can tell that the studies in the 1940s mainly aimed to emulate intelligence at the level of animals. As we mentioned before, the research agendas about intelligence around the 1940s were very much dominated by Norbert Wiener‚Äôs Cybernetics movement. Alan Turing was one of the first to notice this limitation. In his famous 1950 paper ‚Äú Computing Machinery and Intelligence ‚Äù [ Tur50 ] , Turing formally posted the question whether machines can imitate i"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S1.SS0.SSS0.Px5", "title": "The renaissance of ‚ÄúArtificial Intelligence‚Äù or ‚ÄúCybernetics‚Äù?", "snippet": "The renaissance of ‚ÄúArtificial Intelligence‚Äù or ‚ÄúCybernetics‚Äù? As the readers may have known, in the past decade or so, machine intelligence has undergone explosive development, powered mainly by the practice of deep artificial neural networks, triggered by the work of Geoffrey Hinton and students in 2012 [ KSH12 ] . This era is also hailed as the ‚ÄúRenaissance‚Äù of Artificial Intelligence (AI). However, in terms of tasks that people have actually tried to tackle (recognition, generation, and prediction) and techniques that people have developed and implemented so far (reinforcing, imitating, en"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS1.SSS0.Px1", "title": "Scalar Case.", "snippet": "Scalar Case. The simplest predictable discrete sequence is arguably the sequence of natural numbers: S = 1 , 2 , 3 , 4 , 5 , 6 , ‚Ä¶ , n , n + 1 , ‚Ä¶ {S}=1,2,3,4,5,6,\\ldots,n,n+1,\\ldots italic_S = 1 , 2 , 3 , 4 , 5 , 6 , ‚Ä¶ , italic_n , italic_n + 1 , ‚Ä¶ (1.2.1) in which the next number x n + 1 x_{n+1} italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT is defined to be its previous number x n x_{n} italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT plus 1: x n + 1 = x n + 1 . x_{n+1}=x_{n}+1. italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT it"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS1.SSS0.Px2", "title": "Multi-Variable Case.", "snippet": "Multi-Variable Case. Of course, the value of the next number can also depend on two of its predecessors. For example, the famous Fibonacci sequence is defined to be: S = 1 , 1 , 2 , 3 , 5 , 8 , 13 , 21 , 34 , 55 , ‚Ä¶ {S}=1,1,2,3,5,8,13,21,34,55,\\ldots italic_S = 1 , 1 , 2 , 3 , 5 , 8 , 13 , 21 , 34 , 55 , ‚Ä¶ (1.2.4) where one can easily see: x n + 2 = x n + 1 + x n , x n ‚àà ‚Ñù , n = 1 , 2 , 3 , ‚Ä¶ x_{n+2}=x_{n+1}+x_{n},\\quad x_{n}\\in\\mathbb{R},\\;n=1,2,3,\\ldots italic_x start_POSTSUBSCRIPT italic_n + 2 end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT + italic_x start_P"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS1.SSS0.Px3", "title": "Vector Case.", "snippet": "Vector Case. To simplify the notation, we may define a vector ùíô ‚àà ‚Ñù d \\bm{x}\\in\\mathbb{R}^{d} bold_italic_x ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT that collects d d italic_d consecutive values in the sequence ùíô n ‚âê [ x n + d ‚àí 1 , ‚Ä¶ , x n ] ‚ä§ , ùíô n ‚àà ‚Ñù d , n = 1 , 2 , 3 , ‚Ä¶ \\bm{x}_{n}\\doteq[x_{n+d-1},\\ldots,x_{n}]^{\\top},\\quad\\bm{x}_{n}\\in\\mathbb{R}^{d},\\;n=1,2,3,\\ldots bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ‚âê [ italic_x start_POSTSUBSCRIPT italic_n + italic_d - 1 end_POSTSUBSCRIPT , ‚Ä¶ , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ] "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS1.SSS0.Px4", "title": "Controlled Prediction.", "snippet": "Controlled Prediction. We may also define a predictable sequence that depends on another predictable sequence as input: ùíô n + 1 = f ‚Äã ( ùíô n , ùíñ n ) ‚àà ‚Ñù d , n = 1 , 2 , 3 , ‚Ä¶ , \\bm{x}_{n+1}=f(\\bm{x}_{n},\\bm{u}_{n})\\;\\in\\mathbb{R}^{d},\\quad n=1,2,3,\\ldots, bold_italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT = italic_f ( bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , bold_italic_u start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , italic_n = 1 , 2 , 3 , ‚Ä¶ , (1.2.10) where { ùíñ n } \\{\\bm{u}_{n}\\} { bold_i"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS1.SSS0.Px5", "title": "Continuous Processes.", "snippet": "Continuous Processes. Predictable sequences have their natural counterparts in the continuous case. We may refer to them as predictable processes. Similar to the sequence of natural numbers, the simplest predictable continuous process is time itself x = t x=t italic_x = italic_t . More generally, we say a process, denoted by ùíô ‚Äã ( t ) \\bm{x}(t) bold_italic_x ( italic_t ) , is predictable if at any time t t italic_t , the value of the process at t + Œ¥ ‚Äã t t+\\delta t italic_t + italic_Œ¥ italic_t , where Œ¥ ‚Äã t \\delta t italic_Œ¥ italic_t is an infinitesimal increment, is determined by its value at"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS2.SSS0.Px1", "title": "Learn to Predict.", "snippet": "Learn to Predict. Now suppose you have observed or are given many sequence segments: { S 1 , S 2 , ‚Ä¶ , S i , ‚Ä¶ , S N } \\{S_{1},S_{2},\\ldots,S_{i},\\ldots,S_{N}\\} { italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ‚Ä¶ , italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , ‚Ä¶ , italic_S start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT } (1.2.19) all from some predictable sequence { x n } n = 1 ‚àû \\{x_{n}\\}_{n=1}^{\\infty} { italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_n = 1 end_POSTSUBSCRIPT start_POSTSUPER"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS2.SSS0.Px2", "title": "Predictability and Low-Dimensionality.", "snippet": "Predictability and Low-Dimensionality. To identify the predictive function f f italic_f , we may notice a common characteristic of segments of any predictable sequence, say given by ( 1.2.21 ). If we take a long segment, say with a length D ‚â´ d D\\gg d italic_D ‚â´ italic_d , of the sequence and view it as a vector: ùíô i = [ x i , x i + 1 , ‚Ä¶ ‚Äã x i + D ‚àí 1 ] ‚ä§ ‚àà ‚Ñù D . \\bm{x}_{i}=[x_{i},x_{i+1},\\ldots x_{i+D-1}]^{\\top}\\in\\mathbb{R}^{D}. bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = [ italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_i + 1"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS2.SSS0.Px3", "title": "Properties of Low-Dimensionality.", "snippet": "Properties of Low-Dimensionality. Of course, temporal correlation in predictable sequences is not the only reason why data are low-dimensional. For example, the space of all images is humongous but most of the space consists of images that resemble structureless random images as shown in Figure 1.8 left. Natural images and videos however are highly redundant because there is a strong spatial and temporal correlation among all pixel values. This is the reason why we can easily recognize whether an image is noisy or clean, as shown in Figure 1.8 middle and right. Hence the distribution of natura"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx1.Px1", "title": "Wiener Filter.", "snippet": "Wiener Filter. As we have discussed before in Section 1.2.1 , a main task of intelligence is to learn what is predictable in sequences of observations. Probably the simplest class of predictable sequences, or signals, are generated via a linear time-invariant (LTI) process: x ‚Äã [ n ] = h ‚Äã [ n ] ‚àó z ‚Äã [ n ] + œµ ‚Äã [ n ] , x[n]=h[n]*z[n]+\\epsilon[n], italic_x [ italic_n ] = italic_h [ italic_n ] ‚àó italic_z [ italic_n ] + italic_œµ [ italic_n ] , (1.3.1) where z z italic_z is the input and h h italic_h is the impulse response function. 14 14 14 Normally h h italic_h is assumed to have certain nice"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx1.Px2", "title": "Kalman Filter.", "snippet": "Kalman Filter. The idea of denoising or filtering for a dynamic process was later extended to a linear time-invariant system described by a (finite-dimensional) state-space model by Rudolph Kalman in the 1960s: ùíõ ‚Äã [ n ] = ùë® ‚Äã ùíõ ‚Äã [ n ‚àí 1 ] + ùë© ‚Äã ùíñ ‚Äã [ n ] + œµ ‚Äã [ n ] . \\bm{z}[n]=\\bm{A}\\bm{z}[n-1]+\\bm{B}\\bm{u}[n]+\\bm{\\epsilon}[n]. bold_italic_z [ italic_n ] = bold_italic_A bold_italic_z [ italic_n - 1 ] + bold_italic_B bold_italic_u [ italic_n ] + bold_italic_œµ [ italic_n ] . (1.3.3) The problem is how we can estimate the state of the system ùíõ ‚Äã [ n ] \\bm{z}[n] bold_italic_z [ italic_n ] from "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx1.Px3", "title": "Identification of Linear Dynamical Systems.", "snippet": "Identification of Linear Dynamical Systems. To derive the Kalman filter, the system parameters ( ùë® , ùë© , ùë™ ) (\\bm{A},\\bm{B},\\bm{C}) ( bold_italic_A , bold_italic_B , bold_italic_C ) are assumed to be known. If they are not given in advance, it would be a more challenging problem known as system identification : how to learn the parameters ( ùë® , ùë© , ùë™ ) (\\bm{A},\\bm{B},\\bm{C}) ( bold_italic_A , bold_italic_B , bold_italic_C ) from (many samples of) the input sequence { ùíñ ‚Äã [ n ] } \\{\\bm{u}[n]\\} { bold_italic_u [ italic_n ] } and observation sequence { ùíô ‚Äã [ n ] } \\{\\bm{x}[n]\\} { bold_italic_x [ "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx2.Px1", "title": "Principal Component Analysis.", "snippet": "Principal Component Analysis. From the above problems in classical signal processing and system identification, we see that the main task behind of all these problems is to learn from noisy observations a single low-dimensional linear subspace. Mathematically, we may model such a structure as: ùíô = ùíñ 1 ‚Äã z 1 + ùíñ 2 ‚Äã z 2 + ‚ãØ + ùíñ d ‚Äã z d + œµ = ùëº ‚Äã ùíõ + œµ , ùëº ‚àà ‚Ñù D √ó d \\bm{x}=\\bm{u}_{1}z_{1}+\\bm{u}_{2}z_{2}+\\cdots+\\bm{u}_{d}z_{d}+\\bm{\\epsilon}=\\bm{U}\\bm{z}+\\bm{\\epsilon},\\quad\\bm{U}\\in\\mathbb{R}^{D\\times d} bold_italic_x = bold_italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_z start_POSTSUBS"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx2.Px2", "title": "Independent Component Analysis.", "snippet": "Independent Component Analysis. Independent component analysis (ICA) was originally proposed by [ BJC85 ] as a classic model for learning a good representation . In fact it was originally proposed as a simple mathematical model for our memory. The ICA model takes a deceivingly similar form as the above PCA model ( 1.3.7 ) by assuming that the observed random variable ùíô \\bm{x} bold_italic_x is a linear superposition of multiple independent components z i z_{i} italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT : ùíô = ùíñ 1 ‚Äã z 1 + ùíñ 2 ‚Äã z 2 + ‚ãØ + ùíñ d ‚Äã z d + œµ = ùëº ‚Äã ùíõ + œµ . \\bm{x}=\\bm{u}_{1}z"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx2.Px3", "title": "Sparse Structures and Compressive Sensing.", "snippet": "Sparse Structures and Compressive Sensing. As one may see, if p p italic_p in ( 1.3.13 ) is very small, the probability that any of the components is non-zero is small. In this case, we say ùíô \\bm{x} bold_italic_x is sparsely generated and it concentrates on a set of linear subspaces of dimension k = p ‚ãÖ d k=p\\cdot d italic_k = italic_p ‚ãÖ italic_d . Hence, to some extent, we may extend the above ICA model to a more general family of low-dimensional structures known as sparse models. A k k italic_k -sparse model is defined to be consisting of the set of all k k italic_k -sparse vectors : ùíµ = { ùíõ"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx2.Px4", "title": "Dictionary Learning.", "snippet": "Dictionary Learning. Conceptually, an even harder problem than the sparse coding problem ( 1.3.16 ) is when the observation matrix ùë® \\bm{A} bold_italic_A is not even known in advance and we need to learn ùë® \\bm{A} bold_italic_A from a set of (possibly noisy) observations, say ùëø = [ ùíô 1 , ùíô 2 , ‚Ä¶ , ùíô n ] \\bm{X}=[\\bm{x}_{1},\\bm{x}_{2},\\ldots,\\bm{x}_{n}] bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ] : ùëø = ùë® ‚Äã ùíÅ + ùë¨ , ùë® ‚àà ‚Ñù m √ó n . \\bm{X}=\\bm{A}\\bm{Z"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx3.Px1", "title": "Denoising.", "snippet": "Denoising. In the 1950s, statisticians became interested in the problem of denoising data drawn from an arbitrary distribution. Let ùíô o \\bm{x}_{o} bold_italic_x start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT be a random variable with probability density function p o ‚Äã ( ‚ãÖ ) p_{o}(\\cdot) italic_p start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT ( ‚ãÖ ) . So if we observe a noisy version of ùíô o \\bm{x}_{o} bold_italic_x start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT : ùíô = ùíô o + œÉ ‚Äã ùíà , \\bm{x}=\\bm{x}_{o}+\\sigma\\bm{g}, bold_italic_x = bold_italic_x start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT + itali"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx3.Px2", "title": "Entropy minimization.", "snippet": "Entropy minimization. In fact, this function has a very intuitive information-theoretic and geometric interpretation. Note that in information theory ‚àí log ‚Å° p ‚Äã ( ùíô ) -\\log p(\\bm{x}) - roman_log italic_p ( bold_italic_x ) generally corresponds to the number of bits needed to encode ùíô \\bm{x} bold_italic_x 23 23 23 at least in the case of a discrete variable, as we will explain in more details in Chapter 3 . . The gradient ‚àá log ‚Å° p ‚Äã ( ùíô ) \\nabla\\log p(\\bm{x}) ‚àá roman_log italic_p ( bold_italic_x ) points to a direction in which the density is higher, as shown in Figure 1.12 left. The number o"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx1.Px1", "title": "Artificial neuron.", "snippet": "Artificial neuron. Figure 1.13 : The first mathematical model of an artificial neuron (right) that emulates how a neuron (left) processes signals. Inspired by the nerve system in the brain, the mathematical model of the first artificial neuron 25 25 25 known as the Linear Threshold Unit, or a perceptron. was proposed by Warren McCulloch 26 26 26 A professor of psychiatry at the University of Chicago at the time and Walter Pitts in 1943 [ MP43 ] . It describes the relationship between the input x i x_{i} italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and output o j o_{j} italic_o start"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx1.Px2", "title": "Artificial neural network.", "snippet": "Artificial neural network. In the 1950s, Frank Rosenblatt was the first to build a machine with a network of such artificial neurons, shown in Figure 1.15 . The machine is called Mark I Perceptron which consists of an input layer, an output layer, and a single hidden layer consisting of 512 artificial neurons, as shown in Figure 1.15 left, which is similar to what is illustrated in Figure 1.14 left. It was designed to classify optical images of letters. However, the capacity of a single-layer network is limited and can only learn linearly separable patterns. In a 1969 book Perceptrons: An Intr"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx1.Px3", "title": "Convolutional neural networks.", "snippet": "Convolutional neural networks. The somewhat disappointing early experimentation with artificial neural networks like Mark I Perceptron in the 50s and 60s suggested that it might not be enough to simply connect neurons in a general fashion as multi-layer perceptrons (MLP). In order to build effective and efficient networks, we need to understand what purpose or function neurons in a network need to achieve collectively so that they should be organized and learned in a certain special way. Once again, the study of machine intelligence turned to draw inspiration from how the animal‚Äôs nerve system"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx1.Px4", "title": "Backpropagation.", "snippet": "Backpropagation. In history, the fate of deep neural networks seems to be tied closely to how they can be trained easily and efficiently. Back propagation (BP) was introduced for this reason. We know that a multiple layer perceptron can be expressed as a composition of a sequence of linear mappings and nonlinear activations as follows: h ‚Äã ( ùëæ 1 , ‚Ä¶ , ùëæ L ) = f L ‚Äã ( ùëæ L ‚Äã f L ‚àí 1 ‚Äã ( ùëæ L ‚àí 1 ‚Äã ‚ãØ ‚Äã f 2 ‚Äã ( ùëæ 2 ‚Äã f 1 ‚Äã ( ùëæ 1 ‚Äã ùíô ) ) ) ) . h(\\bm{W}_{1},\\ldots,\\bm{W}_{L})=f^{L}(\\bm{W}_{L}f^{L-1}(\\bm{W}_{L-1}\\cdots f^{2}(\\bm{W}_{2}f^{1}(\\bm{W}_{1}\\bm{x})))). italic_h ( bold_italic_W start_POSTSUBS"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx1.Px5", "title": "Compressive autoencoding.", "snippet": "Compressive autoencoding. In the late 1980s and 1990s, artificial neural networks were already adopted to learn low-dimensional representations of high-dimensional data such as images. It had been shown that neural networks can be used to learn PCA from the data [ Oja82 , BH89 ] , instead of using the classic methods discussed in Section 1.3.1 . It was also argued during late 1980s that due to its capability to model nonlinear transforms, neural networks were suggested to learn low-dimensional representations for data with nonlinear distributions. Similar to the linear PCA case, one can try to"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx2.Px1", "title": "Classification and recognition.", "snippet": "Classification and recognition. As it turns out, the tremendous potential of deep neural networks could only be unleashed once there are enough data and computing power. Fast forward to 2010s, much larger datasets such as ImageNet became available, and GPUs became powerful enough to make BP much more affordable, even for networks much larger than LeNet. Around 2012, a deep convolutional neural network known as the AlexNet drew attention as it surpassed extent classification methods by a significant margin with the ImageNet dataset [ KSH12 ] . 34 34 34 In fact, before this, deep networks had de"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx2.Px2", "title": "Reinforcement learning.", "snippet": "Reinforcement learning. The early successes of deep networks were mainly for classification tasks in a supervised learning setting, such as speech recognition and image recognition. Deep networks were later adopted by the DeepMind team, led by Demis Hassabis, to learn decision-making or control policies for playing games. In this context, deep networks are used to model the optimal decision/control policy or the associated optimal value function, as shown in Figure 1.19 . These network parameters are incrementally optimized 35 35 35 Say based on back propagation (BP). based on reward returned "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx2.Px3", "title": "Generation and prediction.", "snippet": "Generation and prediction. One may view early practices of deep networks in the 2010s focused more on extracting relevant information from the data ùëø \\bm{X} bold_italic_X and encoding it for certain task-specific representation ùíÅ \\bm{Z} bold_italic_Z (say ùíÅ \\bm{Z} bold_italic_Z represent class labels in classification tasks): ùëø ‚Üí ùëì ùíÅ . \\bm{X}\\xrightarrow{\\hskip 5.69054ptf\\hskip 5.69054pt}\\bm{Z}. bold_italic_X start_ARROW start_OVERACCENT italic_f end_OVERACCENT ‚Üí end_ARROW bold_italic_Z . (1.3.32) In this setting, the mapping f f italic_f to be learned does not need to preserve most distributi"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx2.Px4", "title": "Generation via discriminative approaches.", "snippet": "Generation via discriminative approaches. In order for the generated images ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG to be similar to the true natural images ùëø \\bm{X} bold_italic_X , we need to be able to evaluate and minimize some distance: min ‚Å° d ‚Äã ( ùëø , ùëø ^ ) . \\min d(\\bm{X},\\hat{\\bm{X}}). roman_min italic_d ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG ) . (1.3.34) As it turns out, most theoretically motivated distances are extremely difficult, if not impossible, to compute and optimize for distributions in high-dimensional space but with a low intrinsic dimension."}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx2.Px5", "title": "Generation via denoising and diffusion.", "snippet": "Generation via denoising and diffusion. In 2015, shortly after GAN was introduced and became popular, Surya Ganguli and his students realized and suggested that an iterative denoising process modeled by a deep network can be used to learn a general distribution, such as that of natural images [ SWM+15 ] . Their method was inspired by properties of the special Gaussian and binomial processes, studied by William Feller back in 1949 [ Fel49 ] . 39 39 39 Again, in the magical era of 1940s! Figure 1.20 : Illustration of an iterative denoising and compressing process that, starting from a Gaussian d"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.SS1.SSS0.Px1", "title": "Pursuing low-dimensionality via compression.", "snippet": "Pursuing low-dimensionality via compression. From the examples of sequences we gave in Section 1.2.1 , it is clear that some sequences are easy to model and compute and others are more difficult. Obviously, the computational cost of a sequence depends on how complex the predicting function f f italic_f is. The higher the degree of regression d d italic_d , the more costly it is to compute. f f italic_f can be a simple linear function, and it can also be a nonlinear function that can be arbitrarily difficult to specify and compute. It is reasonable to believe that if a sequence is harder, by wh"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.SS1.SSS0.Px2", "title": "Computable measure of parsimony.", "snippet": "Computable measure of parsimony. Hence for practical purposes, we need an efficiently computable measure of complexity for sequences that are generated from the same predicting function. 45 45 45 Note that in practice, we typically care about learning the predicting function f f italic_f , instead of any particular sequence generated by f f italic_f . Note that part of the reason why Kolmogorov complexity is not computable is because its definition is non-constructive. So to introduce a computable measure of complexity, we may take a more constructive approach, as advocated by Claude Shannon t"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.SS3.SSS0.Px1", "title": "Bidirectional Autoencoding for Consistency.", "snippet": "Bidirectional Autoencoding for Consistency. In a broader learning context, the main goal of a compressive coding scheme ‚Ñ∞ \\mathcal{E} caligraphic_E is to identify the low-dimensional structures in the data ùëø \\bm{X} bold_italic_X so that they can be used to predict things in the original data space. This requires that the learned encoding scheme ‚Ñ∞ \\mathcal{E} caligraphic_E allows an efficient decoding scheme, denoted as ùíü \\mathcal{D} caligraphic_D . It maps ùíÅ \\bm{Z} bold_italic_Z , often known as a latent representation, back to the data space: ùëø ‚Üí ‚Ñ∞ ùíÅ ‚Üí ùíü ùëø ^ . \\bm{X}\\xrightarrow{\\hskip 5.6905"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S5.SS0.SSS0.Px1", "title": "Back to Intelligence.", "snippet": "Back to Intelligence. As we have mentioned in the beginning, a common and fundamental task of any intelligent being is to learn predictable information from its sensed data. Now we have understood a little about the computational nature of this task, and one should realize that this is a never-ending process, for the following reasons: ‚Ä¢ The knowledge learned so far from the data, say by the encoding and decoding schemes, is unlikely to be correct or optimal. Intelligence should have the ability to improve if there are still errors in predicting new observations. ‚Ä¢ The data observed so far do "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#p1", "title": "‚Äú Just as the constant increase of entropy is the basic law of the universe, so it is the basic law of life to be ever more highly structured and to struggle against entropy. ‚Äù ‚Äì V√°clav Havel", "snippet": "‚Äú Just as the constant increase of entropy is the basic law of the universe, so it is the basic law of life to be ever more highly structured and to struggle against entropy. ‚Äù ‚Äì V√°clav Havel"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S1.p1", "title": "The world in which we are living is neither fully random nor completely unpredictable. 1 1 1 Note there is no need for an intelligent being to learn or memorize anything if the world is fully random. ", "snippet": "The world in which we are living is neither fully random nor completely unpredictable. 1 1 1 Note there is no need for an intelligent being to learn or memorize anything if the world is fully random. Instead, it follows certain orders, patterns, and laws that make it largely predictable. 2 2 2 Some deterministic and some probabilistic. The very emergence and existence of life depend on a predictable living environment. Only by learning and memorizing what is predictable in the environment can life survive and thrive since good decisions and actions depend on reliable predictions. Because there"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S1.I1.i1.p1", "title": "How to model and represent such predictable information in the data mathematically?", "snippet": "How to model and represent such predictable information in the data mathematically?"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S1.I1.i2.p1", "title": "How can such information be learned effectively and efficiently from the data computationally?", "snippet": "How can such information be learned effectively and efficiently from the data computationally?"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S1.I1.i3.p1", "title": "How should such information be best organized for future prediction and inference?", "snippet": "How should such information be best organized for future prediction and inference?"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S1.SS0.SSS0.Px1.p1", "title": "A necessary condition for the emergence of life on earth about 4 billion years ago is that the earth‚Äôs environment is largely predictable. In the environment, life has developed mechanisms that allow ", "snippet": "A necessary condition for the emergence of life on earth about 4 billion years ago is that the earth‚Äôs environment is largely predictable. In the environment, life has developed mechanisms that allow it to learn what is predictable about the environment, encode the information in a certain way, and use it for survival. Generally speaking, we call the ability to learn intelligence . To a large extent, the evolution of life is the mechanism of intelligence at work. In nature, intelligence is mainly developed through two types of learning mechanisms: phylogenetic and ontogenetic [ Wie61 ] ."}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S1.SS0.SSS0.Px1.p2", "title": "The phylogenetic intelligence refers to learning through the evolution of species. Species inherit and survive mainly based on knowledge inherited from DNAs or genes of their parents. To a large exten", "snippet": "The phylogenetic intelligence refers to learning through the evolution of species. Species inherit and survive mainly based on knowledge inherited from DNAs or genes of their parents. To a large extent, we may call DNAs nature‚Äôs pre-trained large models because they play a very similar role. The main characteristic of phylogenetic intelligence is that individuals do not have much learning capacity. Learning is carried out with a ‚Äútrial-and-error‚Äù mechanism based on random mutation of the genes, and then species evolve based on the process of natural selection ‚Äì survival of the fittest, as show"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S1.SS0.SSS0.Px1.p3", "title": "The ontogenetic intelligence refers to the learning mechanisms that allow an individual to learn through its own senses, memories, and predictions within its specific living environment and to improve", "snippet": "The ontogenetic intelligence refers to the learning mechanisms that allow an individual to learn through its own senses, memories, and predictions within its specific living environment and to improve and adapt its behaviors. The ontogenetic learning became possible after the emergence of the nervous system about 550 to 600 million years ago (in worm-like organisms), shown in Figure 1.2 middle. That is, with a sensory and nervous system, an individual is capable of continuously forming and improving its own knowledge about the world, also known as a memory, in addition to what is inherited fro"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S1.SS0.SSS0.Px1.p4", "title": "Notice that both types of learning mechanisms rely on some form of feedback (from the external environment), in terms of a penalty (death) or reward (food), of a species or individuals‚Äô actions 4 4 4 ", "snippet": "Notice that both types of learning mechanisms rely on some form of feedback (from the external environment), in terms of a penalty (death) or reward (food), of a species or individuals‚Äô actions 4 4 4 Gene mutation of the species or actions made by the individual. to learn. As a result, all intelligent beings, as species or as individuals, rely on a closed-loop feedback mechanism to learn and improve their knowledge about the world. We also notice that from plants, to fish, to birds, and to mammals, more advanced species rely more and more on their ontogenetic learning capabilities. They stay w"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S1.SS0.SSS0.Px2.p1", "title": "Since the emergence of homo sapiens about 315 thousand years ago, a new and higher form of intelligence emerged which evolves more efficiently and economically. Languages, first spoken 5 5 5 It was be", "snippet": "Since the emergence of homo sapiens about 315 thousand years ago, a new and higher form of intelligence emerged which evolves more efficiently and economically. Languages, first spoken 5 5 5 It was believed that Sanskrit was the first spoken language, dated as back as 5000 BC. and then written 6 6 6 Sumerian language is believed to be one of the oldest written language in existence, first attested about 3100 BC in southern Mesopotamia. , were developed a few thousand years ago. See Figure 1.3 . This allows individuals to communicate and share useful information with others. Therefore, a human "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S1.SS0.SSS0.Px2.p2", "title": "Quite miraculously, about a few thousand years ago, another quantum leap in human intelligence occurred, which allowed philosophers and mathematicians to develop knowledge that seem to go way beyond d", "snippet": "Quite miraculously, about a few thousand years ago, another quantum leap in human intelligence occurred, which allowed philosophers and mathematicians to develop knowledge that seem to go way beyond developing empirical knowledge. The development of abstract mathematical concepts and symbols, such as numbers, space and time, as well as mathematical logic, serve as a new precise language for modern science. In addition, the development of the ability to generate new hypotheses and verify their correctness based on logic deduction or scientific experimentation. This, for the first time, has enab"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S1.SS0.SSS0.Px2.p3", "title": "Hence, from what we can learn from the nature, from now on, whenever we use the word ‚Äúintelligence,‚Äù we need to be very specific about which level/form of intelligence we mean: phylogentic ‚üπ ontogenet", "snippet": "Hence, from what we can learn from the nature, from now on, whenever we use the word ‚Äúintelligence,‚Äù we need to be very specific about which level/form of intelligence we mean: phylogentic ‚üπ ontogenetic ‚üπ societal ‚üπ artificial intelligence . \\mbox{{phylogentic}}\\;\\Longrightarrow\\;\\mbox{{ontogenetic}}\\;\\Longrightarrow\\;\\mbox{{societal}}\\;\\Longrightarrow\\;\\mbox{{artificial intelligence}}. phylogentic ‚üπ ontogenetic ‚üπ societal ‚üπ artificial intelligence . (1.1.1) A clear characterization and distinction are necessary and important because we want to study intelligence as a scientific and mathematic"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S1.SS0.SSS0.Px3.p1", "title": "In 1940s, partly due to the war effort, intelligence in nature had inspired scientists in the 1940s to emulate animal intelligence by machines, which led to the ‚ÄúCybernetics‚Äù movement advocated by Nor", "snippet": "In 1940s, partly due to the war effort, intelligence in nature had inspired scientists in the 1940s to emulate animal intelligence by machines, which led to the ‚ÄúCybernetics‚Äù movement advocated by Norbert Wiener. Wiener studied zoology at Harvard as an undergraduate but later became a mathematician and control theorist. Wiener had a life long passion in understanding and developing autonomous systems that could emulate intelligent behaviors of animals. Today, the Cybernetics program is often narrowly interpreted by people as mainly about feedback control systems for which Wiener indeed made hi"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S1.SS0.SSS0.Px3.p2", "title": "Wiener was arguably the first person who studied intelligence as a system , instead of paying attention to only one component or aspect of it. His comprehensive views on intelligence were elaborated i", "snippet": "Wiener was arguably the first person who studied intelligence as a system , instead of paying attention to only one component or aspect of it. His comprehensive views on intelligence were elaborated in his famous 1948 book ‚ÄúCybernetics: or Control and Communication in the Animal and the Machine‚Äù [ Wie48 ] . In this book and its second edition published in 1961 [ Wie61 ] (see Figure 1.4 ), he tried to identify several necessary characteristics and mechanisms of intelligent systems, which include (but are not limited to): ‚Ä¢ How to measure and store information (in the brain) and how to communica"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S1.I2.i1.p1", "title": "How to measure and store information (in the brain) and how to communicate with others. 8 8 8 Norbert Wiener was the first to point out ‚Äúinformation‚Äù is not matter nor energy, but an independent quant", "snippet": "How to measure and store information (in the brain) and how to communicate with others. 8 8 8 Norbert Wiener was the first to point out ‚Äúinformation‚Äù is not matter nor energy, but an independent quantity for study. This led to the formulation of information theory and coding theory by Claude Shannon in 1948."}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S1.I2.i2.p1", "title": "How to correct errors in prediction and estimation based on existing information. Norbert Wiener himself helped formalize the theory for control systems based on closed-loop feedback in the 1940s.", "snippet": "How to correct errors in prediction and estimation based on existing information. Norbert Wiener himself helped formalize the theory for control systems based on closed-loop feedback in the 1940s."}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S1.I2.i3.p1", "title": "How to learn to make better decisions from interacting with a potentially non-cooperative opponent or adversarial environment. This was formalized by John von Neumann as game theory in 1944.", "snippet": "How to learn to make better decisions from interacting with a potentially non-cooperative opponent or adversarial environment. This was formalized by John von Neumann as game theory in 1944."}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S1.SS0.SSS0.Px3.p3", "title": "Acute readers probably have noticed that the 1940s was truly a magical era: So many fundamental ideas were invented and influential theories formalized in that era, including the mathematical model of", "snippet": "Acute readers probably have noticed that the 1940s was truly a magical era: So many fundamental ideas were invented and influential theories formalized in that era, including the mathematical model of neurons, artificial neural networks, information theory, control theory, game theory, and computing machines. Figure 1.5 shows some of the pioneers of these theories. As we now know, each work above had grown to become the foundation of a scientific or engineering field for the following many decades and has tremendous impact on us. All these fundamental theories were inspired and motivated by th"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S1.SS0.SSS0.Px3.p4", "title": "Although Wiener had identified in his work many key characteristics and mechanisms of (embodied) intelligence, there was no indication that he knew how to properly integrate all these mechanisms toget", "snippet": "Although Wiener had identified in his work many key characteristics and mechanisms of (embodied) intelligence, there was no indication that he knew how to properly integrate all these mechanisms together to build a complete autonomous intelligent system. Judging from today‚Äôs knowledge, some of his views on these mechanisms were not entirely accurate or complete. In particular, in the last chapter of the second edition of Cybernetics [ Wie61 ] , he pointed out that it is crucial to deal with nonlinearality if a machine learning system is designed to emulate typical learning mechanisms in nature"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S1.SS0.SSS0.Px3.p5", "title": "Nevertheless, we could not help but marvel at Wiener‚Äôs foresight about the importance of nonlinearity. As we will see in this book, the answer was found only recently: nonlinearity can be effectively ", "snippet": "Nevertheless, we could not help but marvel at Wiener‚Äôs foresight about the importance of nonlinearity. As we will see in this book, the answer was found only recently: nonlinearity can be effectively dealt with through progressive linearization and transformation realized by deep neural networks (see Chapter 4 ). In addition, we will attempt to show in this book how all these mechanisms listed above can be naturally integrated into a complete system which would exhibit characteristics of an autonomous intelligent system (see Chapter 5 )."}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S1.SS0.SSS0.Px4.p1", "title": "From the subtitle of Wiener‚Äôs Cybernetics book: ‚ÄúControl and Communication in the Animal and the Machine‚Äù , one can tell that the studies in the 1940s mainly aimed to emulate intelligence at the level", "snippet": "From the subtitle of Wiener‚Äôs Cybernetics book: ‚ÄúControl and Communication in the Animal and the Machine‚Äù , one can tell that the studies in the 1940s mainly aimed to emulate intelligence at the level of animals. As we mentioned before, the research agendas about intelligence around the 1940s were very much dominated by Norbert Wiener‚Äôs Cybernetics movement."}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S1.SS0.SSS0.Px4.p2", "title": "Alan Turing was one of the first to notice this limitation. In his famous 1950 paper ‚Äú Computing Machinery and Intelligence ‚Äù [ Tur50 ] , Turing formally posted the question whether machines can imita", "snippet": "Alan Turing was one of the first to notice this limitation. In his famous 1950 paper ‚Äú Computing Machinery and Intelligence ‚Äù [ Tur50 ] , Turing formally posted the question whether machines can imitate intelligence even at the human level, to the point of being indistinguishable from the intelligent capabilities of humans. This is now known as the Turing test ."}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S1.SS0.SSS0.Px4.p3", "title": "Around 1955, a group of young and ambitious scientists tried to break away from the then dominating Cybernetics movement and research agendas so that they would have a chance to create their own legac", "snippet": "Around 1955, a group of young and ambitious scientists tried to break away from the then dominating Cybernetics movement and research agendas so that they would have a chance to create their own legacy. They decided to take on Turing‚Äôs challenge of imitating human intelligence and proposed a workshop to be held at Dartmouth College in the summer of 1956. They made their intention clear with a statement in their proposal: ‚Äú The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a m"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S1.SS0.SSS0.Px5.p1", "title": "As the readers may have known, in the past decade or so, machine intelligence has undergone explosive development, powered mainly by the practice of deep artificial neural networks, triggered by the w", "snippet": "As the readers may have known, in the past decade or so, machine intelligence has undergone explosive development, powered mainly by the practice of deep artificial neural networks, triggered by the work of Geoffrey Hinton and students in 2012 [ KSH12 ] . This era is also hailed as the ‚ÄúRenaissance‚Äù of Artificial Intelligence (AI). However, in terms of tasks that people have actually tried to tackle (recognition, generation, and prediction) and techniques that people have developed and implemented so far (reinforcing, imitating, encoding, decoding, denoising, and compression), we are very much"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S1.SS0.SSS0.Px5.p2", "title": "Hence, strictly speaking, the advancement of machine intelligence in the past decade does not align well with the ‚ÄúArtificial Intelligence‚Äù program laid out in the 1956 Dartmouth workshop. Instead, wh", "snippet": "Hence, strictly speaking, the advancement of machine intelligence in the past decade does not align well with the ‚ÄúArtificial Intelligence‚Äù program laid out in the 1956 Dartmouth workshop. Instead, what has been predominantly accomplished so far is more closely related to the objectives of the classic ‚ÄúCybernetics‚Äù program laid out by Norbert Wiener in the 1940s. It is probably more appropriate to call the current era the ‚ÄúRenaissance of Cybernetics‚Äù. 9 9 9 The recent rise of the so-called ‚ÄúEmbodied AI‚Äù for autonomous intelligent robots share even more similarity with the goals of the Cybernet"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS1.p1", "title": "Data that carry useful information manifest in many different forms. In the most natural form, they can be modeled as sequences that are predictable and computable. The notion and properties of a pred", "snippet": "Data that carry useful information manifest in many different forms. In the most natural form, they can be modeled as sequences that are predictable and computable. The notion and properties of a predictable and computable sequence were at the heart of the theory of computing and very much led to the invention of computers [ Tur36 ] . The role of predictable sequences in (inductive) inference was studied by Ray Solomonoff, Andrey Kolmogorov, and many others in the 1960s [ Kol98 ] as a generalization to Claude Shannon‚Äôs classic Information Theory [ Sha48 ] . To understand the concept of predict"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS1.SSS0.Px1.p1", "title": "The simplest predictable discrete sequence is arguably the sequence of natural numbers: S = 1 , 2 , 3 , 4 , 5 , 6 , ‚Ä¶ , n , n + 1 , ‚Ä¶ {S}=1,2,3,4,5,6,\\ldots,n,n+1,\\ldots italic_S = 1 , 2 , 3 , 4 , 5 ,", "snippet": "The simplest predictable discrete sequence is arguably the sequence of natural numbers: S = 1 , 2 , 3 , 4 , 5 , 6 , ‚Ä¶ , n , n + 1 , ‚Ä¶ {S}=1,2,3,4,5,6,\\ldots,n,n+1,\\ldots italic_S = 1 , 2 , 3 , 4 , 5 , 6 , ‚Ä¶ , italic_n , italic_n + 1 , ‚Ä¶ (1.2.1) in which the next number x n + 1 x_{n+1} italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT is defined to be its previous number x n x_{n} italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT plus 1: x n + 1 = x n + 1 . x_{n+1}=x_{n}+1. italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT italic_n end_PO"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS1.SSS0.Px2.p1", "title": "Of course, the value of the next number can also depend on two of its predecessors. For example, the famous Fibonacci sequence is defined to be: S = 1 , 1 , 2 , 3 , 5 , 8 , 13 , 21 , 34 , 55 , ‚Ä¶ {S}=1", "snippet": "Of course, the value of the next number can also depend on two of its predecessors. For example, the famous Fibonacci sequence is defined to be: S = 1 , 1 , 2 , 3 , 5 , 8 , 13 , 21 , 34 , 55 , ‚Ä¶ {S}=1,1,2,3,5,8,13,21,34,55,\\ldots italic_S = 1 , 1 , 2 , 3 , 5 , 8 , 13 , 21 , 34 , 55 , ‚Ä¶ (1.2.4) where one can easily see: x n + 2 = x n + 1 + x n , x n ‚àà ‚Ñù , n = 1 , 2 , 3 , ‚Ä¶ x_{n+2}=x_{n+1}+x_{n},\\quad x_{n}\\in\\mathbb{R},\\;n=1,2,3,\\ldots italic_x start_POSTSUBSCRIPT italic_n + 2 end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT + italic_x start_POSTSUBSCRIPT italic_n"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS1.SSS0.Px3.p1", "title": "To simplify the notation, we may define a vector ùíô ‚àà ‚Ñù d \\bm{x}\\in\\mathbb{R}^{d} bold_italic_x ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT that collects d d italic_d consecutive ", "snippet": "To simplify the notation, we may define a vector ùíô ‚àà ‚Ñù d \\bm{x}\\in\\mathbb{R}^{d} bold_italic_x ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT that collects d d italic_d consecutive values in the sequence ùíô n ‚âê [ x n + d ‚àí 1 , ‚Ä¶ , x n ] ‚ä§ , ùíô n ‚àà ‚Ñù d , n = 1 , 2 , 3 , ‚Ä¶ \\bm{x}_{n}\\doteq[x_{n+d-1},\\ldots,x_{n}]^{\\top},\\quad\\bm{x}_{n}\\in\\mathbb{R}^{d},\\;n=1,2,3,\\ldots bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ‚âê [ italic_x start_POSTSUBSCRIPT italic_n + italic_d - 1 end_POSTSUBSCRIPT , ‚Ä¶ , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ] start_POSTSUP"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS1.SSS0.Px4.p1", "title": "We may also define a predictable sequence that depends on another predictable sequence as input: ùíô n + 1 = f ‚Äã ( ùíô n , ùíñ n ) ‚àà ‚Ñù d , n = 1 , 2 , 3 , ‚Ä¶ , \\bm{x}_{n+1}=f(\\bm{x}_{n},\\bm{u}_{n})\\;\\in\\math", "snippet": "We may also define a predictable sequence that depends on another predictable sequence as input: ùíô n + 1 = f ‚Äã ( ùíô n , ùíñ n ) ‚àà ‚Ñù d , n = 1 , 2 , 3 , ‚Ä¶ , \\bm{x}_{n+1}=f(\\bm{x}_{n},\\bm{u}_{n})\\;\\in\\mathbb{R}^{d},\\quad n=1,2,3,\\ldots, bold_italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT = italic_f ( bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , bold_italic_u start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , italic_n = 1 , 2 , 3 , ‚Ä¶ , (1.2.10) where { ùíñ n } \\{\\bm{u}_{n}\\} { bold_italic_u start_POSTSUBSC"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS1.SSS0.Px4.p2", "title": "Very often the control input is given by a computable function of the state ùíô n \\bm{x}_{n} bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT itself, say: ùíñ n = h ‚Äã ( ùíô n ) , n = 1 , 2 , 3 ,", "snippet": "Very often the control input is given by a computable function of the state ùíô n \\bm{x}_{n} bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT itself, say: ùíñ n = h ‚Äã ( ùíô n ) , n = 1 , 2 , 3 , ‚Ä¶ \\bm{u}_{n}=h(\\bm{x}_{n}),\\quad n=1,2,3,\\ldots bold_italic_u start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT = italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) , italic_n = 1 , 2 , 3 , ‚Ä¶ (1.2.12) As a result, the sequence { ùíô n } \\{\\bm{x}_{n}\\} { bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } is given by composing the two computable functions f f italic_f "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS1.SSS0.Px5.p1", "title": "Predictable sequences have their natural counterparts in the continuous case. We may refer to them as predictable processes. Similar to the sequence of natural numbers, the simplest predictable contin", "snippet": "Predictable sequences have their natural counterparts in the continuous case. We may refer to them as predictable processes. Similar to the sequence of natural numbers, the simplest predictable continuous process is time itself x = t x=t italic_x = italic_t ."}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS1.SSS0.Px5.p2", "title": "More generally, we say a process, denoted by ùíô ‚Äã ( t ) \\bm{x}(t) bold_italic_x ( italic_t ) , is predictable if at any time t t italic_t , the value of the process at t + Œ¥ ‚Äã t t+\\delta t italic_t + i", "snippet": "More generally, we say a process, denoted by ùíô ‚Äã ( t ) \\bm{x}(t) bold_italic_x ( italic_t ) , is predictable if at any time t t italic_t , the value of the process at t + Œ¥ ‚Äã t t+\\delta t italic_t + italic_Œ¥ italic_t , where Œ¥ ‚Äã t \\delta t italic_Œ¥ italic_t is an infinitesimal increment, is determined by its value at t t italic_t . Typically, the change in value Œ¥ ‚Äã ùíô ‚Äã ( t ) \\delta\\bm{x}(t) italic_Œ¥ bold_italic_x ( italic_t ) is continuous and smooth. So Œ¥ ‚Äã ùíô ‚Äã ( t ) = ùíô ‚Äã ( t + Œ¥ ‚Äã t ) ‚àí ùíô ‚Äã ( t ) \\delta\\bm{x}(t)=\\bm{x}(t+\\delta t)-\\bm{x}(t) italic_Œ¥ bold_italic_x ( italic_t ) = bold_italic"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS1.SSS0.Px5.p3", "title": "In the context of systems theory [ CD91 , Sas99 ] , the above equation is also known as a state-space model. Similar to the discrete case, a controlled process can be given by: ùíô Àô ‚Äã ( t ) = f ‚Äã ( ùíô ‚Äã", "snippet": "In the context of systems theory [ CD91 , Sas99 ] , the above equation is also known as a state-space model. Similar to the discrete case, a controlled process can be given by: ùíô Àô ‚Äã ( t ) = f ‚Äã ( ùíô ‚Äã ( t ) , ùíñ ‚Äã ( t ) ) , ùíô ‚àà ‚Ñù d , ùíñ ‚àà ‚Ñù k , \\dot{\\bm{x}}(t)=f(\\bm{x}(t),\\bm{u}(t)),\\quad\\bm{x}\\in\\mathbb{R}^{d},\\bm{u}\\in\\mathbb{R}^{k}, overÀô start_ARG bold_italic_x end_ARG ( italic_t ) = italic_f ( bold_italic_x ( italic_t ) , bold_italic_u ( italic_t ) ) , bold_italic_x ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , bold_italic_u ‚àà blackboard_R start_POSTSUPERSCRIPT italic_"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#Thmexample1.p1", "title": "For example in physics, Newton‚Äôs second law of motion describes how to predict the trajectory ùíô ‚Äã ( t ) ‚àà ‚Ñù 3 \\bm{x}(t)\\in\\mathbb{R}^{3} bold_italic_x ( italic_t ) ‚àà blackboard_R start_POSTSUPERSCRIPT", "snippet": "For example in physics, Newton‚Äôs second law of motion describes how to predict the trajectory ùíô ‚Äã ( t ) ‚àà ‚Ñù 3 \\bm{x}(t)\\in\\mathbb{R}^{3} bold_italic_x ( italic_t ) ‚àà blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT of a moving object under a force input ùë≠ ‚Äã ( t ) ‚àà ‚Ñù 3 \\bm{F}(t)\\in\\mathbb{R}^{3} bold_italic_F ( italic_t ) ‚àà blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT : m ‚Äã ùíô ¬® ‚Äã ( t ) = ùë≠ ‚Äã ( t ) . m\\ddot{\\bm{x}}(t)=\\bm{F}(t). italic_m over¬® start_ARG bold_italic_x end_ARG ( italic_t ) = bold_italic_F ( italic_t ) . (1.2.17) When there is no force ùë≠ ‚Äã ( t ) ‚â° 0 \\bm{F}(t)\\e"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS2.SSS0.Px1.p1", "title": "Now suppose you have observed or are given many sequence segments: { S 1 , S 2 , ‚Ä¶ , S i , ‚Ä¶ , S N } \\{S_{1},S_{2},\\ldots,S_{i},\\ldots,S_{N}\\} { italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ital", "snippet": "Now suppose you have observed or are given many sequence segments: { S 1 , S 2 , ‚Ä¶ , S i , ‚Ä¶ , S N } \\{S_{1},S_{2},\\ldots,S_{i},\\ldots,S_{N}\\} { italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ‚Ä¶ , italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , ‚Ä¶ , italic_S start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT } (1.2.19) all from some predictable sequence { x n } n = 1 ‚àû \\{x_{n}\\}_{n=1}^{\\infty} { italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_n = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚àû end_POSTS"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS2.SSS0.Px1.p2", "title": "One difficulty here is that you normally do not know the function f f italic_f and the degree d d italic_d from which the sequence is generated: x n + d = f ‚Äã ( x n + d ‚àí 1 , ‚Ä¶ , x n ) . x_{n+d}=f(x_{", "snippet": "One difficulty here is that you normally do not know the function f f italic_f and the degree d d italic_d from which the sequence is generated: x n + d = f ‚Äã ( x n + d ‚àí 1 , ‚Ä¶ , x n ) . x_{n+d}=f(x_{n+d-1},\\ldots,x_{n}). italic_x start_POSTSUBSCRIPT italic_n + italic_d end_POSTSUBSCRIPT = italic_f ( italic_x start_POSTSUBSCRIPT italic_n + italic_d - 1 end_POSTSUBSCRIPT , ‚Ä¶ , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) . (1.2.21) So the hope is somehow ‚Äúto learn‚Äù f f italic_f and d d italic_d from the given sample segments S 1 , S 2 , ‚Ä¶ , S N S_{1},S_{2},\\ldots,S_{N} italic_S sta"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS2.SSS0.Px2.p1", "title": "To identify the predictive function f f italic_f , we may notice a common characteristic of segments of any predictable sequence, say given by ( 1.2.21 ). If we take a long segment, say with a length ", "snippet": "To identify the predictive function f f italic_f , we may notice a common characteristic of segments of any predictable sequence, say given by ( 1.2.21 ). If we take a long segment, say with a length D ‚â´ d D\\gg d italic_D ‚â´ italic_d , of the sequence and view it as a vector: ùíô i = [ x i , x i + 1 , ‚Ä¶ ‚Äã x i + D ‚àí 1 ] ‚ä§ ‚àà ‚Ñù D . \\bm{x}_{i}=[x_{i},x_{i+1},\\ldots x_{i+D-1}]^{\\top}\\in\\mathbb{R}^{D}. bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = [ italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT , ‚Ä¶ italic_x start_P"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS2.SSS0.Px2.p2", "title": "In practice, if we choose the segment length D D italic_D to be large enough, then all segments sampled from the same predicting function lie on a surface with an intrinsic dimension d d italic_d , si", "snippet": "In practice, if we choose the segment length D D italic_D to be large enough, then all segments sampled from the same predicting function lie on a surface with an intrinsic dimension d d italic_d , significantly lower than that of the ambient space D D italic_D . For example, if the sequence is given by the following linear autoregression: x n + 2 = a ‚ãÖ x n + 1 + b ‚ãÖ x n , x_{n+2}=a\\cdot x_{n+1}+b\\cdot x_{n}, italic_x start_POSTSUBSCRIPT italic_n + 2 end_POSTSUBSCRIPT = italic_a ‚ãÖ italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT + italic_b ‚ãÖ italic_x start_POSTSUBSCRIPT italic_n end"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS2.SSS0.Px2.p3", "title": "It is easy to see that if the predicting function f f italic_f is linear, such as the case with the linear systems given in ( 1.2.11 ) and ( 1.2.14 ), the long segments always lie on certain low-dimen", "snippet": "It is easy to see that if the predicting function f f italic_f is linear, such as the case with the linear systems given in ( 1.2.11 ) and ( 1.2.14 ), the long segments always lie on certain low-dimensional linear subspace. Identifying the predicting function is largely equivalent to identifying this low-dimensional subspace, a problem generally known as principal component analysis. We will discuss such classic models and methods in Chapter 2 ."}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS2.SSS0.Px2.p4", "title": "As it turns out, this is largely true for general predictable sequences too: if one can identify the low-dimensional surface on which all the segment samples lie, then one can identify the associated ", "snippet": "As it turns out, this is largely true for general predictable sequences too: if one can identify the low-dimensional surface on which all the segment samples lie, then one can identify the associated predictive function f f italic_f . 11 11 11 Under mild conditions, there is a one-to-one mapping between the low-dimensional surface and the function f f italic_f . This fact has been exploited in problems such as system identification which we will discuss later. We cannot over-emphasize the importance of this property of segments from a predictable sequence: All samples of long segments of a pre"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS2.SSS0.Px2.p5", "title": "In real-world scenarios, the data we observe often do not come from a single predictable sequence. Typically they contain observations of multiple predictable sequences. For example, a video sequence ", "snippet": "In real-world scenarios, the data we observe often do not come from a single predictable sequence. Typically they contain observations of multiple predictable sequences. For example, a video sequence can contain multiple moving objects. It is easy to see that in such scenarios, the data lie on a mixture of multiple low-dimensional linear subspaces or nonlinear submanifolds, as illustrated in Figure 1.7 ."}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS2.SSS0.Px3.p1", "title": "Of course, temporal correlation in predictable sequences is not the only reason why data are low-dimensional. For example, the space of all images is humongous but most of the space consists of images", "snippet": "Of course, temporal correlation in predictable sequences is not the only reason why data are low-dimensional. For example, the space of all images is humongous but most of the space consists of images that resemble structureless random images as shown in Figure 1.8 left. Natural images and videos however are highly redundant because there is a strong spatial and temporal correlation among all pixel values. This is the reason why we can easily recognize whether an image is noisy or clean, as shown in Figure 1.8 middle and right. Hence the distribution of natural images has a very low intrinsic "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS2.SSS0.Px3.p2", "title": "Due to the importance and ubiquity of the task of learning low-dimensional structures, the book ‚ÄúHigh-Dimensional Data Analysis with Low-Dimensional Models: Principles, Computation, and Applications‚Äù ", "snippet": "Due to the importance and ubiquity of the task of learning low-dimensional structures, the book ‚ÄúHigh-Dimensional Data Analysis with Low-Dimensional Models: Principles, Computation, and Applications‚Äù [ WM22 ] has stated in the beginning with a statement: ‚Äú The problem of identifying the low-dimensional structure of signals or data in high-dimensional spaces is one of the most fundamental problems that, through a long history, interweaves many engineering and mathematical fields such as system theory, signal processing, pattern recognition, machine learning, and statistics. ‚Äù"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS2.SSS0.Px3.p3", "title": "Note that by enforcing the observed data point ùíô \\bm{x} bold_italic_x to be on a low-dimensional surface, we essentially have made the entries of ùíô \\bm{x} bold_italic_x very dependent on each other an", "snippet": "Note that by enforcing the observed data point ùíô \\bm{x} bold_italic_x to be on a low-dimensional surface, we essentially have made the entries of ùíô \\bm{x} bold_italic_x very dependent on each other and in some sense have made the entries very ‚Äúpredictable‚Äù from values of other entries. For example, if we know that the data are constrained on a d d italic_d -dimensional surface in ‚Ñù d \\mathbb{R}^{d} blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , then it allows us to do many useful things with the data besides prediction: ‚Ä¢ completion : in general, given more than d d italic_d"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.I1.i1.p1", "title": "completion : in general, given more than d d italic_d entries of a typical sample ùíô \\bm{x} bold_italic_x , the rest of its entries usually can be uniquely determined. 12 12 12 Prediction becomes a spe", "snippet": "completion : in general, given more than d d italic_d entries of a typical sample ùíô \\bm{x} bold_italic_x , the rest of its entries usually can be uniquely determined. 12 12 12 Prediction becomes a special case of this property."}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.I1.i2.p1", "title": "denoising : suppose entries of a sample ùíô \\bm{x} bold_italic_x are perturbed by small noises, they can be effectively removed by projecting ùíô \\bm{x} bold_italic_x back onto the surface.", "snippet": "denoising : suppose entries of a sample ùíô \\bm{x} bold_italic_x are perturbed by small noises, they can be effectively removed by projecting ùíô \\bm{x} bold_italic_x back onto the surface."}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.I1.i3.p1", "title": "error correction : suppose a small number of unknown entries of ùíô \\bm{x} bold_italic_x are arbitrarily corrupted, they can be effectively and efficiently corrected.", "snippet": "error correction : suppose a small number of unknown entries of ùíô \\bm{x} bold_italic_x are arbitrarily corrupted, they can be effectively and efficiently corrected."}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS2.SSS0.Px3.p4", "title": "In fact, under mild conditions, the above properties are generalizable to many other low-dimensional structures in high-dimensional spaces [ WM22 ] . Interestingly, as we will see in this book, these ", "snippet": "In fact, under mild conditions, the above properties are generalizable to many other low-dimensional structures in high-dimensional spaces [ WM22 ] . Interestingly, as we will see in this book, these useful properties such as completion and denoising will inspire effective methods to learn such low-dimensional structures."}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS2.SSS0.Px3.p5", "title": "In the above, for simplicity, we have only used the deterministic case to introduce the important notion of predictability and low-dimensionality. Hence, the data (or sampled segments) precisely lie o", "snippet": "In the above, for simplicity, we have only used the deterministic case to introduce the important notion of predictability and low-dimensionality. Hence, the data (or sampled segments) precisely lie on some geometric structures such as subspaces or surfaces. In practice, as we have alluded to before, there is always a certain level of uncertainty or randomness in the data. In this case, we may assume that the data have a probability distribution, with a probability density function p ‚Äã ( ùíô ) p(\\bm{x}) italic_p ( bold_italic_x ) . We say that a distribution is ‚Äúlow-dimensional‚Äù if its density c"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.SS2.SSS0.Px3.p6", "title": "Our discussions above have led to the main and only assumption on which this book will make for a deductive approach to understand intelligence and deep networks in particular: The Main Assumption: An", "snippet": "Our discussions above have led to the main and only assumption on which this book will make for a deductive approach to understand intelligence and deep networks in particular: The Main Assumption: Any intelligent systems or learning methods should and could rely on is that the world is predictable hence the distribution of the observed high-dimensional data samples have low-dimensional supports. The remaining question is how to learn such low-dimensional structures correctly from the high-dimensional data, via effective and efficient computable means. As we will see in this book, parametric m"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.p1", "title": "Note even if a predictive function is tractable to compute, it does not imply it is tractable or scalable to learn this function from a number of sampled segments. Of course, one classical approach to", "snippet": "Note even if a predictive function is tractable to compute, it does not imply it is tractable or scalable to learn this function from a number of sampled segments. Of course, one classical approach to ensure the problems are tractable or amenable to efficient solutions is to make explicit assumptions about the family of low-dimensional structures we are dealing with. Historically, due to limited computation and data, simple and idealistic analytical models were always the first to be studied as they often offer efficient closed-form or numerical solutions. In addition, they can provide insight"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.p2", "title": "For example, arguably the simplest case is to assume the data is distributed around a single low-dimensional subspace in a high-dimensional space. Or somewhat equivalently, one may assume the data is ", "snippet": "For example, arguably the simplest case is to assume the data is distributed around a single low-dimensional subspace in a high-dimensional space. Or somewhat equivalently, one may assume the data is distributed according to an almost degenerate low-dimensional Gaussian. Identifying such a subspace or Gaussian from a finite number of (noisy) samples is then the classical problem of principal component analysis (PCA) and effective algorithms have been developed for this class of models [ Jol02 ] . One can make the family of models increasingly more complex and expressive. For instance, one may "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.p3", "title": "Around all these analytical model families, the central problem of study is always how to identify the most compact model within each family that best fits the given data. Below, we give a very brief ", "snippet": "Around all these analytical model families, the central problem of study is always how to identify the most compact model within each family that best fits the given data. Below, we give a very brief account of these classical analytical models but leave a more systematic study to Chapter 2 . In theory, these analytical models have provided us with tremendous insights into the geometric and statistical properties of low-dimensional structures. They often give us closed-form solutions or efficient and scalable algorithms which are very useful for data whose distributions can be well approximate"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx1.Px1.p1", "title": "As we have discussed before in Section 1.2.1 , a main task of intelligence is to learn what is predictable in sequences of observations. Probably the simplest class of predictable sequences, or signal", "snippet": "As we have discussed before in Section 1.2.1 , a main task of intelligence is to learn what is predictable in sequences of observations. Probably the simplest class of predictable sequences, or signals, are generated via a linear time-invariant (LTI) process: x ‚Äã [ n ] = h ‚Äã [ n ] ‚àó z ‚Äã [ n ] + œµ ‚Äã [ n ] , x[n]=h[n]*z[n]+\\epsilon[n], italic_x [ italic_n ] = italic_h [ italic_n ] ‚àó italic_z [ italic_n ] + italic_œµ [ italic_n ] , (1.3.1) where z z italic_z is the input and h h italic_h is the impulse response function. 14 14 14 Normally h h italic_h is assumed to have certain nice structures, sa"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx1.Px2.p1", "title": "The idea of denoising or filtering for a dynamic process was later extended to a linear time-invariant system described by a (finite-dimensional) state-space model by Rudolph Kalman in the 1960s: ùíõ ‚Äã ", "snippet": "The idea of denoising or filtering for a dynamic process was later extended to a linear time-invariant system described by a (finite-dimensional) state-space model by Rudolph Kalman in the 1960s: ùíõ ‚Äã [ n ] = ùë® ‚Äã ùíõ ‚Äã [ n ‚àí 1 ] + ùë© ‚Äã ùíñ ‚Äã [ n ] + œµ ‚Äã [ n ] . \\bm{z}[n]=\\bm{A}\\bm{z}[n-1]+\\bm{B}\\bm{u}[n]+\\bm{\\epsilon}[n]. bold_italic_z [ italic_n ] = bold_italic_A bold_italic_z [ italic_n - 1 ] + bold_italic_B bold_italic_u [ italic_n ] + bold_italic_œµ [ italic_n ] . (1.3.3) The problem is how we can estimate the state of the system ùíõ ‚Äã [ n ] \\bm{z}[n] bold_italic_z [ italic_n ] from noisy observati"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx1.Px3.p1", "title": "To derive the Kalman filter, the system parameters ( ùë® , ùë© , ùë™ ) (\\bm{A},\\bm{B},\\bm{C}) ( bold_italic_A , bold_italic_B , bold_italic_C ) are assumed to be known. If they are not given in advance, it ", "snippet": "To derive the Kalman filter, the system parameters ( ùë® , ùë© , ùë™ ) (\\bm{A},\\bm{B},\\bm{C}) ( bold_italic_A , bold_italic_B , bold_italic_C ) are assumed to be known. If they are not given in advance, it would be a more challenging problem known as system identification : how to learn the parameters ( ùë® , ùë© , ùë™ ) (\\bm{A},\\bm{B},\\bm{C}) ( bold_italic_A , bold_italic_B , bold_italic_C ) from (many samples of) the input sequence { ùíñ ‚Äã [ n ] } \\{\\bm{u}[n]\\} { bold_italic_u [ italic_n ] } and observation sequence { ùíô ‚Äã [ n ] } \\{\\bm{x}[n]\\} { bold_italic_x [ italic_n ] } . This is a classic problem in "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx1.Px3.p2", "title": "Note that the above problems have two things in common: first, the (noise-free) sequences or signals are assumed to be generated by an explicit family of parametric models; second, these models essent", "snippet": "Note that the above problems have two things in common: first, the (noise-free) sequences or signals are assumed to be generated by an explicit family of parametric models; second, these models essentially are all linear. So conceptually, let ùíô o \\bm{x}_{o} bold_italic_x start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT be a random variable whose ‚Äútrue‚Äù distribution is supported on a low-dimensional linear subspace, say S S italic_S . To a large extent, Wiener filter and Kalman filter all try to estimate such an ùíô o \\bm{x}_{o} bold_italic_x start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT from its "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx2.Px1.p1", "title": "From the above problems in classical signal processing and system identification, we see that the main task behind of all these problems is to learn from noisy observations a single low-dimensional li", "snippet": "From the above problems in classical signal processing and system identification, we see that the main task behind of all these problems is to learn from noisy observations a single low-dimensional linear subspace. Mathematically, we may model such a structure as: ùíô = ùíñ 1 ‚Äã z 1 + ùíñ 2 ‚Äã z 2 + ‚ãØ + ùíñ d ‚Äã z d + œµ = ùëº ‚Äã ùíõ + œµ , ùëº ‚àà ‚Ñù D √ó d \\bm{x}=\\bm{u}_{1}z_{1}+\\bm{u}_{2}z_{2}+\\cdots+\\bm{u}_{d}z_{d}+\\bm{\\epsilon}=\\bm{U}\\bm{z}+\\bm{\\epsilon},\\quad\\bm{U}\\in\\mathbb{R}^{D\\times d} bold_italic_x = bold_italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + bo"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx2.Px1.p2", "title": "The problem is to find the subspace basis ùëº \\bm{U} bold_italic_U from many samples of ùíô \\bm{x} bold_italic_x . A typical approach to estimate the subspace ùëº \\bm{U} bold_italic_U is to minimize the var", "snippet": "The problem is to find the subspace basis ùëº \\bm{U} bold_italic_U from many samples of ùíô \\bm{x} bold_italic_x . A typical approach to estimate the subspace ùëº \\bm{U} bold_italic_U is to minimize the variance of the noise, also known as the minimum mean square error (MMSE): min ùëº ‚Å° ùîº ‚Äã [ ‚Äñ œµ ‚Äñ 2 2 ] = ùîº ‚Äã [ ‚Äñ ùíô ‚àí ùëº ‚Äã ùíõ ‚Äñ 2 2 ] . \\min_{\\bm{U}}\\mathbb{E}\\big{[}\\|\\bm{\\epsilon}\\|_{2}^{2}\\big{]}=\\mathbb{E}\\big{[}\\|\\bm{x}-\\bm{U}\\bm{z}\\|_{2}^{2}\\big{]}. roman_min start_POSTSUBSCRIPT bold_italic_U end_POSTSUBSCRIPT blackboard_E [ ‚à• bold_italic_œµ ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSC"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx2.Px1.p3", "title": "This is a classic problem in statistics known as the Principal Component Analysis (PCA). It was first studied by Pearson in 1901 [ Pea01 ] and later independently by Hotelling in 1933 [ Hot33 ] . This", "snippet": "This is a classic problem in statistics known as the Principal Component Analysis (PCA). It was first studied by Pearson in 1901 [ Pea01 ] and later independently by Hotelling in 1933 [ Hot33 ] . This topic is systematically summarized in the classic book [ Jol86 , Jol02 ] . In addition, one may explicitly assume the data ùíô \\bm{x} bold_italic_x is distributed according to a single low-dimensional Gaussian: ùíô ‚àº ùí© ‚Äã ( ùüé , ùëº ‚Äã ùëº ‚ä§ + œÉ ‚Äã ùë∞ ) , ùëº ‚àà ‚Ñù D √ó d , \\bm{x}\\sim\\mathcal{N}(\\bm{0},\\bm{U}\\bm{U}^{\\top}+\\sigma\\bm{I}),\\quad\\bm{U}\\in\\mathbb{R}^{D\\times d}, bold_italic_x ‚àº caligraphic_N ( bold_0 , "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx2.Px1.p4", "title": "In this book, we will revisit PCA in Chapter 2 , from the perspective of learning a low-dimensional distribution. Our goal is to use this simple and idealistic model to convey some of the most fundame", "snippet": "In this book, we will revisit PCA in Chapter 2 , from the perspective of learning a low-dimensional distribution. Our goal is to use this simple and idealistic model to convey some of the most fundamental ideas for learning a compact representation for a low-dimensional distribution, including the important notion of compression via denoising and autoencoding for a consistent representation."}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx2.Px2.p1", "title": "Independent component analysis (ICA) was originally proposed by [ BJC85 ] as a classic model for learning a good representation . In fact it was originally proposed as a simple mathematical model for ", "snippet": "Independent component analysis (ICA) was originally proposed by [ BJC85 ] as a classic model for learning a good representation . In fact it was originally proposed as a simple mathematical model for our memory. The ICA model takes a deceivingly similar form as the above PCA model ( 1.3.7 ) by assuming that the observed random variable ùíô \\bm{x} bold_italic_x is a linear superposition of multiple independent components z i z_{i} italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT : ùíô = ùíñ 1 ‚Äã z 1 + ùíñ 2 ‚Äã z 2 + ‚ãØ + ùíñ d ‚Äã z d + œµ = ùëº ‚Äã ùíõ + œµ . \\bm{x}=\\bm{u}_{1}z_{1}+\\bm{u}_{2}z_{2}+\\cdots+\\bm{"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx2.Px2.p2", "title": "Although the (decoding) mapping from ùíõ \\bm{z} bold_italic_z to ùíô \\bm{x} bold_italic_x seems linear and easy once ùëº \\bm{U} bold_italic_U and ùíõ \\bm{z} bold_italic_z are learned, the (encoding) mapping f", "snippet": "Although the (decoding) mapping from ùíõ \\bm{z} bold_italic_z to ùíô \\bm{x} bold_italic_x seems linear and easy once ùëº \\bm{U} bold_italic_U and ùíõ \\bm{z} bold_italic_z are learned, the (encoding) mapping from ùíô \\bm{x} bold_italic_x to ùíõ \\bm{z} bold_italic_z can be complicated and may not be represented by a simple linear mapping. Hence ICA generally gives an autoencoding of the form: ùíô ‚Üí ‚Ñ∞ ùíõ ‚Üí ùëº ùíô ^ . \\bm{x}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}\\hskip 5.69054pt}\\bm{z}\\xrightarrow{\\hskip 5.69054pt\\bm{U}\\hskip 5.69054pt}\\hat{\\bm{x}}. bold_italic_x start_ARROW start_OVERACCENT caligraphic_E end_OVER"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx2.Px3.p1", "title": "As one may see, if p p italic_p in ( 1.3.13 ) is very small, the probability that any of the components is non-zero is small. In this case, we say ùíô \\bm{x} bold_italic_x is sparsely generated and it c", "snippet": "As one may see, if p p italic_p in ( 1.3.13 ) is very small, the probability that any of the components is non-zero is small. In this case, we say ùíô \\bm{x} bold_italic_x is sparsely generated and it concentrates on a set of linear subspaces of dimension k = p ‚ãÖ d k=p\\cdot d italic_k = italic_p ‚ãÖ italic_d . Hence, to some extent, we may extend the above ICA model to a more general family of low-dimensional structures known as sparse models."}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx2.Px3.p2", "title": "A k k italic_k -sparse model is defined to be consisting of the set of all k k italic_k -sparse vectors : ùíµ = { ùíõ ‚àà ‚Ñù n ‚à£ ‚Äñ ùíõ ‚Äñ 0 ‚â§ k } , \\mathcal{Z}=\\{\\bm{z}\\in\\mathbb{R}^{n}\\mid\\|\\bm{z}\\|_{0}\\leq k\\", "snippet": "A k k italic_k -sparse model is defined to be consisting of the set of all k k italic_k -sparse vectors : ùíµ = { ùíõ ‚àà ‚Ñù n ‚à£ ‚Äñ ùíõ ‚Äñ 0 ‚â§ k } , \\mathcal{Z}=\\{\\bm{z}\\in\\mathbb{R}^{n}\\mid\\|\\bm{z}\\|_{0}\\leq k\\}, caligraphic_Z = { bold_italic_z ‚àà blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ‚à£ ‚à• bold_italic_z ‚à• start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ‚â§ italic_k } , (1.3.15) where ‚à• ‚ãÖ ‚à• 0 \\|\\cdot\\|_{0} ‚à• ‚ãÖ ‚à• start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT is the ‚Ñì 0 \\ell^{0} roman_‚Ñì start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT -norm that counts the number of non-zero entries in a vector ùíõ \\bm{z}"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx2.Px3.p3", "title": "So despite a very rich and long history of study which can be dated back as early as the 18th century [ Bos50 ] , there was no provably efficient algorithm to solve this class of problems, although ma", "snippet": "So despite a very rich and long history of study which can be dated back as early as the 18th century [ Bos50 ] , there was no provably efficient algorithm to solve this class of problems, although many heuristic algorithms have been proposed and developed between the 1960s and the 1990s. Some are rather effective in practice but without any rigorous justification. A major breakthrough came in the early 2000s when a few renowned mathematicians including David Donoho, Emmanuel Cand√®s, and Terence Tao [ Don05 , CT05a , CT05 ] established a rigorous theoretical framework that allows us to charact"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx2.Px3.p4", "title": "As it turns out, conditions under which ‚Ñì 1 \\ell^{1} roman_‚Ñì start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT minimization succeeds are surprisingly general. The minimum number of measurements m m italic_m", "snippet": "As it turns out, conditions under which ‚Ñì 1 \\ell^{1} roman_‚Ñì start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT minimization succeeds are surprisingly general. The minimum number of measurements m m italic_m required for a successful recovery is only proportional to the intrinsic dimension of the data k k italic_k . This is now known as the compressive sensing phenomenon [ Can06 ] . Moreover, this phenomenon is not unique to sparse structures. It also applies to very broad families of low-dimensional structures such as low-rank matrices etc. These results have fundamentally changed our understanding "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx2.Px3.p5", "title": "From a computational perspective, one cannot over-estimate the significance of this new framework. It has fundamentally changed our views about an important class of problems previously believed to be", "snippet": "From a computational perspective, one cannot over-estimate the significance of this new framework. It has fundamentally changed our views about an important class of problems previously believed to be largely intractable. It has enabled us to develop extremely efficient algorithms that scale gracefully with the dimension of the problem, hence making the problem of sparse recovery from: intractable ‚üπ tractable ‚üπ scalable . \\mbox{{intractable}}\\;\\Longrightarrow\\;\\mbox{{tractable}}\\;\\Longrightarrow\\;\\mbox{{scalable}}. intractable ‚üπ tractable ‚üπ scalable . (1.3.19) These algorithms also come with r"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx2.Px4.p1", "title": "Conceptually, an even harder problem than the sparse coding problem ( 1.3.16 ) is when the observation matrix ùë® \\bm{A} bold_italic_A is not even known in advance and we need to learn ùë® \\bm{A} bold_ita", "snippet": "Conceptually, an even harder problem than the sparse coding problem ( 1.3.16 ) is when the observation matrix ùë® \\bm{A} bold_italic_A is not even known in advance and we need to learn ùë® \\bm{A} bold_italic_A from a set of (possibly noisy) observations, say ùëø = [ ùíô 1 , ùíô 2 , ‚Ä¶ , ùíô n ] \\bm{X}=[\\bm{x}_{1},\\bm{x}_{2},\\ldots,\\bm{x}_{n}] bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ] : ùëø = ùë® ‚Äã ùíÅ + ùë¨ , ùë® ‚àà ‚Ñù m √ó n . \\bm{X}=\\bm{A}\\bm{Z}+\\bm{E},\\quad\\bm{A}\\"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx2.Px4.p2", "title": "One can see what is in common with PCA, ICA, and Dictionary Learning is that they all assume that the distribution of the data is supported around low-dimensional linear or mixed linear structures. Th", "snippet": "One can see what is in common with PCA, ICA, and Dictionary Learning is that they all assume that the distribution of the data is supported around low-dimensional linear or mixed linear structures. They all require to learn a (global or local) basis of the linear structures, from probably noisy samples of the distribution. In Chapter 2 , we will study how to identify low-dimensional structures through these classical models. In particular, we will see an interesting and important fact: all these low-dimensional (piecewise) linear models can be learned effectively and efficiently by the same ty"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx3.p1", "title": "The distributions of real-world data such as images, videos, and audio are too complex to be modeled by above, somewhat idealistic, linear models or Gaussian processes. We normally do not know a prior", "snippet": "The distributions of real-world data such as images, videos, and audio are too complex to be modeled by above, somewhat idealistic, linear models or Gaussian processes. We normally do not know a priori they are generated from which family of parametric models. 19 19 19 Although in history there had been many attempts to develop analytical models for these data, such as random fields or stochastic processes for imagery data [ MG99 ] , as we have discussed in the previous section. In practice, we typically only have many samples from their distributions ‚Äì the empirical distributions. Obviously, "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx3.Px1.p1", "title": "In the 1950s, statisticians became interested in the problem of denoising data drawn from an arbitrary distribution. Let ùíô o \\bm{x}_{o} bold_italic_x start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT be ", "snippet": "In the 1950s, statisticians became interested in the problem of denoising data drawn from an arbitrary distribution. Let ùíô o \\bm{x}_{o} bold_italic_x start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT be a random variable with probability density function p o ‚Äã ( ‚ãÖ ) p_{o}(\\cdot) italic_p start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT ( ‚ãÖ ) . So if we observe a noisy version of ùíô o \\bm{x}_{o} bold_italic_x start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT : ùíô = ùíô o + œÉ ‚Äã ùíà , \\bm{x}=\\bm{x}_{o}+\\sigma\\bm{g}, bold_italic_x = bold_italic_x start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT + italic_œÉ bold_it"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx3.Px1.p2", "title": "Because this is such an important and useful result, it has been rediscovered and used in many different contexts and areas. For example, after Tweedie‚Äôs formula [ Rob56 ] , it was rediscovered a few ", "snippet": "Because this is such an important and useful result, it has been rediscovered and used in many different contexts and areas. For example, after Tweedie‚Äôs formula [ Rob56 ] , it was rediscovered a few years later by [ Miy61 ] . In the early 2000s, the function ‚àá log ‚Å° p ‚Äã ( ùíô ) \\nabla\\log p(\\bm{x}) ‚àá roman_log italic_p ( bold_italic_x ) was rediscovered again in the context of learning a general distribution and was named the ‚Äúscore function‚Äù by Aapo Hyv√§rinen [ Hyv05 ] . But its connection to (empirical Bayesian) denoising was soon recognized by [ Vin11 ] . Generalizations to other measurement"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx3.Px2.p1", "title": "In fact, this function has a very intuitive information-theoretic and geometric interpretation. Note that in information theory ‚àí log ‚Å° p ‚Äã ( ùíô ) -\\log p(\\bm{x}) - roman_log italic_p ( bold_italic_x )", "snippet": "In fact, this function has a very intuitive information-theoretic and geometric interpretation. Note that in information theory ‚àí log ‚Å° p ‚Äã ( ùíô ) -\\log p(\\bm{x}) - roman_log italic_p ( bold_italic_x ) generally corresponds to the number of bits needed to encode ùíô \\bm{x} bold_italic_x 23 23 23 at least in the case of a discrete variable, as we will explain in more details in Chapter 3 . . The gradient ‚àá log ‚Å° p ‚Äã ( ùíô ) \\nabla\\log p(\\bm{x}) ‚àá roman_log italic_p ( bold_italic_x ) points to a direction in which the density is higher, as shown in Figure 1.12 left. The number of bits needed to encod"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx3.Px2.p2", "title": "We will discuss later in this chapter and in Chapter 3 , how such a seemingly simple concept of denoising and compression leads to a very unifying and powerful method for learning general low-dimensio", "snippet": "We will discuss later in this chapter and in Chapter 3 , how such a seemingly simple concept of denoising and compression leads to a very unifying and powerful method for learning general low-dimensional distributions in a high-dimensional space, including the distribution of natural images."}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.p1", "title": "In practice, for many important real-world data such as images, sounds, and texts, it is difficult to model them with idealistic linear or mixed linear models. For example, there has been a long and r", "snippet": "In practice, for many important real-world data such as images, sounds, and texts, it is difficult to model them with idealistic linear or mixed linear models. For example, there has been a long and rich history in the fields of image processing and computer vision that tries to model the distribution of natural images analytically. David Mumford, a Fields Medalist, spent considerable effort in the 1990s trying to understand and model the statistics of natural images [ Mum96 ] . He and his students, including Song-Chun Zhu, drew inspiration and techniques from statistical physics and proposed "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.p2", "title": "Hence, historically, many empirical models have been proposed to model important real-world data, including images and texts. These models often drew inspiration from the characteristics of the biolog", "snippet": "Hence, historically, many empirical models have been proposed to model important real-world data, including images and texts. These models often drew inspiration from the characteristics of the biological nerve system because the brain of an animal or human seems to process these data extremely efficiently and effectively."}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx1.Px1.p1", "title": "Inspired by the nerve system in the brain, the mathematical model of the first artificial neuron 25 25 25 known as the Linear Threshold Unit, or a perceptron. was proposed by Warren McCulloch 26 26 26", "snippet": "Inspired by the nerve system in the brain, the mathematical model of the first artificial neuron 25 25 25 known as the Linear Threshold Unit, or a perceptron. was proposed by Warren McCulloch 26 26 26 A professor of psychiatry at the University of Chicago at the time and Walter Pitts in 1943 [ MP43 ] . It describes the relationship between the input x i x_{i} italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and output o j o_{j} italic_o start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT as: o j = œÜ ‚Äã ( ‚àë i w j ‚Äã i ‚Äã x i ) , o_{j}=\\varphi\\Big{(}\\sum_{i}w_{ji}x_{i}\\Big{)}, italic_o start_POST"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx1.Px2.p1", "title": "In the 1950s, Frank Rosenblatt was the first to build a machine with a network of such artificial neurons, shown in Figure 1.15 . The machine is called Mark I Perceptron which consists of an input lay", "snippet": "In the 1950s, Frank Rosenblatt was the first to build a machine with a network of such artificial neurons, shown in Figure 1.15 . The machine is called Mark I Perceptron which consists of an input layer, an output layer, and a single hidden layer consisting of 512 artificial neurons, as shown in Figure 1.15 left, which is similar to what is illustrated in Figure 1.14 left. It was designed to classify optical images of letters. However, the capacity of a single-layer network is limited and can only learn linearly separable patterns. In a 1969 book Perceptrons: An Introduction to Computational G"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx1.Px3.p1", "title": "The somewhat disappointing early experimentation with artificial neural networks like Mark I Perceptron in the 50s and 60s suggested that it might not be enough to simply connect neurons in a general ", "snippet": "The somewhat disappointing early experimentation with artificial neural networks like Mark I Perceptron in the 50s and 60s suggested that it might not be enough to simply connect neurons in a general fashion as multi-layer perceptrons (MLP). In order to build effective and efficient networks, we need to understand what purpose or function neurons in a network need to achieve collectively so that they should be organized and learned in a certain special way. Once again, the study of machine intelligence turned to draw inspiration from how the animal‚Äôs nerve system works."}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx1.Px3.p2", "title": "It is known that most of our brain is dedicated to process visual information. In 1950s and 1960s, David Hubel and Torsten Wiesel systematically studied the visual cortices of cats. It was discovered ", "snippet": "It is known that most of our brain is dedicated to process visual information. In 1950s and 1960s, David Hubel and Torsten Wiesel systematically studied the visual cortices of cats. It was discovered that the visual cortex contains different types of cells (known as simple cells and complex cells), which are sensitive to visual stimuli of different orientations and locations [ HW59 ] . Hubel and Wiesel won the 1981 Nobel Prize in Physiology or Medicine for their ground-breaking discovery."}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx1.Px3.p3", "title": "On the artificial neural network side, Hubel and Wiesel‚Äôs work had inspired Kunihiko Fukushima who designed the ‚Äúneocognitron‚Äù network in 1980 which consists of artificial neurons that emulate biologi", "snippet": "On the artificial neural network side, Hubel and Wiesel‚Äôs work had inspired Kunihiko Fukushima who designed the ‚Äúneocognitron‚Äù network in 1980 which consists of artificial neurons that emulate biological neurons in the visual cortices [ Fuk80 ] . This is known as the first convolutional neural network (CNN), and its architecture is illustrated in Figure 1.16 . Unlike the perceptron, the neocognitron had more than one hidden layer and could be viewed as a deep network, as compared in Figure 1.14 right."}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx1.Px3.p4", "title": "Also inspired by how neurons work in the cat‚Äôs visual cortex, he was also the first to introduce the use of rectified linear unit (ReLU): œÜ ‚Äã ( x ) = max ‚Å° { 0 , x } = { x , if ‚Äã x > 0 , 0 , if ‚Äã x ‚â§ ", "snippet": "Also inspired by how neurons work in the cat‚Äôs visual cortex, he was also the first to introduce the use of rectified linear unit (ReLU): œÜ ‚Äã ( x ) = max ‚Å° { 0 , x } = { x , if ‚Äã x > 0 , 0 , if ‚Äã x ‚â§ 0 , \\varphi(x)=\\max\\{0,x\\}=\\begin{cases}x,&\\text{if}\\,x>0,\\\\ 0,\\quad&\\text{if}\\,x\\leq 0,\\end{cases} italic_œÜ ( italic_x ) = roman_max { 0 , italic_x } = { start_ROW start_CELL italic_x , end_CELL start_CELL if italic_x > 0 , end_CELL end_ROW start_ROW start_CELL 0 , end_CELL start_CELL if italic_x ‚â§ 0 , end_CELL end_ROW (1.3.28) for the activation function œÜ ‚Äã ( ‚ãÖ ) \\varphi(\\cdot) italic_œÜ ( ‚ãÖ ) i"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx1.Px3.p5", "title": "CNN-type networks continued to evolve in the 1980s and many different variants had been introduced and studied. However, despite the remarkable capacities of deep networks and the improved architectur", "snippet": "CNN-type networks continued to evolve in the 1980s and many different variants had been introduced and studied. However, despite the remarkable capacities of deep networks and the improved architectures inspired by neuroscience, it remained extremely difficult to train such deep networks for a real task such as image classification. How to get a network to work depended on many unexplainable heuristics and tricks that really limited the appeal and applicability of neural networks. One major breakthrough came around 1989 when Yann LeCun successfully used back propagation (BP) to learn a deep co"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx1.Px4.p1", "title": "In history, the fate of deep neural networks seems to be tied closely to how they can be trained easily and efficiently. Back propagation (BP) was introduced for this reason. We know that a multiple l", "snippet": "In history, the fate of deep neural networks seems to be tied closely to how they can be trained easily and efficiently. Back propagation (BP) was introduced for this reason. We know that a multiple layer perceptron can be expressed as a composition of a sequence of linear mappings and nonlinear activations as follows: h ‚Äã ( ùëæ 1 , ‚Ä¶ , ùëæ L ) = f L ‚Äã ( ùëæ L ‚Äã f L ‚àí 1 ‚Äã ( ùëæ L ‚àí 1 ‚Äã ‚ãØ ‚Äã f 2 ‚Äã ( ùëæ 2 ‚Äã f 1 ‚Äã ( ùëæ 1 ‚Äã ùíô ) ) ) ) . h(\\bm{W}_{1},\\ldots,\\bm{W}_{L})=f^{L}(\\bm{W}_{L}f^{L-1}(\\bm{W}_{L-1}\\cdots f^{2}(\\bm{W}_{2}f^{1}(\\bm{W}_{1}\\bm{x})))). italic_h ( bold_italic_W start_POSTSUBSCRIPT 1 end_POSTS"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx1.Px4.p2", "title": "However, despite the above algorithmic progress and promising practice in the 1980s, training deep neural networks remained extremely finicky and expensive for computing systems in the 1980s and 1990s", "snippet": "However, despite the above algorithmic progress and promising practice in the 1980s, training deep neural networks remained extremely finicky and expensive for computing systems in the 1980s and 1990s. In late 1990s, support vector machines (SVM) [ CV95 ] had become very popular as they were viewed as a better alternative to neural networks for tasks such as classification. 32 32 32 In fact, similar ideas to solve classification problems can be traced back to the PhD dissertation work of Thomas Cover, which as condensed and published in a paper in 1964 [ Cov64 ] . There were two main reasons: "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx1.Px5.p1", "title": "In the late 1980s and 1990s, artificial neural networks were already adopted to learn low-dimensional representations of high-dimensional data such as images. It had been shown that neural networks ca", "snippet": "In the late 1980s and 1990s, artificial neural networks were already adopted to learn low-dimensional representations of high-dimensional data such as images. It had been shown that neural networks can be used to learn PCA from the data [ Oja82 , BH89 ] , instead of using the classic methods discussed in Section 1.3.1 . It was also argued during late 1980s that due to its capability to model nonlinear transforms, neural networks were suggested to learn low-dimensional representations for data with nonlinear distributions. Similar to the linear PCA case, one can try to simultaneously learn a no"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx1.Px5.p2", "title": "But how can we guarantee that such an autoencoding indeed captures the true low-dimensional structures in ùëø \\bm{X} bold_italic_X instead of giving a trivial redundant representation? For example, we c", "snippet": "But how can we guarantee that such an autoencoding indeed captures the true low-dimensional structures in ùëø \\bm{X} bold_italic_X instead of giving a trivial redundant representation? For example, we can simply choose f f italic_f and g g italic_g to be the identity map and ùíÅ = ùëø \\bm{Z}=\\bm{X} bold_italic_Z = bold_italic_X . So to ensure the autoencoding to be worthwhile, one wishes the resulting representation to be compressive, in terms of a certain computable measure of complexity. In 1993, Geoffrey Hinton and colleagues proposed to use the coding length as such a measure, hence the objectiv"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx2.p1", "title": "For nearly 30 years, from 1980s to 2010s, for the study of machine learning and machine intelligence, neural networks were not considered seriously by the mainstream. Early (deep) neural networks, suc", "snippet": "For nearly 30 years, from 1980s to 2010s, for the study of machine learning and machine intelligence, neural networks were not considered seriously by the mainstream. Early (deep) neural networks, such as the LeNet, have shown promising performance for small-scale classification problems such as recognizing digits. However, the design and practice of the networks were rather empirical, datasets available at the time were small, and the BP algorithm was a huge computational burden for computers then. These factors had resulted in the lack of interest in neural networks, and progress had been st"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx2.Px1.p1", "title": "As it turns out, the tremendous potential of deep neural networks could only be unleashed once there are enough data and computing power. Fast forward to 2010s, much larger datasets such as ImageNet b", "snippet": "As it turns out, the tremendous potential of deep neural networks could only be unleashed once there are enough data and computing power. Fast forward to 2010s, much larger datasets such as ImageNet became available, and GPUs became powerful enough to make BP much more affordable, even for networks much larger than LeNet. Around 2012, a deep convolutional neural network known as the AlexNet drew attention as it surpassed extent classification methods by a significant margin with the ImageNet dataset [ KSH12 ] . 34 34 34 In fact, before this, deep networks had demonstrated state of the art perf"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx2.Px1.p2", "title": "This early success inspired the machine intelligence community, in the next few years, to explore new variations and improvements to the network design. In particular, people had discovered empiricall", "snippet": "This early success inspired the machine intelligence community, in the next few years, to explore new variations and improvements to the network design. In particular, people had discovered empirically that the larger and deeper the networks, the better the performance in tasks such as image classification. Many deep network architectures have been tried, tested, and popularized. A few notable ones include VGG [ SZ15 ] , GoogLeNet [ SLJ+14 ] , ResNet [ HZR+16 ] , and more recently Transformers [ VSP+17 ] etc. Despite fast progress in empirical performance, there was a lack of theoretical expla"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx2.Px2.p1", "title": "The early successes of deep networks were mainly for classification tasks in a supervised learning setting, such as speech recognition and image recognition. Deep networks were later adopted by the De", "snippet": "The early successes of deep networks were mainly for classification tasks in a supervised learning setting, such as speech recognition and image recognition. Deep networks were later adopted by the DeepMind team, led by Demis Hassabis, to learn decision-making or control policies for playing games. In this context, deep networks are used to model the optimal decision/control policy or the associated optimal value function, as shown in Figure 1.19 . These network parameters are incrementally optimized 35 35 35 Say based on back propagation (BP). based on reward returned from the success or fail"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx2.Px2.p2", "title": "From an implementation perspective, the combination of deep networks and reinforcement learning turned out to be rather powerful: deep networks can be used to approximate control policy and value func", "snippet": "From an implementation perspective, the combination of deep networks and reinforcement learning turned out to be rather powerful: deep networks can be used to approximate control policy and value function for real-world environments that are difficult to model analytically. This practice had eventually led to the AlphaGo system, developed by the company DeepMind, which surprised the world in 2016 by beating a top human player Lee Sedol in the game Go and then the world champion Jie Ke in 2017. 36 36 36 In 1996, IBM‚Äôs Deep Blue system made history by defeating Russian grandmaster Garry Kasparov"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx2.Px2.p3", "title": "The success of AlphaGo came as a big surprise to the computing society which generally believes that the state space for search is too prohibitively large to admit any efficient solution, in terms of ", "snippet": "The success of AlphaGo came as a big surprise to the computing society which generally believes that the state space for search is too prohibitively large to admit any efficient solution, in terms of computation and sample size. The only reasonable explanation for its success is that there must be very good structures in the optimal value/policy function of the game Go. Their intrinsic dimension is not so high and they can be approximated well by a neural network, learnable from not so prohibitively many samples."}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx2.Px3.p1", "title": "One may view early practices of deep networks in the 2010s focused more on extracting relevant information from the data ùëø \\bm{X} bold_italic_X and encoding it for certain task-specific representation", "snippet": "One may view early practices of deep networks in the 2010s focused more on extracting relevant information from the data ùëø \\bm{X} bold_italic_X and encoding it for certain task-specific representation ùíÅ \\bm{Z} bold_italic_Z (say ùíÅ \\bm{Z} bold_italic_Z represent class labels in classification tasks): ùëø ‚Üí ùëì ùíÅ . \\bm{X}\\xrightarrow{\\hskip 5.69054ptf\\hskip 5.69054pt}\\bm{Z}. bold_italic_X start_ARROW start_OVERACCENT italic_f end_OVERACCENT ‚Üí end_ARROW bold_italic_Z . (1.3.32) In this setting, the mapping f f italic_f to be learned does not need to preserve most distributional information about ùëø \\b"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx2.Px3.p2", "title": "However, in many modern situations such as those so-called large foundation models, people often need to decode ùíÅ \\bm{Z} bold_italic_Z to recover the corresponding ùëø \\bm{X} bold_italic_X to a certain ", "snippet": "However, in many modern situations such as those so-called large foundation models, people often need to decode ùíÅ \\bm{Z} bold_italic_Z to recover the corresponding ùëø \\bm{X} bold_italic_X to a certain degree of precision: ùíÅ ‚Üí ùëî ùëø ^ . \\bm{Z}\\xrightarrow{\\hskip 5.69054ptg\\hskip 5.69054pt}\\hat{\\bm{X}}. bold_italic_Z start_ARROW start_OVERACCENT italic_g end_OVERACCENT ‚Üí end_ARROW over^ start_ARG bold_italic_X end_ARG . (1.3.33) As ùëø \\bm{X} bold_italic_X typically represents data observed from the external world, a good decoder would allow us to simulate or predict what happens in the world. For ex"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx2.Px4.p1", "title": "In order for the generated images ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG to be similar to the true natural images ùëø \\bm{X} bold_italic_X , we need to be able to evaluate and minimize s", "snippet": "In order for the generated images ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG to be similar to the true natural images ùëø \\bm{X} bold_italic_X , we need to be able to evaluate and minimize some distance: min ‚Å° d ‚Äã ( ùëø , ùëø ^ ) . \\min d(\\bm{X},\\hat{\\bm{X}}). roman_min italic_d ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG ) . (1.3.34) As it turns out, most theoretically motivated distances are extremely difficult, if not impossible, to compute and optimize for distributions in high-dimensional space but with a low intrinsic dimension. 37 37 37 This is the case even if a param"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx2.Px4.p2", "title": "In 2007, Zhuowen Tu, a former student of Song-Chun Zhu, probably disappointed by early analytical attempts to model and generate natural images (discussed earlier), decided to try a drastically differ", "snippet": "In 2007, Zhuowen Tu, a former student of Song-Chun Zhu, probably disappointed by early analytical attempts to model and generate natural images (discussed earlier), decided to try a drastically different approach. In a paper published in CVPR 2007 [ Tu07 ] , he was the first to suggest that one could learn a generative model for images via a discriminative approach. The idea is simple: if it is difficult to evaluate the distance d ‚Äã ( ùëø , ùëø ^ ) d(\\bm{X},\\hat{\\bm{X}}) italic_d ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG ) , one could try to learn a discriminator d d italic_d to sepa"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx2.Px4.p3", "title": "Tu‚Äôs work [ Tu07 ] was the first to demonstrate the feasibility of learning a generative model from a discriminative approach. However, the work adopted traditional methods to generate images and clas", "snippet": "Tu‚Äôs work [ Tu07 ] was the first to demonstrate the feasibility of learning a generative model from a discriminative approach. However, the work adopted traditional methods to generate images and classify distributions (such as boosting), and they were slow and hard to implement. After 2012, deep neural networks became very popular for image classification. In 2014, Ian Goodfellow and colleagues proposed again to generate natural images with a discriminative approach [ GPM+14 ] . They suggested using deep neural networks to model the generator g g italic_g and the discriminator d d italic_d in"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx2.Px4.p4", "title": "The discriminative approach seems to be a rather clever way to bypass a fundamental difficulty in distribution learning. However, rigorously speaking, this approach does not fully resolve this fundame", "snippet": "The discriminative approach seems to be a rather clever way to bypass a fundamental difficulty in distribution learning. However, rigorously speaking, this approach does not fully resolve this fundamental difficulty at all. It is shown by [ GPM+14 ] that with a properly chosen loss, the minimax formulation is mathematically equivalent to minimize the Jensen-Shannon distance (see [ CT91 ] ) between ùëø \\bm{X} bold_italic_X and ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG . This is known to be a hard problem for two low-dimensional distributions in a high-dimensional space. As a result, "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx2.Px5.p1", "title": "In 2015, shortly after GAN was introduced and became popular, Surya Ganguli and his students realized and suggested that an iterative denoising process modeled by a deep network can be used to learn a", "snippet": "In 2015, shortly after GAN was introduced and became popular, Surya Ganguli and his students realized and suggested that an iterative denoising process modeled by a deep network can be used to learn a general distribution, such as that of natural images [ SWM+15 ] . Their method was inspired by properties of the special Gaussian and binomial processes, studied by William Feller back in 1949 [ Fel49 ] . 39 39 39 Again, in the magical era of 1940s!"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx2.Px5.p2", "title": "Soon, denoising operators based on the score function [ Hyv05 ] , briefly introduced in Section 1.3.1 , were shown to be more general and unified the denoising and diffusion processes and algorithms [", "snippet": "Soon, denoising operators based on the score function [ Hyv05 ] , briefly introduced in Section 1.3.1 , were shown to be more general and unified the denoising and diffusion processes and algorithms [ SE19 , SSK+21 , HJA20 ] . Figure 1.20 gives an illustration of the process that transforms a generic Gaussian distribution q 0 = ùí© ‚Äã ( ùüé , ùë∞ ) q^{0}=\\mathcal{N}(\\bm{0},\\bm{I}) italic_q start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT = caligraphic_N ( bold_0 , bold_italic_I ) to an (arbitrary) empirical distribution p ‚Äã ( ùíô ) p(\\bm{x}) italic_p ( bold_italic_x ) by performing a sequence of iterative d"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.p1", "title": "So far, we have given a brief account of the main objective and history of machine intelligence and many important ideas and approaches associated with it. In recent years, after the empirical success", "snippet": "So far, we have given a brief account of the main objective and history of machine intelligence and many important ideas and approaches associated with it. In recent years, after the empirical success of deep neural networks, tremendous efforts have been made to develop theoretical frameworks that can help us understand all the empirically designed deep neural networks, either certain seemingly necessary components (e.g., dropout, normalization, attention, etc.) or their overall behaviors (e.g., double descent, neural collapse, etc.)."}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.p2", "title": "Partly motivated by this, this book aims to achieve several important and challenging goals: ‚Ä¢ Develop a theoretical framework that would allow us to derive rigorous mathematical interpretation of dee", "snippet": "Partly motivated by this, this book aims to achieve several important and challenging goals: ‚Ä¢ Develop a theoretical framework that would allow us to derive rigorous mathematical interpretation of deep neural networks. ‚Ä¢ Ensure correctness of the learned data distribution and consistency with the learned representation. ‚Ä¢ Demonstrate that the framework can lead to performant architectures and can guide further improvements in practice. Within the past few years, there is mounting evidence that these goals can indeed be achieved, by leveraging the theory and solutions to the classical analytica"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.I1.i1.p1", "title": "Develop a theoretical framework that would allow us to derive rigorous mathematical interpretation of deep neural networks.", "snippet": "Develop a theoretical framework that would allow us to derive rigorous mathematical interpretation of deep neural networks."}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.I1.i2.p1", "title": "Ensure correctness of the learned data distribution and consistency with the learned representation.", "snippet": "Ensure correctness of the learned data distribution and consistency with the learned representation."}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.I1.i3.p1", "title": "Demonstrate that the framework can lead to performant architectures and can guide further improvements in practice.", "snippet": "Demonstrate that the framework can lead to performant architectures and can guide further improvements in practice."}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.SS1.p1", "title": "One necessary condition for any learning task to be possible is that the sequences of interest must be computable , at least in the sense of Alan Turing [ Tur36 ] . That is, a sequence can be computed", "snippet": "One necessary condition for any learning task to be possible is that the sequences of interest must be computable , at least in the sense of Alan Turing [ Tur36 ] . That is, a sequence can be computed via a program on a typical computer. 40 40 40 There are indeed well-defined sequences that are not computable. In addition to being computable, we require computation be tractable . 41 41 41 We do not need to consider predicting things whose computational complexity is intractable, say grows exponentially in the length or dimension of the sequence. That is, the computational cost (space and time)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.SS1.p2", "title": "This is because whatever algorithms intelligent beings use to learn useful information must be scalable . More specifically, the computational complexity of the algorithms would better scale gracefull", "snippet": "This is because whatever algorithms intelligent beings use to learn useful information must be scalable . More specifically, the computational complexity of the algorithms would better scale gracefully, typically linear or even sublinear, in the size and dimension of the data. On the technical level, this requires that the operations that the algorithms rely on to learn could only utilize oracle information that can be efficiently computed from the data. More specifically, when the dimension is high and the scale is large, the only oracle one could afford to compute is either the first-order g"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.SS1.SSS0.Px1.p1", "title": "From the examples of sequences we gave in Section 1.2.1 , it is clear that some sequences are easy to model and compute and others are more difficult. Obviously, the computational cost of a sequence d", "snippet": "From the examples of sequences we gave in Section 1.2.1 , it is clear that some sequences are easy to model and compute and others are more difficult. Obviously, the computational cost of a sequence depends on how complex the predicting function f f italic_f is. The higher the degree of regression d d italic_d , the more costly it is to compute. f f italic_f can be a simple linear function, and it can also be a nonlinear function that can be arbitrarily difficult to specify and compute."}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.SS1.SSS0.Px1.p2", "title": "It is reasonable to believe that if a sequence is harder, by whatever measure we may choose, to specify and compute, then it will also be more difficult to learn from its sampled segments. Nevertheles", "snippet": "It is reasonable to believe that if a sequence is harder, by whatever measure we may choose, to specify and compute, then it will also be more difficult to learn from its sampled segments. Nevertheless, for any given predictable sequence, there are in fact many, often infinitely many, ways to specify it. For example, for a simple sequence x n + 1 = a ‚Äã x n x_{n+1}=ax_{n} italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT = italic_a italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , we could also define the same sequence with x n + 1 = a ‚Äã x n + b ‚Äã x n ‚àí 1 ‚àí b ‚Äã x n ‚àí 1 . x_{n+"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.SS1.SSS0.Px1.p3", "title": "Andrey Kolmogorov, a Russian mathematician, was one of the first to give a definition of complexity for any computable sequence. 44 44 44 Many have contributed to this notion of sequence complexity, m", "snippet": "Andrey Kolmogorov, a Russian mathematician, was one of the first to give a definition of complexity for any computable sequence. 44 44 44 Many have contributed to this notion of sequence complexity, most notably including Ray Solomonoff and Greg Chaitin. All three are believed to have developed and studied algorithmic information theory independently, Ray Solomonoff in 1960, Andrey Kolmogorov in 1965 [ Kol98 ] and Gregory Chaitin around 1966 [ Cha66 ] . He suggested that among all programs that can compute the same sequence, we may use the length of the shortest program as a measure for its co"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.SS1.SSS0.Px1.p4", "title": "The length of the shortest program can be viewed as the ultimate compression of the sequence considered, providing a quantitative measure of how much we have gained by having learned the correct gener", "snippet": "The length of the shortest program can be viewed as the ultimate compression of the sequence considered, providing a quantitative measure of how much we have gained by having learned the correct generative mechanism of the sequence. However, despite its theoretical importance, the Kolmogorov complexity is in general not a computable function [ CT91 ] (even intractable to approximate accurately). As a result, this measure of complexity is of little practical use. Neither can it tell us in advance how difficult it is to learn a given sequence nor can it tell us how well we have learned."}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.SS1.SSS0.Px2.p1", "title": "Hence for practical purposes, we need an efficiently computable measure of complexity for sequences that are generated from the same predicting function. 45 45 45 Note that in practice, we typically c", "snippet": "Hence for practical purposes, we need an efficiently computable measure of complexity for sequences that are generated from the same predicting function. 45 45 45 Note that in practice, we typically care about learning the predicting function f f italic_f , instead of any particular sequence generated by f f italic_f . Note that part of the reason why Kolmogorov complexity is not computable is because its definition is non-constructive."}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.SS1.SSS0.Px2.p2", "title": "So to introduce a computable measure of complexity, we may take a more constructive approach, as advocated by Claude Shannon through the framework of information theory [ Sha48 , CT91 ] . 46 46 46 whi", "snippet": "So to introduce a computable measure of complexity, we may take a more constructive approach, as advocated by Claude Shannon through the framework of information theory [ Sha48 , CT91 ] . 46 46 46 which has successfully guided the engineering practice of the communication industry for the past over 80 years. In essence, by assuming the sequence S S italic_S is drawn from a probabilistic distribution p ‚Äã ( S ) p(S) italic_p ( italic_S ) , the so-called entropy of the distribution: 47 47 47 Here we consider differential entropy as we assume the sequence consists of continuous variables. If it co"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.SS1.SSS0.Px2.p3", "title": "To illustrate the main ideas of this view, let us take a large number of long sequence segments: { S 1 , S 2 , ‚Ä¶ , S i , ‚Ä¶ , S N } ‚äÇ ‚Ñù D , \\{S_{1},S_{2},\\ldots,S_{i},\\ldots,S_{N}\\}\\subset\\mathbb{R}^{D", "snippet": "To illustrate the main ideas of this view, let us take a large number of long sequence segments: { S 1 , S 2 , ‚Ä¶ , S i , ‚Ä¶ , S N } ‚äÇ ‚Ñù D , \\{S_{1},S_{2},\\ldots,S_{i},\\ldots,S_{N}\\}\\subset\\mathbb{R}^{D}, { italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ‚Ä¶ , italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , ‚Ä¶ , italic_S start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT } ‚äÇ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT , (1.4.4) generated by a predicting function f f italic_f . Note that without loss of generality, here "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.SS1.SSS0.Px2.p4", "title": "Then the complexity of the predicting function f f italic_f can be evaluated as the average coding length of all sequences, known as the coding rate: 49 49 49 One may make this more precise by taking ", "snippet": "Then the complexity of the predicting function f f italic_f can be evaluated as the average coding length of all sequences, known as the coding rate: 49 49 49 One may make this more precise by taking R ‚Äã ( f ‚à£ ‚Ñ∞ ) R(f\\mid\\mathcal{E}) italic_R ( italic_f ‚à£ caligraphic_E ) to be the expected coding length for all segments of length D D italic_D . R ‚Äã ( f ‚à£ ‚Ñ∞ ) = ùîº ‚Äã [ L ‚Äã ( ‚Ñ∞ ‚Äã ( S ) ) ] ‚âà 1 N ‚Äã ‚àë i = 1 N L ‚Äã ( ‚Ñ∞ ‚Äã ( S i ) ) . R(f\\mid\\mathcal{E})=\\mathbb{E}[L(\\mathcal{E}(S))]\\approx\\frac{1}{N}\\sum_{i=1}^{N}L(\\mathcal{E}(S_{i})). italic_R ( italic_f ‚à£ caligraphic_E ) = blackboard_E [ italic_L ( c"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.SS1.SSS0.Px2.p5", "title": "Given two coding schemes ‚Ñ∞ 1 \\mathcal{E}_{1} caligraphic_E start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and ‚Ñ∞ 2 \\mathcal{E}_{2} caligraphic_E start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT for the segments, if th", "snippet": "Given two coding schemes ‚Ñ∞ 1 \\mathcal{E}_{1} caligraphic_E start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and ‚Ñ∞ 2 \\mathcal{E}_{2} caligraphic_E start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT for the segments, if the difference in the coding rates is positive: R ‚Äã ( f ‚à£ ‚Ñ∞ 1 ) ‚àí R ‚Äã ( f ‚à£ ‚Ñ∞ 2 ) > 0 , R(f\\mid\\mathcal{E}_{1})-R(f\\mid\\mathcal{E}_{2})>0, italic_R ( italic_f ‚à£ caligraphic_E start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) - italic_R ( italic_f ‚à£ caligraphic_E start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) > 0 , (1.4.6) we may say the coding scheme ‚Ñ∞ 2 \\mathcal{E}_{2} caligraphic_E start_POSTSUBSCRIPT 2 end_PO"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#Thmremark1.p1", "title": "The perspective of measuring data complexity with explicit encoding schemes has motivated several learning objectives that were proposed to revise the Kolmogorov complexity for better computability [ ", "snippet": "The perspective of measuring data complexity with explicit encoding schemes has motivated several learning objectives that were proposed to revise the Kolmogorov complexity for better computability [ WD99 ] , including the minimum message length (MML) proposed later in 1968 [ WB68 ] and the minimum description length (MDL) in 1978 [ Ris78 , HY01 ] . These objectives normally count the coding length for the coding scheme ‚Ñ∞ \\mathcal{E} caligraphic_E itself (including its code book) in addition to the data S S italic_S of interest: L ‚Äã ( ‚Ñ∞ ‚Äã ( S ) ) + L ‚Äã ( ‚Ñ∞ ) L(\\mathcal{E}(S))+L(\\mathcal{E}) it"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.SS1.SSS0.Px2.p6", "title": "Again, one may view the resulting optimal coding scheme as the one that achieves the best compression of the observed data. In general, compared to the Kolmogorov complexity, the coding length given b", "snippet": "Again, one may view the resulting optimal coding scheme as the one that achieves the best compression of the observed data. In general, compared to the Kolmogorov complexity, the coding length given by any encoding scheme will always be larger: K ‚Äã ( S ) < L ‚Äã ( ‚Ñ∞ ‚Äã ( S ) ) . K(S)<L(\\mathcal{E}(S)). italic_K ( italic_S ) < italic_L ( caligraphic_E ( italic_S ) ) . (1.4.8) Therefore, minimizing the coding rate/length is essentially to minimize an upper bound of the otherwise uncomputable Kolmogorov complexity."}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.SS2.p1", "title": "Note that if the goal was simply to compress the given data just for the sake of compression, then in theory the optimal codes that approach the Kolmogorov complexity would become nearly random or str", "snippet": "Note that if the goal was simply to compress the given data just for the sake of compression, then in theory the optimal codes that approach the Kolmogorov complexity would become nearly random or structureless [ Cha66 ] . 50 50 50 Because any codes with structures can be further compressed. However, our true purpose of learning the predictive function f f italic_f is to use it repeatedly with ease in future predictions. Hence, while compression allows us to identify the low-dimensional distribution in the data, we would like to encode the distribution in a structured and organized way so that"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.SS2.p2", "title": "As we will show in Chapter 3 , these desired structures in the final representation can be precisely promoted by choosing a natural measure of information gain based on the coding rates of the chosen ", "snippet": "As we will show in Chapter 3 , these desired structures in the final representation can be precisely promoted by choosing a natural measure of information gain based on the coding rates of the chosen coding schemes. As we see throughout this book, such an explicit and constructive coding approach provides a powerful computational framework for learning good representations of low-dimensional structures for real-world data, as in many cases of practical importance, the coding length function can be efficiently computed or approximated accurately. In some benign cases, we can even obtain closed-"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.SS2.p3", "title": "In addition, such a computational framework leads to a principled approach that naturally reveals the role that deep networks play in this learning process. As we will derive systematically in Chapter", "snippet": "In addition, such a computational framework leads to a principled approach that naturally reveals the role that deep networks play in this learning process. As we will derive systematically in Chapter 4 , the layers of a deep network are trying to perform operations that optimize the objective function of interest in an incremental manner. From this perspective, the role of deep networks can be precisely interpreted as to emulate a certain iterative optimization algorithm, say gradient descent, to optimize the objective of information gain. Layers of the resulting deep architectures can be end"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.SS3.p1", "title": "To summarize our discussions so far, let us denote the data as: ùëø = { S 1 , S 2 , ‚Ä¶ , S i , ‚Ä¶ , S N } ‚äÇ ‚Ñù D , \\bm{X}=\\{S_{1},S_{2},\\ldots,S_{i},\\ldots,S_{N}\\}\\subset\\mathbb{R}^{D}, bold_italic_X = { i", "snippet": "To summarize our discussions so far, let us denote the data as: ùëø = { S 1 , S 2 , ‚Ä¶ , S i , ‚Ä¶ , S N } ‚äÇ ‚Ñù D , \\bm{X}=\\{S_{1},S_{2},\\ldots,S_{i},\\ldots,S_{N}\\}\\subset\\mathbb{R}^{D}, bold_italic_X = { italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ‚Ä¶ , italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , ‚Ä¶ , italic_S start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT } ‚äÇ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT , (1.4.9) and let ùíÅ = ‚Ñ∞ ‚Äã ( ùëø ) \\bm{Z}=\\mathcal{E}(\\bm{X}) bold_italic_Z = caligraphic_E ( bold_italic_X ) be"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#Thmexample2.p1", "title": "For example, image classification is such a case: we assign all images in the same class to a single code and images in different classes to different codes, say ‚Äúone-hot‚Äù vectors: ùíô ‚Ü¶ ùíõ ‚àà { [ 1 , 0 ,", "snippet": "For example, image classification is such a case: we assign all images in the same class to a single code and images in different classes to different codes, say ‚Äúone-hot‚Äù vectors: ùíô ‚Ü¶ ùíõ ‚àà { [ 1 , 0 , 0 , ‚Ä¶ , 0 , 0 ] , [ 0 , 1 , 0 ‚Ä¶ , 0 , 0 ] , ‚Ä¶ , [ 0 , 0 , 0 , ‚Ä¶ , 0 , 1 ] . } \\bm{x}\\mapsto\\bm{z}\\in\\{[1,0,0,\\ldots,0,0],\\;[0,1,0\\ldots,0,0],\\;\\ldots,\\;[0,0,0,\\ldots,0,1].\\} bold_italic_x ‚Ü¶ bold_italic_z ‚àà { [ 1 , 0 , 0 , ‚Ä¶ , 0 , 0 ] , [ 0 , 1 , 0 ‚Ä¶ , 0 , 0 ] , ‚Ä¶ , [ 0 , 0 , 0 , ‚Ä¶ , 0 , 1 ] . } (1.4.12) Now, a classifier f ‚Äã ( ‚ãÖ ) f(\\cdot) italic_f ( ‚ãÖ ) can be modeled as a function that predicts"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.SS3.p2", "title": "The cross-entropy loss L ‚Äã ( ùíõ ^ , ùíõ ) L(\\hat{\\bm{z}},\\bm{z}) italic_L ( over^ start_ARG bold_italic_z end_ARG , bold_italic_z ) can be viewed as a special measure of parsimony œÅ ‚Äã ( ùíõ ) \\rho(\\bm{z}) ", "snippet": "The cross-entropy loss L ‚Äã ( ùíõ ^ , ùíõ ) L(\\hat{\\bm{z}},\\bm{z}) italic_L ( over^ start_ARG bold_italic_z end_ARG , bold_italic_z ) can be viewed as a special measure of parsimony œÅ ‚Äã ( ùíõ ) \\rho(\\bm{z}) italic_œÅ ( bold_italic_z ) associated with a particular family of encoding schemes that are suitable for classification. However, such an encoding is obviously very lossy . The learned ùíõ \\bm{z} bold_italic_z does not contain any other information about ùíô \\bm{x} bold_italic_x except for its class type. For example, by assigning an image with (a code representing) the class label ‚Äúapple‚Äù, we no long"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.SS3.p3", "title": "Of course, the other extreme is to require the coding scheme to be lossless . That is, there is a one-to-one mapping between ùíô \\bm{x} bold_italic_x and its code ùíõ \\bm{z} bold_italic_z . However, as we", "snippet": "Of course, the other extreme is to require the coding scheme to be lossless . That is, there is a one-to-one mapping between ùíô \\bm{x} bold_italic_x and its code ùíõ \\bm{z} bold_italic_z . However, as we will see in Chapter 3 , lossless coding (or compression) is impractical unless ùíô \\bm{x} bold_italic_x is discrete. For a continuous random variable, we may only consider lossy coding schemes so that the coding length for the data can be finite. That is, we only encode the data up to a certain prescribed precision. As we will elaborate more in Chapter 3 , lossy coding is not merely a practical cho"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.SS3.p4", "title": "For many purposes of learning, we want the feature ùíõ \\bm{z} bold_italic_z , although lossy , to keep more information about ùíô \\bm{x} bold_italic_x than just its class type. In this book, we will intro", "snippet": "For many purposes of learning, we want the feature ùíõ \\bm{z} bold_italic_z , although lossy , to keep more information about ùíô \\bm{x} bold_italic_x than just its class type. In this book, we will introduce a more general measure of parsimony based on coding length/rate associated with a more general family of coding schemes ‚Äì coding with a mixture of subspaces or Gaussians. This family has the capability to closely approximate arbitrary real-world distributions up to certain precision. As we will see in Chapter 3 and Chapter 4 , such a measure will not only preserve most information about the d"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.SS3.SSS0.Px1.p1", "title": "In a broader learning context, the main goal of a compressive coding scheme ‚Ñ∞ \\mathcal{E} caligraphic_E is to identify the low-dimensional structures in the data ùëø \\bm{X} bold_italic_X so that they ca", "snippet": "In a broader learning context, the main goal of a compressive coding scheme ‚Ñ∞ \\mathcal{E} caligraphic_E is to identify the low-dimensional structures in the data ùëø \\bm{X} bold_italic_X so that they can be used to predict things in the original data space. This requires that the learned encoding scheme ‚Ñ∞ \\mathcal{E} caligraphic_E allows an efficient decoding scheme, denoted as ùíü \\mathcal{D} caligraphic_D . It maps ùíÅ \\bm{Z} bold_italic_Z , often known as a latent representation, back to the data space: ùëø ‚Üí ‚Ñ∞ ùíÅ ‚Üí ùíü ùëø ^ . \\bm{X}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}\\hskip 5.69054pt}\\bm{Z}\\xright"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.SS3.SSS0.Px1.p2", "title": "Generally, we would prefer that the decoding is approximately an ‚Äúinverse‚Äù to the encoding such that the data (distribution) ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG decoded from ùíÅ \\bm{Z", "snippet": "Generally, we would prefer that the decoding is approximately an ‚Äúinverse‚Äù to the encoding such that the data (distribution) ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG decoded from ùíÅ \\bm{Z} bold_italic_Z would be similar to the original data (distribution) ùëø \\bm{X} bold_italic_X to some extent. 53 53 53 We will make it more precise what we mean by being similar later. If so, we would be able to recover or predict from ùíÅ \\bm{Z} bold_italic_Z what is going on in the original data space. In this case, we say the pair ( ‚Ñ∞ , ùíü ) (\\mathcal{E},\\mathcal{D}) ( caligraphic_E , caligraphic_D "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.SS3.SSS0.Px1.p3", "title": "Generally speaking, as we will see, both encoder and decoder could be modeled and realized by deep networks and learned by solving an optimization problem of the following form: min ‚Å° d ‚Äã ( ùëø , ùëø ^ ) ", "snippet": "Generally speaking, as we will see, both encoder and decoder could be modeled and realized by deep networks and learned by solving an optimization problem of the following form: min ‚Å° d ‚Äã ( ùëø , ùëø ^ ) + œÅ ‚Äã ( ùíÅ ) , \\min\\,d(\\bm{X},\\hat{\\bm{X}})+\\rho(\\bm{Z}), roman_min italic_d ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG ) + italic_œÅ ( bold_italic_Z ) , (1.4.15) where d ‚Äã ( ‚ãÖ , ‚ãÖ ) d(\\cdot,\\cdot) italic_d ( ‚ãÖ , ‚ãÖ ) is a certain distance function that promotes similarity between ùëø \\bm{X} bold_italic_X and ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG 54 54 54 Either sample-wis"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.SS4.p1", "title": "Note that in the above autoencoding objective, one needs to evaluate how close or consistent the decoded data ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG is to the original ùëø \\bm{X} bold_it", "snippet": "Note that in the above autoencoding objective, one needs to evaluate how close or consistent the decoded data ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG is to the original ùëø \\bm{X} bold_italic_X . This often requires some external supervision or knowledge on what similarity measure to use. Computing similarity between ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG and ùëø \\bm{X} bold_italic_X can be very expensive, if not entirely impossible or intractable. 55 55 55 Say one wants to minimize certain distributional distance between the two. Note that in nature, animals are cap"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.SS4.p2", "title": "Then how is a system able to learn autonomously without external supervision or comparison? How can they know that ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG is consistent with ùëø \\bm{X} bo", "snippet": "Then how is a system able to learn autonomously without external supervision or comparison? How can they know that ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG is consistent with ùëø \\bm{X} bold_italic_X even without directly comparing them? That leads to the idea of ‚Äúclosing the loop‚Äù. As it turns out, under the mild conditions that we will make precise in Chapter 5 , to ensure ùëø \\bm{X} bold_italic_X and ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG are consistent, one only has to encode ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG as ùíÅ ^ \\hat{\\bm{Z}} over^ start_AR"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.SS4.p3", "title": "It is arguably true that any autonomous intelligent being only needs to learn a self-consistent representation ùíÅ \\bm{Z} bold_italic_Z of the observed data ùëø \\bm{X} bold_italic_X , because checking con", "snippet": "It is arguably true that any autonomous intelligent being only needs to learn a self-consistent representation ùíÅ \\bm{Z} bold_italic_Z of the observed data ùëø \\bm{X} bold_italic_X , because checking consistency in the original data space (often meaning in the external world) is either too expensive or even not physically feasible. The closed-loop formulation allows one to learn an optimal encoding f ‚Äã ( ‚ãÖ , Œ∏ ) f(\\cdot,\\theta) italic_f ( ‚ãÖ , italic_Œ∏ ) and decoding g ‚Äã ( ‚ãÖ , Œ∑ ) g(\\cdot,\\eta) italic_g ( ‚ãÖ , italic_Œ∑ ) via a minmax game that depends only on the internal (learned) feature ùíÅ \\bm{Z}"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S5.p1", "title": "So far, we have introduced three related frameworks for learning a compact and structured representation ùíÅ \\bm{Z} bold_italic_Z for a given data distribution ùëø \\bm{X} bold_italic_X : ‚Ä¢ The open-ended ", "snippet": "So far, we have introduced three related frameworks for learning a compact and structured representation ùíÅ \\bm{Z} bold_italic_Z for a given data distribution ùëø \\bm{X} bold_italic_X : ‚Ä¢ The open-ended encoding ( 1.4.10 ); ‚Ä¢ The bi-directional autoencoding ( 1.4.14 ); ‚Ä¢ The closed-loop transcription ( 1.4.16 ). In this book, we will systematically study all three frameworks, one after another: open-ended ‚üπ bi-directional ‚üπ closed-loop , \\mbox{{open-ended}}\\;\\Longrightarrow\\;\\mbox{{bi-directional}}\\;\\Longrightarrow\\;\\mbox{{closed-loop}}, open-ended ‚üπ bi-directional ‚üπ closed-loop , (1.5.1) in Chap"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S5.I1.i1.p1", "title": "The open-ended encoding ( 1.4.10 );", "snippet": "The open-ended encoding ( 1.4.10 );"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S5.I1.i2.p1", "title": "The bi-directional autoencoding ( 1.4.14 );", "snippet": "The bi-directional autoencoding ( 1.4.14 );"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S5.I1.i3.p1", "title": "The closed-loop transcription ( 1.4.16 ).", "snippet": "The closed-loop transcription ( 1.4.16 )."}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S5.p2", "title": "In the past few years, many theoretical frameworks have been proposed and developed to help understand deep networks. However, many were unable to provide scalable solutions that matched the performan", "snippet": "In the past few years, many theoretical frameworks have been proposed and developed to help understand deep networks. However, many were unable to provide scalable solutions that matched the performance of empirical methods on real-world data and tasks. Many theories do not provide useful guidance on how to further improve practice. Chapters 6 and 7 will show how the framework presented in this book may help bridge the gap between theory and practice. Chapter 6 will show how to use the learned distribution and its representation to conduct (Bayesian) inference for almost all practical tasks th"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S5.SS0.SSS0.Px1.p1", "title": "As we have mentioned in the beginning, a common and fundamental task of any intelligent being is to learn predictable information from its sensed data. Now we have understood a little about the comput", "snippet": "As we have mentioned in the beginning, a common and fundamental task of any intelligent being is to learn predictable information from its sensed data. Now we have understood a little about the computational nature of this task, and one should realize that this is a never-ending process, for the following reasons: ‚Ä¢ The knowledge learned so far from the data, say by the encoding and decoding schemes, is unlikely to be correct or optimal. Intelligence should have the ability to improve if there are still errors in predicting new observations. ‚Ä¢ The data observed so far do not yet cover all the "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S5.I2.i1.p1", "title": "The knowledge learned so far from the data, say by the encoding and decoding schemes, is unlikely to be correct or optimal. Intelligence should have the ability to improve if there are still errors in", "snippet": "The knowledge learned so far from the data, say by the encoding and decoding schemes, is unlikely to be correct or optimal. Intelligence should have the ability to improve if there are still errors in predicting new observations."}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S5.I2.i2.p1", "title": "The data observed so far do not yet cover all the predictable information. Intelligence should be able to recognize that current knowledge is inadequate and have the capability to learn and acquire ne", "snippet": "The data observed so far do not yet cover all the predictable information. Intelligence should be able to recognize that current knowledge is inadequate and have the capability to learn and acquire new information whenever available."}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S5.SS0.SSS0.Px1.p2", "title": "Hence, intelligence is not about simply collecting all data in advance and training a model to memorize all the predictable information in the data. In contrast, it is about being equipped with comput", "snippet": "Hence, intelligence is not about simply collecting all data in advance and training a model to memorize all the predictable information in the data. In contrast, it is about being equipped with computational mechanisms that can constantly improve current knowledge and acquire new information when available and needed. That is, a fundamental characteristic of any intelligent being or system 58 58 58 An animal, a human being, an intelligent robot, the scientific community, and even the entire civilization. is being able to continuously improve or gain information (or knowledge) on its own . Conc"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S5.SS0.SSS0.Px1.p3", "title": "We hope that this book will help people better understand the objectives, principles, and computational mechanisms behind intelligence. It serves as a foundation for further study of higher-level huma", "snippet": "We hope that this book will help people better understand the objectives, principles, and computational mechanisms behind intelligence. It serves as a foundation for further study of higher-level human intelligence, the true ‚Äúartificial‚Äù intelligence, in the future, which we will layout several significant open problems in these new directions at the end of the book in Chapter 8 ."}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#Thmexample1", "title": "Example 1.1 .", "snippet": "Example 1.1 . For example in physics, Newton‚Äôs second law of motion describes how to predict the trajectory ùíô ‚Äã ( t ) ‚àà ‚Ñù 3 \\bm{x}(t)\\in\\mathbb{R}^{3} bold_italic_x ( italic_t ) ‚àà blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT of a moving object under a force input ùë≠ ‚Äã ( t ) ‚àà ‚Ñù 3 \\bm{F}(t)\\in\\mathbb{R}^{3} bold_italic_F ( italic_t ) ‚àà blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT : m ‚Äã ùíô ¬® ‚Äã ( t ) = ùë≠ ‚Äã ( t ) . m\\ddot{\\bm{x}}(t)=\\bm{F}(t). italic_m over¬® start_ARG bold_italic_x end_ARG ( italic_t ) = bold_italic_F ( italic_t ) . (1.2.17) When there is no force ùë≠ ‚Äã ( t ) ‚â°"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#Thmremark1", "title": "Remark 1.1 .", "snippet": "Remark 1.1 . The perspective of measuring data complexity with explicit encoding schemes has motivated several learning objectives that were proposed to revise the Kolmogorov complexity for better computability [ WD99 ] , including the minimum message length (MML) proposed later in 1968 [ WB68 ] and the minimum description length (MDL) in 1978 [ Ris78 , HY01 ] . These objectives normally count the coding length for the coding scheme ‚Ñ∞ \\mathcal{E} caligraphic_E itself (including its code book) in addition to the data S S italic_S of interest: L ‚Äã ( ‚Ñ∞ ‚Äã ( S ) ) + L ‚Äã ( ‚Ñ∞ ) L(\\mathcal{E}(S))+L(\\m"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#Thmexample2", "title": "Example 1.2 .", "snippet": "Example 1.2 . For example, image classification is such a case: we assign all images in the same class to a single code and images in different classes to different codes, say ‚Äúone-hot‚Äù vectors: ùíô ‚Ü¶ ùíõ ‚àà { [ 1 , 0 , 0 , ‚Ä¶ , 0 , 0 ] , [ 0 , 1 , 0 ‚Ä¶ , 0 , 0 ] , ‚Ä¶ , [ 0 , 0 , 0 , ‚Ä¶ , 0 , 1 ] . } \\bm{x}\\mapsto\\bm{z}\\in\\{[1,0,0,\\ldots,0,0],\\;[0,1,0\\ldots,0,0],\\;\\ldots,\\;[0,0,0,\\ldots,0,1].\\} bold_italic_x ‚Ü¶ bold_italic_z ‚àà { [ 1 , 0 , 0 , ‚Ä¶ , 0 , 0 ] , [ 0 , 1 , 0 ‚Ä¶ , 0 , 0 ] , ‚Ä¶ , [ 0 , 0 , 0 , ‚Ä¶ , 0 , 1 ] . } (1.4.12) Now, a classifier f ‚Äã ( ‚ãÖ ) f(\\cdot) italic_f ( ‚ãÖ ) can be modeled as a function"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S1.E1", "title": "phylogentic ‚üπ ontogenetic ‚üπ societal ‚üπ artificial intelligence . \\mbox{{phylogentic}}\\;\\Longrightarrow\\;\\mbox{{ontogenetic}}\\;\\Longrightarrow\\;\\mbox{{societal}}\\;\\Longrightarrow\\;\\mbox{{artificial int", "snippet": "phylogentic ‚üπ ontogenetic ‚üπ societal ‚üπ artificial intelligence . \\mbox{{phylogentic}}\\;\\Longrightarrow\\;\\mbox{{ontogenetic}}\\;\\Longrightarrow\\;\\mbox{{societal}}\\;\\Longrightarrow\\;\\mbox{{artificial intelligence}}. phylogentic ‚üπ ontogenetic ‚üπ societal ‚üπ artificial intelligence . (1.1.1)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E1", "title": "S = 1 , 2 , 3 , 4 , 5 , 6 , ‚Ä¶ , n , n + 1 , ‚Ä¶ {S}=1,2,3,4,5,6,\\ldots,n,n+1,\\ldots italic_S = 1 , 2 , 3 , 4 , 5 , 6 , ‚Ä¶ , italic_n , italic_n + 1 , ‚Ä¶ (1.2.1)", "snippet": "S = 1 , 2 , 3 , 4 , 5 , 6 , ‚Ä¶ , n , n + 1 , ‚Ä¶ {S}=1,2,3,4,5,6,\\ldots,n,n+1,\\ldots italic_S = 1 , 2 , 3 , 4 , 5 , 6 , ‚Ä¶ , italic_n , italic_n + 1 , ‚Ä¶ (1.2.1)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E2", "title": "x n + 1 = x n + 1 . x_{n+1}=x_{n}+1. italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT + 1 . (1.2.2)", "snippet": "x n + 1 = x n + 1 . x_{n+1}=x_{n}+1. italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT + 1 . (1.2.2)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E3", "title": "x n + 1 = f ‚Äã ( x n ) , x n ‚àà ‚Ñù , n = 1 , 2 , 3 , ‚Ä¶ x_{n+1}=f(x_{n}),\\quad x_{n}\\in\\mathbb{R},\\;n=1,2,3,\\ldots italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT = italic_f ( italic_x start_P", "snippet": "x n + 1 = f ‚Äã ( x n ) , x n ‚àà ‚Ñù , n = 1 , 2 , 3 , ‚Ä¶ x_{n+1}=f(x_{n}),\\quad x_{n}\\in\\mathbb{R},\\;n=1,2,3,\\ldots italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT = italic_f ( italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ‚àà blackboard_R , italic_n = 1 , 2 , 3 , ‚Ä¶ (1.2.3)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E4", "title": "S = 1 , 1 , 2 , 3 , 5 , 8 , 13 , 21 , 34 , 55 , ‚Ä¶ {S}=1,1,2,3,5,8,13,21,34,55,\\ldots italic_S = 1 , 1 , 2 , 3 , 5 , 8 , 13 , 21 , 34 , 55 , ‚Ä¶ (1.2.4)", "snippet": "S = 1 , 1 , 2 , 3 , 5 , 8 , 13 , 21 , 34 , 55 , ‚Ä¶ {S}=1,1,2,3,5,8,13,21,34,55,\\ldots italic_S = 1 , 1 , 2 , 3 , 5 , 8 , 13 , 21 , 34 , 55 , ‚Ä¶ (1.2.4)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E5", "title": "x n + 2 = x n + 1 + x n , x n ‚àà ‚Ñù , n = 1 , 2 , 3 , ‚Ä¶ x_{n+2}=x_{n+1}+x_{n},\\quad x_{n}\\in\\mathbb{R},\\;n=1,2,3,\\ldots italic_x start_POSTSUBSCRIPT italic_n + 2 end_POSTSUBSCRIPT = italic_x start_POSTS", "snippet": "x n + 2 = x n + 1 + x n , x n ‚àà ‚Ñù , n = 1 , 2 , 3 , ‚Ä¶ x_{n+2}=x_{n+1}+x_{n},\\quad x_{n}\\in\\mathbb{R},\\;n=1,2,3,\\ldots italic_x start_POSTSUBSCRIPT italic_n + 2 end_POSTSUBSCRIPT = italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT + italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ‚àà blackboard_R , italic_n = 1 , 2 , 3 , ‚Ä¶ (1.2.5)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E6", "title": "x n + 2 = f ‚Äã ( x n + 1 , x n ) , x n ‚àà ‚Ñù , n = 1 , 2 , 3 , ‚Ä¶ x_{n+2}=f(x_{n+1},x_{n}),\\quad x_{n}\\in\\mathbb{R},\\;n=1,2,3,\\ldots italic_x start_POSTSUBSCRIPT italic_n + 2 end_POSTSUBSCRIPT = italic_f ", "snippet": "x n + 2 = f ‚Äã ( x n + 1 , x n ) , x n ‚àà ‚Ñù , n = 1 , 2 , 3 , ‚Ä¶ x_{n+2}=f(x_{n+1},x_{n}),\\quad x_{n}\\in\\mathbb{R},\\;n=1,2,3,\\ldots italic_x start_POSTSUBSCRIPT italic_n + 2 end_POSTSUBSCRIPT = italic_f ( italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ‚àà blackboard_R , italic_n = 1 , 2 , 3 , ‚Ä¶ (1.2.6)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E7", "title": "x n + d = f ‚Äã ( x n + d ‚àí 1 , ‚Ä¶ , x n ) , x n ‚àà ‚Ñù , n = 1 , 2 , 3 , ‚Ä¶ x_{n+d}=f(x_{n+d-1},\\ldots,x_{n}),\\quad x_{n}\\in\\mathbb{R},\\;n=1,2,3,\\ldots italic_x start_POSTSUBSCRIPT italic_n + italic_d end_P", "snippet": "x n + d = f ‚Äã ( x n + d ‚àí 1 , ‚Ä¶ , x n ) , x n ‚àà ‚Ñù , n = 1 , 2 , 3 , ‚Ä¶ x_{n+d}=f(x_{n+d-1},\\ldots,x_{n}),\\quad x_{n}\\in\\mathbb{R},\\;n=1,2,3,\\ldots italic_x start_POSTSUBSCRIPT italic_n + italic_d end_POSTSUBSCRIPT = italic_f ( italic_x start_POSTSUBSCRIPT italic_n + italic_d - 1 end_POSTSUBSCRIPT , ‚Ä¶ , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ‚àà blackboard_R , italic_n = 1 , 2 , 3 , ‚Ä¶ (1.2.7)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E8", "title": "ùíô n ‚âê [ x n + d ‚àí 1 , ‚Ä¶ , x n ] ‚ä§ , ùíô n ‚àà ‚Ñù d , n = 1 , 2 , 3 , ‚Ä¶ \\bm{x}_{n}\\doteq[x_{n+d-1},\\ldots,x_{n}]^{\\top},\\quad\\bm{x}_{n}\\in\\mathbb{R}^{d},\\;n=1,2,3,\\ldots bold_italic_x start_POSTSUBSCRIPT it", "snippet": "ùíô n ‚âê [ x n + d ‚àí 1 , ‚Ä¶ , x n ] ‚ä§ , ùíô n ‚àà ‚Ñù d , n = 1 , 2 , 3 , ‚Ä¶ \\bm{x}_{n}\\doteq[x_{n+d-1},\\ldots,x_{n}]^{\\top},\\quad\\bm{x}_{n}\\in\\mathbb{R}^{d},\\;n=1,2,3,\\ldots bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ‚âê [ italic_x start_POSTSUBSCRIPT italic_n + italic_d - 1 end_POSTSUBSCRIPT , ‚Ä¶ , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ] start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT , bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , italic_n = 1 , 2 , 3 , ‚Ä¶ (1.2.8)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E9", "title": "ùíô n + 1 = g ‚Äã ( ùíô n ) ‚àà ‚Ñù d , n = 1 , 2 , 3 , ‚Ä¶ \\bm{x}_{n+1}=g(\\bm{x}_{n})\\;\\in\\mathbb{R}^{d},\\quad n=1,2,3,\\ldots bold_italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT = italic_g ( bold_it", "snippet": "ùíô n + 1 = g ‚Äã ( ùíô n ) ‚àà ‚Ñù d , n = 1 , 2 , 3 , ‚Ä¶ \\bm{x}_{n+1}=g(\\bm{x}_{n})\\;\\in\\mathbb{R}^{d},\\quad n=1,2,3,\\ldots bold_italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT = italic_g ( bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , italic_n = 1 , 2 , 3 , ‚Ä¶ (1.2.9)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E10", "title": "ùíô n + 1 = f ‚Äã ( ùíô n , ùíñ n ) ‚àà ‚Ñù d , n = 1 , 2 , 3 , ‚Ä¶ , \\bm{x}_{n+1}=f(\\bm{x}_{n},\\bm{u}_{n})\\;\\in\\mathbb{R}^{d},\\quad n=1,2,3,\\ldots, bold_italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT ", "snippet": "ùíô n + 1 = f ‚Äã ( ùíô n , ùíñ n ) ‚àà ‚Ñù d , n = 1 , 2 , 3 , ‚Ä¶ , \\bm{x}_{n+1}=f(\\bm{x}_{n},\\bm{u}_{n})\\;\\in\\mathbb{R}^{d},\\quad n=1,2,3,\\ldots, bold_italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT = italic_f ( bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , bold_italic_u start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , italic_n = 1 , 2 , 3 , ‚Ä¶ , (1.2.10)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E11", "title": "ùíô n + 1 = ùë® ‚Äã ùíô n + ùë© ‚Äã ùíñ n , ùë® ‚àà ‚Ñù d √ó d , ùë© ‚àà ‚Ñù d √ó k , \\bm{x}_{n+1}=\\bm{A}\\bm{x}_{n}+\\bm{B}\\bm{u}_{n},\\quad\\bm{A}\\in\\mathbb{R}^{d\\times d},\\bm{B}\\in\\mathbb{R}^{d\\times k}, bold_italic_x start_POSTS", "snippet": "ùíô n + 1 = ùë® ‚Äã ùíô n + ùë© ‚Äã ùíñ n , ùë® ‚àà ‚Ñù d √ó d , ùë© ‚àà ‚Ñù d √ó k , \\bm{x}_{n+1}=\\bm{A}\\bm{x}_{n}+\\bm{B}\\bm{u}_{n},\\quad\\bm{A}\\in\\mathbb{R}^{d\\times d},\\bm{B}\\in\\mathbb{R}^{d\\times k}, bold_italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT = bold_italic_A bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT + bold_italic_B bold_italic_u start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , bold_italic_A ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d √ó italic_d end_POSTSUPERSCRIPT , bold_italic_B ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d √ó italic_k end_POSTSUPERSCRIPT , (1.2.11)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E12", "title": "ùíñ n = h ‚Äã ( ùíô n ) , n = 1 , 2 , 3 , ‚Ä¶ \\bm{u}_{n}=h(\\bm{x}_{n}),\\quad n=1,2,3,\\ldots bold_italic_u start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT = italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_n", "snippet": "ùíñ n = h ‚Äã ( ùíô n ) , n = 1 , 2 , 3 , ‚Ä¶ \\bm{u}_{n}=h(\\bm{x}_{n}),\\quad n=1,2,3,\\ldots bold_italic_u start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT = italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) , italic_n = 1 , 2 , 3 , ‚Ä¶ (1.2.12)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E13", "title": "ùíô n + 1 = f ‚Äã ( ùíô n , h ‚Äã ( ùíô n ) ) , n = 1 , 2 , 3 , ‚Ä¶ \\bm{x}_{n+1}=f\\big{(}\\bm{x}_{n},h(\\bm{x}_{n})\\big{)},\\quad n=1,2,3,\\ldots bold_italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT = ita", "snippet": "ùíô n + 1 = f ‚Äã ( ùíô n , h ‚Äã ( ùíô n ) ) , n = 1 , 2 , 3 , ‚Ä¶ \\bm{x}_{n+1}=f\\big{(}\\bm{x}_{n},h(\\bm{x}_{n})\\big{)},\\quad n=1,2,3,\\ldots bold_italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT = italic_f ( bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) ) , italic_n = 1 , 2 , 3 , ‚Ä¶ (1.2.13)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E14", "title": "ùíô n + 1 = ùë® ‚Äã ùíô n + ùë© ‚Äã ùíñ n = ùë® ‚Äã ùíô n + ùë© ‚Äã ùë≠ ‚Äã ùíô n = ( ùë® + ùë© ‚Äã ùë≠ ) ‚Äã ùíô n , \\bm{x}_{n+1}=\\bm{A}\\bm{x}_{n}+\\bm{B}\\bm{u}_{n}=\\bm{A}\\bm{x}_{n}+\\bm{B}\\bm{F}\\bm{x}_{n}=(\\bm{A}+\\bm{B}\\bm{F})\\bm{x}_{n}, bold", "snippet": "ùíô n + 1 = ùë® ‚Äã ùíô n + ùë© ‚Äã ùíñ n = ùë® ‚Äã ùíô n + ùë© ‚Äã ùë≠ ‚Äã ùíô n = ( ùë® + ùë© ‚Äã ùë≠ ) ‚Äã ùíô n , \\bm{x}_{n+1}=\\bm{A}\\bm{x}_{n}+\\bm{B}\\bm{u}_{n}=\\bm{A}\\bm{x}_{n}+\\bm{B}\\bm{F}\\bm{x}_{n}=(\\bm{A}+\\bm{B}\\bm{F})\\bm{x}_{n}, bold_italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT = bold_italic_A bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT + bold_italic_B bold_italic_u start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT = bold_italic_A bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT + bold_italic_B bold_italic_F bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT = ( bold_italic_A"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E15", "title": "ùíô Àô ‚Äã ( t ) = f ‚Äã ( ùíô ‚Äã ( t ) ) , ùíô ‚àà ‚Ñù d . \\dot{\\bm{x}}(t)=f(\\bm{x}(t)),\\quad\\bm{x}\\in\\mathbb{R}^{d}. overÀô start_ARG bold_italic_x end_ARG ( italic_t ) = italic_f ( bold_italic_x ( italic_t ) ) , bo", "snippet": "ùíô Àô ‚Äã ( t ) = f ‚Äã ( ùíô ‚Äã ( t ) ) , ùíô ‚àà ‚Ñù d . \\dot{\\bm{x}}(t)=f(\\bm{x}(t)),\\quad\\bm{x}\\in\\mathbb{R}^{d}. overÀô start_ARG bold_italic_x end_ARG ( italic_t ) = italic_f ( bold_italic_x ( italic_t ) ) , bold_italic_x ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT . (1.2.15)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E16", "title": "ùíô Àô ‚Äã ( t ) = f ‚Äã ( ùíô ‚Äã ( t ) , ùíñ ‚Äã ( t ) ) , ùíô ‚àà ‚Ñù d , ùíñ ‚àà ‚Ñù k , \\dot{\\bm{x}}(t)=f(\\bm{x}(t),\\bm{u}(t)),\\quad\\bm{x}\\in\\mathbb{R}^{d},\\bm{u}\\in\\mathbb{R}^{k}, overÀô start_ARG bold_italic_x end_ARG ( i", "snippet": "ùíô Àô ‚Äã ( t ) = f ‚Äã ( ùíô ‚Äã ( t ) , ùíñ ‚Äã ( t ) ) , ùíô ‚àà ‚Ñù d , ùíñ ‚àà ‚Ñù k , \\dot{\\bm{x}}(t)=f(\\bm{x}(t),\\bm{u}(t)),\\quad\\bm{x}\\in\\mathbb{R}^{d},\\bm{u}\\in\\mathbb{R}^{k}, overÀô start_ARG bold_italic_x end_ARG ( italic_t ) = italic_f ( bold_italic_x ( italic_t ) , bold_italic_u ( italic_t ) ) , bold_italic_x ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , bold_italic_u ‚àà blackboard_R start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT , (1.2.16)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E17", "title": "m ‚Äã ùíô ¬® ‚Äã ( t ) = ùë≠ ‚Äã ( t ) . m\\ddot{\\bm{x}}(t)=\\bm{F}(t). italic_m over¬® start_ARG bold_italic_x end_ARG ( italic_t ) = bold_italic_F ( italic_t ) . (1.2.17)", "snippet": "m ‚Äã ùíô ¬® ‚Äã ( t ) = ùë≠ ‚Äã ( t ) . m\\ddot{\\bm{x}}(t)=\\bm{F}(t). italic_m over¬® start_ARG bold_italic_x end_ARG ( italic_t ) = bold_italic_F ( italic_t ) . (1.2.17)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E18", "title": "ùíô ¬® ‚Äã ( t ) = ùüé ‚áî ùíô Àô ‚Äã ( t ) = ùíó \\ddot{\\bm{x}}(t)=\\bm{0}\\;\\Leftrightarrow\\;\\dot{\\bm{x}}(t)=\\bm{v} over¬® start_ARG bold_italic_x end_ARG ( italic_t ) = bold_0 ‚áî overÀô start_ARG bold_italic_x end_ARG (", "snippet": "ùíô ¬® ‚Äã ( t ) = ùüé ‚áî ùíô Àô ‚Äã ( t ) = ùíó \\ddot{\\bm{x}}(t)=\\bm{0}\\;\\Leftrightarrow\\;\\dot{\\bm{x}}(t)=\\bm{v} over¬® start_ARG bold_italic_x end_ARG ( italic_t ) = bold_0 ‚áî overÀô start_ARG bold_italic_x end_ARG ( italic_t ) = bold_italic_v (1.2.18)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E19", "title": "{ S 1 , S 2 , ‚Ä¶ , S i , ‚Ä¶ , S N } \\{S_{1},S_{2},\\ldots,S_{i},\\ldots,S_{N}\\} { italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ‚Ä¶ , italic_S start_P", "snippet": "{ S 1 , S 2 , ‚Ä¶ , S i , ‚Ä¶ , S N } \\{S_{1},S_{2},\\ldots,S_{i},\\ldots,S_{N}\\} { italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ‚Ä¶ , italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , ‚Ä¶ , italic_S start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT } (1.2.19)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E20", "title": "S i = [ x j ‚Äã ( i ) , x j ‚Äã ( i ) + 1 , ‚Ä¶ , x j ‚Äã ( i ) + D ‚àí 1 ] ‚ä§ ‚àà ‚Ñù D S_{i}=[x_{j(i)},x_{j(i)+1},\\ldots,x_{j(i)+D-1}]^{\\top}\\in\\mathbb{R}^{D} italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIP", "snippet": "S i = [ x j ‚Äã ( i ) , x j ‚Äã ( i ) + 1 , ‚Ä¶ , x j ‚Äã ( i ) + D ‚àí 1 ] ‚ä§ ‚àà ‚Ñù D S_{i}=[x_{j(i)},x_{j(i)+1},\\ldots,x_{j(i)+D-1}]^{\\top}\\in\\mathbb{R}^{D} italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = [ italic_x start_POSTSUBSCRIPT italic_j ( italic_i ) end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_j ( italic_i ) + 1 end_POSTSUBSCRIPT , ‚Ä¶ , italic_x start_POSTSUBSCRIPT italic_j ( italic_i ) + italic_D - 1 end_POSTSUBSCRIPT ] start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT (1.2.20)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E21", "title": "x n + d = f ‚Äã ( x n + d ‚àí 1 , ‚Ä¶ , x n ) . x_{n+d}=f(x_{n+d-1},\\ldots,x_{n}). italic_x start_POSTSUBSCRIPT italic_n + italic_d end_POSTSUBSCRIPT = italic_f ( italic_x start_POSTSUBSCRIPT italic_n + ita", "snippet": "x n + d = f ‚Äã ( x n + d ‚àí 1 , ‚Ä¶ , x n ) . x_{n+d}=f(x_{n+d-1},\\ldots,x_{n}). italic_x start_POSTSUBSCRIPT italic_n + italic_d end_POSTSUBSCRIPT = italic_f ( italic_x start_POSTSUBSCRIPT italic_n + italic_d - 1 end_POSTSUBSCRIPT , ‚Ä¶ , italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) . (1.2.21)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E22", "title": "ùíô i = [ x i , x i + 1 , ‚Ä¶ ‚Äã x i + D ‚àí 1 ] ‚ä§ ‚àà ‚Ñù D . \\bm{x}_{i}=[x_{i},x_{i+1},\\ldots x_{i+D-1}]^{\\top}\\in\\mathbb{R}^{D}. bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = [ italic_x start", "snippet": "ùíô i = [ x i , x i + 1 , ‚Ä¶ ‚Äã x i + D ‚àí 1 ] ‚ä§ ‚àà ‚Ñù D . \\bm{x}_{i}=[x_{i},x_{i+1},\\ldots x_{i+D-1}]^{\\top}\\in\\mathbb{R}^{D}. bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = [ italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT , ‚Ä¶ italic_x start_POSTSUBSCRIPT italic_i + italic_D - 1 end_POSTSUBSCRIPT ] start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT . (1.2.22)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E23", "title": "x n + 2 = a ‚ãÖ x n + 1 + b ‚ãÖ x n , x_{n+2}=a\\cdot x_{n+1}+b\\cdot x_{n}, italic_x start_POSTSUBSCRIPT italic_n + 2 end_POSTSUBSCRIPT = italic_a ‚ãÖ italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCR", "snippet": "x n + 2 = a ‚ãÖ x n + 1 + b ‚ãÖ x n , x_{n+2}=a\\cdot x_{n+1}+b\\cdot x_{n}, italic_x start_POSTSUBSCRIPT italic_n + 2 end_POSTSUBSCRIPT = italic_a ‚ãÖ italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT + italic_b ‚ãÖ italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , (1.2.23)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S2.E24", "title": "ùíö = f ‚Äã ( ùíô ) + ùíè , \\bm{y}=f(\\bm{x})+\\bm{n}, bold_italic_y = italic_f ( bold_italic_x ) + bold_italic_n , (1.2.24)", "snippet": "ùíö = f ‚Äã ( ùíô ) + ùíè , \\bm{y}=f(\\bm{x})+\\bm{n}, bold_italic_y = italic_f ( bold_italic_x ) + bold_italic_n , (1.2.24)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E1", "title": "x ‚Äã [ n ] = h ‚Äã [ n ] ‚àó z ‚Äã [ n ] + œµ ‚Äã [ n ] , x[n]=h[n]*z[n]+\\epsilon[n], italic_x [ italic_n ] = italic_h [ italic_n ] ‚àó italic_z [ italic_n ] + italic_œµ [ italic_n ] , (1.3.1)", "snippet": "x ‚Äã [ n ] = h ‚Äã [ n ] ‚àó z ‚Äã [ n ] + œµ ‚Äã [ n ] , x[n]=h[n]*z[n]+\\epsilon[n], italic_x [ italic_n ] = italic_h [ italic_n ] ‚àó italic_z [ italic_n ] + italic_œµ [ italic_n ] , (1.3.1)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E2", "title": "min h ‚Å° ùîº ‚Äã [ œµ ‚Äã [ n ] 2 ] = ùîº ‚Äã [ ‚Äñ x ‚Äã [ n ] ‚àí h ‚Äã [ n ] ‚àó z ‚Äã [ n ] ‚Äñ 2 2 ] . \\min_{h}\\mathbb{E}\\big{[}\\epsilon[n]^{2}\\big{]}=\\mathbb{E}\\big{[}\\|x[n]-h[n]*z[n]\\|_{2}^{2}\\big{]}. roman_min start_PO", "snippet": "min h ‚Å° ùîº ‚Äã [ œµ ‚Äã [ n ] 2 ] = ùîº ‚Äã [ ‚Äñ x ‚Äã [ n ] ‚àí h ‚Äã [ n ] ‚àó z ‚Äã [ n ] ‚Äñ 2 2 ] . \\min_{h}\\mathbb{E}\\big{[}\\epsilon[n]^{2}\\big{]}=\\mathbb{E}\\big{[}\\|x[n]-h[n]*z[n]\\|_{2}^{2}\\big{]}. roman_min start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT blackboard_E [ italic_œµ [ italic_n ] start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] = blackboard_E [ ‚à• italic_x [ italic_n ] - italic_h [ italic_n ] ‚àó italic_z [ italic_n ] ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] . (1.3.2)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E3", "title": "ùíõ ‚Äã [ n ] = ùë® ‚Äã ùíõ ‚Äã [ n ‚àí 1 ] + ùë© ‚Äã ùíñ ‚Äã [ n ] + œµ ‚Äã [ n ] . \\bm{z}[n]=\\bm{A}\\bm{z}[n-1]+\\bm{B}\\bm{u}[n]+\\bm{\\epsilon}[n]. bold_italic_z [ italic_n ] = bold_italic_A bold_italic_z [ italic_n - 1 ] + bo", "snippet": "ùíõ ‚Äã [ n ] = ùë® ‚Äã ùíõ ‚Äã [ n ‚àí 1 ] + ùë© ‚Äã ùíñ ‚Äã [ n ] + œµ ‚Äã [ n ] . \\bm{z}[n]=\\bm{A}\\bm{z}[n-1]+\\bm{B}\\bm{u}[n]+\\bm{\\epsilon}[n]. bold_italic_z [ italic_n ] = bold_italic_A bold_italic_z [ italic_n - 1 ] + bold_italic_B bold_italic_u [ italic_n ] + bold_italic_œµ [ italic_n ] . (1.3.3)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E4", "title": "ùíô ‚Äã [ n ] = ùë™ ‚Äã ùíõ ‚Äã [ n ] + ùíò ‚Äã [ n ] , \\bm{x}[n]=\\bm{C}\\bm{z}[n]+\\bm{w}[n], bold_italic_x [ italic_n ] = bold_italic_C bold_italic_z [ italic_n ] + bold_italic_w [ italic_n ] , (1.3.4)", "snippet": "ùíô ‚Äã [ n ] = ùë™ ‚Äã ùíõ ‚Äã [ n ] + ùíò ‚Äã [ n ] , \\bm{x}[n]=\\bm{C}\\bm{z}[n]+\\bm{w}[n], bold_italic_x [ italic_n ] = bold_italic_C bold_italic_z [ italic_n ] + bold_italic_w [ italic_n ] , (1.3.4)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E5", "title": "min ‚Å° ùîº ‚Äã [ ‚Äñ ùíô ‚Äã [ n ] ‚àí ùë™ ‚Äã ùíõ ‚Äã [ n ] ‚Äñ 2 2 ] \\min\\mathbb{E}\\big{[}\\|\\bm{x}[n]-\\bm{C}\\bm{z}[n]\\|_{2}^{2}\\big{]} roman_min blackboard_E [ ‚à• bold_italic_x [ italic_n ] - bold_italic_C bold_italic_z [ ", "snippet": "min ‚Å° ùîº ‚Äã [ ‚Äñ ùíô ‚Äã [ n ] ‚àí ùë™ ‚Äã ùíõ ‚Äã [ n ] ‚Äñ 2 2 ] \\min\\mathbb{E}\\big{[}\\|\\bm{x}[n]-\\bm{C}\\bm{z}[n]\\|_{2}^{2}\\big{]} roman_min blackboard_E [ ‚à• bold_italic_x [ italic_n ] - bold_italic_C bold_italic_z [ italic_n ] ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] (1.3.5)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E6", "title": "ùíô = ùíô o + œµ , ùíô o ‚àº S , \\bm{x}=\\bm{x}_{o}+\\bm{\\epsilon},\\quad\\bm{x}_{o}\\sim S, bold_italic_x = bold_italic_x start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT + bold_italic_œµ , bold_italic_x start_POSTSU", "snippet": "ùíô = ùíô o + œµ , ùíô o ‚àº S , \\bm{x}=\\bm{x}_{o}+\\bm{\\epsilon},\\quad\\bm{x}_{o}\\sim S, bold_italic_x = bold_italic_x start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT + bold_italic_œµ , bold_italic_x start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT ‚àº italic_S , (1.3.6)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E7", "title": "ùíô = ùíñ 1 ‚Äã z 1 + ùíñ 2 ‚Äã z 2 + ‚ãØ + ùíñ d ‚Äã z d + œµ = ùëº ‚Äã ùíõ + œµ , ùëº ‚àà ‚Ñù D √ó d \\bm{x}=\\bm{u}_{1}z_{1}+\\bm{u}_{2}z_{2}+\\cdots+\\bm{u}_{d}z_{d}+\\bm{\\epsilon}=\\bm{U}\\bm{z}+\\bm{\\epsilon},\\quad\\bm{U}\\in\\mathbb{R}^", "snippet": "ùíô = ùíñ 1 ‚Äã z 1 + ùíñ 2 ‚Äã z 2 + ‚ãØ + ùíñ d ‚Äã z d + œµ = ùëº ‚Äã ùíõ + œµ , ùëº ‚àà ‚Ñù D √ó d \\bm{x}=\\bm{u}_{1}z_{1}+\\bm{u}_{2}z_{2}+\\cdots+\\bm{u}_{d}z_{d}+\\bm{\\epsilon}=\\bm{U}\\bm{z}+\\bm{\\epsilon},\\quad\\bm{U}\\in\\mathbb{R}^{D\\times d} bold_italic_x = bold_italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + bold_italic_u start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_z start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + ‚ãØ + bold_italic_u start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT + bold_italic_œµ = bold_italic_U bold_ital"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E8", "title": "min ùëº ‚Å° ùîº ‚Äã [ ‚Äñ œµ ‚Äñ 2 2 ] = ùîº ‚Äã [ ‚Äñ ùíô ‚àí ùëº ‚Äã ùíõ ‚Äñ 2 2 ] . \\min_{\\bm{U}}\\mathbb{E}\\big{[}\\|\\bm{\\epsilon}\\|_{2}^{2}\\big{]}=\\mathbb{E}\\big{[}\\|\\bm{x}-\\bm{U}\\bm{z}\\|_{2}^{2}\\big{]}. roman_min start_POSTSUBS", "snippet": "min ùëº ‚Å° ùîº ‚Äã [ ‚Äñ œµ ‚Äñ 2 2 ] = ùîº ‚Äã [ ‚Äñ ùíô ‚àí ùëº ‚Äã ùíõ ‚Äñ 2 2 ] . \\min_{\\bm{U}}\\mathbb{E}\\big{[}\\|\\bm{\\epsilon}\\|_{2}^{2}\\big{]}=\\mathbb{E}\\big{[}\\|\\bm{x}-\\bm{U}\\bm{z}\\|_{2}^{2}\\big{]}. roman_min start_POSTSUBSCRIPT bold_italic_U end_POSTSUBSCRIPT blackboard_E [ ‚à• bold_italic_œµ ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] = blackboard_E [ ‚à• bold_italic_x - bold_italic_U bold_italic_z ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] . (1.3.8)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E9", "title": "ùíô ‚Üí ùíô ^ = ùëº ‚Äã ùëº ‚ä§ ‚Äã ùíô . \\bm{x}\\rightarrow\\hat{\\bm{x}}=\\bm{U}\\bm{U}^{\\top}\\bm{x}. bold_italic_x ‚Üí over^ start_ARG bold_italic_x end_ARG = bold_italic_U bold_italic_U start_POSTSUPERSCRIPT ‚ä§ end_POSTSUP", "snippet": "ùíô ‚Üí ùíô ^ = ùëº ‚Äã ùëº ‚ä§ ‚Äã ùíô . \\bm{x}\\rightarrow\\hat{\\bm{x}}=\\bm{U}\\bm{U}^{\\top}\\bm{x}. bold_italic_x ‚Üí over^ start_ARG bold_italic_x end_ARG = bold_italic_U bold_italic_U start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_x . (1.3.9)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E10", "title": "ùíô ‚Üí ùëº ‚ä§ ùíõ ‚Üí ùëº ùíô ^ . \\bm{x}\\xrightarrow{\\hskip 5.69054pt\\bm{U}^{\\top}\\hskip 5.69054pt}\\bm{z}\\xrightarrow{\\hskip 5.69054pt\\bm{U}\\hskip 5.69054pt}\\hat{\\bm{x}}. bold_italic_x start_ARROW start_OVERACCENT ", "snippet": "ùíô ‚Üí ùëº ‚ä§ ùíõ ‚Üí ùëº ùíô ^ . \\bm{x}\\xrightarrow{\\hskip 5.69054pt\\bm{U}^{\\top}\\hskip 5.69054pt}\\bm{z}\\xrightarrow{\\hskip 5.69054pt\\bm{U}\\hskip 5.69054pt}\\hat{\\bm{x}}. bold_italic_x start_ARROW start_OVERACCENT bold_italic_U start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT end_OVERACCENT ‚Üí end_ARROW bold_italic_z start_ARROW start_OVERACCENT bold_italic_U end_OVERACCENT ‚Üí end_ARROW over^ start_ARG bold_italic_x end_ARG . (1.3.10)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E11", "title": "ùíô ‚àº ùí© ‚Äã ( ùüé , ùëº ‚Äã ùëº ‚ä§ + œÉ ‚Äã ùë∞ ) , ùëº ‚àà ‚Ñù D √ó d , \\bm{x}\\sim\\mathcal{N}(\\bm{0},\\bm{U}\\bm{U}^{\\top}+\\sigma\\bm{I}),\\quad\\bm{U}\\in\\mathbb{R}^{D\\times d}, bold_italic_x ‚àº caligraphic_N ( bold_0 , bold_itali", "snippet": "ùíô ‚àº ùí© ‚Äã ( ùüé , ùëº ‚Äã ùëº ‚ä§ + œÉ ‚Äã ùë∞ ) , ùëº ‚àà ‚Ñù D √ó d , \\bm{x}\\sim\\mathcal{N}(\\bm{0},\\bm{U}\\bm{U}^{\\top}+\\sigma\\bm{I}),\\quad\\bm{U}\\in\\mathbb{R}^{D\\times d}, bold_italic_x ‚àº caligraphic_N ( bold_0 , bold_italic_U bold_italic_U start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT + italic_œÉ bold_italic_I ) , bold_italic_U ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D √ó italic_d end_POSTSUPERSCRIPT , (1.3.11)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E12", "title": "ùíô = ùíñ 1 ‚Äã z 1 + ùíñ 2 ‚Äã z 2 + ‚ãØ + ùíñ d ‚Äã z d + œµ = ùëº ‚Äã ùíõ + œµ . \\bm{x}=\\bm{u}_{1}z_{1}+\\bm{u}_{2}z_{2}+\\cdots+\\bm{u}_{d}z_{d}+\\bm{\\epsilon}=\\bm{U}\\bm{z}+\\bm{\\epsilon}. bold_italic_x = bold_italic_u start_", "snippet": "ùíô = ùíñ 1 ‚Äã z 1 + ùíñ 2 ‚Äã z 2 + ‚ãØ + ùíñ d ‚Äã z d + œµ = ùëº ‚Äã ùíõ + œµ . \\bm{x}=\\bm{u}_{1}z_{1}+\\bm{u}_{2}z_{2}+\\cdots+\\bm{u}_{d}z_{d}+\\bm{\\epsilon}=\\bm{U}\\bm{z}+\\bm{\\epsilon}. bold_italic_x = bold_italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + bold_italic_u start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_z start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + ‚ãØ + bold_italic_u start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT + bold_italic_œµ = bold_italic_U bold_italic_z + bold_italic_œµ . (1.3.12)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E13", "title": "z i = œÉ i ‚ãÖ w i , œÉ i ‚àº B ‚Äã ( 1 , p ) , z_{i}=\\sigma_{i}\\cdot w_{i},\\quad\\sigma_{i}\\sim B(1,p), italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_œÉ start_POSTSUBSCRIPT italic_i end_POST", "snippet": "z i = œÉ i ‚ãÖ w i , œÉ i ‚àº B ‚Äã ( 1 , p ) , z_{i}=\\sigma_{i}\\cdot w_{i},\\quad\\sigma_{i}\\sim B(1,p), italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_œÉ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ‚ãÖ italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_œÉ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ‚àº italic_B ( 1 , italic_p ) , (1.3.13)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E14", "title": "ùíô ‚Üí ‚Ñ∞ ùíõ ‚Üí ùëº ùíô ^ . \\bm{x}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}\\hskip 5.69054pt}\\bm{z}\\xrightarrow{\\hskip 5.69054pt\\bm{U}\\hskip 5.69054pt}\\hat{\\bm{x}}. bold_italic_x start_ARROW start_OVERACCENT cali", "snippet": "ùíô ‚Üí ‚Ñ∞ ùíõ ‚Üí ùëº ùíô ^ . \\bm{x}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}\\hskip 5.69054pt}\\bm{z}\\xrightarrow{\\hskip 5.69054pt\\bm{U}\\hskip 5.69054pt}\\hat{\\bm{x}}. bold_italic_x start_ARROW start_OVERACCENT caligraphic_E end_OVERACCENT ‚Üí end_ARROW bold_italic_z start_ARROW start_OVERACCENT bold_italic_U end_OVERACCENT ‚Üí end_ARROW over^ start_ARG bold_italic_x end_ARG . (1.3.14)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E15", "title": "ùíµ = { ùíõ ‚àà ‚Ñù n ‚à£ ‚Äñ ùíõ ‚Äñ 0 ‚â§ k } , \\mathcal{Z}=\\{\\bm{z}\\in\\mathbb{R}^{n}\\mid\\|\\bm{z}\\|_{0}\\leq k\\}, caligraphic_Z = { bold_italic_z ‚àà blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ‚à£ ‚à• b", "snippet": "ùíµ = { ùíõ ‚àà ‚Ñù n ‚à£ ‚Äñ ùíõ ‚Äñ 0 ‚â§ k } , \\mathcal{Z}=\\{\\bm{z}\\in\\mathbb{R}^{n}\\mid\\|\\bm{z}\\|_{0}\\leq k\\}, caligraphic_Z = { bold_italic_z ‚àà blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ‚à£ ‚à• bold_italic_z ‚à• start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ‚â§ italic_k } , (1.3.15)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E16", "title": "ùíô = ùë® ‚Äã ùíõ + œµ , ùë® ‚àà ‚Ñù m √ó n \\bm{x}=\\bm{A}\\bm{z}+\\bm{\\epsilon},\\quad\\bm{A}\\in\\mathbb{R}^{m\\times n} bold_italic_x = bold_italic_A bold_italic_z + bold_italic_œµ , bold_italic_A ‚àà blackboard_R start_POST", "snippet": "ùíô = ùë® ‚Äã ùíõ + œµ , ùë® ‚àà ‚Ñù m √ó n \\bm{x}=\\bm{A}\\bm{z}+\\bm{\\epsilon},\\quad\\bm{A}\\in\\mathbb{R}^{m\\times n} bold_italic_x = bold_italic_A bold_italic_z + bold_italic_œµ , bold_italic_A ‚àà blackboard_R start_POSTSUPERSCRIPT italic_m √ó italic_n end_POSTSUPERSCRIPT (1.3.16)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E17", "title": "min ‚Å° ‚Äñ ùíõ ‚Äñ 1 subject to ‚Äñ ùíô ‚àí ùë® ‚Äã ùíõ ‚Äñ 2 ‚â§ œµ , \\min\\|\\bm{z}\\|_{1}\\quad\\mbox{subject to}\\quad\\|\\bm{x}-\\bm{A}\\bm{z}\\|_{2}\\leq\\epsilon, roman_min ‚à• bold_italic_z ‚à• start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT", "snippet": "min ‚Å° ‚Äñ ùíõ ‚Äñ 1 subject to ‚Äñ ùíô ‚àí ùë® ‚Äã ùíõ ‚Äñ 2 ‚â§ œµ , \\min\\|\\bm{z}\\|_{1}\\quad\\mbox{subject to}\\quad\\|\\bm{x}-\\bm{A}\\bm{z}\\|_{2}\\leq\\epsilon, roman_min ‚à• bold_italic_z ‚à• start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT subject to ‚à• bold_italic_x - bold_italic_A bold_italic_z ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ‚â§ italic_œµ , (1.3.17)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E18", "title": "ùíô ‚Üí ‚Ñ∞ ùíõ . \\bm{x}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}\\hskip 5.69054pt}\\bm{z}. bold_italic_x start_ARROW start_OVERACCENT caligraphic_E end_OVERACCENT ‚Üí end_ARROW bold_italic_z . (1.3.18)", "snippet": "ùíô ‚Üí ‚Ñ∞ ùíõ . \\bm{x}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}\\hskip 5.69054pt}\\bm{z}. bold_italic_x start_ARROW start_OVERACCENT caligraphic_E end_OVERACCENT ‚Üí end_ARROW bold_italic_z . (1.3.18)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E19", "title": "intractable ‚üπ tractable ‚üπ scalable . \\mbox{{intractable}}\\;\\Longrightarrow\\;\\mbox{{tractable}}\\;\\Longrightarrow\\;\\mbox{{scalable}}. intractable ‚üπ tractable ‚üπ scalable . (1.3.19)", "snippet": "intractable ‚üπ tractable ‚üπ scalable . \\mbox{{intractable}}\\;\\Longrightarrow\\;\\mbox{{tractable}}\\;\\Longrightarrow\\;\\mbox{{scalable}}. intractable ‚üπ tractable ‚üπ scalable . (1.3.19)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E20", "title": "ùëø = ùë® ‚Äã ùíÅ + ùë¨ , ùë® ‚àà ‚Ñù m √ó n . \\bm{X}=\\bm{A}\\bm{Z}+\\bm{E},\\quad\\bm{A}\\in\\mathbb{R}^{m\\times n}. bold_italic_X = bold_italic_A bold_italic_Z + bold_italic_E , bold_italic_A ‚àà blackboard_R start_POSTSUPE", "snippet": "ùëø = ùë® ‚Äã ùíÅ + ùë¨ , ùë® ‚àà ‚Ñù m √ó n . \\bm{X}=\\bm{A}\\bm{Z}+\\bm{E},\\quad\\bm{A}\\in\\mathbb{R}^{m\\times n}. bold_italic_X = bold_italic_A bold_italic_Z + bold_italic_E , bold_italic_A ‚àà blackboard_R start_POSTSUPERSCRIPT italic_m √ó italic_n end_POSTSUPERSCRIPT . (1.3.20)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E21", "title": "ùëø ‚Üí ‚Ñ∞ ùíÅ ‚Üí ùë® ùëø ‚Äã ? \\bm{X}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}\\hskip 5.69054pt}\\bm{Z}\\xrightarrow{\\hskip 5.69054pt\\bm{A}\\hskip 5.69054pt}\\bm{X}? bold_italic_X start_ARROW start_OVERACCENT caligraphi", "snippet": "ùëø ‚Üí ‚Ñ∞ ùíÅ ‚Üí ùë® ùëø ‚Äã ? \\bm{X}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}\\hskip 5.69054pt}\\bm{Z}\\xrightarrow{\\hskip 5.69054pt\\bm{A}\\hskip 5.69054pt}\\bm{X}? bold_italic_X start_ARROW start_OVERACCENT caligraphic_E end_OVERACCENT ‚Üí end_ARROW bold_italic_Z start_ARROW start_OVERACCENT bold_italic_A end_OVERACCENT ‚Üí end_ARROW bold_italic_X ? (1.3.21)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E22", "title": "ùíô = ùíô o + œÉ ‚Äã ùíà , \\bm{x}=\\bm{x}_{o}+\\sigma\\bm{g}, bold_italic_x = bold_italic_x start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT + italic_œÉ bold_italic_g , (1.3.22)", "snippet": "ùíô = ùíô o + œÉ ‚Äã ùíà , \\bm{x}=\\bm{x}_{o}+\\sigma\\bm{g}, bold_italic_x = bold_italic_x start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT + italic_œÉ bold_italic_g , (1.3.22)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E23", "title": "ùíô ^ o = ùîº ‚Äã [ ùíô o ‚à£ ùíô ] = ùíô + œÉ 2 ‚Äã ‚àá log ‚Å° p ‚Äã ( ùíô ) . \\hat{\\bm{x}}_{o}=\\mathbb{E}[\\bm{x}_{o}\\mid\\bm{x}]=\\bm{x}+\\sigma^{2}\\nabla\\log p(\\bm{x}). over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRI", "snippet": "ùíô ^ o = ùîº ‚Äã [ ùíô o ‚à£ ùíô ] = ùíô + œÉ 2 ‚Äã ‚àá log ‚Å° p ‚Äã ( ùíô ) . \\hat{\\bm{x}}_{o}=\\mathbb{E}[\\bm{x}_{o}\\mid\\bm{x}]=\\bm{x}+\\sigma^{2}\\nabla\\log p(\\bm{x}). over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT = blackboard_E [ bold_italic_x start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT ‚à£ bold_italic_x ] = bold_italic_x + italic_œÉ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ‚àá roman_log italic_p ( bold_italic_x ) . (1.3.23)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E24", "title": "ùíà ^ = ùíô ‚àí ùíô ^ o œÉ = ‚àí œÉ ‚Äã ‚àá log ‚Å° p ‚Äã ( ùíô ) , \\hat{\\bm{g}}=\\frac{\\bm{x}-\\hat{\\bm{x}}_{o}}{\\sigma}=-\\sigma\\nabla\\log p(\\bm{x}), over^ start_ARG bold_italic_g end_ARG = divide start_ARG bold_italic_x - ", "snippet": "ùíà ^ = ùíô ‚àí ùíô ^ o œÉ = ‚àí œÉ ‚Äã ‚àá log ‚Å° p ‚Äã ( ùíô ) , \\hat{\\bm{g}}=\\frac{\\bm{x}-\\hat{\\bm{x}}_{o}}{\\sigma}=-\\sigma\\nabla\\log p(\\bm{x}), over^ start_ARG bold_italic_g end_ARG = divide start_ARG bold_italic_x - over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT end_ARG start_ARG italic_œÉ end_ARG = - italic_œÉ ‚àá roman_log italic_p ( bold_italic_x ) , (1.3.24)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E25", "title": "H ‚Äã ( ùíô ) = ‚àí ‚à´ p ‚Äã ( ùíò ) ‚Äã log ‚Å° p ‚Äã ( ùíò ) ‚Äã d ùíò H(\\bm{x})=-\\int p(\\bm{w})\\log p(\\bm{w})\\mathrm{d}\\bm{w} italic_H ( bold_italic_x ) = - ‚à´ italic_p ( bold_italic_w ) roman_log italic_p ( bold_italic_w", "snippet": "H ‚Äã ( ùíô ) = ‚àí ‚à´ p ‚Äã ( ùíò ) ‚Äã log ‚Å° p ‚Äã ( ùíò ) ‚Äã d ùíò H(\\bm{x})=-\\int p(\\bm{w})\\log p(\\bm{w})\\mathrm{d}\\bm{w} italic_H ( bold_italic_x ) = - ‚à´ italic_p ( bold_italic_w ) roman_log italic_p ( bold_italic_w ) roman_d bold_italic_w (1.3.25)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E26", "title": "H ‚Äã ( ùíô ) = ‚àí ‚à´ p ‚Äã ( ùíò ) ‚Äã log ‚Å° p ‚Äã ( ùíò ) ‚Äã d ùíò ‚Üí decreasing H ‚àó ‚Äã ( ùíô ) = ‚àí ‚à´ p ‚àó ‚Äã ( ùíò ) ‚Äã log ‚Å° p ‚àó ‚Äã ( ùíò ) ‚Äã d ùíò . H(\\bm{x})=-\\int p(\\bm{w})\\log p(\\bm{w})\\mathrm{d}\\bm{w}\\quad\\xrightarrow{\\hskip", "snippet": "H ‚Äã ( ùíô ) = ‚àí ‚à´ p ‚Äã ( ùíò ) ‚Äã log ‚Å° p ‚Äã ( ùíò ) ‚Äã d ùíò ‚Üí decreasing H ‚àó ‚Äã ( ùíô ) = ‚àí ‚à´ p ‚àó ‚Äã ( ùíò ) ‚Äã log ‚Å° p ‚àó ‚Äã ( ùíò ) ‚Äã d ùíò . H(\\bm{x})=-\\int p(\\bm{w})\\log p(\\bm{w})\\mathrm{d}\\bm{w}\\quad\\xrightarrow{\\hskip 2.84526pt\\mbox{decreasing}\\hskip 2.84526pt}\\quad H^{*}(\\bm{x})=-\\int p^{*}(\\bm{w})\\log p^{*}(\\bm{w})\\mathrm{d}\\bm{w}. italic_H ( bold_italic_x ) = - ‚à´ italic_p ( bold_italic_w ) roman_log italic_p ( bold_italic_w ) roman_d bold_italic_w start_ARROW start_OVERACCENT decreasing end_OVERACCENT ‚Üí end_ARROW italic_H start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT ( bold_italic_x ) = - ‚à´ italic_p start_POS"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E27", "title": "o j = œÜ ‚Äã ( ‚àë i w j ‚Äã i ‚Äã x i ) , o_{j}=\\varphi\\Big{(}\\sum_{i}w_{ji}x_{i}\\Big{)}, italic_o start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = italic_œÜ ( ‚àë start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ", "snippet": "o j = œÜ ‚Äã ( ‚àë i w j ‚Äã i ‚Äã x i ) , o_{j}=\\varphi\\Big{(}\\sum_{i}w_{ji}x_{i}\\Big{)}, italic_o start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = italic_œÜ ( ‚àë start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_j italic_i end_POSTSUBSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) , (1.3.27)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E28", "title": "œÜ ‚Äã ( x ) = max ‚Å° { 0 , x } = { x , if ‚Äã x > 0 , 0 , if ‚Äã x ‚â§ 0 , \\varphi(x)=\\max\\{0,x\\}=\\begin{cases}x,&\\text{if}\\,x>0,\\\\ 0,\\quad&\\text{if}\\,x\\leq 0,\\end{cases} italic_œÜ ( italic_x ) = roman_max { 0 ", "snippet": "œÜ ‚Äã ( x ) = max ‚Å° { 0 , x } = { x , if ‚Äã x > 0 , 0 , if ‚Äã x ‚â§ 0 , \\varphi(x)=\\max\\{0,x\\}=\\begin{cases}x,&\\text{if}\\,x>0,\\\\ 0,\\quad&\\text{if}\\,x\\leq 0,\\end{cases} italic_œÜ ( italic_x ) = roman_max { 0 , italic_x } = { start_ROW start_CELL italic_x , end_CELL start_CELL if italic_x > 0 , end_CELL end_ROW start_ROW start_CELL 0 , end_CELL start_CELL if italic_x ‚â§ 0 , end_CELL end_ROW (1.3.28)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E29", "title": "h ‚Äã ( ùëæ 1 , ‚Ä¶ , ùëæ L ) = f L ‚Äã ( ùëæ L ‚Äã f L ‚àí 1 ‚Äã ( ùëæ L ‚àí 1 ‚Äã ‚ãØ ‚Äã f 2 ‚Äã ( ùëæ 2 ‚Äã f 1 ‚Äã ( ùëæ 1 ‚Äã ùíô ) ) ) ) . h(\\bm{W}_{1},\\ldots,\\bm{W}_{L})=f^{L}(\\bm{W}_{L}f^{L-1}(\\bm{W}_{L-1}\\cdots f^{2}(\\bm{W}_{2}f^{1}", "snippet": "h ‚Äã ( ùëæ 1 , ‚Ä¶ , ùëæ L ) = f L ‚Äã ( ùëæ L ‚Äã f L ‚àí 1 ‚Äã ( ùëæ L ‚àí 1 ‚Äã ‚ãØ ‚Äã f 2 ‚Äã ( ùëæ 2 ‚Äã f 1 ‚Äã ( ùëæ 1 ‚Äã ùíô ) ) ) ) . h(\\bm{W}_{1},\\ldots,\\bm{W}_{L})=f^{L}(\\bm{W}_{L}f^{L-1}(\\bm{W}_{L-1}\\cdots f^{2}(\\bm{W}_{2}f^{1}(\\bm{W}_{1}\\bm{x})))). italic_h ( bold_italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_W start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT ) = italic_f start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT ( bold_italic_W start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT italic_f start_POSTSUPERSCRIPT italic_L - 1 end_POSTSUPERSCRIPT ( bold_italic_W start_POSTSUBSCRIPT italic_L - 1 end_P"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E30", "title": "ùëø ‚Üí ùëì ùíÅ ‚Üí ùëî ùëø ^ . \\bm{X}\\xrightarrow{\\hskip 5.69054ptf\\hskip 5.69054pt}\\bm{Z}\\xrightarrow{\\hskip 5.69054ptg\\hskip 5.69054pt}\\hat{\\bm{X}}. bold_italic_X start_ARROW start_OVERACCENT italic_f end_OVERAC", "snippet": "ùëø ‚Üí ùëì ùíÅ ‚Üí ùëî ùëø ^ . \\bm{X}\\xrightarrow{\\hskip 5.69054ptf\\hskip 5.69054pt}\\bm{Z}\\xrightarrow{\\hskip 5.69054ptg\\hskip 5.69054pt}\\hat{\\bm{X}}. bold_italic_X start_ARROW start_OVERACCENT italic_f end_OVERACCENT ‚Üí end_ARROW bold_italic_Z start_ARROW start_OVERACCENT italic_g end_OVERACCENT ‚Üí end_ARROW over^ start_ARG bold_italic_X end_ARG . (1.3.30)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E31", "title": "min f , g ‚Å° ‚Äñ ùëø ‚àí ùëø ^ ‚Äñ 2 2 = ‚Äñ ùëø ‚àí g ‚Äã ( f ‚Äã ( ùëø ) ) ‚Äñ 2 2 , \\min_{f,g}\\big{\\|}\\bm{X}-\\hat{\\bm{X}}\\big{\\|}_{2}^{2}=\\big{\\|}\\bm{X}-g(f(\\bm{X}))\\big{\\|}_{2}^{2}, roman_min start_POSTSUBSCRIPT italic_f ", "snippet": "min f , g ‚Å° ‚Äñ ùëø ‚àí ùëø ^ ‚Äñ 2 2 = ‚Äñ ùëø ‚àí g ‚Äã ( f ‚Äã ( ùëø ) ) ‚Äñ 2 2 , \\min_{f,g}\\big{\\|}\\bm{X}-\\hat{\\bm{X}}\\big{\\|}_{2}^{2}=\\big{\\|}\\bm{X}-g(f(\\bm{X}))\\big{\\|}_{2}^{2}, roman_min start_POSTSUBSCRIPT italic_f , italic_g end_POSTSUBSCRIPT ‚à• bold_italic_X - over^ start_ARG bold_italic_X end_ARG ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = ‚à• bold_italic_X - italic_g ( italic_f ( bold_italic_X ) ) ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , (1.3.31)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E32", "title": "ùëø ‚Üí ùëì ùíÅ . \\bm{X}\\xrightarrow{\\hskip 5.69054ptf\\hskip 5.69054pt}\\bm{Z}. bold_italic_X start_ARROW start_OVERACCENT italic_f end_OVERACCENT ‚Üí end_ARROW bold_italic_Z . (1.3.32)", "snippet": "ùëø ‚Üí ùëì ùíÅ . \\bm{X}\\xrightarrow{\\hskip 5.69054ptf\\hskip 5.69054pt}\\bm{Z}. bold_italic_X start_ARROW start_OVERACCENT italic_f end_OVERACCENT ‚Üí end_ARROW bold_italic_Z . (1.3.32)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E33", "title": "ùíÅ ‚Üí ùëî ùëø ^ . \\bm{Z}\\xrightarrow{\\hskip 5.69054ptg\\hskip 5.69054pt}\\hat{\\bm{X}}. bold_italic_Z start_ARROW start_OVERACCENT italic_g end_OVERACCENT ‚Üí end_ARROW over^ start_ARG bold_italic_X end_ARG . (1", "snippet": "ùíÅ ‚Üí ùëî ùëø ^ . \\bm{Z}\\xrightarrow{\\hskip 5.69054ptg\\hskip 5.69054pt}\\hat{\\bm{X}}. bold_italic_Z start_ARROW start_OVERACCENT italic_g end_OVERACCENT ‚Üí end_ARROW over^ start_ARG bold_italic_X end_ARG . (1.3.33)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E34", "title": "min ‚Å° d ‚Äã ( ùëø , ùëø ^ ) . \\min d(\\bm{X},\\hat{\\bm{X}}). roman_min italic_d ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG ) . (1.3.34)", "snippet": "min ‚Å° d ‚Äã ( ùëø , ùëø ^ ) . \\min d(\\bm{X},\\hat{\\bm{X}}). roman_min italic_d ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG ) . (1.3.34)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E35", "title": "ùíÅ ‚Üí ùëî ùëø ^ , ùëø ‚Üí ùëë ùüé , ùüè , \\bm{Z}\\xrightarrow{\\hskip 5.69054ptg\\hskip 5.69054pt}\\hat{\\bm{X}},\\bm{X}\\xrightarrow{\\hskip 5.69054ptd\\hskip 5.69054pt}\\bm{0},\\bm{1}, bold_italic_Z start_ARROW start_OVERACCE", "snippet": "ùíÅ ‚Üí ùëî ùëø ^ , ùëø ‚Üí ùëë ùüé , ùüè , \\bm{Z}\\xrightarrow{\\hskip 5.69054ptg\\hskip 5.69054pt}\\hat{\\bm{X}},\\bm{X}\\xrightarrow{\\hskip 5.69054ptd\\hskip 5.69054pt}\\bm{0},\\bm{1}, bold_italic_Z start_ARROW start_OVERACCENT italic_g end_OVERACCENT ‚Üí end_ARROW over^ start_ARG bold_italic_X end_ARG , bold_italic_X start_ARROW start_OVERACCENT italic_d end_OVERACCENT ‚Üí end_ARROW bold_0 , bold_1 , (1.3.35)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E36", "title": "min g ‚Å° max d ‚Å° ‚Ñì ‚Äã ( ùëø , ùëø ^ ) , \\min_{g}\\max_{d}\\ell(\\bm{X},\\hat{\\bm{X}}), roman_min start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT roman_max start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT roman_‚Ñì (", "snippet": "min g ‚Å° max d ‚Å° ‚Ñì ‚Äã ( ùëø , ùëø ^ ) , \\min_{g}\\max_{d}\\ell(\\bm{X},\\hat{\\bm{X}}), roman_min start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT roman_max start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT roman_‚Ñì ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG ) , (1.3.36)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.E37", "title": "ùíõ 0 ‚àº ùí© ‚Äã ( ùüé , ùë∞ ) ‚Üí g 0 ùíõ 1 ‚Üí g 1 ‚ãØ ‚Üí g L ‚àí 1 ùíõ L ‚àº p ‚Äã ( ùíô ) . \\bm{z}^{0}\\sim\\mathcal{N}(\\bm{0},\\bm{I})\\xrightarrow{\\hskip 5.69054ptg^{0}\\hskip 5.69054pt}\\bm{z}^{1}\\xrightarrow{\\hskip 5.69054ptg^{1", "snippet": "ùíõ 0 ‚àº ùí© ‚Äã ( ùüé , ùë∞ ) ‚Üí g 0 ùíõ 1 ‚Üí g 1 ‚ãØ ‚Üí g L ‚àí 1 ùíõ L ‚àº p ‚Äã ( ùíô ) . \\bm{z}^{0}\\sim\\mathcal{N}(\\bm{0},\\bm{I})\\xrightarrow{\\hskip 5.69054ptg^{0}\\hskip 5.69054pt}\\bm{z}^{1}\\xrightarrow{\\hskip 5.69054ptg^{1}\\hskip 5.69054pt}\\cdots\\xrightarrow{\\hskip 5.69054ptg^{L-1}\\hskip 5.69054pt}\\bm{z}^{L}\\sim p(\\bm{x}). bold_italic_z start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT ‚àº caligraphic_N ( bold_0 , bold_italic_I ) start_ARROW start_OVERACCENT italic_g start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT end_OVERACCENT ‚Üí end_ARROW bold_italic_z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_ARROW start_OVERACCENT "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.E1", "title": "computable ‚üπ tractable ‚üπ scalable . \\mbox{{computable}}\\;\\Longrightarrow\\;\\mbox{{tractable}}\\;\\Longrightarrow\\;\\mbox{{scalable}}. computable ‚üπ tractable ‚üπ scalable . (1.4.1)", "snippet": "computable ‚üπ tractable ‚üπ scalable . \\mbox{{computable}}\\;\\Longrightarrow\\;\\mbox{{tractable}}\\;\\Longrightarrow\\;\\mbox{{scalable}}. computable ‚üπ tractable ‚üπ scalable . (1.4.1)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.E2", "title": "K ‚Äã ( S ) = min p : ùí∞ ‚Äã ( p ) = S ‚Å° L ‚Äã ( p ) . K(S)=\\min_{p\\,:\\,\\mathcal{U}(p)=S}L(p). italic_K ( italic_S ) = roman_min start_POSTSUBSCRIPT italic_p : caligraphic_U ( italic_p ) = italic_S end_POSTS", "snippet": "K ‚Äã ( S ) = min p : ùí∞ ‚Äã ( p ) = S ‚Å° L ‚Äã ( p ) . K(S)=\\min_{p\\,:\\,\\mathcal{U}(p)=S}L(p). italic_K ( italic_S ) = roman_min start_POSTSUBSCRIPT italic_p : caligraphic_U ( italic_p ) = italic_S end_POSTSUBSCRIPT italic_L ( italic_p ) . (1.4.2)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.E3", "title": "h ‚Äã ( S ) ‚âê ‚àí ‚à´ p ‚Äã ( s ) ‚Äã log ‚Å° p ‚Äã ( s ) ‚Äã d s h(S)\\doteq-\\int p(s)\\log p(s)\\mathrm{d}s italic_h ( italic_S ) ‚âê - ‚à´ italic_p ( italic_s ) roman_log italic_p ( italic_s ) roman_d italic_s (1.4.3)", "snippet": "h ‚Äã ( S ) ‚âê ‚àí ‚à´ p ‚Äã ( s ) ‚Äã log ‚Å° p ‚Äã ( s ) ‚Äã d s h(S)\\doteq-\\int p(s)\\log p(s)\\mathrm{d}s italic_h ( italic_S ) ‚âê - ‚à´ italic_p ( italic_s ) roman_log italic_p ( italic_s ) roman_d italic_s (1.4.3)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.E4", "title": "{ S 1 , S 2 , ‚Ä¶ , S i , ‚Ä¶ , S N } ‚äÇ ‚Ñù D , \\{S_{1},S_{2},\\ldots,S_{i},\\ldots,S_{N}\\}\\subset\\mathbb{R}^{D}, { italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT 2 end_POSTSU", "snippet": "{ S 1 , S 2 , ‚Ä¶ , S i , ‚Ä¶ , S N } ‚äÇ ‚Ñù D , \\{S_{1},S_{2},\\ldots,S_{i},\\ldots,S_{N}\\}\\subset\\mathbb{R}^{D}, { italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ‚Ä¶ , italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , ‚Ä¶ , italic_S start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT } ‚äÇ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT , (1.4.4)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.E5", "title": "R ‚Äã ( f ‚à£ ‚Ñ∞ ) = ùîº ‚Äã [ L ‚Äã ( ‚Ñ∞ ‚Äã ( S ) ) ] ‚âà 1 N ‚Äã ‚àë i = 1 N L ‚Äã ( ‚Ñ∞ ‚Äã ( S i ) ) . R(f\\mid\\mathcal{E})=\\mathbb{E}[L(\\mathcal{E}(S))]\\approx\\frac{1}{N}\\sum_{i=1}^{N}L(\\mathcal{E}(S_{i})). italic_R ( ita", "snippet": "R ‚Äã ( f ‚à£ ‚Ñ∞ ) = ùîº ‚Äã [ L ‚Äã ( ‚Ñ∞ ‚Äã ( S ) ) ] ‚âà 1 N ‚Äã ‚àë i = 1 N L ‚Äã ( ‚Ñ∞ ‚Äã ( S i ) ) . R(f\\mid\\mathcal{E})=\\mathbb{E}[L(\\mathcal{E}(S))]\\approx\\frac{1}{N}\\sum_{i=1}^{N}L(\\mathcal{E}(S_{i})). italic_R ( italic_f ‚à£ caligraphic_E ) = blackboard_E [ italic_L ( caligraphic_E ( italic_S ) ) ] ‚âà divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_L ( caligraphic_E ( italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) . (1.4.5)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.E6", "title": "R ‚Äã ( f ‚à£ ‚Ñ∞ 1 ) ‚àí R ‚Äã ( f ‚à£ ‚Ñ∞ 2 ) > 0 , R(f\\mid\\mathcal{E}_{1})-R(f\\mid\\mathcal{E}_{2})>0, italic_R ( italic_f ‚à£ caligraphic_E start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) - italic_R ( italic_f ‚à£ caligra", "snippet": "R ‚Äã ( f ‚à£ ‚Ñ∞ 1 ) ‚àí R ‚Äã ( f ‚à£ ‚Ñ∞ 2 ) > 0 , R(f\\mid\\mathcal{E}_{1})-R(f\\mid\\mathcal{E}_{2})>0, italic_R ( italic_f ‚à£ caligraphic_E start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) - italic_R ( italic_f ‚à£ caligraphic_E start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) > 0 , (1.4.6)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.E7", "title": "min ‚Ñ∞ ‚Å° R ‚Äã ( f ‚à£ ‚Ñ∞ ) . \\min_{\\mathcal{E}}R(f\\mid\\mathcal{E}). roman_min start_POSTSUBSCRIPT caligraphic_E end_POSTSUBSCRIPT italic_R ( italic_f ‚à£ caligraphic_E ) . (1.4.7)", "snippet": "min ‚Ñ∞ ‚Å° R ‚Äã ( f ‚à£ ‚Ñ∞ ) . \\min_{\\mathcal{E}}R(f\\mid\\mathcal{E}). roman_min start_POSTSUBSCRIPT caligraphic_E end_POSTSUBSCRIPT italic_R ( italic_f ‚à£ caligraphic_E ) . (1.4.7)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.Ex1", "title": "1 N ‚Äã ( L ‚Äã ( ‚Ñ∞ ) + ‚àë i = 1 N L ‚Äã ( ‚Ñ∞ ‚Äã ( S i ) ) ) ‚âà 1 N ‚Äã ‚àë i = 1 N L ‚Äã ( ‚Ñ∞ ‚Äã ( S i ) ) \\frac{1}{N}\\Big{(}L(\\mathcal{E})+\\sum_{i=1}^{N}L(\\mathcal{E}(S_{i}))\\Big{)}\\approx\\frac{1}{N}\\sum_{i=1}^{N}L(\\", "snippet": "1 N ‚Äã ( L ‚Äã ( ‚Ñ∞ ) + ‚àë i = 1 N L ‚Äã ( ‚Ñ∞ ‚Äã ( S i ) ) ) ‚âà 1 N ‚Äã ‚àë i = 1 N L ‚Äã ( ‚Ñ∞ ‚Äã ( S i ) ) \\frac{1}{N}\\Big{(}L(\\mathcal{E})+\\sum_{i=1}^{N}L(\\mathcal{E}(S_{i}))\\Big{)}\\approx\\frac{1}{N}\\sum_{i=1}^{N}L(\\mathcal{E}(S_{i})) divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ( italic_L ( caligraphic_E ) + ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_L ( caligraphic_E ( italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) ) ‚âà divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBS"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.E8", "title": "K ‚Äã ( S ) < L ‚Äã ( ‚Ñ∞ ‚Äã ( S ) ) . K(S)<L(\\mathcal{E}(S)). italic_K ( italic_S ) < italic_L ( caligraphic_E ( italic_S ) ) . (1.4.8)", "snippet": "K ‚Äã ( S ) < L ‚Äã ( ‚Ñ∞ ‚Äã ( S ) ) . K(S)<L(\\mathcal{E}(S)). italic_K ( italic_S ) < italic_L ( caligraphic_E ( italic_S ) ) . (1.4.8)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.E9", "title": "ùëø = { S 1 , S 2 , ‚Ä¶ , S i , ‚Ä¶ , S N } ‚äÇ ‚Ñù D , \\bm{X}=\\{S_{1},S_{2},\\ldots,S_{i},\\ldots,S_{N}\\}\\subset\\mathbb{R}^{D}, bold_italic_X = { italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_S start", "snippet": "ùëø = { S 1 , S 2 , ‚Ä¶ , S i , ‚Ä¶ , S N } ‚äÇ ‚Ñù D , \\bm{X}=\\{S_{1},S_{2},\\ldots,S_{i},\\ldots,S_{N}\\}\\subset\\mathbb{R}^{D}, bold_italic_X = { italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ‚Ä¶ , italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , ‚Ä¶ , italic_S start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT } ‚äÇ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT , (1.4.9)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.E10", "title": "ùëø ‚Üí ‚Ñ∞ ùíÅ . \\bm{X}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}\\hskip 5.69054pt}\\bm{Z}. bold_italic_X start_ARROW start_OVERACCENT caligraphic_E end_OVERACCENT ‚Üí end_ARROW bold_italic_Z . (1.4.10)", "snippet": "ùëø ‚Üí ‚Ñ∞ ùíÅ . \\bm{X}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}\\hskip 5.69054pt}\\bm{Z}. bold_italic_X start_ARROW start_OVERACCENT caligraphic_E end_OVERACCENT ‚Üí end_ARROW bold_italic_Z . (1.4.10)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.E11", "title": "min ‚Å° œÅ ‚Äã ( ùíÅ ) . \\min\\rho(\\bm{Z}). roman_min italic_œÅ ( bold_italic_Z ) . (1.4.11)", "snippet": "min ‚Å° œÅ ‚Äã ( ùíÅ ) . \\min\\rho(\\bm{Z}). roman_min italic_œÅ ( bold_italic_Z ) . (1.4.11)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.E12", "title": "ùíô ‚Ü¶ ùíõ ‚àà { [ 1 , 0 , 0 , ‚Ä¶ , 0 , 0 ] , [ 0 , 1 , 0 ‚Ä¶ , 0 , 0 ] , ‚Ä¶ , [ 0 , 0 , 0 , ‚Ä¶ , 0 , 1 ] . } \\bm{x}\\mapsto\\bm{z}\\in\\{[1,0,0,\\ldots,0,0],\\;[0,1,0\\ldots,0,0],\\;\\ldots,\\;[0,0,0,\\ldots,0,1].\\} bold_i", "snippet": "ùíô ‚Ü¶ ùíõ ‚àà { [ 1 , 0 , 0 , ‚Ä¶ , 0 , 0 ] , [ 0 , 1 , 0 ‚Ä¶ , 0 , 0 ] , ‚Ä¶ , [ 0 , 0 , 0 , ‚Ä¶ , 0 , 1 ] . } \\bm{x}\\mapsto\\bm{z}\\in\\{[1,0,0,\\ldots,0,0],\\;[0,1,0\\ldots,0,0],\\;\\ldots,\\;[0,0,0,\\ldots,0,1].\\} bold_italic_x ‚Ü¶ bold_italic_z ‚àà { [ 1 , 0 , 0 , ‚Ä¶ , 0 , 0 ] , [ 0 , 1 , 0 ‚Ä¶ , 0 , 0 ] , ‚Ä¶ , [ 0 , 0 , 0 , ‚Ä¶ , 0 , 1 ] . } (1.4.12)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.E13", "title": "L ‚Äã ( ùíõ ^ , ùíõ ) = ‚àë k = 1 K ‚àí z k ‚Äã log ‚Å° z ^ k , L(\\hat{\\bm{z}},\\bm{z})=\\sum_{k=1}^{K}-z_{k}\\log\\hat{z}_{k}, italic_L ( over^ start_ARG bold_italic_z end_ARG , bold_italic_z ) = ‚àë start_POSTSUBSCRIPT", "snippet": "L ‚Äã ( ùíõ ^ , ùíõ ) = ‚àë k = 1 K ‚àí z k ‚Äã log ‚Å° z ^ k , L(\\hat{\\bm{z}},\\bm{z})=\\sum_{k=1}^{K}-z_{k}\\log\\hat{z}_{k}, italic_L ( over^ start_ARG bold_italic_z end_ARG , bold_italic_z ) = ‚àë start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT - italic_z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT roman_log over^ start_ARG italic_z end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , (1.4.13)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.E14", "title": "ùëø ‚Üí ‚Ñ∞ ùíÅ ‚Üí ùíü ùëø ^ . \\bm{X}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}\\hskip 5.69054pt}\\bm{Z}\\xrightarrow{\\hskip 5.69054pt\\mathcal{D}\\hskip 5.69054pt}\\hat{\\bm{X}}. bold_italic_X start_ARROW start_OVERACCENT", "snippet": "ùëø ‚Üí ‚Ñ∞ ùíÅ ‚Üí ùíü ùëø ^ . \\bm{X}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}\\hskip 5.69054pt}\\bm{Z}\\xrightarrow{\\hskip 5.69054pt\\mathcal{D}\\hskip 5.69054pt}\\hat{\\bm{X}}. bold_italic_X start_ARROW start_OVERACCENT caligraphic_E end_OVERACCENT ‚Üí end_ARROW bold_italic_Z start_ARROW start_OVERACCENT caligraphic_D end_OVERACCENT ‚Üí end_ARROW over^ start_ARG bold_italic_X end_ARG . (1.4.14)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.E15", "title": "min ‚Å° d ‚Äã ( ùëø , ùëø ^ ) + œÅ ‚Äã ( ùíÅ ) , \\min\\,d(\\bm{X},\\hat{\\bm{X}})+\\rho(\\bm{Z}), roman_min italic_d ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG ) + italic_œÅ ( bold_italic_Z ) , (1.4.15)", "snippet": "min ‚Å° d ‚Äã ( ùëø , ùëø ^ ) + œÅ ‚Äã ( ùíÅ ) , \\min\\,d(\\bm{X},\\hat{\\bm{X}})+\\rho(\\bm{Z}), roman_min italic_d ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG ) + italic_œÅ ( bold_italic_Z ) , (1.4.15)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.E16", "title": "ùëø ‚Üí ‚Ñ∞ ùíÅ ‚Üí ùíü ùëø ^ ‚Üí ‚Ñ∞ ùíÅ ^ , \\bm{X}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}\\hskip 5.69054pt}\\bm{Z}\\xrightarrow{\\hskip 5.69054pt\\mathcal{D}\\hskip 5.69054pt}\\hat{\\bm{X}}\\xrightarrow{\\hskip 5.69054pt\\mathca", "snippet": "ùëø ‚Üí ‚Ñ∞ ùíÅ ‚Üí ùíü ùëø ^ ‚Üí ‚Ñ∞ ùíÅ ^ , \\bm{X}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}\\hskip 5.69054pt}\\bm{Z}\\xrightarrow{\\hskip 5.69054pt\\mathcal{D}\\hskip 5.69054pt}\\hat{\\bm{X}}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}\\hskip 5.69054pt}\\hat{\\bm{Z}}, bold_italic_X start_ARROW start_OVERACCENT caligraphic_E end_OVERACCENT ‚Üí end_ARROW bold_italic_Z start_ARROW start_OVERACCENT caligraphic_D end_OVERACCENT ‚Üí end_ARROW over^ start_ARG bold_italic_X end_ARG start_ARROW start_OVERACCENT caligraphic_E end_OVERACCENT ‚Üí end_ARROW over^ start_ARG bold_italic_Z end_ARG , (1.4.16)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S4.E17", "title": "max Œ∏ ‚Å° min Œ∑ ‚Å° ‚Ñì ‚Äã ( ùíÅ , ùíÅ ^ ) + œÅ ‚Äã ( ùíÅ ) , \\max_{\\theta}\\min_{\\eta}\\ell(\\bm{Z},\\hat{\\bm{Z}})+\\rho(\\bm{Z}), roman_max start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT ita", "snippet": "max Œ∏ ‚Å° min Œ∑ ‚Å° ‚Ñì ‚Äã ( ùíÅ , ùíÅ ^ ) + œÅ ‚Äã ( ùíÅ ) , \\max_{\\theta}\\min_{\\eta}\\ell(\\bm{Z},\\hat{\\bm{Z}})+\\rho(\\bm{Z}), roman_max start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT italic_Œ∑ end_POSTSUBSCRIPT roman_‚Ñì ( bold_italic_Z , over^ start_ARG bold_italic_Z end_ARG ) + italic_œÅ ( bold_italic_Z ) , (1.4.17)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S5.E1", "title": "open-ended ‚üπ bi-directional ‚üπ closed-loop , \\mbox{{open-ended}}\\;\\Longrightarrow\\;\\mbox{{bi-directional}}\\;\\Longrightarrow\\;\\mbox{{closed-loop}}, open-ended ‚üπ bi-directional ‚üπ closed-loop , (1.5.1)", "snippet": "open-ended ‚üπ bi-directional ‚üπ closed-loop , \\mbox{{open-ended}}\\;\\Longrightarrow\\;\\mbox{{bi-directional}}\\;\\Longrightarrow\\;\\mbox{{closed-loop}}, open-ended ‚üπ bi-directional ‚üπ closed-loop , (1.5.1)"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S5.E2", "title": "Intelligence ‚Å° ( t ) = d d ‚Äã t ‚Äã Information ‚Å° ( t ) , Information ‚Å° ( t ) = ‚à´ 0 t Intelligence ‚Å° ( s ) ‚Äã d s . \\operatorname{Intelligence}(t)=\\frac{\\mathrm{d}}{\\mathrm{d}t}\\operatorname{Information}(", "snippet": "Intelligence ‚Å° ( t ) = d d ‚Äã t ‚Äã Information ‚Å° ( t ) , Information ‚Å° ( t ) = ‚à´ 0 t Intelligence ‚Å° ( s ) ‚Äã d s . \\operatorname{Intelligence}(t)=\\frac{\\mathrm{d}}{\\mathrm{d}t}\\operatorname{Information}(t),\\qquad\\operatorname{Information}(t)=\\int_{0}^{t}\\operatorname{Intelligence}(s)\\mathrm{d}s. roman_Intelligence ( italic_t ) = divide start_ARG roman_d end_ARG start_ARG roman_d italic_t end_ARG roman_Information ( italic_t ) , roman_Information ( italic_t ) = ‚à´ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT roman_Intelligence ( italic_s ) roman_d itali"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx1", "title": "Linear Dynamical Systems", "snippet": "Linear Dynamical Systems Wiener Filter. As we have discussed before in Section 1.2.1 , a main task of intelligence is to learn what is predictable in sequences of observations. Probably the simplest class of predictable sequences, or signals, are generated via a linear time-invariant (LTI) process: x ‚Äã [ n ] = h ‚Äã [ n ] ‚àó z ‚Äã [ n ] + œµ ‚Äã [ n ] , x[n]=h[n]*z[n]+\\epsilon[n], italic_x [ italic_n ] = italic_h [ italic_n ] ‚àó italic_z [ italic_n ] + italic_œµ [ italic_n ] , (1.3.1) where z z italic_z is the input and h h italic_h is the impulse response function. 14 14 14 Normally h h italic_h is ass"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx2", "title": "Linear and Mixed Linear Models", "snippet": "Linear and Mixed Linear Models Principal Component Analysis. From the above problems in classical signal processing and system identification, we see that the main task behind of all these problems is to learn from noisy observations a single low-dimensional linear subspace. Mathematically, we may model such a structure as: ùíô = ùíñ 1 ‚Äã z 1 + ùíñ 2 ‚Äã z 2 + ‚ãØ + ùíñ d ‚Äã z d + œµ = ùëº ‚Äã ùíõ + œµ , ùëº ‚àà ‚Ñù D √ó d \\bm{x}=\\bm{u}_{1}z_{1}+\\bm{u}_{2}z_{2}+\\cdots+\\bm{u}_{d}z_{d}+\\bm{\\epsilon}=\\bm{U}\\bm{z}+\\bm{\\epsilon},\\quad\\bm{U}\\in\\mathbb{R}^{D\\times d} bold_italic_x = bold_italic_u start_POSTSUBSCRIPT 1 end_POSTSU"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS1.SSSx3", "title": "General Distributions", "snippet": "General Distributions The distributions of real-world data such as images, videos, and audio are too complex to be modeled by above, somewhat idealistic, linear models or Gaussian processes. We normally do not know a priori they are generated from which family of parametric models. 19 19 19 Although in history there had been many attempts to develop analytical models for these data, such as random fields or stochastic processes for imagery data [ MG99 ] , as we have discussed in the previous section. In practice, we typically only have many samples from their distributions ‚Äì the empirical dist"}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx1", "title": "Classic Artificial Neural Networks", "snippet": "Classic Artificial Neural Networks Artificial neuron. Figure 1.13 : The first mathematical model of an artificial neuron (right) that emulates how a neuron (left) processes signals. Inspired by the nerve system in the brain, the mathematical model of the first artificial neuron 25 25 25 known as the Linear Threshold Unit, or a perceptron. was proposed by Warren McCulloch 26 26 26 A professor of psychiatry at the University of Chicago at the time and Walter Pitts in 1943 [ MP43 ] . It describes the relationship between the input x i x_{i} italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT "}, {"page": "Chapter 1 Introduction", "href": "Ch1.html#S3.SS2.SSSx2", "title": "Modern Deep Neural Networks", "snippet": "Modern Deep Neural Networks For nearly 30 years, from 1980s to 2010s, for the study of machine learning and machine intelligence, neural networks were not considered seriously by the mainstream. Early (deep) neural networks, such as the LeNet, have shown promising performance for small-scale classification problems such as recognizing digits. However, the design and practice of the networks were rather empirical, datasets available at the time were small, and the BP algorithm was a huge computational burden for computers then. These factors had resulted in the lack of interest in neural networ"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#top", "title": "Chapter 2 Learning Linear and Independent Structures", "snippet": ""}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1", "title": "2.1 A Low-Dimensional Subspace", "snippet": "2.1 A Low-Dimensional Subspace 2.1.1 Principal Components Analysis (PCA) Perhaps the simplest modeling assumption possible for low-dimensional structure is the so-called low-rank assumption. Letting D D italic_D be the dimension of our data space, we assume that our data belong to a low-dimensional subspace of dimension d ‚â™ D d\\ll D italic_d ‚â™ italic_D , possibly plus some small disturbances. This ends up being a nearly valid assumption for some surprisingly complex data, such as images of handwritten digits and face data [ RD03 ] as shown in Figure 2.1 , yet as we will see, it will lend itsel"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2", "title": "2.2 A Mixture of Complete Low-Dimensional Subspaces", "snippet": "2.2 A Mixture of Complete Low-Dimensional Subspaces As we have seen, low-rank signal models are rich enough to provide a full picture of the interplay between low-dimensionality in data and efficient and scalable computational algorithms for representation and recovery under errors. These models imply a linear and symmetric representation learning pipeline ( 2.1.6 ): ùíõ = ‚Ñ∞ ‚Äã ( ùíô ) = ùëº ‚ä§ ‚Äã ùíô , ùíô ^ = ùíü ‚Äã ( ùíõ ) = ùëº ‚Äã ùíõ , \\bm{z}=\\mathcal{E}(\\bm{x})=\\bm{U}^{\\top}\\bm{x},\\quad\\hat{\\bm{x}}=\\mathcal{D}(\\bm{z})=\\bm{U}\\bm{z}, bold_italic_z = caligraphic_E ( bold_italic_x ) = bold_italic_U start_POSTSUPER"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3", "title": "2.3 A Mixture of Overcomplete Low-Dimensional Subspaces", "snippet": "2.3 A Mixture of Overcomplete Low-Dimensional Subspaces As we have seen, complete dictionary learning enjoys an elegant computational theory in which we maintain a symmetric autoencoding structure ‚Ñ∞ ‚Äã ( ùíô ) = ùëº ‚ä§ ‚Äã ùíô \\mathcal{E}(\\bm{x})=\\bm{U}^{\\top}\\bm{x} caligraphic_E ( bold_italic_x ) = bold_italic_U start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_x , ùíü ‚Äã ( ùíõ ) = ùëº ‚Äã ùíõ \\mathcal{D}(\\bm{z})=\\bm{U}\\bm{z} caligraphic_D ( bold_italic_z ) = bold_italic_U bold_italic_z , with a scalable power-method-like algorithm (the MSP algorithm) for learning an orthogonal dictionary/codebook ùëº \\bm{U} "}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S4", "title": "2.4 Summary and Notes", "snippet": "2.4 Summary and Notes The idealistic models we have presented in this chapter‚ÄîPCA, ICA, and dictionary learning‚Äîwere developed over the course of the twentieth century. Many books have been written solely about each method, so we will only attempt here to give a broad overview of the key works and history. Jolliffe [ Jol86 ] attributes principal component analysis to Pearson [ Pea01 ] , and independently Hotelling [ Hot33 ] . In mathematics, the main result on the related problem of low-rank approximation in unitarily invariant norms is attributed to Eckart and Young [ EY36 ] , and to Mirsky f"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5", "title": "2.5 Exercises and Extensions", "snippet": "2.5 Exercises and Extensions Exercise 2.1 . Prove that, for any symmetric matrix ùë® \\bm{A} bold_italic_A , the solution to the problem max ùëº ‚àà ùñÆ ‚Äã ( D , d ) ‚Å° tr ‚Å° ( ùëº ‚ä§ ‚Äã ùë® ‚Äã ùëº ) \\max_{\\bm{U}\\in\\mathsf{O}(D,d)}\\operatorname{tr}\\left(\\bm{U}^{\\top}\\bm{A}\\bm{U}\\right) roman_max start_POSTSUBSCRIPT bold_italic_U ‚àà sansserif_O ( italic_D , italic_d ) end_POSTSUBSCRIPT roman_tr ( bold_italic_U start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_A bold_italic_U ) is the matrix ùëº ‚ãÜ \\bm{U}^{\\star} bold_italic_U start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT whose columns are the top d d italic_d unit "}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.SS1", "title": "2.1.1 Principal Components Analysis (PCA)", "snippet": "2.1.1 Principal Components Analysis (PCA) Perhaps the simplest modeling assumption possible for low-dimensional structure is the so-called low-rank assumption. Letting D D italic_D be the dimension of our data space, we assume that our data belong to a low-dimensional subspace of dimension d ‚â™ D d\\ll D italic_d ‚â™ italic_D , possibly plus some small disturbances. This ends up being a nearly valid assumption for some surprisingly complex data, such as images of handwritten digits and face data [ RD03 ] as shown in Figure 2.1 , yet as we will see, it will lend itself extremely well to comprehensi"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.SS2", "title": "2.1.2 Pursuing Low-rank Structure via Power Iteration", "snippet": "2.1.2 Pursuing Low-rank Structure via Power Iteration There is a computationally efficient way to estimate the top eigenvectors of ùëø ‚Äã ùëø ‚ä§ / N \\bm{X}\\bm{X}^{\\top}/N bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT / italic_N or any symmetric positive semidefinite matrix ùë¥ \\bm{M} bold_italic_M , called power iteration . This method is the building block of several algorithmic approaches to high-dimensional data analysis that we discuss later in the Chapter, so we discuss it here. Let ùë¥ \\bm{M} bold_italic_M be a symmetric positive semidefinite matrix. There exists an ortho"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.SS3", "title": "2.1.3 Probabilistic PCA", "snippet": "2.1.3 Probabilistic PCA Notice that the above formulation makes no statistical assumptions on the data-generating process. However, it is common to include statistical elements within a given data model, as it may add further enlightening interpretations about the result of the analysis. As such, we ask the natural question: what is the statistical analogue to low-dimensional structure? Our answer is that a low-dimensional distribution is one whose support is concentrated around a low-dimensional geometric structure. To illustrate this point, we discuss probabilistic principal component analys"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.SS4", "title": "2.1.4 Matrix Completion", "snippet": "2.1.4 Matrix Completion In the previous Subsections, we discussed the problem of learning a low-rank geometric or statistical distribution , where the data were sampled from a subspace with additive noise. But this is not the only type of disturbance from a low-dimensional distribution that is worthwhile to study. In this subsection, we introduce one more class of non-additive errors which become increasingly important in deep learning. Let us consider the case where we have some data { ùíô i } i = 1 n \\{\\bm{x}_{i}\\}_{i=1}^{n} { bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } star"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.SS1", "title": "2.2.1 Mixtures of Subspaces and Sparsely-Used Dictionaries", "snippet": "2.2.1 Mixtures of Subspaces and Sparsely-Used Dictionaries Let ùëº 1 , ‚Ä¶ , ùëº K \\bm{U}_{1},\\dots,\\bm{U}_{K} bold_italic_U start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_U start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT , each of size D √ó d D\\times d italic_D √ó italic_d , denote a collection of orthonormal bases for K K italic_K subspaces of dimension d d italic_d in ‚Ñù D \\mathbb{R}^{D} blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT . To say that ùíô \\bm{x} bold_italic_x follows a mixture-of-subspaces distribution parameterized by ùëº 1 , ‚Ä¶ , ùëº K \\bm{U}_{1},\\dots,\\bm{U}_{K} b"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.SS2", "title": "2.2.2 Complete Dictionary Learning", "snippet": "2.2.2 Complete Dictionary Learning In this section, we will derive algorithms for solving the orthogonal dictionary learning problem. To be more precise, we assume that the observed vector ùíô ‚àà ‚Ñù D \\bm{x}\\in\\mathbb{R}^{D} bold_italic_x ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT follows a statistical model ùíô = ùëº ‚Äã ùíõ + ùú∫ , \\bm{x}=\\bm{U}\\bm{z}+\\bm{\\varepsilon}, bold_italic_x = bold_italic_U bold_italic_z + bold_italic_Œµ , (2.2.9) where ùëº ‚àà ‚Ñù D √ó D \\bm{U}\\in\\mathbb{R}^{D\\times D} bold_italic_U ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D √ó italic_D end_POSTSUPERSCRIPT is an "}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.SS3", "title": "2.2.3 Connection to ICA and Kurtosis", "snippet": "2.2.3 Connection to ICA and Kurtosis With the Bernoulli-Gaussian model, the variables z i z_{i} italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT are independent and non-Gaussian. Then, there is a clear correspondence between the dictionary learning and the classic independent component analysis (ICA), to the extent that algorithms to solve one problem can be used to solve the other. 8 8 8 We explore this issue in more depth in Exercise 2.3 , where a connection between non-Gaussianity of the independent components and the purely geometric notion of symmetry is made. This issue is related"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.SS1", "title": "2.3.1 Sparse Coding with an Overcomplete Dictionary", "snippet": "2.3.1 Sparse Coding with an Overcomplete Dictionary In this section, we will consider the data model ( 2.3.1 ), which accommodates sparse linear combinations of many motifs, or atoms . Given data { ùíô i } i = 1 N ‚äÜ ‚Ñù D \\{\\bm{x}_{i}\\}_{i=1}^{N}\\subseteq\\mathbb{R}^{D} { bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ‚äÜ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT satisfying this model, i.e. expressible as ùíô i = ùë® ‚Äã ùíõ i + ùú∫ i , ‚àÄ i ‚àà [ N ] \\bm{x}_{i}=\\bm{A}\\bm{z}_"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.SS2", "title": "2.3.2 Overcomplete Dictionary Learning", "snippet": "2.3.2 Overcomplete Dictionary Learning Recall that we have the data model ùëø = ùë® ‚Äã ùíÅ + ùë¨ , \\bm{X}=\\bm{A}\\bm{Z}+\\bm{E}, bold_italic_X = bold_italic_A bold_italic_Z + bold_italic_E , (2.3.14) where ùíÅ \\bm{Z} bold_italic_Z is sparse, and our goal previously was to estimate ùíÅ \\bm{Z} bold_italic_Z given knowledge of the data ùëø \\bm{X} bold_italic_X and the dictionary atoms ùë® \\bm{A} bold_italic_A . Now we turn to the more practical and more difficult case where we do not know either ùë® \\bm{A} bold_italic_A or ùíÅ \\bm{Z} bold_italic_Z and seek to learn them from a large dataset. A direct generalization of "}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.SS3", "title": "2.3.3 Learned Deep Sparse Coding", "snippet": "2.3.3 Learned Deep Sparse Coding The main insight from the alternating minimization algorithm for overcomplete dictionary learning in the previous section ( Equations 2.3.18 and 2.3.20 ) is to notice that when we fix ùêÄ \\bm{A} bold_italic_A , the ISTA update for ùêô ‚Ñì \\bm{Z}^{\\ell} bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ( 2.3.18 ) looks like the forward pass of a deep neural network with weights given by ùêÄ \\bm{A} bold_italic_A (and ùêÄ ‚ä§ \\bm{A}^{\\top} bold_italic_A start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ) . But in general, we do not know the true ùë® \\bm{A} bold_italic_A"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.SS1.SSS0.Px1", "title": "Problem formulation.", "snippet": "Problem formulation. To write this in mathematical notation, we represent a subspace ùíÆ ‚äÜ ‚Ñù D \\mathcal{S}\\subseteq\\mathbb{R}^{D} caligraphic_S ‚äÜ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT of dimension d d italic_d by an orthonormal matrix ùëº ‚àà ùñÆ ‚Äã ( D , d ) ‚äÜ ‚Ñù D √ó d \\bm{U}\\in\\mathsf{O}(D,d)\\subseteq\\mathbb{R}^{D\\times d} bold_italic_U ‚àà sansserif_O ( italic_D , italic_d ) ‚äÜ blackboard_R start_POSTSUPERSCRIPT italic_D √ó italic_d end_POSTSUPERSCRIPT such that the columns of ùëº \\bm{U} bold_italic_U span ùíÆ \\mathcal{S} caligraphic_S . Then, we say that our data { ùíô i } i = 1 N ‚äÜ "}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.SS1.SSS0.Px2", "title": "Subspace encoding-decoding via denoising.", "snippet": "Subspace encoding-decoding via denoising. To simplify this problem, for a fixed ùëº ~ \\tilde{\\bm{U}} over~ start_ARG bold_italic_U end_ARG , we have (proof as exercise): min { ùíõ ~ i } i = 1 N ‚Å° 1 N ‚Äã ‚àë i = 1 N ‚Äñ ùíô i ‚àí ùëº ~ ‚Äã ùíõ ~ i ‚Äñ 2 2 \\displaystyle\\min_{\\{\\tilde{\\bm{z}}_{i}\\}_{i=1}^{N}}\\frac{1}{N}\\sum_{i=1}^{N}\\|\\bm{x}_{i}-\\tilde{\\bm{U}}\\tilde{\\bm{z}}_{i}\\|_{2}^{2} roman_min start_POSTSUBSCRIPT { over~ start_ARG bold_italic_z end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT end_POSTSUBS"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.SS1.SSS0.Px3", "title": "Computing the subspace basis.", "snippet": "Computing the subspace basis. For now, we continue our calculation. Let ùëø = [ ùíô 1 , ‚Ä¶ , ùíô N ] ‚àà ‚Ñù D √ó N \\bm{X}=\\begin{bmatrix}\\bm{x}_{1},\\dots,\\bm{x}_{N}\\end{bmatrix}\\in\\mathbb{R}^{D\\times N} bold_italic_X = [ start_ARG start_ROW start_CELL bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D √ó italic_N end_POSTSUPERSCRIPT be the matrix whose columns are the observations ùíô i \\bm{x}_{i} bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT . We have"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.SS1.SSS0.Px1", "title": "Orthogonal dictionary for sparse coding.", "snippet": "Orthogonal dictionary for sparse coding. Now we can formulate the structured representation learning problem for mixtures of low-dimensional subspaces that we will study in this section. We assume that we have samples ùëø = [ ùíô 1 , ‚Ä¶ ‚Äã ùíô N ] \\bm{X}=[\\bm{x}_{1},\\dots\\bm{x}_{N}] bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ] from an unknown sparse dictionary model ( 2.2.7 ), possibly with added noises ùú∫ i \\bm{\\varepsilon}_{i} bold_italic_Œµ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT . Let us begin from t"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.SS2.SSS0.Px1", "title": "Dictionary learning via the MSP algorithm.", "snippet": "Dictionary learning via the MSP algorithm. Now suppose that we are given a set of observations: ùíô i = ùëº ‚Äã ùíõ i + ùú∫ i , ‚àÄ i ‚àà [ N ] . \\bm{x}_{i}=\\bm{U}\\bm{z}_{i}+\\bm{\\varepsilon}_{i},\\ \\forall i\\in[N]. bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_U bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + bold_italic_Œµ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , ‚àÄ italic_i ‚àà [ italic_N ] . (2.2.12) Let ùëø = [ ùíô 1 , ‚Ä¶ , ùíô N ] \\bm{X}=[\\bm{x}_{1},\\dots,\\bm{x}_{N}] bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_x start"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.SS3.SSS0.Px1", "title": "Incremental ICA: correctness and FastICA algorithm.", "snippet": "Incremental ICA: correctness and FastICA algorithm. The FastICA algorithm, advanced by Hyv√§rinen and Oja [ HO97 ] , is a fast fixed-point algorithm for solving the k = 1 k=1 italic_k = 1 kurtosis maximization scheme for ICA. The problem at hand is max ‚Äñ ùíó ‚Äñ 2 2 = 1 ‚Äã kurt ( ùëø ‚ä§ ‚Äã ùíó ) . \\max_{\\|\\bm{v}\\|_{2}^{2}=1}\\,\\mathop{\\mathrm{kurt}}(\\bm{X}^{\\top}\\bm{v}). roman_max start_POSTSUBSCRIPT ‚à• bold_italic_v ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 1 end_POSTSUBSCRIPT roman_kurt ( bold_italic_X start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_v "}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.SS3.SSS0.Px1", "title": "Learned ISTA.", "snippet": "Learned ISTA. The above deep-network interpretation of the alternating minimization is more conceptual than practical, as the process could be rather inefficient and take many layers or iterations to converge. But this is mainly because we try to infer both ùíÅ \\bm{Z} bold_italic_Z and ùë® \\bm{A} bold_italic_A from ùëø \\bm{X} bold_italic_X . The problem can be significantly simplified and the above iterations can be made much more efficient in the supervised setting, where we have a dataset of input and output pairs ( ùëø , ùíÅ ) (\\bm{X},\\bm{Z}) ( bold_italic_X , bold_italic_Z ) distributed according to"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.SS3.SSS0.Px2", "title": "Sparse Autoencoders.", "snippet": "Sparse Autoencoders. The original motivation for overcomplete dictionary learning was to provide a simple generative model for high-dimensional data. We have seen with LISTA that, in addition, iterative algorithms for learning sparsely-used overcomplete dictionaries provide an interpretation for ReLU-like deep networks, which we will generalize in later chapters to more complex data distributions than ( 2.3.14 ). But it is also worth noting that even in the modern era of large models, the data generating model ( 2.3.14 ) provides a useful practical basis for interpreting features in pretrained"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.SS3.SSS0.Px3", "title": "Layerwise learned sparse coding?", "snippet": "Layerwise learned sparse coding? In the supervised setting, LISTA provides a deep neural network analogue of the sparse coding iteration, with layerwise-learned dictionaries, inspired by alternating minimization; even in the unsupervised setting, the same methodology can be applied to learning, as with sparse autoencoders. But the connection between low-dimensional-structure-seeking optimization algorithms and deep network architectures goes much deeper than this, and suggests an array of scalable and natural neural learning architectures which may even be usable without backpropagation. As a "}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#p1", "title": "‚Äú The art of doing mathematics consists in finding that special case which contains all the germs of generality .‚Äù ‚Äì David Hilbert", "snippet": "‚Äú The art of doing mathematics consists in finding that special case which contains all the germs of generality .‚Äù ‚Äì David Hilbert"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#p2", "title": "Real data has low-dimensional structure. To see why this is true, let us consider the unassuming case of static on a TV when the satellite isn‚Äôt working. At each frame (approximately every 1 30 \\frac{", "snippet": "Real data has low-dimensional structure. To see why this is true, let us consider the unassuming case of static on a TV when the satellite isn‚Äôt working. At each frame (approximately every 1 30 \\frac{1}{30} divide start_ARG 1 end_ARG start_ARG 30 end_ARG seconds), the RGB static on a screen of size H √ó W H\\times W italic_H √ó italic_W is, roughly, sampled independently from a uniform distribution on [ 0 , 1 ] 3 √ó H √ó W [0,1]^{3\\times H\\times W} [ 0 , 1 ] start_POSTSUPERSCRIPT 3 √ó italic_H √ó italic_W end_POSTSUPERSCRIPT . In theory, the static could resolve to a natural image on any given frame,"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#p3", "title": "Therefore, our central task is to learn a distribution that has low intrinsic dimension in a high-dimensional space. In the remainder of this section, we discuss several classical methods to perform t", "snippet": "Therefore, our central task is to learn a distribution that has low intrinsic dimension in a high-dimensional space. In the remainder of this section, we discuss several classical methods to perform this task for several somewhat idealistic models for the distribution, namely models that are geometrically linear or statistically independent. While these models and methods are important and useful in their own right, we discuss them here as they motivate, inspire, and serve as a predecessor or analogue to more modern methods for more general distributions that involve deep (representation) lear"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#p4", "title": "Our main approach (and general problem formulation) can be summarized as:", "snippet": "Our main approach (and general problem formulation) can be summarized as:"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#p5", "title": "Problem: Given one or several (noisy or incomplete) observations of a ground truth sample from the data distribution, obtain an estimate of this sample.", "snippet": "Problem: Given one or several (noisy or incomplete) observations of a ground truth sample from the data distribution, obtain an estimate of this sample."}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#p6", "title": "This approach underpins several classical methods for data processing, which we discuss in this chapter. ‚Ä¢ Section 2.1 ‚Äî Principal Components Analysis (PCA): Given noisy samples from a distribution su", "snippet": "This approach underpins several classical methods for data processing, which we discuss in this chapter. ‚Ä¢ Section 2.1 ‚Äî Principal Components Analysis (PCA): Given noisy samples from a distribution supported on one low-dimensional subspace , obtain an estimate of the true sample that lies on this subspace. ‚Ä¢ Section 2.2 ‚Äî Complete Dictionary Learning and Independent Components Analysis (ICA): Given noisy samples from a distribution supported on a union (not the span) of few low-dimensional subspaces , obtain an estimate of the true samples. ‚Ä¢ Section 2.3 ‚Äî Sparse Coding and Overcomplete Dictio"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S0.I1.i1.p1", "title": "Section 2.1 ‚Äî Principal Components Analysis (PCA): Given noisy samples from a distribution supported on one low-dimensional subspace , obtain an estimate of the true sample that lies on this subspace.", "snippet": "Section 2.1 ‚Äî Principal Components Analysis (PCA): Given noisy samples from a distribution supported on one low-dimensional subspace , obtain an estimate of the true sample that lies on this subspace."}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S0.I1.i2.p1", "title": "Section 2.2 ‚Äî Complete Dictionary Learning and Independent Components Analysis (ICA): Given noisy samples from a distribution supported on a union (not the span) of few low-dimensional subspaces , obt", "snippet": "Section 2.2 ‚Äî Complete Dictionary Learning and Independent Components Analysis (ICA): Given noisy samples from a distribution supported on a union (not the span) of few low-dimensional subspaces , obtain an estimate of the true samples."}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S0.I1.i3.p1", "title": "Section 2.3 ‚Äî Sparse Coding and Overcomplete Dictionary Learning: Given noisy samples from a distribution supported on combinations of a few incoherent vectors , such as the coordinate axes, obtain an", "snippet": "Section 2.3 ‚Äî Sparse Coding and Overcomplete Dictionary Learning: Given noisy samples from a distribution supported on combinations of a few incoherent vectors , such as the coordinate axes, obtain an estimate of the true sample, which also has this property."}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#p7", "title": "In this chapter, as described above, we make simplifying modeling assumptions that essentially assume the data have geometrically (nearly, piece-wise) linear structures and statistically independent c", "snippet": "In this chapter, as described above, we make simplifying modeling assumptions that essentially assume the data have geometrically (nearly, piece-wise) linear structures and statistically independent components. In Chapter 1 , we have referred to such models of the data as ‚Äúanalytical models‚Äù. These modeling assumptions allow us to derive efficient algorithms with provable efficiency guarantees 1 1 1 Both in terms of data and computational complexity. for processing data at scale. However, they are imperfect models for often-complex real-world data distributions, and so their underlying assumpt"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.SS1.p1", "title": "Perhaps the simplest modeling assumption possible for low-dimensional structure is the so-called low-rank assumption. Letting D D italic_D be the dimension of our data space, we assume that our data b", "snippet": "Perhaps the simplest modeling assumption possible for low-dimensional structure is the so-called low-rank assumption. Letting D D italic_D be the dimension of our data space, we assume that our data belong to a low-dimensional subspace of dimension d ‚â™ D d\\ll D italic_d ‚â™ italic_D , possibly plus some small disturbances. This ends up being a nearly valid assumption for some surprisingly complex data, such as images of handwritten digits and face data [ RD03 ] as shown in Figure 2.1 , yet as we will see, it will lend itself extremely well to comprehensive analysis."}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.SS1.SSS0.Px1.p1", "title": "To write this in mathematical notation, we represent a subspace ùíÆ ‚äÜ ‚Ñù D \\mathcal{S}\\subseteq\\mathbb{R}^{D} caligraphic_S ‚äÜ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT of dimension ", "snippet": "To write this in mathematical notation, we represent a subspace ùíÆ ‚äÜ ‚Ñù D \\mathcal{S}\\subseteq\\mathbb{R}^{D} caligraphic_S ‚äÜ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT of dimension d d italic_d by an orthonormal matrix ùëº ‚àà ùñÆ ‚Äã ( D , d ) ‚äÜ ‚Ñù D √ó d \\bm{U}\\in\\mathsf{O}(D,d)\\subseteq\\mathbb{R}^{D\\times d} bold_italic_U ‚àà sansserif_O ( italic_D , italic_d ) ‚äÜ blackboard_R start_POSTSUPERSCRIPT italic_D √ó italic_d end_POSTSUPERSCRIPT such that the columns of ùëº \\bm{U} bold_italic_U span ùíÆ \\mathcal{S} caligraphic_S . Then, we say that our data { ùíô i } i = 1 N ‚äÜ ‚Ñù D \\{\\bm{x}_{i}\\}_{i"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.SS1.SSS0.Px1.p2", "title": "Given data { ùíô i } i = 1 N \\{\\bm{x}_{i}\\}_{i=1}^{N} { bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N e", "snippet": "Given data { ùíô i } i = 1 N \\{\\bm{x}_{i}\\}_{i=1}^{N} { bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT distributed as in ( 2.1.1 ), we aim to recover the model ùëº \\bm{U} bold_italic_U . A natural approach is to find the subspace ùëº ‚ãÜ \\bm{U}^{\\star} bold_italic_U start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT and latent vectors { ùíõ i ‚ãÜ } i = 1 N \\{\\bm{z}_{i}^{\\star}\\}_{i=1}^{N} { bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT } "}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.SS1.SSS0.Px2.p1", "title": "To simplify this problem, for a fixed ùëº ~ \\tilde{\\bm{U}} over~ start_ARG bold_italic_U end_ARG , we have (proof as exercise): min { ùíõ ~ i } i = 1 N ‚Å° 1 N ‚Äã ‚àë i = 1 N ‚Äñ ùíô i ‚àí ùëº ~ ‚Äã ùíõ ~ i ‚Äñ 2 2 \\display", "snippet": "To simplify this problem, for a fixed ùëº ~ \\tilde{\\bm{U}} over~ start_ARG bold_italic_U end_ARG , we have (proof as exercise): min { ùíõ ~ i } i = 1 N ‚Å° 1 N ‚Äã ‚àë i = 1 N ‚Äñ ùíô i ‚àí ùëº ~ ‚Äã ùíõ ~ i ‚Äñ 2 2 \\displaystyle\\min_{\\{\\tilde{\\bm{z}}_{i}\\}_{i=1}^{N}}\\frac{1}{N}\\sum_{i=1}^{N}\\|\\bm{x}_{i}-\\tilde{\\bm{U}}\\tilde{\\bm{z}}_{i}\\|_{2}^{2} roman_min start_POSTSUBSCRIPT { over~ start_ARG bold_italic_z end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.SS1.SSS0.Px2.p2", "title": "Now, we can write the original optimization problem in ùëº ~ ‚ãÜ \\tilde{\\bm{U}}^{\\star} over~ start_ARG bold_italic_U end_ARG start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT and { ùíõ ~ i } i = 1 N \\{\\tilde{\\bm", "snippet": "Now, we can write the original optimization problem in ùëº ~ ‚ãÜ \\tilde{\\bm{U}}^{\\star} over~ start_ARG bold_italic_U end_ARG start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT and { ùíõ ~ i } i = 1 N \\{\\tilde{\\bm{z}}_{i}\\}_{i=1}^{N} { over~ start_ARG bold_italic_z end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT as an optimization problem just over ùëº ~ \\tilde{\\bm{U}} over~ start_ARG bold_italic_U end_ARG , i.e., to obtain the basis ùëº ‚ãÜ \\bm{U}^{\\star} bold_italic_U start_POSTSUPERSCRIPT ‚ãÜ end_POSTS"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.SS1.SSS0.Px2.p3", "title": "Putting the above process together, we essentially obtain a simple encoding-decoding scheme that maps a data point ùíô \\bm{x} bold_italic_x in ‚Ñù D \\mathbb{R}^{D} blackboard_R start_POSTSUPERSCRIPT itali", "snippet": "Putting the above process together, we essentially obtain a simple encoding-decoding scheme that maps a data point ùíô \\bm{x} bold_italic_x in ‚Ñù D \\mathbb{R}^{D} blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT to a lower-dimensional (latent) space ‚Ñù d \\mathbb{R}^{d} blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT and then back to ‚Ñù D \\mathbb{R}^{D} blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT : ùíô ‚Üí ‚Ñ∞ = ( ùëº ‚ãÜ ) ‚ä§ ùíõ ‚Üí ùíü = ùëº ‚ãÜ ùíô ^ . \\bm{x}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}=(\\bm{U}^{\\star})^{\\top}\\hskip 5.69054pt}\\bm{z}\\xrightarrow{\\hskip 5.69"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.SS1.SSS0.Px3.p1", "title": "For now, we continue our calculation. Let ùëø = [ ùíô 1 , ‚Ä¶ , ùíô N ] ‚àà ‚Ñù D √ó N \\bm{X}=\\begin{bmatrix}\\bm{x}_{1},\\dots,\\bm{x}_{N}\\end{bmatrix}\\in\\mathbb{R}^{D\\times N} bold_italic_X = [ start_ARG start_ROW ", "snippet": "For now, we continue our calculation. Let ùëø = [ ùíô 1 , ‚Ä¶ , ùíô N ] ‚àà ‚Ñù D √ó N \\bm{X}=\\begin{bmatrix}\\bm{x}_{1},\\dots,\\bm{x}_{N}\\end{bmatrix}\\in\\mathbb{R}^{D\\times N} bold_italic_X = [ start_ARG start_ROW start_CELL bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D √ó italic_N end_POSTSUPERSCRIPT be the matrix whose columns are the observations ùíô i \\bm{x}_{i} bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT . We have (proof as exercise) arg ‚Äã min"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmtheorem1.p1", "title": "Suppose that our dataset { ùê± i } i = 1 N ‚äÜ ‚Ñù D \\{\\bm{x}_{i}\\}_{i=1}^{N}\\subseteq\\mathbb{R}^{D} { bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POS", "snippet": "Suppose that our dataset { ùê± i } i = 1 N ‚äÜ ‚Ñù D \\{\\bm{x}_{i}\\}_{i=1}^{N}\\subseteq\\mathbb{R}^{D} { bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ‚äÜ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT can be written as ùíô i = ùëº ‚Äã ùíõ i + ùú∫ i , ‚àÄ i ‚àà [ N ] , \\bm{x}_{i}=\\bm{U}\\bm{z}_{i}+\\bm{\\varepsilon}_{i},\\qquad\\forall i\\in[N], bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_U bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + bold_i"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.SS1.SSS0.Px3.p2", "title": "We do not give explicit rates of approximation here as they can become rather technical. In the special case that ùú∫ i = ùüé \\bm{\\varepsilon}_{i}=\\bm{0} bold_italic_Œµ start_POSTSUBSCRIPT italic_i end_POS", "snippet": "We do not give explicit rates of approximation here as they can become rather technical. In the special case that ùú∫ i = ùüé \\bm{\\varepsilon}_{i}=\\bm{0} bold_italic_Œµ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_0 for all i i italic_i , the learned ùëº ‚ãÜ \\bm{U}^{\\star} bold_italic_U start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT spans the support of the samples { ùíô i } i = 1 N \\{\\bm{x}_{i}\\}_{i=1}^{N} { bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT . If in addition the ùíõ i \\b"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmremark1.p1", "title": "In some data analysis tasks, the data matrix ùëø \\bm{X} bold_italic_X is formatted such that each data point is a row rather than a column as is presented here. In this case the principal components are", "snippet": "In some data analysis tasks, the data matrix ùëø \\bm{X} bold_italic_X is formatted such that each data point is a row rather than a column as is presented here. In this case the principal components are the top d d italic_d eigenvectors of ùëø ‚ä§ ‚Äã ùëø / N \\bm{X}^{\\top}\\bm{X}/N bold_italic_X start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_X / italic_N ."}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmremark2.p1", "title": "In many cases, either our data will not truly be distributed according to a subspace-plus-noise model, or we will not know the true underlying dimension d d italic_d of the subspace. In this case, we ", "snippet": "In many cases, either our data will not truly be distributed according to a subspace-plus-noise model, or we will not know the true underlying dimension d d italic_d of the subspace. In this case, we have to choose d d italic_d ; this problem is called model selection . In the restricted case of PCA, one way to perform model selection is to compute ùëø ‚Äã ùëø ‚ä§ / N \\bm{X}\\bm{X}^{\\top}/N bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT / italic_N and look for instances where adjacent eigenvalues sharply decrease; this is one indicator that the index of the larger eigenvalue is"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmremark3.p1", "title": "The expression on the right-hand side of ( 2.1.5 ), that is, min ùëº ~ ‚Å° 1 N ‚Äã ‚àë i = 1 N ‚Äñ ùíô i ‚àí ùëº ~ ‚Äã ùëº ~ ‚ä§ ‚Äã ùíô i ‚Äñ 2 2 , \\min_{\\tilde{\\bm{U}}}\\frac{1}{N}\\sum_{i=1}^{N}\\|\\bm{x}_{i}-\\tilde{\\bm{U}}\\tilde", "snippet": "The expression on the right-hand side of ( 2.1.5 ), that is, min ùëº ~ ‚Å° 1 N ‚Äã ‚àë i = 1 N ‚Äñ ùíô i ‚àí ùëº ~ ‚Äã ùëº ~ ‚ä§ ‚Äã ùíô i ‚Äñ 2 2 , \\min_{\\tilde{\\bm{U}}}\\frac{1}{N}\\sum_{i=1}^{N}\\|\\bm{x}_{i}-\\tilde{\\bm{U}}\\tilde{\\bm{U}}^{\\top}\\bm{x}_{i}\\|_{2}^{2}, roman_min start_POSTSUBSCRIPT over~ start_ARG bold_italic_U end_ARG end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ‚à• bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over~ start_ARG bold_italic_U end_ARG over~ start_ARG "}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmremark4.p1", "title": "If we do a PCA, we approximately recover the support of the distribution encoded by the parameter ùëº ‚ãÜ \\bm{U}^{\\star} bold_italic_U start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT . The learned denoising m", "snippet": "If we do a PCA, we approximately recover the support of the distribution encoded by the parameter ùëº ‚ãÜ \\bm{U}^{\\star} bold_italic_U start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT . The learned denoising map then takes the form ùëº ‚ãÜ ‚Äã ( ùëº ‚ãÜ ) ‚ä§ \\bm{U}^{\\star}(\\bm{U}^{\\star})^{\\top} bold_italic_U start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ( bold_italic_U start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT . On top of being a denoiser, this can be viewed as a simple two-layer weight-tied linear neural network : the first layer multiplies by ( ùëº ‚ãÜ ) ‚ä§ (\\bm{U}^{\\st"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.SS2.p1", "title": "There is a computationally efficient way to estimate the top eigenvectors of ùëø ‚Äã ùëø ‚ä§ / N \\bm{X}\\bm{X}^{\\top}/N bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT / italic_N or any", "snippet": "There is a computationally efficient way to estimate the top eigenvectors of ùëø ‚Äã ùëø ‚ä§ / N \\bm{X}\\bm{X}^{\\top}/N bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT / italic_N or any symmetric positive semidefinite matrix ùë¥ \\bm{M} bold_italic_M , called power iteration . This method is the building block of several algorithmic approaches to high-dimensional data analysis that we discuss later in the Chapter, so we discuss it here."}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.SS2.p2", "title": "Let ùë¥ \\bm{M} bold_italic_M be a symmetric positive semidefinite matrix. There exists an orthonormal basis for ‚Ñù D \\mathbb{R}^{D} blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT consist", "snippet": "Let ùë¥ \\bm{M} bold_italic_M be a symmetric positive semidefinite matrix. There exists an orthonormal basis for ‚Ñù D \\mathbb{R}^{D} blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT consisting of eigenvectors ( ùíò i ) i = 1 D (\\bm{w}_{i})_{i=1}^{D} ( bold_italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT of ùë¥ \\bm{M} bold_italic_M , with corresponding eigenvalues Œª 1 ‚â• ‚ãØ ‚â• Œª D ‚â• 0 \\lambda_{1}\\geq\\cdots\\geq\\lambda_{D}\\geq 0 italic_Œª start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ‚â• ‚ãØ ‚â•"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmtheorem2.p1", "title": "Assume that Œª 1 > Œª i \\lambda_{1}>\\lambda_{i} italic_Œª start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT > italic_Œª start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT for all i > 1 i>1 italic_i > 1 . If we compute ", "snippet": "Assume that Œª 1 > Œª i \\lambda_{1}>\\lambda_{i} italic_Œª start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT > italic_Œª start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT for all i > 1 i>1 italic_i > 1 . If we compute the fixed point of ( 2.1.16 ) using the following iteration: ùíó 0 ‚àº ùí© ‚Å° ( ùüé , ùüè ) , ùíó t + 1 ‚Üê ùë¥ ‚Äã ùíó t ‚Äñ ùë¥ ‚Äã ùíó t ‚Äñ 2 , \\bm{v}_{0}\\sim\\operatorname{\\mathcal{N}}(\\bm{0},\\bm{1}),\\qquad\\bm{v}_{t+1}\\leftarrow\\frac{\\bm{M}\\bm{v}_{t}}{\\|\\bm{M}\\bm{v}_{t}\\|_{2}}, bold_italic_v start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ‚àº caligraphic_N ( bold_0 , bold_1 ) , bold_italic_v start_POSTSUBSCRIPT italic_t + 1 end_POSTS"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.SS2.p3", "title": "First, note that for all t t italic_t , we have ùíó t = ùë¥ ‚Äã ùíó t ‚àí 1 ‚Äñ ùë¥ ‚Äã ùíó t ‚àí 1 ‚Äñ 2 = ùë¥ 2 ‚Äã ùíó t ‚àí 2 ‚Äñ ùë¥ ‚Äã ùíó t ‚àí 1 ‚Äñ 2 ‚Äã ‚Äñ ùë¥ ‚Äã ùíó t ‚àí 2 ‚Äñ 2 = ‚ãØ = ùë¥ t ‚Äã ùíó 0 ‚àè s = 1 t ‚Äñ ùë¥ ‚Äã ùíó s ‚Äñ 2 . \\bm{v}_{t}=\\frac{\\bm", "snippet": "First, note that for all t t italic_t , we have ùíó t = ùë¥ ‚Äã ùíó t ‚àí 1 ‚Äñ ùë¥ ‚Äã ùíó t ‚àí 1 ‚Äñ 2 = ùë¥ 2 ‚Äã ùíó t ‚àí 2 ‚Äñ ùë¥ ‚Äã ùíó t ‚àí 1 ‚Äñ 2 ‚Äã ‚Äñ ùë¥ ‚Äã ùíó t ‚àí 2 ‚Äñ 2 = ‚ãØ = ùë¥ t ‚Äã ùíó 0 ‚àè s = 1 t ‚Äñ ùë¥ ‚Äã ùíó s ‚Äñ 2 . \\bm{v}_{t}=\\frac{\\bm{M}\\bm{v}_{t-1}}{\\|\\bm{M}\\bm{v}_{t-1}\\|_{2}}=\\frac{\\bm{M}^{2}\\bm{v}_{t-2}}{\\|\\bm{M}\\bm{v}_{t-1}\\|_{2}\\|\\bm{M}\\bm{v}_{t-2}\\|_{2}}=\\cdots=\\frac{\\bm{M}^{t}\\bm{v}_{0}}{\\prod_{s=1}^{t}\\|\\bm{M}\\bm{v}_{s}\\|_{2}}. bold_italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = divide start_ARG bold_italic_M bold_italic_v start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT end_ARG start_ARG ‚à• bold_italic_M b"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.SS2.p4", "title": "To find the second top eigenvector, we apply the power iteration algorithm to ùë¥ ‚àí Œª 1 ‚Äã ùíó 1 ‚Äã ùíó 1 ‚ä§ \\bm{M}-\\lambda_{1}\\bm{v}_{1}\\bm{v}_{1}^{\\top} bold_italic_M - italic_Œª start_POSTSUBSCRIPT 1 end_POS", "snippet": "To find the second top eigenvector, we apply the power iteration algorithm to ùë¥ ‚àí Œª 1 ‚Äã ùíó 1 ‚Äã ùíó 1 ‚ä§ \\bm{M}-\\lambda_{1}\\bm{v}_{1}\\bm{v}_{1}^{\\top} bold_italic_M - italic_Œª start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_italic_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT , which has eigenvectors ( ùíò i ) i = 2 D (\\bm{w}_{i})_{i=2}^{D} ( bold_italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i = 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT and co"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.SS3.p1", "title": "Notice that the above formulation makes no statistical assumptions on the data-generating process. However, it is common to include statistical elements within a given data model, as it may add furthe", "snippet": "Notice that the above formulation makes no statistical assumptions on the data-generating process. However, it is common to include statistical elements within a given data model, as it may add further enlightening interpretations about the result of the analysis. As such, we ask the natural question: what is the statistical analogue to low-dimensional structure? Our answer is that a low-dimensional distribution is one whose support is concentrated around a low-dimensional geometric structure."}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.SS3.p2", "title": "To illustrate this point, we discuss probabilistic principal component analysis (PPCA). This formulation can be viewed as a statistical variant of regular PCA. Mathematically, we now consider our data", "snippet": "To illustrate this point, we discuss probabilistic principal component analysis (PPCA). This formulation can be viewed as a statistical variant of regular PCA. Mathematically, we now consider our data as samples from a random variable ùíô \\bm{x} bold_italic_x taking values in ‚Ñù D \\mathbb{R}^{D} blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT (also sometimes called a random vector ). We say that ùíô \\bm{x} bold_italic_x has (approximate) low-rank statistical structure if and only if there exists an orthonormal matrix ùëº ‚àà ùñÆ ‚Äã ( D , d ) \\bm{U}\\in\\mathsf{O}(D,d) bold_italic_U ‚àà sansser"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmremark5.p1", "title": "By using the probabilistic viewpoint of PCA, we achieve a clearer and more quantitative understanding of how it relates to denoising. First, consider the denoising problem in ( 2.1.26 ), namely min ùëº ", "snippet": "By using the probabilistic viewpoint of PCA, we achieve a clearer and more quantitative understanding of how it relates to denoising. First, consider the denoising problem in ( 2.1.26 ), namely min ùëº ~ ‚Å° ùîº ‚Å° ‚Äñ ùíô ‚àí ùëº ~ ‚Äã ùëº ~ ‚ä§ ‚Äã ùíô ‚Äñ 2 2 . \\min_{\\tilde{\\bm{U}}}\\operatorname{\\mathbb{E}}\\|\\bm{x}-\\tilde{\\bm{U}}\\tilde{\\bm{U}}^{\\top}\\bm{x}\\|_{2}^{2}. roman_min start_POSTSUBSCRIPT over~ start_ARG bold_italic_U end_ARG end_POSTSUBSCRIPT blackboard_E ‚à• bold_italic_x - over~ start_ARG bold_italic_U end_ARG over~ start_ARG bold_italic_U end_ARG start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_x ‚à• s"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmtheorem3.p1", "title": "Suppose that the random variable ùê± \\bm{x} bold_italic_x can be written as ùíô = ùëº ‚Äã ùíõ + ùú∫ \\bm{x}=\\bm{U}\\bm{z}+\\bm{\\varepsilon} bold_italic_x = bold_italic_U bold_italic_z + bold_italic_Œµ (2.1.34) where ", "snippet": "Suppose that the random variable ùê± \\bm{x} bold_italic_x can be written as ùíô = ùëº ‚Äã ùíõ + ùú∫ \\bm{x}=\\bm{U}\\bm{z}+\\bm{\\varepsilon} bold_italic_x = bold_italic_U bold_italic_z + bold_italic_Œµ (2.1.34) where ùêî ‚àà ùñÆ ‚Äã ( D , d ) \\bm{U}\\in\\mathsf{O}(D,d) bold_italic_U ‚àà sansserif_O ( italic_D , italic_d ) captures the low-rank structure, ùê≥ \\bm{z} bold_italic_z is a random vector taking values in ‚Ñù d \\mathbb{R}^{d} blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , and ùõÜ \\bm{\\varepsilon} bold_italic_Œµ is a random vector taking values in ‚Ñù D \\mathbb{R}^{D} blackboard_R start_POSTSUPERSCRIPT i"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.SS4.p1", "title": "In the previous Subsections, we discussed the problem of learning a low-rank geometric or statistical distribution , where the data were sampled from a subspace with additive noise. But this is not th", "snippet": "In the previous Subsections, we discussed the problem of learning a low-rank geometric or statistical distribution , where the data were sampled from a subspace with additive noise. But this is not the only type of disturbance from a low-dimensional distribution that is worthwhile to study. In this subsection, we introduce one more class of non-additive errors which become increasingly important in deep learning. Let us consider the case where we have some data { ùíô i } i = 1 n \\{\\bm{x}_{i}\\}_{i=1}^{n} { bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.SS4.p2", "title": "There are many excellent resources discussing algorithms and approaches to solve this problem [ WM22 ] . Indeed, this and similar generalizations of this low-rank structure recovery problem are resolv", "snippet": "There are many excellent resources discussing algorithms and approaches to solve this problem [ WM22 ] . Indeed, this and similar generalizations of this low-rank structure recovery problem are resolved by ‚Äúrobust PCA‚Äù. We will not go into the solution method here. Instead, we will discuss under what conditions this problem is plausible to solve. On one hand, in the most absurd case, suppose that each entry of the matrix ùëø \\bm{X} bold_italic_X were chosen independently from all the others. Then there would be no hope of recovering ùëø \\bm{X} bold_italic_X exactly even if only one entry were miss"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.SS4.p3", "title": "In the real world, actual problems are somewhere in between the two limiting cases discussed above. Yet the differences between these two extremes, as well as the earlier discussion of PCA, reveal a g", "snippet": "In the real world, actual problems are somewhere in between the two limiting cases discussed above. Yet the differences between these two extremes, as well as the earlier discussion of PCA, reveal a general kernel of truth: The lower-dimensional and more structured the data distribution is, the easier it is to process, and the fewer observations are needed‚Äîprovided that the algorithm effectively utilizes this low-dimensional structure. As is perhaps predictable, we will encounter this motif repeatedly in the remainder of the manuscript, starting in the very next section."}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.p1", "title": "As we have seen, low-rank signal models are rich enough to provide a full picture of the interplay between low-dimensionality in data and efficient and scalable computational algorithms for representa", "snippet": "As we have seen, low-rank signal models are rich enough to provide a full picture of the interplay between low-dimensionality in data and efficient and scalable computational algorithms for representation and recovery under errors. These models imply a linear and symmetric representation learning pipeline ( 2.1.6 ): ùíõ = ‚Ñ∞ ‚Äã ( ùíô ) = ùëº ‚ä§ ‚Äã ùíô , ùíô ^ = ùíü ‚Äã ( ùíõ ) = ùëº ‚Äã ùíõ , \\bm{z}=\\mathcal{E}(\\bm{x})=\\bm{U}^{\\top}\\bm{x},\\quad\\hat{\\bm{x}}=\\mathcal{D}(\\bm{z})=\\bm{U}\\bm{z}, bold_italic_z = caligraphic_E ( bold_italic_x ) = bold_italic_U start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_x , over^ s"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.p2", "title": "Even accounting for its elegance and simplicity, the low-rank assumption is too restrictive to be broadly applicable to modeling real-world data. A key limitation is the assumption of a single linear ", "snippet": "Even accounting for its elegance and simplicity, the low-rank assumption is too restrictive to be broadly applicable to modeling real-world data. A key limitation is the assumption of a single linear subspace that is responsible for generating the structured observations. In many practical applications, structure generated by a mixture of distinct low-dimensional subspaces provides a more realistic model. For example, consider a video sequence that captures the motion of several distinct objects, each subject to its own independent displacement (Fig. 2.3 left). Under suitable assumptions on th"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.p3", "title": "In this section, we will begin by discussing the conceptual and algorithmic foundations for structured representation learning when the data distribution can be modeled by a mixture of low-dimensional", "snippet": "In this section, we will begin by discussing the conceptual and algorithmic foundations for structured representation learning when the data distribution can be modeled by a mixture of low-dimensional subspaces , as illustrated in Figure 2.4 . In this setting, the decoder mapping will be almost as simple as the case of a single subspace: we simply represent via ùíô ^ = ùíü ‚Äã ( ùíõ ) = ( ‚àë k = 1 K œÄ k ‚Äã ( ùíõ ) ‚Äã ùëº k ) ‚Äã ùíõ , \\hat{\\bm{x}}=\\mathcal{D}(\\bm{z})=\\left(\\sum_{k=1}^{K}\\pi_{k}(\\bm{z})\\bm{U}_{k}\\right)\\bm{z}, over^ start_ARG bold_italic_x end_ARG = caligraphic_D ( bold_italic_z ) = ( ‚àë start_POS"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.p4", "title": "We will see how ideas from the rich literature on sparse representation and independent component analysis lead to a natural reformulation of the above decoder architecture through the lens of sparsit", "snippet": "We will see how ideas from the rich literature on sparse representation and independent component analysis lead to a natural reformulation of the above decoder architecture through the lens of sparsity, the corresponding encoder architecture (obtained through a power-method-like algorithm analogous to that of principal component analysis), and strong guarantees of correctness and efficiency for learning such encoder-decoder pairs from data. In this sense, the case of mixed linear low-dimensional structure already leads to many of the key conceptual components of structured representation learn"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.SS1.p1", "title": "Let ùëº 1 , ‚Ä¶ , ùëº K \\bm{U}_{1},\\dots,\\bm{U}_{K} bold_italic_U start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_U start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT , each of size D √ó d D\\times d it", "snippet": "Let ùëº 1 , ‚Ä¶ , ùëº K \\bm{U}_{1},\\dots,\\bm{U}_{K} bold_italic_U start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_U start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT , each of size D √ó d D\\times d italic_D √ó italic_d , denote a collection of orthonormal bases for K K italic_K subspaces of dimension d d italic_d in ‚Ñù D \\mathbb{R}^{D} blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT . To say that ùíô \\bm{x} bold_italic_x follows a mixture-of-subspaces distribution parameterized by ùëº 1 , ‚Ä¶ , ùëº K \\bm{U}_{1},\\dots,\\bm{U}_{K} bold_italic_U start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , "}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmremark6.p1", "title": "One should be aware that the above model ( 2.2.3 ) is a mixture of Gaussian distributions, not to be confused with a mixture of Gaussian variables by superposition, say ùíô = ‚àë i = 1 n w i ‚Äã ùíô i , ùíô i ‚àº", "snippet": "One should be aware that the above model ( 2.2.3 ) is a mixture of Gaussian distributions, not to be confused with a mixture of Gaussian variables by superposition, say ùíô = ‚àë i = 1 n w i ‚Äã ùíô i , ùíô i ‚àº ùí© ‚Äã ( ùüé , ùëº i ‚Äã ùëº i ‚ä§ ) , \\bm{x}=\\sum_{i=1}^{n}w_{i}\\bm{x}_{i},\\quad\\bm{x}_{i}\\sim\\mathcal{N}(\\mathbf{0},\\bm{U}_{i}\\bm{U}_{i}^{\\top}), bold_italic_x = ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_italic_x start_POSTS"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.SS1.p2", "title": "For now, we focus on the geometric perspective offered by ( 2.2.2 ). There is an algebraically convenient alternative to this conditional representation. Consider a lifted representation vector ùíõ = [ ", "snippet": "For now, we focus on the geometric perspective offered by ( 2.2.2 ). There is an algebraically convenient alternative to this conditional representation. Consider a lifted representation vector ùíõ = [ ùíõ 1 ‚ä§ , ‚Ä¶ , ùíõ K ‚ä§ ] ‚ä§ ‚àà ‚Ñù d ‚Äã K \\bm{z}=[\\bm{z}_{1}^{\\top},\\dots,\\bm{z}_{K}^{\\top}]^{\\top}\\in\\mathbb{R}^{dK} bold_italic_z = [ bold_italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT , ‚Ä¶ , bold_italic_z start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ‚àà blackboard_R star"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.SS1.p3", "title": "Now, ( 2.2.5 ) suggests a convenient relaxation for tractability of analysis: rather than modeling ùíô \\bm{x} bold_italic_x as coming from a mixture of K K italic_K specific subspaces ùëº 1 , ‚Ä¶ , ùëº K \\bm{", "snippet": "Now, ( 2.2.5 ) suggests a convenient relaxation for tractability of analysis: rather than modeling ùíô \\bm{x} bold_italic_x as coming from a mixture of K K italic_K specific subspaces ùëº 1 , ‚Ä¶ , ùëº K \\bm{U}_{1},\\dots,\\bm{U}_{K} bold_italic_U start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_U start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT , we may instead start with a dictionary ùëº ‚àà ‚Ñù D √ó m \\bm{U}\\in\\mathbb{R}^{D\\times m} bold_italic_U ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D √ó italic_m end_POSTSUPERSCRIPT , where m m italic_m may be smaller or larger than D D italic_D , and simply se"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.SS1.SSS0.Px1.p1", "title": "Now we can formulate the structured representation learning problem for mixtures of low-dimensional subspaces that we will study in this section. We assume that we have samples ùëø = [ ùíô 1 , ‚Ä¶ ‚Äã ùíô N ] \\", "snippet": "Now we can formulate the structured representation learning problem for mixtures of low-dimensional subspaces that we will study in this section. We assume that we have samples ùëø = [ ùíô 1 , ‚Ä¶ ‚Äã ùíô N ] \\bm{X}=[\\bm{x}_{1},\\dots\\bm{x}_{N}] bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ] from an unknown sparse dictionary model ( 2.2.7 ), possibly with added noises ùú∫ i \\bm{\\varepsilon}_{i} bold_italic_Œµ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT . Let us begin from the assumption that the dictionary ùëº \\bm{U"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.SS2.p1", "title": "In this section, we will derive algorithms for solving the orthogonal dictionary learning problem. To be more precise, we assume that the observed vector ùíô ‚àà ‚Ñù D \\bm{x}\\in\\mathbb{R}^{D} bold_italic_x ", "snippet": "In this section, we will derive algorithms for solving the orthogonal dictionary learning problem. To be more precise, we assume that the observed vector ùíô ‚àà ‚Ñù D \\bm{x}\\in\\mathbb{R}^{D} bold_italic_x ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT follows a statistical model ùíô = ùëº ‚Äã ùíõ + ùú∫ , \\bm{x}=\\bm{U}\\bm{z}+\\bm{\\varepsilon}, bold_italic_x = bold_italic_U bold_italic_z + bold_italic_Œµ , (2.2.9) where ùëº ‚àà ‚Ñù D √ó D \\bm{U}\\in\\mathbb{R}^{D\\times D} bold_italic_U ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D √ó italic_D end_POSTSUPERSCRIPT is an unknown orthogonal dictionary, ùíõ \\b"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.SS2.p2", "title": "Here we assume that each independent component z i z_{i} italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is distributed as z i ‚àº Bern ‚Äã ( Œ∏ ) ‚ãÖ ùí© ‚Äã ( 0 , 1 / Œ∏ ) . z_{i}\\sim\\mathrm{Bern}(\\thet", "snippet": "Here we assume that each independent component z i z_{i} italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is distributed as z i ‚àº Bern ‚Äã ( Œ∏ ) ‚ãÖ ùí© ‚Äã ( 0 , 1 / Œ∏ ) . z_{i}\\sim\\mathrm{Bern}(\\theta)\\cdot\\mathcal{N}(0,1/\\theta). italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ‚àº roman_Bern ( italic_Œ∏ ) ‚ãÖ caligraphic_N ( 0 , 1 / italic_Œ∏ ) . That is, it is the product of a Bernoulli random variable with probability Œ∏ \\theta italic_Œ∏ of being 1 1 1 and 1 ‚àí Œ∏ 1-\\theta 1 - italic_Œ∏ of being 0 , and an independent Gaussian random variable with variance 1 / Œ∏ 1/\\theta 1 / italic_Œ∏ . This d"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmremark7.p1", "title": "At first sight, the assumption that the dictionary ùëº \\bm{U} bold_italic_U is orthogonal might seem to be somewhat restrictive. But there is actually no loss of generality. One may consider a complete ", "snippet": "At first sight, the assumption that the dictionary ùëº \\bm{U} bold_italic_U is orthogonal might seem to be somewhat restrictive. But there is actually no loss of generality. One may consider a complete dictionary to be any square invertible matrix ùëº \\bm{U} bold_italic_U . With samples generated from this dictionary: ùëø = ùëº ‚Äã ùíÅ ‚àà ‚Ñù D √ó N \\bm{X}=\\bm{U}\\bm{Z}\\in\\mathbb{R}^{D\\times N} bold_italic_X = bold_italic_U bold_italic_Z ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D √ó italic_N end_POSTSUPERSCRIPT , it is easy to show that with some preconditioning of the data matrix ùëø \\bm{X} bold_italic_X : ùëø "}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.SS2.SSS0.Px1.p1", "title": "Now suppose that we are given a set of observations: ùíô i = ùëº ‚Äã ùíõ i + ùú∫ i , ‚àÄ i ‚àà [ N ] . \\bm{x}_{i}=\\bm{U}\\bm{z}_{i}+\\bm{\\varepsilon}_{i},\\ \\forall i\\in[N]. bold_italic_x start_POSTSUBSCRIPT italic_i ", "snippet": "Now suppose that we are given a set of observations: ùíô i = ùëº ‚Äã ùíõ i + ùú∫ i , ‚àÄ i ‚àà [ N ] . \\bm{x}_{i}=\\bm{U}\\bm{z}_{i}+\\bm{\\varepsilon}_{i},\\ \\forall i\\in[N]. bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_U bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + bold_italic_Œµ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , ‚àÄ italic_i ‚àà [ italic_N ] . (2.2.12) Let ùëø = [ ùíô 1 , ‚Ä¶ , ùíô N ] \\bm{X}=[\\bm{x}_{1},\\dots,\\bm{x}_{N}] bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ]"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.SS2.SSS0.Px1.p2", "title": "Also, given ùëº \\bm{U} bold_italic_U is orthogonal and the fact ùú∫ \\bm{\\varepsilon} bold_italic_Œµ is small, the vector ùíô \\bm{x} bold_italic_x has a predictable expected norm, i.e., ùîº ‚Äã [ ‚Äñ ùíô ‚Äñ 2 2 ] ‚âà ùîº ", "snippet": "Also, given ùëº \\bm{U} bold_italic_U is orthogonal and the fact ùú∫ \\bm{\\varepsilon} bold_italic_Œµ is small, the vector ùíô \\bm{x} bold_italic_x has a predictable expected norm, i.e., ùîº ‚Äã [ ‚Äñ ùíô ‚Äñ 2 2 ] ‚âà ùîº ‚Äã [ ‚Äñ ùíõ ‚Äñ 2 2 ] = d \\mathbb{E}[\\|\\bm{x}\\|_{2}^{2}]\\approx\\mathbb{E}[\\|\\bm{z}\\|_{2}^{2}]=d blackboard_E [ ‚à• bold_italic_x ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] ‚âà blackboard_E [ ‚à• bold_italic_z ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] = italic_d . It is a known fact that for vectors on a sphere, maximizi"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.SS2.SSS0.Px1.p3", "title": "An orthogonal matrix ùë® \\bm{A} bold_italic_A preserves the Euclidean ( ‚Ñì 2 \\ell^{2} roman_‚Ñì start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) norm: ‚Äñ ùë® ‚Äã ùíô ‚Äñ 2 2 = ‚Äñ ùíô ‚Äñ 2 2 \\|\\bm{A}\\bm{x}\\|_{2}^{2}=\\|\\bm{", "snippet": "An orthogonal matrix ùë® \\bm{A} bold_italic_A preserves the Euclidean ( ‚Ñì 2 \\ell^{2} roman_‚Ñì start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) norm: ‚Äñ ùë® ‚Äã ùíô ‚Äñ 2 2 = ‚Äñ ùíô ‚Äñ 2 2 \\|\\bm{A}\\bm{x}\\|_{2}^{2}=\\|\\bm{x}\\|_{2}^{2} ‚à• bold_italic_A bold_italic_x ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = ‚à• bold_italic_x ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . Therefore, to find the correct orthogonal dictionary ùëº \\bm{U} bold_italic_U from ùëø \\bm{X} bold_italic_X , we may try to solve the following optimization program: max"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmremark8.p1", "title": "It is also known that for vectors on a sphere, minimizing the ‚Ñì 1 \\ell^{1} roman_‚Ñì start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT norm is equivalent to minimizing the ‚Ñì 0 \\ell^{0} roman_‚Ñì start_POSTSUPER", "snippet": "It is also known that for vectors on a sphere, minimizing the ‚Ñì 1 \\ell^{1} roman_‚Ñì start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT norm is equivalent to minimizing the ‚Ñì 0 \\ell^{0} roman_‚Ñì start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT norm (for promoting sparsity), arg ‚Äã min ùíõ ‚àà ùïä n ‚Å° ‚Äñ ùíõ ‚Äñ 1 ‚áî arg ‚Äã min ùíõ ‚àà ùïä n ‚Å° ‚Äñ ùíõ ‚Äñ 0 , \\operatorname*{arg\\ min}_{\\bm{z}\\in\\mathbb{S}^{n}}\\|\\bm{z}\\|_{1}\\quad\\Leftrightarrow\\quad\\operatorname*{arg\\ min}_{\\bm{z}\\in\\mathbb{S}^{n}}\\|\\bm{z}\\|_{0}, start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT bold_italic_z ‚àà blackboard_S start_POSTSUPERSCRIPT italic"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.SS2.SSS0.Px1.p4", "title": "Note that the above problem is equivalent to the following constrained optimization problem: min ‚àí 1 4 ‚Äã ‚Äñ ùë® ~ ‚Äã ùëø ‚Äñ 4 4 subject to ùë® ~ ‚ä§ ‚Äã ùë® ~ = ùë∞ . \\min\\,-\\frac{1}{4}\\left\\|\\tilde{\\bm{A}}\\bm{X}\\righ", "snippet": "Note that the above problem is equivalent to the following constrained optimization problem: min ‚àí 1 4 ‚Äã ‚Äñ ùë® ~ ‚Äã ùëø ‚Äñ 4 4 subject to ùë® ~ ‚ä§ ‚Äã ùë® ~ = ùë∞ . \\min\\,-\\frac{1}{4}\\left\\|\\tilde{\\bm{A}}\\bm{X}\\right\\|_{4}^{4}\\quad\\mbox{subject to}\\quad\\tilde{\\bm{A}}^{\\top}\\tilde{\\bm{A}}=\\bm{I}. roman_min - divide start_ARG 1 end_ARG start_ARG 4 end_ARG ‚à• over~ start_ARG bold_italic_A end_ARG bold_italic_X ‚à• start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT subject to over~ start_ARG bold_italic_A end_ARG start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT over~ start_ARG bold_itali"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.SS2.SSS0.Px1.p5", "title": "To compute the fixed point for the above equation, similar to how we computed eigenvectors for PCA ( 2.1.16 ), we may take the following power iteration: ùë® t + 1 = ùí´ O ‚Äã ( D ) ‚Äã [ ( ùë® t ‚Äã ùëø ) ‚äô 3 ‚Äã ùëø ", "snippet": "To compute the fixed point for the above equation, similar to how we computed eigenvectors for PCA ( 2.1.16 ), we may take the following power iteration: ùë® t + 1 = ùí´ O ‚Äã ( D ) ‚Äã [ ( ùë® t ‚Äã ùëø ) ‚äô 3 ‚Äã ùëø ‚ä§ ] . \\bm{A}_{t+1}=\\mathcal{P}_{\\mathrm{O}(D)}[({\\bm{A}_{t}\\bm{X}})^{\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\scalebox{0.8}{$\\scriptstyle\\odot$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\odot$}}$}}{\\raisebox{1.3pt}{$\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\scalebo"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmremark9.p1", "title": "The constrained ‚Ñì 4 \\ell^{4} roman_‚Ñì start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT maximization problem is a nonconvex program. In general one should not expect that any greedy (say gradient-descent typ", "snippet": "The constrained ‚Ñì 4 \\ell^{4} roman_‚Ñì start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT maximization problem is a nonconvex program. In general one should not expect that any greedy (say gradient-descent type) algorithms would converge to the globally optimal solution. Surprisingly, one can show that, unlike general nonconvex programs, the landscape of ‚Ñì 4 \\ell^{4} roman_‚Ñì start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT maximization over a sphere min ‚àí 1 4 ‚Äã ‚Äñ ùíí ‚ä§ ‚Äã ùëø ‚Äñ 4 4 subject to ùíí ‚ä§ ‚Äã ùíí = 1 . \\min\\,-\\frac{1}{4}\\left\\|\\bm{q}^{\\top}\\bm{X}\\right\\|_{4}^{4}\\quad\\mbox{subject to}\\quad\\bm{q}^{\\top}\\bm{q}="}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmremark10.p1", "title": "The above iterative process of computing the dictionary has a natural incremental ‚Äúdeep learning‚Äù interpretation. Let us define Œ¥ ‚Äã ùë® t + 1 = ùë® t + 1 ‚Äã ùë® t ‚ä§ \\delta\\bm{A}_{t+1}=\\bm{A}_{t+1}\\bm{A}_{t}^", "snippet": "The above iterative process of computing the dictionary has a natural incremental ‚Äúdeep learning‚Äù interpretation. Let us define Œ¥ ‚Äã ùë® t + 1 = ùë® t + 1 ‚Äã ùë® t ‚ä§ \\delta\\bm{A}_{t+1}=\\bm{A}_{t+1}\\bm{A}_{t}^{\\top} italic_Œ¥ bold_italic_A start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT = bold_italic_A start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT bold_italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT and ùíÅ t = ùë® t ‚Äã ùëø \\bm{Z}_{t}=\\bm{A}_{t}\\bm{X} bold_italic_Z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_A start_POSTSUBSCRIPT ital"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.SS3.p1", "title": "With the Bernoulli-Gaussian model, the variables z i z_{i} italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT are independent and non-Gaussian. Then, there is a clear correspondence between the d", "snippet": "With the Bernoulli-Gaussian model, the variables z i z_{i} italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT are independent and non-Gaussian. Then, there is a clear correspondence between the dictionary learning and the classic independent component analysis (ICA), to the extent that algorithms to solve one problem can be used to solve the other. 8 8 8 We explore this issue in more depth in Exercise 2.3 , where a connection between non-Gaussianity of the independent components and the purely geometric notion of symmetry is made. This issue is related to our observation above that PCA do"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.SS3.p2", "title": "Towards deriving an algorithm based on ICA, we focus on an objective function known as kurtosis , which is used in ICA as a direct consequence of the non-Gaussianity of the components. The kurtosis , ", "snippet": "Towards deriving an algorithm based on ICA, we focus on an objective function known as kurtosis , which is used in ICA as a direct consequence of the non-Gaussianity of the components. The kurtosis , or fourth-order cumulant, of a zero-mean random variable X X italic_X is defined as kurt ( X ) = ùîº ‚Å° X 4 ‚àí 3 ‚Äã ( ùîº ‚Å° X 2 ) 2 . \\mathop{\\mathrm{kurt}}(X)=\\operatorname{\\mathbb{E}}{X^{4}}-3(\\operatorname{\\mathbb{E}}{X^{2}})^{2}. roman_kurt ( italic_X ) = blackboard_E italic_X start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT - 3 ( blackboard_E italic_X start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) start_P"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.SS3.SSS0.Px1.p1", "title": "The FastICA algorithm, advanced by Hyv√§rinen and Oja [ HO97 ] , is a fast fixed-point algorithm for solving the k = 1 k=1 italic_k = 1 kurtosis maximization scheme for ICA. The problem at hand is max ", "snippet": "The FastICA algorithm, advanced by Hyv√§rinen and Oja [ HO97 ] , is a fast fixed-point algorithm for solving the k = 1 k=1 italic_k = 1 kurtosis maximization scheme for ICA. The problem at hand is max ‚Äñ ùíó ‚Äñ 2 2 = 1 ‚Äã kurt ( ùëø ‚ä§ ‚Äã ùíó ) . \\max_{\\|\\bm{v}\\|_{2}^{2}=1}\\,\\mathop{\\mathrm{kurt}}(\\bm{X}^{\\top}\\bm{v}). roman_max start_POSTSUBSCRIPT ‚à• bold_italic_v ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 1 end_POSTSUBSCRIPT roman_kurt ( bold_italic_X start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_v ) . (2.2.23) First, we will perform some very basic "}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.p1", "title": "As we have seen, complete dictionary learning enjoys an elegant computational theory in which we maintain a symmetric autoencoding structure ‚Ñ∞ ‚Äã ( ùíô ) = ùëº ‚ä§ ‚Äã ùíô \\mathcal{E}(\\bm{x})=\\bm{U}^{\\top}\\bm{x}", "snippet": "As we have seen, complete dictionary learning enjoys an elegant computational theory in which we maintain a symmetric autoencoding structure ‚Ñ∞ ‚Äã ( ùíô ) = ùëº ‚ä§ ‚Äã ùíô \\mathcal{E}(\\bm{x})=\\bm{U}^{\\top}\\bm{x} caligraphic_E ( bold_italic_x ) = bold_italic_U start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_x , ùíü ‚Äã ( ùíõ ) = ùëº ‚Äã ùíõ \\mathcal{D}(\\bm{z})=\\bm{U}\\bm{z} caligraphic_D ( bold_italic_z ) = bold_italic_U bold_italic_z , with a scalable power-method-like algorithm (the MSP algorithm) for learning an orthogonal dictionary/codebook ùëº \\bm{U} bold_italic_U from data. Nevertheless, for learning repr"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmexample1.p1", "title": "Given sampled images of hand-written digits, Figure 2.6 (a) shows the result of fitting an orthogonal dictionary to the dataset. In contrast, Figure 2.6 (b) shows the result of running an optimization", "snippet": "Given sampled images of hand-written digits, Figure 2.6 (a) shows the result of fitting an orthogonal dictionary to the dataset. In contrast, Figure 2.6 (b) shows the result of running an optimization algorithm for learning overcomplete dictionaries (which we will present in detail later in the Chapter) on these samples. Notice that the representations become far sparser and the codebooks far more interpretable‚Äîthey consist of fundamental primitives for the strokes composing the digits, i.e. oriented edges. ‚ñ† \\blacksquare ‚ñ†"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.p2", "title": "In fact, overcomplete dictionary learning was originally proposed as a biologically plausible algorithm for image representation based on empirical evidence of how early stages of the visual cortex re", "snippet": "In fact, overcomplete dictionary learning was originally proposed as a biologically plausible algorithm for image representation based on empirical evidence of how early stages of the visual cortex represent visual stimuli [ OF96 , OF97 ] ."}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.p3", "title": "In the remainder of this section, we will overview the conceptual and computational foundations of overcomplete dictionary learning. Supposing that the model ( 2.3.1 ) is satisfied with sparse codes ùíõ", "snippet": "In the remainder of this section, we will overview the conceptual and computational foundations of overcomplete dictionary learning. Supposing that the model ( 2.3.1 ) is satisfied with sparse codes ùíõ \\bm{z} bold_italic_z , overcomplete dictionary ùë® \\bm{A} bold_italic_A , and sparsity level d d italic_d , and given samples ùëø = [ ùíô 1 , ‚Ä¶ , ùíô N ] \\bm{X}=[\\bm{x}_{1},\\dots,\\bm{x}_{N}] bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ] of ùíô \\bm{x} bold_italic_x , we want to learn an encoder ‚Ñ∞ : ‚Ñù D ‚Üí ‚Ñù m \\math"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.p4", "title": "We will start from the construction of the encoder ‚Ñ∞ \\mathcal{E} caligraphic_E . We will work incrementally: first, given the true dictionary ùêÄ \\bm{A} bold_italic_A , we will show how the problem of s", "snippet": "We will start from the construction of the encoder ‚Ñ∞ \\mathcal{E} caligraphic_E . We will work incrementally: first, given the true dictionary ùêÄ \\bm{A} bold_italic_A , we will show how the problem of sparse coding gives an elegant, scalable, and provably-correct algorithm for recovering the sparse code ùíõ \\bm{z} bold_italic_z of ùíô \\bm{x} bold_italic_x . Although this problem is NP-hard in the worst case, it can be solved efficiently and scalably for dictionaries ùë® \\bm{A} bold_italic_A which are incoherent , i.e. having columns that are not too correlated. The encoder architecture encompassed by "}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.p5", "title": "Then we will proceed to the task of learning the decoder ùíü \\mathcal{D} caligraphic_D , or equivalently the overcomplete dictionary ùë® \\bm{A} bold_italic_A . We will derive an algorithm for overcomplete", "snippet": "Then we will proceed to the task of learning the decoder ùíü \\mathcal{D} caligraphic_D , or equivalently the overcomplete dictionary ùë® \\bm{A} bold_italic_A . We will derive an algorithm for overcomplete dictionary learning that allows us to simultaneously learn the codebook ùë® \\bm{A} bold_italic_A and the sparse codes ùíõ \\bm{z} bold_italic_z , using ideas from sparse coding. Finally, we will discuss a more modern perspective on learnable sparse coding that leads us to a fully asymmetric encoder-decoder structure, as an alternative to ( 2.3.2 ). Here, the decoder will correspond to an incremental s"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.SS1.p1", "title": "In this section, we will consider the data model ( 2.3.1 ), which accommodates sparse linear combinations of many motifs, or atoms . Given data { ùíô i } i = 1 N ‚äÜ ‚Ñù D \\{\\bm{x}_{i}\\}_{i=1}^{N}\\subseteq\\", "snippet": "In this section, we will consider the data model ( 2.3.1 ), which accommodates sparse linear combinations of many motifs, or atoms . Given data { ùíô i } i = 1 N ‚äÜ ‚Ñù D \\{\\bm{x}_{i}\\}_{i=1}^{N}\\subseteq\\mathbb{R}^{D} { bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ‚äÜ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT satisfying this model, i.e. expressible as ùíô i = ùë® ‚Äã ùíõ i + ùú∫ i , ‚àÄ i ‚àà [ N ] \\bm{x}_{i}=\\bm{A}\\bm{z}_{i}+\\bm{\\varepsilon}_{i},\\qquad\\forall i\\in[N] bold_"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.SS1.p2", "title": "Note that we can collect the ùíô i \\bm{x}_{i} bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT into ùëø = [ ùíô 1 , ‚Ä¶ , ùíô N ] ‚àà ‚Ñù D √ó N \\bm{X}=\\begin{bmatrix}\\bm{x}_{1},\\dots,\\bm{x}_{N}\\end{bmat", "snippet": "Note that we can collect the ùíô i \\bm{x}_{i} bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT into ùëø = [ ùíô 1 , ‚Ä¶ , ùíô N ] ‚àà ‚Ñù D √ó N \\bm{X}=\\begin{bmatrix}\\bm{x}_{1},\\dots,\\bm{x}_{N}\\end{bmatrix}\\in\\mathbb{R}^{D\\times N} bold_italic_X = [ start_ARG start_ROW start_CELL bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D √ó italic_N end_POSTSUPERSCRIPT , collect the ùíõ i \\bm{z}_{i} bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT into ùíÅ"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.SS1.p3", "title": "However, unlike PCA or the complete dictionary learning problem, there is no clear power iteration-type algorithm to recover ùíÅ ‚ãÜ \\bm{Z}^{\\star} bold_italic_Z start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIP", "snippet": "However, unlike PCA or the complete dictionary learning problem, there is no clear power iteration-type algorithm to recover ùíÅ ‚ãÜ \\bm{Z}^{\\star} bold_italic_Z start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT . A natural alternative is to consider solving the above optimization problem with gradient descent type algorithms. Let f ‚Äã ( ùíÅ ) = ‚Äñ ùëø ‚àí ùë® ‚Äã ùíÅ ‚Äñ 2 2 + Œª ‚Äã ‚Äñ ùíÅ ‚Äñ 1 f(\\bm{Z})=\\|\\bm{X}-\\bm{A}\\bm{Z}\\|_{2}^{2}+\\lambda\\|\\bm{Z}\\|_{1} italic_f ( bold_italic_Z ) = ‚à• bold_italic_X - bold_italic_A bold_italic_Z ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.SS1.p4", "title": "Applying proximal gradient to the LASSO objective function ( 2.3.5 ) leads to the classic iterative shrinkage-thresholding algorithm (ISTA), which implements the iteration ùíÅ 1 \\displaystyle\\bm{Z}_{1} ", "snippet": "Applying proximal gradient to the LASSO objective function ( 2.3.5 ) leads to the classic iterative shrinkage-thresholding algorithm (ISTA), which implements the iteration ùíÅ 1 \\displaystyle\\bm{Z}_{1} bold_italic_Z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ‚àº \\displaystyle\\sim ‚àº ùí© ‚Å° ( ùüé , ùë∞ ) , \\displaystyle\\operatorname{\\mathcal{N}}(\\bm{0},\\bm{I}), caligraphic_N ( bold_0 , bold_italic_I ) , (2.3.8) ùíÅ t + 1 \\displaystyle\\bm{Z}_{t+1} bold_italic_Z start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT = \\displaystyle= = S Œ∑ ‚Äã Œª ‚Äã ( ùíÅ t ‚àí 2 ‚Äã Œ∑ ‚Äã ùë® ‚ä§ ‚Äã ( ùë® ‚Äã ùíÅ t ‚àí ùëø ) ) , ‚àÄ t ‚â• 1 , \\displaystyle S_{\\eta"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.SS1.p5", "title": "We now take a moment to remark on the functional form of the update operator in ( 2.3.9 ). It takes the form ùíÅ t + 1 = nonlinearity ‚Äã ( ùíÅ t + linear ‚ä§ ‚Äã ( linear ‚Äã ( ùíÅ t ) + bias ) ) . \\bm{Z}_{t+1}=\\t", "snippet": "We now take a moment to remark on the functional form of the update operator in ( 2.3.9 ). It takes the form ùíÅ t + 1 = nonlinearity ‚Äã ( ùíÅ t + linear ‚ä§ ‚Äã ( linear ‚Äã ( ùíÅ t ) + bias ) ) . \\bm{Z}_{t+1}=\\texttt{nonlinearity}(\\bm{Z}_{t}+\\texttt{linear}^{\\top}(\\texttt{linear}(\\bm{Z}_{t})+\\texttt{bias})). bold_italic_Z start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT = nonlinearity ( bold_italic_Z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + linear start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ( linear ( bold_italic_Z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) + bias ) ) . (2.3.12) This func"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.SS2.p1", "title": "Recall that we have the data model ùëø = ùë® ‚Äã ùíÅ + ùë¨ , \\bm{X}=\\bm{A}\\bm{Z}+\\bm{E}, bold_italic_X = bold_italic_A bold_italic_Z + bold_italic_E , (2.3.14) where ùíÅ \\bm{Z} bold_italic_Z is sparse, and our go", "snippet": "Recall that we have the data model ùëø = ùë® ‚Äã ùíÅ + ùë¨ , \\bm{X}=\\bm{A}\\bm{Z}+\\bm{E}, bold_italic_X = bold_italic_A bold_italic_Z + bold_italic_E , (2.3.14) where ùíÅ \\bm{Z} bold_italic_Z is sparse, and our goal previously was to estimate ùíÅ \\bm{Z} bold_italic_Z given knowledge of the data ùëø \\bm{X} bold_italic_X and the dictionary atoms ùë® \\bm{A} bold_italic_A . Now we turn to the more practical and more difficult case where we do not know either ùë® \\bm{A} bold_italic_A or ùíÅ \\bm{Z} bold_italic_Z and seek to learn them from a large dataset."}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.SS2.p2", "title": "A direct generalization of Equation 2.3.5 suggests solving the problem min ùë® ~ , ùíÅ ~ ‚Å° { ‚Äñ ùëø ‚àí ùë® ~ ‚Äã ùíÅ ~ ‚Äñ F 2 + Œª ‚Äã ‚Äñ ùíÅ ~ ‚Äñ 1 } . \\min_{\\tilde{\\bm{A}},\\tilde{\\bm{Z}}}\\left\\{\\|\\bm{X}-\\tilde{\\bm{A}}\\ti", "snippet": "A direct generalization of Equation 2.3.5 suggests solving the problem min ùë® ~ , ùíÅ ~ ‚Å° { ‚Äñ ùëø ‚àí ùë® ~ ‚Äã ùíÅ ~ ‚Äñ F 2 + Œª ‚Äã ‚Äñ ùíÅ ~ ‚Äñ 1 } . \\min_{\\tilde{\\bm{A}},\\tilde{\\bm{Z}}}\\left\\{\\|\\bm{X}-\\tilde{\\bm{A}}\\tilde{\\bm{Z}}\\|_{F}^{2}+\\lambda\\|\\tilde{\\bm{Z}}\\|_{1}\\right\\}. roman_min start_POSTSUBSCRIPT over~ start_ARG bold_italic_A end_ARG , over~ start_ARG bold_italic_Z end_ARG end_POSTSUBSCRIPT { ‚à• bold_italic_X - over~ start_ARG bold_italic_A end_ARG over~ start_ARG bold_italic_Z end_ARG ‚à• start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_Œª ‚à• over~ start"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.SS2.p3", "title": "This issue with ( 2.3.15 ) is dealt with by adding a constraint on the scale of the columns of the dictionary ùë® ~ \\tilde{\\bm{A}} over~ start_ARG bold_italic_A end_ARG . For example, it is common to as", "snippet": "This issue with ( 2.3.15 ) is dealt with by adding a constraint on the scale of the columns of the dictionary ùë® ~ \\tilde{\\bm{A}} over~ start_ARG bold_italic_A end_ARG . For example, it is common to assume that each column ùë® j \\bm{A}_{j} bold_italic_A start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT of the dictionary ùë® \\bm{A} bold_italic_A in ( 2.3.14 ) has bounded ‚Ñì 2 \\ell^{2} roman_‚Ñì start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT norm‚Äîsay, without loss of generality, by 1 1 1 . We then enforce this as a constraint, giving the objective min ùíÅ ~ , ùë® ~ : ‚Äñ ùë® ~ j ‚Äñ 2 ‚â§ 1 ‚Å° { ‚Äñ ùëø ‚àí ùë® ~ ‚Äã ùíÅ ~ ‚Äñ F 2 + Œª "}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmremark11.p1", "title": "Note that the above problem formulation follows naturally from the LASSO formulation ( 2.3.5 ) for sparse coding. We promote the sparsity of the solution via the ‚Ñì 1 \\ell^{1} roman_‚Ñì start_POSTSUPERSC", "snippet": "Note that the above problem formulation follows naturally from the LASSO formulation ( 2.3.5 ) for sparse coding. We promote the sparsity of the solution via the ‚Ñì 1 \\ell^{1} roman_‚Ñì start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT norm. Nevertheless, if we are only interested in recovering the over-complete dictionary ùë® \\bm{A} bold_italic_A , the ‚Ñì 4 \\ell^{4} roman_‚Ñì start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT maximization scheme introduced in Section 2.2.2 also generalizes to the over-complete case, without any significant modification. Interested readers may refer to the work of [ QZL+20 ] ."}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.SS2.p4", "title": "The above problem ( 2.3.17 ), which we call overcomplete dictionary learning , is nonconvex as here both ùë® \\bm{A} bold_italic_A and ùíÅ \\bm{Z} bold_italic_Z are unknown. It cannot be solved easily by th", "snippet": "The above problem ( 2.3.17 ), which we call overcomplete dictionary learning , is nonconvex as here both ùë® \\bm{A} bold_italic_A and ùíÅ \\bm{Z} bold_italic_Z are unknown. It cannot be solved easily by the standard convex optimization toolkit. Nevertheless, because it is interesting, simple to state, and practically important, there have been many important works dedicated to different algorithms and theoretical analysis for this problem. Here, for the interest of this manuscript, we present an idiomatic approach to solve this problem which is closer to the spirit of deep learning."}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.SS2.p5", "title": "From our experience with the LASSO problem above, it is easy to see that, for the two unknowns ùë® \\bm{A} bold_italic_A and ùíÅ \\bm{Z} bold_italic_Z , if we fix one and optimize the other, each subproblem", "snippet": "From our experience with the LASSO problem above, it is easy to see that, for the two unknowns ùë® \\bm{A} bold_italic_A and ùíÅ \\bm{Z} bold_italic_Z , if we fix one and optimize the other, each subproblem is in fact convex and easy to solve. This naturally suggests that we could attempt to solve the above program ( 2.3.17 ) by minimizing against ùíÅ \\bm{Z} bold_italic_Z or ùë® \\bm{A} bold_italic_A alternatively, say using gradient descent. Coupled with a natural choice of initialization, this leads to the following iterative scheme: ùíÅ ‚Ñì + 1 = S Œ∑ ‚Äã Œª ‚Äã ( ùíÅ ‚Ñì ‚àí 2 ‚Äã Œ∑ ‚Äã ùë® + ‚ä§ ‚Äã ( ùë® + ‚Äã ùíÅ ‚Ñì ‚àí ùëø ) ) , ùíÅ 1"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.SS2.p6", "title": "Despite the dictionary learning problem being a nonconvex problem, it has been shown that alternating minimization type algorithms indeed converge to the correct solution, at least locally. See, for e", "snippet": "Despite the dictionary learning problem being a nonconvex problem, it has been shown that alternating minimization type algorithms indeed converge to the correct solution, at least locally. See, for example, [ AAJ+16 ] . As a practical demonstration, the above algorithm (with L = T = 1 L=T=1 italic_L = italic_T = 1 ) was used to generate the results for overcomplete dictionary learning in Figure 2.6 ."}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.SS3.p1", "title": "The main insight from the alternating minimization algorithm for overcomplete dictionary learning in the previous section ( Equations 2.3.18 and 2.3.20 ) is to notice that when we fix ùêÄ \\bm{A} bold_it", "snippet": "The main insight from the alternating minimization algorithm for overcomplete dictionary learning in the previous section ( Equations 2.3.18 and 2.3.20 ) is to notice that when we fix ùêÄ \\bm{A} bold_italic_A , the ISTA update for ùêô ‚Ñì \\bm{Z}^{\\ell} bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ( 2.3.18 ) looks like the forward pass of a deep neural network with weights given by ùêÄ \\bm{A} bold_italic_A (and ùêÄ ‚ä§ \\bm{A}^{\\top} bold_italic_A start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ) . But in general, we do not know the true ùë® \\bm{A} bold_italic_A , and the current estimate ùë® + \\"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.SS3.SSS0.Px1.p1", "title": "The above deep-network interpretation of the alternating minimization is more conceptual than practical, as the process could be rather inefficient and take many layers or iterations to converge. But ", "snippet": "The above deep-network interpretation of the alternating minimization is more conceptual than practical, as the process could be rather inefficient and take many layers or iterations to converge. But this is mainly because we try to infer both ùíÅ \\bm{Z} bold_italic_Z and ùë® \\bm{A} bold_italic_A from ùëø \\bm{X} bold_italic_X . The problem can be significantly simplified and the above iterations can be made much more efficient in the supervised setting, where we have a dataset of input and output pairs ( ùëø , ùíÅ ) (\\bm{X},\\bm{Z}) ( bold_italic_X , bold_italic_Z ) distributed according to ( 2.3.14 ) an"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.SS3.SSS0.Px2.p1", "title": "The original motivation for overcomplete dictionary learning was to provide a simple generative model for high-dimensional data. We have seen with LISTA that, in addition, iterative algorithms for lea", "snippet": "The original motivation for overcomplete dictionary learning was to provide a simple generative model for high-dimensional data. We have seen with LISTA that, in addition, iterative algorithms for learning sparsely-used overcomplete dictionaries provide an interpretation for ReLU-like deep networks, which we will generalize in later chapters to more complex data distributions than ( 2.3.14 ). But it is also worth noting that even in the modern era of large models, the data generating model ( 2.3.14 ) provides a useful practical basis for interpreting features in pretrained large-scale deep net"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.SS3.SSS0.Px2.p2", "title": "We can use our development of the LISTA algorithm above to understand common practices in this field of research. In the most straightforward instantiation (see [ HCS+24 , GTT+25 ] ), a large number o", "snippet": "We can use our development of the LISTA algorithm above to understand common practices in this field of research. In the most straightforward instantiation (see [ HCS+24 , GTT+25 ] ), a large number of features from a pretrained deep network h h italic_h are collected from different inputs ùíô i \\bm{x}_{i} bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , which themselves are chosen based on a desired interpretation task. 12 12 12 For example, the inputs ùíô i \\bm{x}_{i} bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT could correspond to texts containing samples of comput"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.SS3.SSS0.Px2.p3", "title": "The parameterization and training procedure ( 2.3.24 ) may initially seem to be an arbitrary application of deep learning to the sparse coding problem, but it is actually highly aligned with the algor", "snippet": "The parameterization and training procedure ( 2.3.24 ) may initially seem to be an arbitrary application of deep learning to the sparse coding problem, but it is actually highly aligned with the algorithms we have studied above for layerwise sparse coding with a learned dictionary. In particular, recall the LISTA architecture ùíÅ L = f ‚Äã ( ùë® L , f ‚Äã ( ùë® L ‚àí 1 , ‚ãØ , f ‚Äã ( ùë® 1 , ùëø ) ‚Äã ‚ãØ ) ) \\bm{Z}^{L}=f(\\bm{A}^{L},f(\\bm{A}^{L-1},\\cdots,f(\\bm{A}^{1},\\bm{X})\\cdots)) bold_italic_Z start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT = italic_f ( bold_italic_A start_POSTSUPERSCRIPT italic_L end_POSTSUPE"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.SS3.SSS0.Px2.p4", "title": "Thus, the SAE parameterization and training procedure coincides with LISTA training with L = 1 L=1 italic_L = 1 , and a modified training objective‚Äîusing the LASSO objective ( 2.3.5 ), which remains u", "snippet": "Thus, the SAE parameterization and training procedure coincides with LISTA training with L = 1 L=1 italic_L = 1 , and a modified training objective‚Äîusing the LASSO objective ( 2.3.5 ), which remains unsupervised , instead of the supervised reconstruction loss ( 2.3.23 ) used in vanilla LISTA. In particular, we can understand the SAE architecture in terms of our interpretation of the LISTA architecture in terms of layerwise sparse coding in ( 2.3.29 ). This connection is suggestive of a host of new design strategies for improving practical interpretability methodology, many of which remain tant"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.SS3.SSS0.Px3.p1", "title": "In the supervised setting, LISTA provides a deep neural network analogue of the sparse coding iteration, with layerwise-learned dictionaries, inspired by alternating minimization; even in the unsuperv", "snippet": "In the supervised setting, LISTA provides a deep neural network analogue of the sparse coding iteration, with layerwise-learned dictionaries, inspired by alternating minimization; even in the unsupervised setting, the same methodology can be applied to learning, as with sparse autoencoders. But the connection between low-dimensional-structure-seeking optimization algorithms and deep network architectures goes much deeper than this, and suggests an array of scalable and natural neural learning architectures which may even be usable without backpropagation."}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.SS3.SSS0.Px3.p2", "title": "As a simple illustration, we return the alternating minimization iterations ( 2.3.18 ) and ( 2.3.20 ). This scheme randomly re-initializes the dictionary ùë® 1 \\bm{A}_{1} bold_italic_A start_POSTSUBSCRI", "snippet": "As a simple illustration, we return the alternating minimization iterations ( 2.3.18 ) and ( 2.3.20 ). This scheme randomly re-initializes the dictionary ùë® 1 \\bm{A}_{1} bold_italic_A start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT on every such update. An improvement uses instead warm starting , where the residual is generated using the previous estimate ùë® + \\bm{A}_{+} bold_italic_A start_POSTSUBSCRIPT + end_POSTSUBSCRIPT for the dictionary. If we then view each ISTA update ( 2.3.18 ) as a layer and allow the associated dictionary, now coupled with the sparse code updates as ùë® ‚Ñì \\bm{A}_{\\ell} bold_ita"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.SS3.SSS0.Px3.p3", "title": "Notice that the above layer-wise scheme also suggests a plausible alternative to the current end-to-end optimization strategy that primarily relies on back propagation (BP) detailed in the Appendix A.", "snippet": "Notice that the above layer-wise scheme also suggests a plausible alternative to the current end-to-end optimization strategy that primarily relies on back propagation (BP) detailed in the Appendix A.2.3 . Freeing training large networks from BP would be one of the biggest challenges and opportunities in the future, as we will discuss more at the end of the book in Chapter 8 ."}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S4.p1", "title": "The idealistic models we have presented in this chapter‚ÄîPCA, ICA, and dictionary learning‚Äîwere developed over the course of the twentieth century. Many books have been written solely about each method", "snippet": "The idealistic models we have presented in this chapter‚ÄîPCA, ICA, and dictionary learning‚Äîwere developed over the course of the twentieth century. Many books have been written solely about each method, so we will only attempt here to give a broad overview of the key works and history."}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S4.p2", "title": "Jolliffe [ Jol86 ] attributes principal component analysis to Pearson [ Pea01 ] , and independently Hotelling [ Hot33 ] . In mathematics, the main result on the related problem of low-rank approximati", "snippet": "Jolliffe [ Jol86 ] attributes principal component analysis to Pearson [ Pea01 ] , and independently Hotelling [ Hot33 ] . In mathematics, the main result on the related problem of low-rank approximation in unitarily invariant norms is attributed to Eckart and Young [ EY36 ] , and to Mirsky for full generality [ Mir60 ] . PCA continues to play an important role in research as perhaps the simplest model problem for unsupervised representation learning: as early as the 1980s, works such as [ Oja82 ] and [ BH89 ] used the problem to understand learning in primitive neural networks, and more recent"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S4.p3", "title": "Independent component analysis was proposed by [ BJC85 ] and pioneered by Aapo Hyv√§rinen in the 1990s and early 2000s in a series of influential works: see [ HO00a ] for a summary. As a simple model f", "snippet": "Independent component analysis was proposed by [ BJC85 ] and pioneered by Aapo Hyv√§rinen in the 1990s and early 2000s in a series of influential works: see [ HO00a ] for a summary. As a simple model for structure that arises in practical data, it initially saw significant use in applications such as blind source separation, where each independent component z i z_{i} italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT represents an independent source (such as sound associated to a distinct instrument in a musical recording) that is superimposed to produce the observation ùíô = ùëº ‚Äã ùíõ \\bm{x}=\\b"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S4.p4", "title": "The problem of dictionary learning can, in the complete or orthogonal case, be seen as one of the foundational problems of twentieth-century signal processing, particularly in linear systems theory, w", "snippet": "The problem of dictionary learning can, in the complete or orthogonal case, be seen as one of the foundational problems of twentieth-century signal processing, particularly in linear systems theory, where the Fourier basis plays the key role; from the 1980s onward, the field of computational harmonic analysis crystallized around the study of alternate such dictionaries for classes of signals in which optimal approximation could only be realized in a basis other than Fourier (e.g., wavelets) [ DVD+98 ] . However, the importance of the case of redundant bases, or overcomplete dictionaries, was o"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S4.p5", "title": "One point that we wish to highlight from the study of these classical analytical models for low-dimensional structure is the common role played by various generalized power methods ‚Äîalgorithms that ve", "snippet": "One point that we wish to highlight from the study of these classical analytical models for low-dimensional structure is the common role played by various generalized power methods ‚Äîalgorithms that very rapidly converge, at least locally, to various types of low-dimensional structures. The terminology for this class of algorithms follows the work of [ MYP+10 ] . At a high level, modeled on the classical power iteration for computation of the top eigenvector of a semidefinite matrix ùë® ‚àà ‚Ñù n √ó n \\bm{A}\\in\\mathbb{R}^{n\\times n} bold_italic_A ‚àà blackboard_R start_POSTSUPERSCRIPT italic_n √ó italic_"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S4.p6", "title": "The connection we make in Section 2.2.1 between the geometric mixture-of-subspaces distributional assumption and the more analytically-convenient sparse dictionary assumption has been mentioned in pri", "snippet": "The connection we make in Section 2.2.1 between the geometric mixture-of-subspaces distributional assumption and the more analytically-convenient sparse dictionary assumption has been mentioned in prior work, especially by those focused on generalized principal component analysis and applications such as subspace clustering, e.g. work of [ VMS16 ] . The mixture of subspaces assumption will continue to play a significant role throughout this manuscript, both as an analytical test case for different algorithmic paradigms, and as a foundation for deriving different deep network architectures, as "}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmexercise1.p1", "title": "Prove that, for any symmetric matrix ùë® \\bm{A} bold_italic_A , the solution to the problem max ùëº ‚àà ùñÆ ‚Äã ( D , d ) ‚Å° tr ‚Å° ( ùëº ‚ä§ ‚Äã ùë® ‚Äã ùëº ) \\max_{\\bm{U}\\in\\mathsf{O}(D,d)}\\operatorname{tr}\\left(\\bm{U}^{\\to", "snippet": "Prove that, for any symmetric matrix ùë® \\bm{A} bold_italic_A , the solution to the problem max ùëº ‚àà ùñÆ ‚Äã ( D , d ) ‚Å° tr ‚Å° ( ùëº ‚ä§ ‚Äã ùë® ‚Äã ùëº ) \\max_{\\bm{U}\\in\\mathsf{O}(D,d)}\\operatorname{tr}\\left(\\bm{U}^{\\top}\\bm{A}\\bm{U}\\right) roman_max start_POSTSUBSCRIPT bold_italic_U ‚àà sansserif_O ( italic_D , italic_d ) end_POSTSUBSCRIPT roman_tr ( bold_italic_U start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_A bold_italic_U ) is the matrix ùëº ‚ãÜ \\bm{U}^{\\star} bold_italic_U start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT whose columns are the top d d italic_d unit eigenvectors of ùë® \\bm{A} bold_italic_A ."}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmexercise2.p1", "title": "Let ùíõ ‚àº ùí© ‚Äã ( ùüé , œÉ 2 ‚Äã ùë∞ ) \\bm{z}\\sim\\mathcal{N}(\\mathbf{0},\\sigma^{2}\\bm{I}) bold_italic_z ‚àº caligraphic_N ( bold_0 , italic_œÉ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) be a Gaussi", "snippet": "Let ùíõ ‚àº ùí© ‚Äã ( ùüé , œÉ 2 ‚Äã ùë∞ ) \\bm{z}\\sim\\mathcal{N}(\\mathbf{0},\\sigma^{2}\\bm{I}) bold_italic_z ‚àº caligraphic_N ( bold_0 , italic_œÉ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) be a Gaussian random variable with independent components, each with variance œÉ 2 \\sigma^{2} italic_œÉ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . Prove that for any orthogonal matrix ùë∏ \\bm{Q} bold_italic_Q (i.e., ùë∏ ‚ä§ ‚Äã ùë∏ = ùë∞ \\bm{Q}^{\\top}\\bm{Q}=\\bm{I} bold_italic_Q start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_Q = bold_italic_I ), the random variable ùë∏ ‚Äã ùíõ \\bm{Q}\\bm{z} bold_italic_Q bold_italic_z"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmexercise3.p1", "title": "The notion of statistical identifiability discussed above can be related to symmetries of the model class, allowing estimation to be understood in a purely deterministic fashion without any statistica", "snippet": "The notion of statistical identifiability discussed above can be related to symmetries of the model class, allowing estimation to be understood in a purely deterministic fashion without any statistical assumptions."}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmexercise3.p2", "title": "Consider the model ùëø = ùëº ‚Äã ùíÅ \\bm{X}=\\bm{U}\\bm{Z} bold_italic_X = bold_italic_U bold_italic_Z for matrices ùëø , ùëº , ùíÅ \\bm{X},\\bm{U},\\bm{Z} bold_italic_X , bold_italic_U , bold_italic_Z of compatible siz", "snippet": "Consider the model ùëø = ùëº ‚Äã ùíÅ \\bm{X}=\\bm{U}\\bm{Z} bold_italic_X = bold_italic_U bold_italic_Z for matrices ùëø , ùëº , ùíÅ \\bm{X},\\bm{U},\\bm{Z} bold_italic_X , bold_italic_U , bold_italic_Z of compatible sizes. 1. Show that if ùë® \\bm{A} bold_italic_A is any square invertible matrix of compatible size, then the pair ( ùëº ‚Äã ùë® , ùë® ‚àí 1 ‚Äã ùíÅ ) (\\bm{U}\\bm{A},\\bm{A}^{-1}\\bm{Z}) ( bold_italic_U bold_italic_A , bold_italic_A start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_italic_Z ) also equals ùëø \\bm{X} bold_italic_X under the model. We call this a ùñ¶ùñ´ ‚Äã ( d ) \\mathsf{GL}(d) sansserif_GL ( italic_d ) symmetry "}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.I1.i1.p1", "title": "Show that if ùë® \\bm{A} bold_italic_A is any square invertible matrix of compatible size, then the pair ( ùëº ‚Äã ùë® , ùë® ‚àí 1 ‚Äã ùíÅ ) (\\bm{U}\\bm{A},\\bm{A}^{-1}\\bm{Z}) ( bold_italic_U bold_italic_A , bold_italic", "snippet": "Show that if ùë® \\bm{A} bold_italic_A is any square invertible matrix of compatible size, then the pair ( ùëº ‚Äã ùë® , ùë® ‚àí 1 ‚Äã ùíÅ ) (\\bm{U}\\bm{A},\\bm{A}^{-1}\\bm{Z}) ( bold_italic_U bold_italic_A , bold_italic_A start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_italic_Z ) also equals ùëø \\bm{X} bold_italic_X under the model. We call this a ùñ¶ùñ´ ‚Äã ( d ) \\mathsf{GL}(d) sansserif_GL ( italic_d ) symmetry ."}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.I1.i2.p1", "title": "Suppose each column of ùíÅ \\bm{Z} bold_italic_Z is an independent and identically distributed observation from a common statistical model ùíõ \\bm{z} bold_italic_z , which moreover has zero mean and indepe", "snippet": "Suppose each column of ùíÅ \\bm{Z} bold_italic_Z is an independent and identically distributed observation from a common statistical model ùíõ \\bm{z} bold_italic_z , which moreover has zero mean and independent components z i z_{i} italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT with positive variance. Show that for any square invertible matrix ùë® \\bm{A} bold_italic_A , if ùë® ‚Äã ùíõ \\bm{A}\\bm{z} bold_italic_A bold_italic_z has uncorrelated components, then ùë® \\bm{A} bold_italic_A can be written as ùë´ 1 ‚Äã ùë∏ ‚Äã ùë´ 2 \\bm{D}_{1}\\bm{Q}\\bm{D}_{2} bold_italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmexercise4.p1", "title": "Consider the model ùíô = ùëº ‚Äã ùíõ \\bm{x}=\\bm{U}\\bm{z} bold_italic_x = bold_italic_U bold_italic_z , where ùëº ‚àà ‚Ñù D √ó d \\bm{U}\\in\\mathbb{R}^{D\\times d} bold_italic_U ‚àà blackboard_R start_POSTSUPERSCRIPT ital", "snippet": "Consider the model ùíô = ùëº ‚Äã ùíõ \\bm{x}=\\bm{U}\\bm{z} bold_italic_x = bold_italic_U bold_italic_z , where ùëº ‚àà ‚Ñù D √ó d \\bm{U}\\in\\mathbb{R}^{D\\times d} bold_italic_U ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D √ó italic_d end_POSTSUPERSCRIPT with D ‚â• d D\\geq d italic_D ‚â• italic_d is fixed and has rank d d italic_d , and ùíõ \\bm{z} bold_italic_z is a zero-mean random variable. Let ùíô 1 , ‚Ä¶ ‚Äã ùíô N \\bm{x}_{1},\\dots\\bm{x}_{N} bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT denote i.i.d. observations from this model. 1. Show that the matr"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.I2.i1.p1", "title": "Show that the matrix ùëø = [ ùíô 1 , ‚Ä¶ , ùíô N ] \\bm{X}=[\\bm{x}_{1},\\dots,\\bm{x}_{N}] bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_x start_POSTSUBSCRIPT italic_N", "snippet": "Show that the matrix ùëø = [ ùíô 1 , ‚Ä¶ , ùíô N ] \\bm{X}=[\\bm{x}_{1},\\dots,\\bm{x}_{N}] bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ] has rank no larger than d d italic_d , and therefore there is an orthonormal matrix ùëΩ ‚àà ‚Ñù D √ó d \\bm{V}\\in\\mathbb{R}^{D\\times d} bold_italic_V ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D √ó italic_d end_POSTSUPERSCRIPT so that ùëø = ùëΩ ‚Äã ùíÄ \\bm{X}=\\bm{V}\\bm{Y} bold_italic_X = bold_italic_V bold_italic_Y , where ùíÄ ‚àà ‚Ñù d √ó N \\bm{Y}\\in\\mathbb{R}^{d\\times N} bold_italic_Y ‚àà blackboard"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.I2.i2.p1", "title": "Show that the whitened matrix ( ùíÄ ‚Äã ùíÄ ‚ä§ ) ‚àí 1 / 2 ‚Äã ùíÄ (\\bm{Y}\\bm{Y}^{\\top})^{-1/2}\\bm{Y} ( bold_italic_Y bold_italic_Y start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT - 1 / 2 end_P", "snippet": "Show that the whitened matrix ( ùíÄ ‚Äã ùíÄ ‚ä§ ) ‚àí 1 / 2 ‚Äã ùíÄ (\\bm{Y}\\bm{Y}^{\\top})^{-1/2}\\bm{Y} ( bold_italic_Y bold_italic_Y start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT - 1 / 2 end_POSTSUPERSCRIPT bold_italic_Y exists in expectation whenever Cov ‚Å° ( ùíõ ) \\operatorname{Cov}(\\bm{z}) roman_Cov ( bold_italic_z ) is nonsingular, and that it has identity empirical covariance. 14 14 14 In particular, it can be proved mathematically that this is enough to guarantee that the whitened matrix exists with high probability whenever ùíõ \\bm{z} bold_italic_z satisfies a suitable concentration "}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.I2.i3.p1", "title": "Show, by using the singular value decomposition of ùëº \\bm{U} bold_italic_U , that the matrix ùëΩ \\bm{V} bold_italic_V can be chosen so that the whitened matrix satisfies ( ùíÄ ‚Äã ùíÄ ‚ä§ ) ‚àí 1 / 2 ‚Äã ùíÄ = ùëæ ‚Äã [ ùíõ", "snippet": "Show, by using the singular value decomposition of ùëº \\bm{U} bold_italic_U , that the matrix ùëΩ \\bm{V} bold_italic_V can be chosen so that the whitened matrix satisfies ( ùíÄ ‚Äã ùíÄ ‚ä§ ) ‚àí 1 / 2 ‚Äã ùíÄ = ùëæ ‚Äã [ ùíõ 1 , ‚Ä¶ , ùíõ N ] (\\bm{Y}\\bm{Y}^{\\top})^{-1/2}\\bm{Y}=\\bm{W}[\\bm{z}_{1},\\dots,\\bm{z}_{N}] ( bold_italic_Y bold_italic_Y start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT - 1 / 2 end_POSTSUPERSCRIPT bold_italic_Y = bold_italic_W [ bold_italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_z start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ] , where ùëæ \\bm{W} bold_italic_W is"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmexercise5.p1", "title": "Let X X italic_X and Y Y italic_Y be zero-mean independent random variables. 1. Show that kurt ( X + Y ) = kurt ( X ) + kurt ( Y ) \\mathop{\\mathrm{kurt}}(X+Y)=\\mathop{\\mathrm{kurt}}(X)+\\mathop{\\mathrm", "snippet": "Let X X italic_X and Y Y italic_Y be zero-mean independent random variables. 1. Show that kurt ( X + Y ) = kurt ( X ) + kurt ( Y ) \\mathop{\\mathrm{kurt}}(X+Y)=\\mathop{\\mathrm{kurt}}(X)+\\mathop{\\mathrm{kurt}}(Y) roman_kurt ( italic_X + italic_Y ) = roman_kurt ( italic_X ) + roman_kurt ( italic_Y ) . 2. For any Œ± ‚àà ‚Ñù \\alpha\\in\\mathbb{R} italic_Œ± ‚àà blackboard_R , show that kurt ( Œ± ‚Äã X ) = Œ± 4 ‚Äã kurt ( X ) \\mathop{\\mathrm{kurt}}(\\alpha X)=\\alpha^{4}\\mathop{\\mathrm{kurt}}(X) roman_kurt ( italic_Œ± italic_X ) = italic_Œ± start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT roman_kurt ( italic_X ) ."}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.I3.i1.p1", "title": "Show that kurt ( X + Y ) = kurt ( X ) + kurt ( Y ) \\mathop{\\mathrm{kurt}}(X+Y)=\\mathop{\\mathrm{kurt}}(X)+\\mathop{\\mathrm{kurt}}(Y) roman_kurt ( italic_X + italic_Y ) = roman_kurt ( italic_X ) + roman_", "snippet": "Show that kurt ( X + Y ) = kurt ( X ) + kurt ( Y ) \\mathop{\\mathrm{kurt}}(X+Y)=\\mathop{\\mathrm{kurt}}(X)+\\mathop{\\mathrm{kurt}}(Y) roman_kurt ( italic_X + italic_Y ) = roman_kurt ( italic_X ) + roman_kurt ( italic_Y ) ."}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.I3.i2.p1", "title": "For any Œ± ‚àà ‚Ñù \\alpha\\in\\mathbb{R} italic_Œ± ‚àà blackboard_R , show that kurt ( Œ± ‚Äã X ) = Œ± 4 ‚Äã kurt ( X ) \\mathop{\\mathrm{kurt}}(\\alpha X)=\\alpha^{4}\\mathop{\\mathrm{kurt}}(X) roman_kurt ( italic_Œ± itali", "snippet": "For any Œ± ‚àà ‚Ñù \\alpha\\in\\mathbb{R} italic_Œ± ‚àà blackboard_R , show that kurt ( Œ± ‚Äã X ) = Œ± 4 ‚Äã kurt ( X ) \\mathop{\\mathrm{kurt}}(\\alpha X)=\\alpha^{4}\\mathop{\\mathrm{kurt}}(X) roman_kurt ( italic_Œ± italic_X ) = italic_Œ± start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT roman_kurt ( italic_X ) ."}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmexercise6.p1", "title": "Let f : ‚Ñù d ‚Üí ‚Ñù f:\\mathbb{R}^{d}\\to\\mathbb{R} italic_f : blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ‚Üí blackboard_R be a given twice-continuously-differentiable objective function.", "snippet": "Let f : ‚Ñù d ‚Üí ‚Ñù f:\\mathbb{R}^{d}\\to\\mathbb{R} italic_f : blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ‚Üí blackboard_R be a given twice-continuously-differentiable objective function. Consider the spherically-constrained optimization problem max ‚Äñ ùíñ ‚Äñ 2 2 = 1 ‚Å° f ‚Äã ( ùíñ ) . \\max_{\\|\\bm{u}\\|_{2}^{2}=1}\\,f(\\bm{u}). roman_max start_POSTSUBSCRIPT ‚à• bold_italic_u ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 1 end_POSTSUBSCRIPT italic_f ( bold_italic_u ) . (2.5.1) In this exercise, we will derive the expressions we gave in the FastICA deriva"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.I4.i1.p1", "title": "For any constraint set ‚Ñ≥ \\mathcal{M} caligraphic_M that is a differentiable submanifold of ‚Ñù d \\mathbb{R}^{d} blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , the tangent space at a p", "snippet": "For any constraint set ‚Ñ≥ \\mathcal{M} caligraphic_M that is a differentiable submanifold of ‚Ñù d \\mathbb{R}^{d} blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , the tangent space at a point ùíñ ‚àà ‚Ñ≥ \\bm{u}\\in\\mathcal{M} bold_italic_u ‚àà caligraphic_M is, informally, the best local linear approximation to the manifold ‚Ñ≥ \\mathcal{M} caligraphic_M at the point ùíñ \\bm{u} bold_italic_u . In the important special case where ‚Ñ≥ \\mathcal{M} caligraphic_M is defined locally at ùíñ \\bm{u} bold_italic_u as a level set of a function F : ‚Ñù d ‚Üí ‚Ñù F:\\mathbb{R}^{d}\\to\\mathbb{R} italic_F : blackboard_R "}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.I4.i2.p1", "title": "The vector field grad ‚Äã f ‚Äã ( ùíñ ) = ùë∑ ùíñ ‚üÇ ‚Äã ‚àá f \\mathrm{grad}\\,f(\\bm{u})=\\bm{P}_{\\bm{u}}^{\\perp}\\nabla f roman_grad italic_f ( bold_italic_u ) = bold_italic_P start_POSTSUBSCRIPT bold_italic_u end_POS", "snippet": "The vector field grad ‚Äã f ‚Äã ( ùíñ ) = ùë∑ ùíñ ‚üÇ ‚Äã ‚àá f \\mathrm{grad}\\,f(\\bm{u})=\\bm{P}_{\\bm{u}}^{\\perp}\\nabla f roman_grad italic_f ( bold_italic_u ) = bold_italic_P start_POSTSUBSCRIPT bold_italic_u end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚üÇ end_POSTSUPERSCRIPT ‚àá italic_f (2.5.2) is known as the Riemannian gradient of the function f f italic_f restricted to the sphere. The first order optimality conditions for the optimization problem ( 2.5.1 ) can be expressed in terms of the Riemann gradient: grad ‚Äã f ‚Äã ( ùíñ ) = ùüé . \\mathrm{grad}\\,f(\\bm{u})=\\mathbf{0}. roman_grad italic_f ( bold_italic_u ) = bold_0 "}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.I4.i3.p1", "title": "In optimization over ‚Ñù d \\mathbb{R}^{d} blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , one checks the second-order optimality conditions (to determine whether a critical point is a ", "snippet": "In optimization over ‚Ñù d \\mathbb{R}^{d} blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , one checks the second-order optimality conditions (to determine whether a critical point is a maximizer, a minimizer, or a saddle point) using the Hessian matrix ‚àá 2 f ‚Äã ( ùíñ ) \\nabla^{2}f(\\bm{u}) ‚àá start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_f ( bold_italic_u ) . Show, by differentiating the Riemann gradient grad ‚Äã f ‚Äã ( ùíñ ) \\mathrm{grad}\\,f(\\bm{u}) roman_grad italic_f ( bold_italic_u ) for the sphere with respect to ùíñ \\bm{u} bold_italic_u as in the first part of this exercise, that"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmexercise7.p1", "title": "In this exercise, we sketch an argument referred to in the literature as a landscape analysis for the spherically-constrained population kurtosis maximization problem ( 2.2.24 ). We will show that whe", "snippet": "In this exercise, we sketch an argument referred to in the literature as a landscape analysis for the spherically-constrained population kurtosis maximization problem ( 2.2.24 ). We will show that when there is at least one independent component with positive kurtosis, its global maximizers indeed lead to the recovery of one column of the dictionary ùëº \\bm{U} bold_italic_U . For simplicity, we will assume that kurt ( z i ) ‚â† 0 \\mathop{\\mathrm{kurt}}(z_{i})\\neq 0 roman_kurt ( italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ‚â† 0 for each i = 1 , ‚Ä¶ , d i=1,\\dots,d italic_i = 1 , ‚Ä¶ , itali"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.I5.i1.p1", "title": "Using the results of Part 1 of Exercise 2.6 , show that the first-order optimality condition for ( 2.2.24 ) is ( ‚àë i = 1 d kurt ( z i ) ‚Äã w i 4 ) ‚Äã ùíò = kurt ( ùíõ ) ‚äô ùíò ‚äô 3 , \\left(\\sum_{i=1}^{d}\\mathop", "snippet": "Using the results of Part 1 of Exercise 2.6 , show that the first-order optimality condition for ( 2.2.24 ) is ( ‚àë i = 1 d kurt ( z i ) ‚Äã w i 4 ) ‚Äã ùíò = kurt ( ùíõ ) ‚äô ùíò ‚äô 3 , \\left(\\sum_{i=1}^{d}\\mathop{\\mathrm{kurt}}(z_{i})w_{i}^{4}\\right)\\bm{w}=\\mathop{\\mathrm{kurt}}(\\bm{z})\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\scalebox{0.8}{$\\scriptstyle\\odot$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\odot$}}$}}{\\raisebox{1.3pt}{$\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\s"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.I5.i2.p1", "title": "Show that the vectors ùíò \\bm{w} bold_italic_w with unit norm that also satisfy ( 2.5.4 ) all take the following form. Let S + = { i ‚àà [ d ] ‚à£ kurt ( z i ) > 0 } S^{+}=\\{i\\in[d]\\mid\\mathop{\\mathrm{kurt}", "snippet": "Show that the vectors ùíò \\bm{w} bold_italic_w with unit norm that also satisfy ( 2.5.4 ) all take the following form. Let S + = { i ‚àà [ d ] ‚à£ kurt ( z i ) > 0 } S^{+}=\\{i\\in[d]\\mid\\mathop{\\mathrm{kurt}}(z_{i})>0\\} italic_S start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT = { italic_i ‚àà [ italic_d ] ‚à£ roman_kurt ( italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) > 0 } , and S ‚àí = { i ‚àà [ d ] ‚à£ kurt ( z i ) < 0 } S^{-}=\\{i\\in[d]\\mid\\mathop{\\mathrm{kurt}}(z_{i})<0\\} italic_S start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT = { italic_i ‚àà [ italic_d ] ‚à£ roman_kurt ( italic_z start_POSTSUBSCRIPT ital"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.I5.i3.p1", "title": "Assume that there is at least one i i italic_i such that kurt ( z i ) > 0 \\mathop{\\mathrm{kurt}}(z_{i})>0 roman_kurt ( italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) > 0 . Using the results", "snippet": "Assume that there is at least one i i italic_i such that kurt ( z i ) > 0 \\mathop{\\mathrm{kurt}}(z_{i})>0 roman_kurt ( italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) > 0 . Using the results of Part 2 of Exercise 2.6 , show that the only local maxima of the objective of ( 2.2.24 ) are the signed one-sparse vectors ¬± ùíÜ i \\pm\\bm{e}_{i} ¬± bold_italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT with i ‚àà S + i\\in S^{+} italic_i ‚àà italic_S start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT . Conclude that the global maximizers of ( 2.2.24 ) are the signed one-sparse vectors corresponding to "}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.I5.i4.p1", "title": "Now assume that kurt ( z j ) < 0 \\mathop{\\mathrm{kurt}}(z_{j})<0 roman_kurt ( italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) < 0 for every j = 1 , ‚Ä¶ , d j=1,\\dots,d italic_j = 1 , ‚Ä¶ , itali", "snippet": "Now assume that kurt ( z j ) < 0 \\mathop{\\mathrm{kurt}}(z_{j})<0 roman_kurt ( italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) < 0 for every j = 1 , ‚Ä¶ , d j=1,\\dots,d italic_j = 1 , ‚Ä¶ , italic_d . This corresponds to an ‚Äúover-deflated‚Äù instantiation of the kurtosis maximization problem. Using again the results of Part 2 of Exercise 2.6 , show that the only local maxima of the objective of ( 2.2.24 ) are the signed dense vectors ‚àë i = 1 d ¬± ùíÜ i \\sum_{i=1}^{d}\\pm\\bm{e}_{i} ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ¬± bold_ital"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmexercise8.p1", "title": "This exercise follows the structure and formalism introduced in Exercise 2.6 , but applies it instead to the orthogonal group ùñÆ ‚Äã ( d ) = { ùëº ‚àà ‚Ñù d √ó d ‚à£ ùëº ‚ä§ ‚Äã ùëº = ùë∞ } \\mathsf{O}(d)=\\{\\bm{U}\\in\\mathbb", "snippet": "This exercise follows the structure and formalism introduced in Exercise 2.6 , but applies it instead to the orthogonal group ùñÆ ‚Äã ( d ) = { ùëº ‚àà ‚Ñù d √ó d ‚à£ ùëº ‚ä§ ‚Äã ùëº = ùë∞ } \\mathsf{O}(d)=\\{\\bm{U}\\in\\mathbb{R}^{d\\times d}\\mid\\bm{U}^{\\top}\\bm{U}=\\bm{I}\\} sansserif_O ( italic_d ) = { bold_italic_U ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d √ó italic_d end_POSTSUPERSCRIPT ‚à£ bold_italic_U start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_U = bold_italic_I } . Consult the description of Exercise 2.6 for the necessary conceptual background; the formalism applies identically to the case where the a"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmexercise8.p2", "title": "Let f : ‚Ñù d √ó d ‚Üí ‚Ñù f:\\mathbb{R}^{d\\times d}\\to\\mathbb{R} italic_f : blackboard_R start_POSTSUPERSCRIPT italic_d √ó italic_d end_POSTSUPERSCRIPT ‚Üí blackboard_R be a given twice-continuously-differentia", "snippet": "Let f : ‚Ñù d √ó d ‚Üí ‚Ñù f:\\mathbb{R}^{d\\times d}\\to\\mathbb{R} italic_f : blackboard_R start_POSTSUPERSCRIPT italic_d √ó italic_d end_POSTSUPERSCRIPT ‚Üí blackboard_R be a given twice-continuously-differentiable objective function. Consider the orthogonally-constrained optimization problem max ùë∏ ‚ä§ ‚Äã ùë∏ = ùë∞ ‚Å° f ‚Äã ( ùë∏ ) . \\max_{\\bm{Q}^{\\top}\\bm{Q}=\\bm{I}}\\,f(\\bm{Q}). roman_max start_POSTSUBSCRIPT bold_italic_Q start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_Q = bold_italic_I end_POSTSUBSCRIPT italic_f ( bold_italic_Q ) . (2.5.6) 1. It is easily seen that the orthogonal group has the defining equa"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.I6.i1.p1", "title": "It is easily seen that the orthogonal group has the defining equation F ‚Äã ( ùë∏ ) = ùë∏ ‚ä§ ‚Äã ùë∏ = ùë∞ F(\\bm{Q})=\\bm{Q}^{\\top}\\bm{Q}=\\bm{I} italic_F ( bold_italic_Q ) = bold_italic_Q start_POSTSUPERSCRIPT ‚ä§ en", "snippet": "It is easily seen that the orthogonal group has the defining equation F ‚Äã ( ùë∏ ) = ùë∏ ‚ä§ ‚Äã ùë∏ = ùë∞ F(\\bm{Q})=\\bm{Q}^{\\top}\\bm{Q}=\\bm{I} italic_F ( bold_italic_Q ) = bold_italic_Q start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_Q = bold_italic_I . Show, using this fact, that the tangent space to the orthogonal group at ùë∏ \\bm{Q} bold_italic_Q is given by T ùë∏ ‚Äã ùñÆ ‚Äã ( d ) = { ùë∏ ‚Äã ùõÄ ‚àà ‚Ñù d √ó d ‚à£ ùõÄ ‚ä§ = ‚àí ùõÄ } , T_{\\bm{Q}}\\mathsf{O}(d)=\\{\\bm{Q}\\bm{\\Omega}\\in\\mathbb{R}^{d\\times d}\\mid\\bm{\\Omega}^{\\top}=-\\bm{\\Omega}\\}, italic_T start_POSTSUBSCRIPT bold_italic_Q end_POSTSUBSCRIPT sansserif_O ( italic_d"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.I6.i2.p1", "title": "Show, by differentiating the Riemann gradient grad ‚Äã f ‚Äã ( ùë∏ ) \\mathrm{grad}\\,f(\\bm{Q}) roman_grad italic_f ( bold_italic_Q ) for the orthogonal group with respect to ùë∏ \\bm{Q} bold_italic_Q as in the ", "snippet": "Show, by differentiating the Riemann gradient grad ‚Äã f ‚Äã ( ùë∏ ) \\mathrm{grad}\\,f(\\bm{Q}) roman_grad italic_f ( bold_italic_Q ) for the orthogonal group with respect to ùë∏ \\bm{Q} bold_italic_Q as in the first part of this exercise, that the Riemannian Hessian is given by Hess ‚Äã f ‚Äã ( ùë∏ ) = ùí´ T ùë∏ ‚Äã ùñÆ ‚Äã ( d ) ‚Äã ( ‚àá 2 f ‚Äã ( ùë∏ ) ‚àí sym ‚Å° ( ùë∏ ‚ä§ ‚Äã ‚àá f ‚Äã ( ùë∏ ) ) ‚äó ùë∞ ) ‚Äã ùí´ T ùë∏ ‚Äã ùñÆ ‚Äã ( d ) , \\mathrm{Hess}\\,f(\\bm{Q})=\\mathcal{P}_{T_{\\bm{Q}}\\mathsf{O}(d)}\\left(\\nabla^{2}f(\\bm{Q})-\\operatorname{sym}(\\bm{Q}^{\\top}\\nabla f(\\bm{Q}))\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.I6.i2.p2", "title": "( Hint: The key is to manipulate one‚Äôs calcluations to obtain the form ( 2.5.8 ), which is as compact as possible. To this end, make use of the following isomorphism of the Kronecker product: if ùêÄ \\bm", "snippet": "( Hint: The key is to manipulate one‚Äôs calcluations to obtain the form ( 2.5.8 ), which is as compact as possible. To this end, make use of the following isomorphism of the Kronecker product: if ùêÄ \\bm{A} bold_italic_A , ùêó \\bm{X} bold_italic_X , and ùêÅ \\bm{B} bold_italic_B are matrices of compatible sizes, then one has ( ùë© ‚ä§ ‚äó ùë® ) ‚Äã vec ‚Å° ( ùëø ) = vec ‚Å° ( ùë® ‚Äã ùëø ‚Äã ùë© ) , (\\bm{B}^{\\top}\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\otimes$}}{\\scalebox{0.8}{$\\textstyle\\otimes$}}{\\scalebox{0.8}{$\\scriptstyle\\otimes$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\ot"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.I6.i3.p1", "title": "Now suppose ùëø ‚àà ‚Ñù d √ó d \\bm{X}\\in\\mathbb{R}^{d\\times d} bold_italic_X ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d √ó italic_d end_POSTSUPERSCRIPT is full-rank. In this and the next part of the exerci", "snippet": "Now suppose ùëø ‚àà ‚Ñù d √ó d \\bm{X}\\in\\mathbb{R}^{d\\times d} bold_italic_X ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d √ó italic_d end_POSTSUPERSCRIPT is full-rank. In this and the next part of the exercise, we consider the projection onto the orthogonal group of ùëø \\bm{X} bold_italic_X : proj ùñÆ ‚Äã ( d ) ‚Äã ( ùëø ) ‚âê min ùë∏ ‚àà ùñÆ ‚Äã ( d ) ‚Å° ‚Äñ ùë∏ ‚àí ùëø ‚Äñ F 2 . \\mathrm{proj}_{\\mathsf{O}(d)}(\\bm{X})\\doteq\\min_{\\bm{Q}\\in\\mathsf{O}(d)}\\,\\|\\bm{Q}-\\bm{X}\\|_{F}^{2}. roman_proj start_POSTSUBSCRIPT sansserif_O ( italic_d ) end_POSTSUBSCRIPT ( bold_italic_X ) ‚âê roman_min start_POSTSUBSCRIPT bold_italic_Q ‚àà sansserif_O ("}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.I6.i3.p2", "title": "(a) Using the first and second-order optimality conditions, show that every local minimizer ùë∏ \\bm{Q} bold_italic_Q of ( 2.5.9 ) satisfies ( ùë∏ ‚ä§ ‚Äã ùëø ) ‚ä§ \\displaystyle\\left(\\bm{Q}^{\\top}\\bm{X}\\right)^{\\", "snippet": "(a) Using the first and second-order optimality conditions, show that every local minimizer ùë∏ \\bm{Q} bold_italic_Q of ( 2.5.9 ) satisfies ( ùë∏ ‚ä§ ‚Äã ùëø ) ‚ä§ \\displaystyle\\left(\\bm{Q}^{\\top}\\bm{X}\\right)^{\\top} ( bold_italic_Q start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_X ) start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT = ùë∏ ‚ä§ ‚Äã ùëø , \\displaystyle=\\bm{Q}^{\\top}\\bm{X}, = bold_italic_Q start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_X , ùë∏ ‚ä§ ‚Äã ùëø \\displaystyle\\bm{Q}^{\\top}\\bm{X} bold_italic_Q start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_X ‚™∞ ùüé . \\displaystyle\\succeq\\mathbf{0}. ‚™∞"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.I6.i3.I1.i1.p1", "title": "Using the first and second-order optimality conditions, show that every local minimizer ùë∏ \\bm{Q} bold_italic_Q of ( 2.5.9 ) satisfies ( ùë∏ ‚ä§ ‚Äã ùëø ) ‚ä§ \\displaystyle\\left(\\bm{Q}^{\\top}\\bm{X}\\right)^{\\top}", "snippet": "Using the first and second-order optimality conditions, show that every local minimizer ùë∏ \\bm{Q} bold_italic_Q of ( 2.5.9 ) satisfies ( ùë∏ ‚ä§ ‚Äã ùëø ) ‚ä§ \\displaystyle\\left(\\bm{Q}^{\\top}\\bm{X}\\right)^{\\top} ( bold_italic_Q start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_X ) start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT = ùë∏ ‚ä§ ‚Äã ùëø , \\displaystyle=\\bm{Q}^{\\top}\\bm{X}, = bold_italic_Q start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_X , ùë∏ ‚ä§ ‚Äã ùëø \\displaystyle\\bm{Q}^{\\top}\\bm{X} bold_italic_Q start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_X ‚™∞ ùüé . \\displaystyle\\succeq\\mathbf{0}. ‚™∞ bol"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.I6.i3.I1.i2.p1", "title": "Using these conditions, argue that at every local minimizer ùë∏ \\bm{Q} bold_italic_Q of ( 2.5.9 ), one has ùë∏ ‚ä§ ‚Äã ùëø = ( ùëø ‚ä§ ‚Äã ùëø ) 1 / 2 \\bm{Q}^{\\top}\\bm{X}=(\\bm{X}^{\\top}\\bm{X})^{1/2} bold_italic_Q start", "snippet": "Using these conditions, argue that at every local minimizer ùë∏ \\bm{Q} bold_italic_Q of ( 2.5.9 ), one has ùë∏ ‚ä§ ‚Äã ùëø = ( ùëø ‚ä§ ‚Äã ùëø ) 1 / 2 \\bm{Q}^{\\top}\\bm{X}=(\\bm{X}^{\\top}\\bm{X})^{1/2} bold_italic_Q start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_X = ( bold_italic_X start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_X ) start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT . ( Hint: Use the fact from linear algebra that if ùêí ‚™∞ ùüé \\bm{S}\\succeq\\mathbf{0} bold_italic_S ‚™∞ bold_0 is a symmetric positive semidefinite matrix, then ( ùêí ‚ä§ ‚Äã ùêí ) 1 / 2 = ùêí (\\bm{S}^{\\top}\\bm{S})^{1/2}=\\bm{S} ( bold_ita"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.I6.i3.I1.i3.p1", "title": "Using the singular value decomposition ùëø = ùëº ‚Äã ùë∫ ‚Äã ùëΩ ‚ä§ \\bm{X}=\\bm{U}\\bm{S}\\bm{V}^{\\top} bold_italic_X = bold_italic_U bold_italic_S bold_italic_V start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT , conclude", "snippet": "Using the singular value decomposition ùëø = ùëº ‚Äã ùë∫ ‚Äã ùëΩ ‚ä§ \\bm{X}=\\bm{U}\\bm{S}\\bm{V}^{\\top} bold_italic_X = bold_italic_U bold_italic_S bold_italic_V start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT , conclude that ùëº ‚Äã ùëΩ ‚ä§ = proj ùñÆ ‚Äã ( d ) ‚Äã ( ùëø ) . \\bm{U}\\bm{V}^{\\top}=\\mathrm{proj}_{\\mathsf{O}(d)}(\\bm{X}). bold_italic_U bold_italic_V start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT = roman_proj start_POSTSUBSCRIPT sansserif_O ( italic_d ) end_POSTSUBSCRIPT ( bold_italic_X ) ."}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmtheorem1", "title": "Theorem 2.1 .", "snippet": "Theorem 2.1 . Suppose that our dataset { ùê± i } i = 1 N ‚äÜ ‚Ñù D \\{\\bm{x}_{i}\\}_{i=1}^{N}\\subseteq\\mathbb{R}^{D} { bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ‚äÜ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT can be written as ùíô i = ùëº ‚Äã ùíõ i + ùú∫ i , ‚àÄ i ‚àà [ N ] , \\bm{x}_{i}=\\bm{U}\\bm{z}_{i}+\\bm{\\varepsilon}_{i},\\qquad\\forall i\\in[N], bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_U bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBS"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmremark1", "title": "Remark 2.1 .", "snippet": "Remark 2.1 . In some data analysis tasks, the data matrix ùëø \\bm{X} bold_italic_X is formatted such that each data point is a row rather than a column as is presented here. In this case the principal components are the top d d italic_d eigenvectors of ùëø ‚ä§ ‚Äã ùëø / N \\bm{X}^{\\top}\\bm{X}/N bold_italic_X start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_X / italic_N ."}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmremark2", "title": "Remark 2.2 (Basis Selection via Denoising Eigenvalues) .", "snippet": "Remark 2.2 (Basis Selection via Denoising Eigenvalues) . In many cases, either our data will not truly be distributed according to a subspace-plus-noise model, or we will not know the true underlying dimension d d italic_d of the subspace. In this case, we have to choose d d italic_d ; this problem is called model selection . In the restricted case of PCA, one way to perform model selection is to compute ùëø ‚Äã ùëø ‚ä§ / N \\bm{X}\\bm{X}^{\\top}/N bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT / italic_N and look for instances where adjacent eigenvalues sharply decrease; this is"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmremark3", "title": "Remark 2.3 (Denoising Samples) .", "snippet": "Remark 2.3 (Denoising Samples) . The expression on the right-hand side of ( 2.1.5 ), that is, min ùëº ~ ‚Å° 1 N ‚Äã ‚àë i = 1 N ‚Äñ ùíô i ‚àí ùëº ~ ‚Äã ùëº ~ ‚ä§ ‚Äã ùíô i ‚Äñ 2 2 , \\min_{\\tilde{\\bm{U}}}\\frac{1}{N}\\sum_{i=1}^{N}\\|\\bm{x}_{i}-\\tilde{\\bm{U}}\\tilde{\\bm{U}}^{\\top}\\bm{x}_{i}\\|_{2}^{2}, roman_min start_POSTSUBSCRIPT over~ start_ARG bold_italic_U end_ARG end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ‚à• bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over~ start_ARG bold_"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmremark4", "title": "Remark 2.4 (Neural Network Interpretation) .", "snippet": "Remark 2.4 (Neural Network Interpretation) . If we do a PCA, we approximately recover the support of the distribution encoded by the parameter ùëº ‚ãÜ \\bm{U}^{\\star} bold_italic_U start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT . The learned denoising map then takes the form ùëº ‚ãÜ ‚Äã ( ùëº ‚ãÜ ) ‚ä§ \\bm{U}^{\\star}(\\bm{U}^{\\star})^{\\top} bold_italic_U start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ( bold_italic_U start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT . On top of being a denoiser, this can be viewed as a simple two-layer weight-tied linear neural network : the fir"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmtheorem2", "title": "Theorem 2.2 (Power Iteration) .", "snippet": "Theorem 2.2 (Power Iteration) . Assume that Œª 1 > Œª i \\lambda_{1}>\\lambda_{i} italic_Œª start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT > italic_Œª start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT for all i > 1 i>1 italic_i > 1 . If we compute the fixed point of ( 2.1.16 ) using the following iteration: ùíó 0 ‚àº ùí© ‚Å° ( ùüé , ùüè ) , ùíó t + 1 ‚Üê ùë¥ ‚Äã ùíó t ‚Äñ ùë¥ ‚Äã ùíó t ‚Äñ 2 , \\bm{v}_{0}\\sim\\operatorname{\\mathcal{N}}(\\bm{0},\\bm{1}),\\qquad\\bm{v}_{t+1}\\leftarrow\\frac{\\bm{M}\\bm{v}_{t}}{\\|\\bm{M}\\bm{v}_{t}\\|_{2}}, bold_italic_v start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ‚àº caligraphic_N ( bold_0 , bold_1 ) , bold_italic_v start_POST"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmremark5", "title": "Remark 2.5 .", "snippet": "Remark 2.5 . By using the probabilistic viewpoint of PCA, we achieve a clearer and more quantitative understanding of how it relates to denoising. First, consider the denoising problem in ( 2.1.26 ), namely min ùëº ~ ‚Å° ùîº ‚Å° ‚Äñ ùíô ‚àí ùëº ~ ‚Äã ùëº ~ ‚ä§ ‚Äã ùíô ‚Äñ 2 2 . \\min_{\\tilde{\\bm{U}}}\\operatorname{\\mathbb{E}}\\|\\bm{x}-\\tilde{\\bm{U}}\\tilde{\\bm{U}}^{\\top}\\bm{x}\\|_{2}^{2}. roman_min start_POSTSUBSCRIPT over~ start_ARG bold_italic_U end_ARG end_POSTSUBSCRIPT blackboard_E ‚à• bold_italic_x - over~ start_ARG bold_italic_U end_ARG over~ start_ARG bold_italic_U end_ARG start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmtheorem3", "title": "Theorem 2.3 .", "snippet": "Theorem 2.3 . Suppose that the random variable ùê± \\bm{x} bold_italic_x can be written as ùíô = ùëº ‚Äã ùíõ + ùú∫ \\bm{x}=\\bm{U}\\bm{z}+\\bm{\\varepsilon} bold_italic_x = bold_italic_U bold_italic_z + bold_italic_Œµ (2.1.34) where ùêî ‚àà ùñÆ ‚Äã ( D , d ) \\bm{U}\\in\\mathsf{O}(D,d) bold_italic_U ‚àà sansserif_O ( italic_D , italic_d ) captures the low-rank structure, ùê≥ \\bm{z} bold_italic_z is a random vector taking values in ‚Ñù d \\mathbb{R}^{d} blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , and ùõÜ \\bm{\\varepsilon} bold_italic_Œµ is a random vector taking values in ‚Ñù D \\mathbb{R}^{D} blackboard_R start_POS"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmremark6", "title": "Remark 2.6 (A Mixture of Gaussians v.s. A Superposition of Gaussians) .", "snippet": "Remark 2.6 (A Mixture of Gaussians v.s. A Superposition of Gaussians) . One should be aware that the above model ( 2.2.3 ) is a mixture of Gaussian distributions, not to be confused with a mixture of Gaussian variables by superposition, say ùíô = ‚àë i = 1 n w i ‚Äã ùíô i , ùíô i ‚àº ùí© ‚Äã ( ùüé , ùëº i ‚Äã ùëº i ‚ä§ ) , \\bm{x}=\\sum_{i=1}^{n}w_{i}\\bm{x}_{i},\\quad\\bm{x}_{i}\\sim\\mathcal{N}(\\mathbf{0},\\bm{U}_{i}\\bm{U}_{i}^{\\top}), bold_italic_x = ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT bold_italic_x st"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmremark7", "title": "Remark 2.7 (The Orthogonal Assumption) .", "snippet": "Remark 2.7 (The Orthogonal Assumption) . At first sight, the assumption that the dictionary ùëº \\bm{U} bold_italic_U is orthogonal might seem to be somewhat restrictive. But there is actually no loss of generality. One may consider a complete dictionary to be any square invertible matrix ùëº \\bm{U} bold_italic_U . With samples generated from this dictionary: ùëø = ùëº ‚Äã ùíÅ ‚àà ‚Ñù D √ó N \\bm{X}=\\bm{U}\\bm{Z}\\in\\mathbb{R}^{D\\times N} bold_italic_X = bold_italic_U bold_italic_Z ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D √ó italic_N end_POSTSUPERSCRIPT , it is easy to show that with some preconditioning of th"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmremark8", "title": "Remark 2.8 .", "snippet": "Remark 2.8 . It is also known that for vectors on a sphere, minimizing the ‚Ñì 1 \\ell^{1} roman_‚Ñì start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT norm is equivalent to minimizing the ‚Ñì 0 \\ell^{0} roman_‚Ñì start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT norm (for promoting sparsity), arg ‚Äã min ùíõ ‚àà ùïä n ‚Å° ‚Äñ ùíõ ‚Äñ 1 ‚áî arg ‚Äã min ùíõ ‚àà ùïä n ‚Å° ‚Äñ ùíõ ‚Äñ 0 , \\operatorname*{arg\\ min}_{\\bm{z}\\in\\mathbb{S}^{n}}\\|\\bm{z}\\|_{1}\\quad\\Leftrightarrow\\quad\\operatorname*{arg\\ min}_{\\bm{z}\\in\\mathbb{S}^{n}}\\|\\bm{z}\\|_{0}, start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT bold_italic_z ‚àà blackboard_S start_POSTSUPER"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmremark9", "title": "Remark 2.9 (Global Optimality of ‚Ñì 4 \\ell^{4} roman_‚Ñì start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT Maximization) .", "snippet": "Remark 2.9 (Global Optimality of ‚Ñì 4 \\ell^{4} roman_‚Ñì start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT Maximization) . The constrained ‚Ñì 4 \\ell^{4} roman_‚Ñì start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT maximization problem is a nonconvex program. In general one should not expect that any greedy (say gradient-descent type) algorithms would converge to the globally optimal solution. Surprisingly, one can show that, unlike general nonconvex programs, the landscape of ‚Ñì 4 \\ell^{4} roman_‚Ñì start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT maximization over a sphere min ‚àí 1 4 ‚Äã ‚Äñ ùíí ‚ä§ ‚Äã ùëø ‚Äñ 4 4 subject to ùíí ‚ä§ ‚Äã ùíí"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmremark10", "title": "Remark 2.10 (Stable Deep Linear Network) .", "snippet": "Remark 2.10 (Stable Deep Linear Network) . The above iterative process of computing the dictionary has a natural incremental ‚Äúdeep learning‚Äù interpretation. Let us define Œ¥ ‚Äã ùë® t + 1 = ùë® t + 1 ‚Äã ùë® t ‚ä§ \\delta\\bm{A}_{t+1}=\\bm{A}_{t+1}\\bm{A}_{t}^{\\top} italic_Œ¥ bold_italic_A start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT = bold_italic_A start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT bold_italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT and ùíÅ t = ùë® t ‚Äã ùëø \\bm{Z}_{t}=\\bm{A}_{t}\\bm{X} bold_italic_Z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRI"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmexample1", "title": "Example 2.1 .", "snippet": "Example 2.1 . Given sampled images of hand-written digits, Figure 2.6 (a) shows the result of fitting an orthogonal dictionary to the dataset. In contrast, Figure 2.6 (b) shows the result of running an optimization algorithm for learning overcomplete dictionaries (which we will present in detail later in the Chapter) on these samples. Notice that the representations become far sparser and the codebooks far more interpretable‚Äîthey consist of fundamental primitives for the strokes composing the digits, i.e. oriented edges. ‚ñ† \\blacksquare ‚ñ†"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmremark11", "title": "Remark 2.11 ( ‚Ñì 4 \\ell^{4} roman_‚Ñì start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT maximization versus ‚Ñì 1 \\ell^{1} roman_‚Ñì start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT minimization) .", "snippet": "Remark 2.11 ( ‚Ñì 4 \\ell^{4} roman_‚Ñì start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT maximization versus ‚Ñì 1 \\ell^{1} roman_‚Ñì start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT minimization) . Note that the above problem formulation follows naturally from the LASSO formulation ( 2.3.5 ) for sparse coding. We promote the sparsity of the solution via the ‚Ñì 1 \\ell^{1} roman_‚Ñì start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT norm. Nevertheless, if we are only interested in recovering the over-complete dictionary ùë® \\bm{A} bold_italic_A , the ‚Ñì 4 \\ell^{4} roman_‚Ñì start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT maximizati"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmexercise1", "title": "Exercise 2.1 .", "snippet": "Exercise 2.1 . Prove that, for any symmetric matrix ùë® \\bm{A} bold_italic_A , the solution to the problem max ùëº ‚àà ùñÆ ‚Äã ( D , d ) ‚Å° tr ‚Å° ( ùëº ‚ä§ ‚Äã ùë® ‚Äã ùëº ) \\max_{\\bm{U}\\in\\mathsf{O}(D,d)}\\operatorname{tr}\\left(\\bm{U}^{\\top}\\bm{A}\\bm{U}\\right) roman_max start_POSTSUBSCRIPT bold_italic_U ‚àà sansserif_O ( italic_D , italic_d ) end_POSTSUBSCRIPT roman_tr ( bold_italic_U start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_A bold_italic_U ) is the matrix ùëº ‚ãÜ \\bm{U}^{\\star} bold_italic_U start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT whose columns are the top d d italic_d unit eigenvectors of ùë® \\bm{A} bold"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmexercise2", "title": "Exercise 2.2 .", "snippet": "Exercise 2.2 . Let ùíõ ‚àº ùí© ‚Äã ( ùüé , œÉ 2 ‚Äã ùë∞ ) \\bm{z}\\sim\\mathcal{N}(\\mathbf{0},\\sigma^{2}\\bm{I}) bold_italic_z ‚àº caligraphic_N ( bold_0 , italic_œÉ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) be a Gaussian random variable with independent components, each with variance œÉ 2 \\sigma^{2} italic_œÉ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . Prove that for any orthogonal matrix ùë∏ \\bm{Q} bold_italic_Q (i.e., ùë∏ ‚ä§ ‚Äã ùë∏ = ùë∞ \\bm{Q}^{\\top}\\bm{Q}=\\bm{I} bold_italic_Q start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_Q = bold_italic_I ), the random variable ùë∏ ‚Äã ùíõ \\bm{Q}\\bm{z} bold_italic_"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmexercise3", "title": "Exercise 2.3 .", "snippet": "Exercise 2.3 . The notion of statistical identifiability discussed above can be related to symmetries of the model class, allowing estimation to be understood in a purely deterministic fashion without any statistical assumptions. Consider the model ùëø = ùëº ‚Äã ùíÅ \\bm{X}=\\bm{U}\\bm{Z} bold_italic_X = bold_italic_U bold_italic_Z for matrices ùëø , ùëº , ùíÅ \\bm{X},\\bm{U},\\bm{Z} bold_italic_X , bold_italic_U , bold_italic_Z of compatible sizes. 1. Show that if ùë® \\bm{A} bold_italic_A is any square invertible matrix of compatible size, then the pair ( ùëº ‚Äã ùë® , ùë® ‚àí 1 ‚Äã ùíÅ ) (\\bm{U}\\bm{A},\\bm{A}^{-1}\\bm{Z}) ( bold"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmexercise4", "title": "Exercise 2.4 .", "snippet": "Exercise 2.4 . Consider the model ùíô = ùëº ‚Äã ùíõ \\bm{x}=\\bm{U}\\bm{z} bold_italic_x = bold_italic_U bold_italic_z , where ùëº ‚àà ‚Ñù D √ó d \\bm{U}\\in\\mathbb{R}^{D\\times d} bold_italic_U ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D √ó italic_d end_POSTSUPERSCRIPT with D ‚â• d D\\geq d italic_D ‚â• italic_d is fixed and has rank d d italic_d , and ùíõ \\bm{z} bold_italic_z is a zero-mean random variable. Let ùíô 1 , ‚Ä¶ ‚Äã ùíô N \\bm{x}_{1},\\dots\\bm{x}_{N} bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT denote i.i.d. observations from this model. 1. Sho"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmexercise5", "title": "Exercise 2.5 .", "snippet": "Exercise 2.5 . Let X X italic_X and Y Y italic_Y be zero-mean independent random variables. 1. Show that kurt ( X + Y ) = kurt ( X ) + kurt ( Y ) \\mathop{\\mathrm{kurt}}(X+Y)=\\mathop{\\mathrm{kurt}}(X)+\\mathop{\\mathrm{kurt}}(Y) roman_kurt ( italic_X + italic_Y ) = roman_kurt ( italic_X ) + roman_kurt ( italic_Y ) . 2. For any Œ± ‚àà ‚Ñù \\alpha\\in\\mathbb{R} italic_Œ± ‚àà blackboard_R , show that kurt ( Œ± ‚Äã X ) = Œ± 4 ‚Äã kurt ( X ) \\mathop{\\mathrm{kurt}}(\\alpha X)=\\alpha^{4}\\mathop{\\mathrm{kurt}}(X) roman_kurt ( italic_Œ± italic_X ) = italic_Œ± start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT roman_kurt ( italic_X"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmexercise6", "title": "Exercise 2.6 .", "snippet": "Exercise 2.6 . Let f : ‚Ñù d ‚Üí ‚Ñù f:\\mathbb{R}^{d}\\to\\mathbb{R} italic_f : blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ‚Üí blackboard_R be a given twice-continuously-differentiable objective function. Consider the spherically-constrained optimization problem max ‚Äñ ùíñ ‚Äñ 2 2 = 1 ‚Å° f ‚Äã ( ùíñ ) . \\max_{\\|\\bm{u}\\|_{2}^{2}=1}\\,f(\\bm{u}). roman_max start_POSTSUBSCRIPT ‚à• bold_italic_u ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 1 end_POSTSUBSCRIPT italic_f ( bold_italic_u ) . (2.5.1) In this exercise, we will derive the expressions we gave in the"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmexercise7", "title": "Exercise 2.7 .", "snippet": "Exercise 2.7 . In this exercise, we sketch an argument referred to in the literature as a landscape analysis for the spherically-constrained population kurtosis maximization problem ( 2.2.24 ). We will show that when there is at least one independent component with positive kurtosis, its global maximizers indeed lead to the recovery of one column of the dictionary ùëº \\bm{U} bold_italic_U . For simplicity, we will assume that kurt ( z i ) ‚â† 0 \\mathop{\\mathrm{kurt}}(z_{i})\\neq 0 roman_kurt ( italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ‚â† 0 for each i = 1 , ‚Ä¶ , d i=1,\\dots,d italic_i "}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#Thmexercise8", "title": "Exercise 2.8 .", "snippet": "Exercise 2.8 . This exercise follows the structure and formalism introduced in Exercise 2.6 , but applies it instead to the orthogonal group ùñÆ ‚Äã ( d ) = { ùëº ‚àà ‚Ñù d √ó d ‚à£ ùëº ‚ä§ ‚Äã ùëº = ùë∞ } \\mathsf{O}(d)=\\{\\bm{U}\\in\\mathbb{R}^{d\\times d}\\mid\\bm{U}^{\\top}\\bm{U}=\\bm{I}\\} sansserif_O ( italic_d ) = { bold_italic_U ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d √ó italic_d end_POSTSUPERSCRIPT ‚à£ bold_italic_U start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_U = bold_italic_I } . Consult the description of Exercise 2.6 for the necessary conceptual background; the formalism applies identically to the c"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E1", "title": "ùíô i = ùëº ‚Äã ùíõ i + ùú∫ i , ‚àÄ i ‚àà [ N ] . \\bm{x}_{i}=\\bm{U}\\bm{z}_{i}+\\bm{\\varepsilon}_{i},\\quad\\forall i\\in[N]. bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_U bold_italic_z st", "snippet": "ùíô i = ùëº ‚Äã ùíõ i + ùú∫ i , ‚àÄ i ‚àà [ N ] . \\bm{x}_{i}=\\bm{U}\\bm{z}_{i}+\\bm{\\varepsilon}_{i},\\quad\\forall i\\in[N]. bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_U bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + bold_italic_Œµ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , ‚àÄ italic_i ‚àà [ italic_N ] . (2.1.1)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E2", "title": "min ùëº ~ , { ùíõ ~ i } i = 1 N ‚Å° 1 N ‚Äã ‚àë i = 1 N ‚Äñ ùíô i ‚àí ùëº ~ ‚Äã ùíõ ~ i ‚Äñ 2 2 , \\min_{\\tilde{\\bm{U}},\\{\\tilde{\\bm{z}}_{i}\\}_{i=1}^{N}}\\frac{1}{N}\\sum_{i=1}^{N}\\|\\bm{x}_{i}-\\tilde{\\bm{U}}\\tilde{\\bm{z}}_{i}\\|", "snippet": "min ùëº ~ , { ùíõ ~ i } i = 1 N ‚Å° 1 N ‚Äã ‚àë i = 1 N ‚Äñ ùíô i ‚àí ùëº ~ ‚Äã ùíõ ~ i ‚Äñ 2 2 , \\min_{\\tilde{\\bm{U}},\\{\\tilde{\\bm{z}}_{i}\\}_{i=1}^{N}}\\frac{1}{N}\\sum_{i=1}^{N}\\|\\bm{x}_{i}-\\tilde{\\bm{U}}\\tilde{\\bm{z}}_{i}\\|_{2}^{2}, roman_min start_POSTSUBSCRIPT over~ start_ARG bold_italic_U end_ARG , { over~ start_ARG bold_italic_z end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT star"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E5", "title": "min ùëº ~ , { ùíõ ~ i } i = 1 N ‚Å° 1 N ‚Äã ‚àë i = 1 N ‚Äñ ùíô i ‚àí ùëº ~ ‚Äã ùíõ ~ i ‚Äñ 2 2 = min ùëº ~ ‚Å° 1 N ‚Äã ‚àë i = 1 N ‚Äñ ùíô i ‚àí ùëº ~ ‚Äã ùëº ~ ‚ä§ ‚Äã ùíô i ‚Äñ 2 2 . \\min_{\\tilde{\\bm{U}},\\{\\tilde{\\bm{z}}_{i}\\}_{i=1}^{N}}\\frac{1}{N}\\", "snippet": "min ùëº ~ , { ùíõ ~ i } i = 1 N ‚Å° 1 N ‚Äã ‚àë i = 1 N ‚Äñ ùíô i ‚àí ùëº ~ ‚Äã ùíõ ~ i ‚Äñ 2 2 = min ùëº ~ ‚Å° 1 N ‚Äã ‚àë i = 1 N ‚Äñ ùíô i ‚àí ùëº ~ ‚Äã ùëº ~ ‚ä§ ‚Äã ùíô i ‚Äñ 2 2 . \\min_{\\tilde{\\bm{U}},\\{\\tilde{\\bm{z}}_{i}\\}_{i=1}^{N}}\\frac{1}{N}\\sum_{i=1}^{N}\\|\\bm{x}_{i}-\\tilde{\\bm{U}}\\tilde{\\bm{z}}_{i}\\|_{2}^{2}=\\min_{\\tilde{\\bm{U}}}\\frac{1}{N}\\sum_{i=1}^{N}\\|\\bm{x}_{i}-\\tilde{\\bm{U}}\\tilde{\\bm{U}}^{\\top}\\bm{x}_{i}\\|_{2}^{2}. roman_min start_POSTSUBSCRIPT over~ start_ARG bold_italic_U end_ARG , { over~ start_ARG bold_italic_z end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E6", "title": "ùíô ‚Üí ‚Ñ∞ = ( ùëº ‚ãÜ ) ‚ä§ ùíõ ‚Üí ùíü = ùëº ‚ãÜ ùíô ^ . \\bm{x}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}=(\\bm{U}^{\\star})^{\\top}\\hskip 5.69054pt}\\bm{z}\\xrightarrow{\\hskip 5.69054pt\\mathcal{D}=\\bm{U}^{\\star}\\hskip 5.69054pt", "snippet": "ùíô ‚Üí ‚Ñ∞ = ( ùëº ‚ãÜ ) ‚ä§ ùíõ ‚Üí ùíü = ùëº ‚ãÜ ùíô ^ . \\bm{x}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}=(\\bm{U}^{\\star})^{\\top}\\hskip 5.69054pt}\\bm{z}\\xrightarrow{\\hskip 5.69054pt\\mathcal{D}=\\bm{U}^{\\star}\\hskip 5.69054pt}\\hat{\\bm{x}}. bold_italic_x start_ARROW start_OVERACCENT caligraphic_E = ( bold_italic_U start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT end_OVERACCENT ‚Üí end_ARROW bold_italic_z start_ARROW start_OVERACCENT caligraphic_D = bold_italic_U start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT end_OVERACCENT ‚Üí end_ARROW over^ start_ARG bold_italic_x end_ARG . (2.1.6)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E9", "title": "max ùíñ ~ : ‚Äñ ùíñ ~ ‚Äñ 2 = 1 ‚Å° ùíñ ~ ‚ä§ ‚Äã ( ùëø ‚Äã ùëø ‚ä§ / N ) ‚Äã ùíñ ~ . \\max_{\\tilde{\\bm{u}}\\colon\\|\\tilde{\\bm{u}}\\|_{2}=1}\\tilde{\\bm{u}}^{\\top}(\\bm{X}\\bm{X}^{\\top}/N)\\tilde{\\bm{u}}. roman_max start_POSTSUBSCRIPT o", "snippet": "max ùíñ ~ : ‚Äñ ùíñ ~ ‚Äñ 2 = 1 ‚Å° ùíñ ~ ‚ä§ ‚Äã ( ùëø ‚Äã ùëø ‚ä§ / N ) ‚Äã ùíñ ~ . \\max_{\\tilde{\\bm{u}}\\colon\\|\\tilde{\\bm{u}}\\|_{2}=1}\\tilde{\\bm{u}}^{\\top}(\\bm{X}\\bm{X}^{\\top}/N)\\tilde{\\bm{u}}. roman_max start_POSTSUBSCRIPT over~ start_ARG bold_italic_u end_ARG : ‚à• over~ start_ARG bold_italic_u end_ARG ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 1 end_POSTSUBSCRIPT over~ start_ARG bold_italic_u end_ARG start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ( bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT / italic_N ) over~ start_ARG bold_italic_u end_ARG . (2.1.9)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E10", "title": "ùíñ ~ ‚ä§ ‚Äã ( ùëø ‚Äã ùëø ‚ä§ / N ) ‚Äã ùíñ ~ = ùíñ ~ ‚ä§ ‚Äã ùëΩ ‚Äã ùö≤ ‚Äã ùëΩ ‚ä§ ‚Äã ùíñ = ( ùëΩ ‚ä§ ‚Äã ùíñ ~ ) ‚ä§ ‚Äã ùö≤ ‚Äã ( ùëΩ ‚ä§ ‚Äã ùíñ ~ ) . \\tilde{\\bm{u}}^{\\top}(\\bm{X}\\bm{X}^{\\top}/N)\\tilde{\\bm{u}}=\\tilde{\\bm{u}}^{\\top}\\bm{V}\\bm{\\Lambda}\\bm{V}", "snippet": "ùíñ ~ ‚ä§ ‚Äã ( ùëø ‚Äã ùëø ‚ä§ / N ) ‚Äã ùíñ ~ = ùíñ ~ ‚ä§ ‚Äã ùëΩ ‚Äã ùö≤ ‚Äã ùëΩ ‚ä§ ‚Äã ùíñ = ( ùëΩ ‚ä§ ‚Äã ùíñ ~ ) ‚ä§ ‚Äã ùö≤ ‚Äã ( ùëΩ ‚ä§ ‚Äã ùíñ ~ ) . \\tilde{\\bm{u}}^{\\top}(\\bm{X}\\bm{X}^{\\top}/N)\\tilde{\\bm{u}}=\\tilde{\\bm{u}}^{\\top}\\bm{V}\\bm{\\Lambda}\\bm{V}^{\\top}\\bm{u}=(\\bm{V}^{\\top}\\tilde{\\bm{u}})^{\\top}\\bm{\\Lambda}(\\bm{V}^{\\top}\\tilde{\\bm{u}}). over~ start_ARG bold_italic_u end_ARG start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ( bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT / italic_N ) over~ start_ARG bold_italic_u end_ARG = over~ start_ARG bold_italic_u end_ARG start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_V bo"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E11", "title": "ùíñ ~ ‚ä§ ‚Äã ( ùëø ‚Äã ùëø ‚ä§ / N ) ‚Äã ùíñ ~ = ùíò ~ ‚ä§ ‚Äã ùö≤ ‚Äã ùíò ~ , \\tilde{\\bm{u}}^{\\top}(\\bm{X}\\bm{X}^{\\top}/N)\\tilde{\\bm{u}}=\\tilde{\\bm{w}}^{\\top}\\bm{\\Lambda}\\tilde{\\bm{w}}, over~ start_ARG bold_italic_u end_ARG star", "snippet": "ùíñ ~ ‚ä§ ‚Äã ( ùëø ‚Äã ùëø ‚ä§ / N ) ‚Äã ùíñ ~ = ùíò ~ ‚ä§ ‚Äã ùö≤ ‚Äã ùíò ~ , \\tilde{\\bm{u}}^{\\top}(\\bm{X}\\bm{X}^{\\top}/N)\\tilde{\\bm{u}}=\\tilde{\\bm{w}}^{\\top}\\bm{\\Lambda}\\tilde{\\bm{w}}, over~ start_ARG bold_italic_u end_ARG start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ( bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT / italic_N ) over~ start_ARG bold_italic_u end_ARG = over~ start_ARG bold_italic_w end_ARG start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_Œõ over~ start_ARG bold_italic_w end_ARG , (2.1.11)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E12", "title": "ùíô i = ùëº ‚Äã ùíõ i + ùú∫ i , ‚àÄ i ‚àà [ N ] , \\bm{x}_{i}=\\bm{U}\\bm{z}_{i}+\\bm{\\varepsilon}_{i},\\qquad\\forall i\\in[N], bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_U bold_italic_z s", "snippet": "ùíô i = ùëº ‚Äã ùíõ i + ùú∫ i , ‚àÄ i ‚àà [ N ] , \\bm{x}_{i}=\\bm{U}\\bm{z}_{i}+\\bm{\\varepsilon}_{i},\\qquad\\forall i\\in[N], bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_U bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + bold_italic_Œµ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , ‚àÄ italic_i ‚àà [ italic_N ] , (2.1.12)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E13", "title": "min ùëº ~ ‚Å° 1 N ‚Äã ‚àë i = 1 N ‚Äñ ùíô i ‚àí ùëº ~ ‚Äã ùëº ~ ‚ä§ ‚Äã ùíô i ‚Äñ 2 2 , \\min_{\\tilde{\\bm{U}}}\\frac{1}{N}\\sum_{i=1}^{N}\\|\\bm{x}_{i}-\\tilde{\\bm{U}}\\tilde{\\bm{U}}^{\\top}\\bm{x}_{i}\\|_{2}^{2}, roman_min start_POSTSUBS", "snippet": "min ùëº ~ ‚Å° 1 N ‚Äã ‚àë i = 1 N ‚Äñ ùíô i ‚àí ùëº ~ ‚Äã ùëº ~ ‚ä§ ‚Äã ùíô i ‚Äñ 2 2 , \\min_{\\tilde{\\bm{U}}}\\frac{1}{N}\\sum_{i=1}^{N}\\|\\bm{x}_{i}-\\tilde{\\bm{U}}\\tilde{\\bm{U}}^{\\top}\\bm{x}_{i}\\|_{2}^{2}, roman_min start_POSTSUBSCRIPT over~ start_ARG bold_italic_U end_ARG end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ‚à• bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over~ start_ARG bold_italic_U end_ARG over~ start_ARG bold_italic_U end_ARG start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSC"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E14", "title": "denoise ‚Å° ( ùíô ) = ùëº ‚ãÜ ‚àò id ‚àò ( ùëº ‚ãÜ ) ‚ä§ ‚Äã ùíô ‚èü first ‚Äúlayer‚Äù ‚èü post-activation of first ‚Äúlayer‚Äù ‚èü output of ‚ÄúNN‚Äù \\operatorname{denoise}(\\bm{x})=\\underbrace{\\bm{U}^{\\star}\\circ\\underbrace{\\operatorname{i", "snippet": "denoise ‚Å° ( ùíô ) = ùëº ‚ãÜ ‚àò id ‚àò ( ùëº ‚ãÜ ) ‚ä§ ‚Äã ùíô ‚èü first ‚Äúlayer‚Äù ‚èü post-activation of first ‚Äúlayer‚Äù ‚èü output of ‚ÄúNN‚Äù \\operatorname{denoise}(\\bm{x})=\\underbrace{\\bm{U}^{\\star}\\circ\\underbrace{\\operatorname{id}\\circ\\underbrace{(\\bm{U}^{\\star})^{\\top}\\bm{x}}_{\\text{first ``layer''}}}_{\\text{post-activation of first ``layer''}}}_{\\text{output of ``NN''}} roman_denoise ( bold_italic_x ) = under‚èü start_ARG bold_italic_U start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ‚àò under‚èü start_ARG roman_id ‚àò under‚èü start_ARG ( bold_italic_U start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ‚ä§ end_POSTSUP"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E15", "title": "NN ‚Å° ( ùíô ) = ùëæ ‚ãÜ ‚àò ReLU ‚àò ( ùëº ‚ãÜ ) ‚ä§ ‚Äã ùíô ‚èü first layer ‚èü post-activation of first layer ‚èü output of NN \\operatorname{NN}(\\bm{x})=\\underbrace{\\bm{W}^{\\star}\\circ\\underbrace{\\mathrm{ReLU}\\circ\\underbrace", "snippet": "NN ‚Å° ( ùíô ) = ùëæ ‚ãÜ ‚àò ReLU ‚àò ( ùëº ‚ãÜ ) ‚ä§ ‚Äã ùíô ‚èü first layer ‚èü post-activation of first layer ‚èü output of NN \\operatorname{NN}(\\bm{x})=\\underbrace{\\bm{W}^{\\star}\\circ\\underbrace{\\mathrm{ReLU}\\circ\\underbrace{(\\bm{U}^{\\star})^{\\top}\\bm{x}}_{\\text{first layer}}}_{\\text{post-activation of first layer}}}_{\\text{output of NN}} roman_NN ( bold_italic_x ) = under‚èü start_ARG bold_italic_W start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ‚àò under‚èü start_ARG roman_ReLU ‚àò under‚èü start_ARG ( bold_italic_U start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_x end_ARG st"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E16", "title": "ùíò = ùë¥ ‚Äã ùíò ‚Äñ ùë¥ ‚Äã ùíò ‚Äñ 2 . \\bm{w}=\\frac{\\bm{M}\\bm{w}}{\\|\\bm{M}\\bm{w}\\|_{2}}. bold_italic_w = divide start_ARG bold_italic_M bold_italic_w end_ARG start_ARG ‚à• bold_italic_M bold_italic_w ‚à• start_POSTSUBSC", "snippet": "ùíò = ùë¥ ‚Äã ùíò ‚Äñ ùë¥ ‚Äã ùíò ‚Äñ 2 . \\bm{w}=\\frac{\\bm{M}\\bm{w}}{\\|\\bm{M}\\bm{w}\\|_{2}}. bold_italic_w = divide start_ARG bold_italic_M bold_italic_w end_ARG start_ARG ‚à• bold_italic_M bold_italic_w ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG . (2.1.16)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E17", "title": "ùíó 0 ‚àº ùí© ‚Å° ( ùüé , ùüè ) , ùíó t + 1 ‚Üê ùë¥ ‚Äã ùíó t ‚Äñ ùë¥ ‚Äã ùíó t ‚Äñ 2 , \\bm{v}_{0}\\sim\\operatorname{\\mathcal{N}}(\\bm{0},\\bm{1}),\\qquad\\bm{v}_{t+1}\\leftarrow\\frac{\\bm{M}\\bm{v}_{t}}{\\|\\bm{M}\\bm{v}_{t}\\|_{2}}, bold_ital", "snippet": "ùíó 0 ‚àº ùí© ‚Å° ( ùüé , ùüè ) , ùíó t + 1 ‚Üê ùë¥ ‚Äã ùíó t ‚Äñ ùë¥ ‚Äã ùíó t ‚Äñ 2 , \\bm{v}_{0}\\sim\\operatorname{\\mathcal{N}}(\\bm{0},\\bm{1}),\\qquad\\bm{v}_{t+1}\\leftarrow\\frac{\\bm{M}\\bm{v}_{t}}{\\|\\bm{M}\\bm{v}_{t}\\|_{2}}, bold_italic_v start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ‚àº caligraphic_N ( bold_0 , bold_1 ) , bold_italic_v start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT ‚Üê divide start_ARG bold_italic_M bold_italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG ‚à• bold_italic_M bold_italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG , (2.1.17)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E18", "title": "ùíó t = ùë¥ ‚Äã ùíó t ‚àí 1 ‚Äñ ùë¥ ‚Äã ùíó t ‚àí 1 ‚Äñ 2 = ùë¥ 2 ‚Äã ùíó t ‚àí 2 ‚Äñ ùë¥ ‚Äã ùíó t ‚àí 1 ‚Äñ 2 ‚Äã ‚Äñ ùë¥ ‚Äã ùíó t ‚àí 2 ‚Äñ 2 = ‚ãØ = ùë¥ t ‚Äã ùíó 0 ‚àè s = 1 t ‚Äñ ùë¥ ‚Äã ùíó s ‚Äñ 2 . \\bm{v}_{t}=\\frac{\\bm{M}\\bm{v}_{t-1}}{\\|\\bm{M}\\bm{v}_{t-1}\\|_{2}}=\\fr", "snippet": "ùíó t = ùë¥ ‚Äã ùíó t ‚àí 1 ‚Äñ ùë¥ ‚Äã ùíó t ‚àí 1 ‚Äñ 2 = ùë¥ 2 ‚Äã ùíó t ‚àí 2 ‚Äñ ùë¥ ‚Äã ùíó t ‚àí 1 ‚Äñ 2 ‚Äã ‚Äñ ùë¥ ‚Äã ùíó t ‚àí 2 ‚Äñ 2 = ‚ãØ = ùë¥ t ‚Äã ùíó 0 ‚àè s = 1 t ‚Äñ ùë¥ ‚Äã ùíó s ‚Äñ 2 . \\bm{v}_{t}=\\frac{\\bm{M}\\bm{v}_{t-1}}{\\|\\bm{M}\\bm{v}_{t-1}\\|_{2}}=\\frac{\\bm{M}^{2}\\bm{v}_{t-2}}{\\|\\bm{M}\\bm{v}_{t-1}\\|_{2}\\|\\bm{M}\\bm{v}_{t-2}\\|_{2}}=\\cdots=\\frac{\\bm{M}^{t}\\bm{v}_{0}}{\\prod_{s=1}^{t}\\|\\bm{M}\\bm{v}_{s}\\|_{2}}. bold_italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = divide start_ARG bold_italic_M bold_italic_v start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT end_ARG start_ARG ‚à• bold_italic_M bold_italic_v start_POSTSUBSCRIPT italic_t - 1 en"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E19", "title": "ùíó t = ùë¥ t ‚Äã ùíó 0 ‚Äñ ùë¥ t ‚Äã ùíó 0 ‚Äñ 2 . \\bm{v}_{t}=\\frac{\\bm{M}^{t}\\bm{v}_{0}}{\\|\\bm{M}^{t}\\bm{v}_{0}\\|_{2}}. bold_italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = divide start_ARG bold_italic_M st", "snippet": "ùíó t = ùë¥ t ‚Äã ùíó 0 ‚Äñ ùë¥ t ‚Äã ùíó 0 ‚Äñ 2 . \\bm{v}_{t}=\\frac{\\bm{M}^{t}\\bm{v}_{0}}{\\|\\bm{M}^{t}\\bm{v}_{0}\\|_{2}}. bold_italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = divide start_ARG bold_italic_M start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT bold_italic_v start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT end_ARG start_ARG ‚à• bold_italic_M start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT bold_italic_v start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG . (2.1.19)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E20", "title": "ùíó 0 = ‚àë i = 1 D Œ± i ‚Äã ùíò i , \\bm{v}_{0}=\\sum_{i=1}^{D}\\alpha_{i}\\bm{w}_{i}, bold_italic_v start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERS", "snippet": "ùíó 0 = ‚àë i = 1 D Œ± i ‚Äã ùíò i , \\bm{v}_{0}=\\sum_{i=1}^{D}\\alpha_{i}\\bm{w}_{i}, bold_italic_v start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT italic_Œ± start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT bold_italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , (2.1.20)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E21", "title": "ùíó t = ùë¥ t ‚Äã ùíó 0 ‚Äñ ùë¥ t ‚Äã ùíó 0 ‚Äñ 2 = ‚àë i = 1 D Œª i t ‚Äã Œ± i ‚Äã ùíò i ‚Äñ ‚àë i = 1 D Œª i t ‚Äã Œ± i ‚Äã ùíò i ‚Äñ 2 = ‚àë i = 1 D Œª i t ‚Äã Œ± i ‚Äã ùíò i ‚àë i = 1 D Œª i t ‚Äã | Œ± i | . \\bm{v}_{t}=\\frac{\\bm{M}^{t}\\bm{v}_{0}}{\\|\\bm{M", "snippet": "ùíó t = ùë¥ t ‚Äã ùíó 0 ‚Äñ ùë¥ t ‚Äã ùíó 0 ‚Äñ 2 = ‚àë i = 1 D Œª i t ‚Äã Œ± i ‚Äã ùíò i ‚Äñ ‚àë i = 1 D Œª i t ‚Äã Œ± i ‚Äã ùíò i ‚Äñ 2 = ‚àë i = 1 D Œª i t ‚Äã Œ± i ‚Äã ùíò i ‚àë i = 1 D Œª i t ‚Äã | Œ± i | . \\bm{v}_{t}=\\frac{\\bm{M}^{t}\\bm{v}_{0}}{\\|\\bm{M}^{t}\\bm{v}_{0}\\|_{2}}=\\frac{\\sum_{i=1}^{D}\\lambda_{i}^{t}\\alpha_{i}\\bm{w}_{i}}{\\|\\sum_{i=1}^{D}\\lambda_{i}^{t}\\alpha_{i}\\bm{w}_{i}\\|_{2}}=\\frac{\\sum_{i=1}^{D}\\lambda_{i}^{t}\\alpha_{i}\\bm{w}_{i}}{\\sum_{i=1}^{D}\\lambda_{i}^{t}\\lvert\\alpha_{i}\\rvert}. bold_italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = divide start_ARG bold_italic_M start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT bold_"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E22", "title": "ùíó t = Œ± 1 ‚Äã ùíò 1 + ‚àë i = 2 D ( Œª i / Œª 1 ) t ‚Äã Œ± i ‚Äã ùíò i | Œ± 1 | + ‚àë i = 2 D ( Œª i / Œª 1 ) t ‚Äã | Œ± i | . \\bm{v}_{t}=\\frac{\\alpha_{1}\\bm{w}_{1}+\\sum_{i=2}^{D}(\\lambda_{i}/\\lambda_{1})^{t}\\alpha_{i}\\bm{w", "snippet": "ùíó t = Œ± 1 ‚Äã ùíò 1 + ‚àë i = 2 D ( Œª i / Œª 1 ) t ‚Äã Œ± i ‚Äã ùíò i | Œ± 1 | + ‚àë i = 2 D ( Œª i / Œª 1 ) t ‚Äã | Œ± i | . \\bm{v}_{t}=\\frac{\\alpha_{1}\\bm{w}_{1}+\\sum_{i=2}^{D}(\\lambda_{i}/\\lambda_{1})^{t}\\alpha_{i}\\bm{w}_{i}}{\\lvert\\alpha_{1}\\rvert+\\sum_{i=2}^{D}(\\lambda_{i}/\\lambda_{1})^{t}\\lvert\\alpha_{i}\\rvert}. bold_italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = divide start_ARG italic_Œ± start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + ‚àë start_POSTSUBSCRIPT italic_i = 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT ( italic_Œª st"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E23", "title": "lim t ‚Üí ‚àû ùíó t = Œ± 1 | Œ± 1 | ‚Äã ùíò 1 = sign ‚Å° ( Œ± 1 ) ‚Äã ùíò 1 , \\lim_{t\\to\\infty}\\bm{v}_{t}=\\frac{\\alpha_{1}}{\\lvert\\alpha_{1}\\rvert}\\bm{w}_{1}=\\operatorname{sign}(\\alpha_{1})\\bm{w}_{1}, roman_lim start_PO", "snippet": "lim t ‚Üí ‚àû ùíó t = Œ± 1 | Œ± 1 | ‚Äã ùíò 1 = sign ‚Å° ( Œ± 1 ) ‚Äã ùíò 1 , \\lim_{t\\to\\infty}\\bm{v}_{t}=\\frac{\\alpha_{1}}{\\lvert\\alpha_{1}\\rvert}\\bm{w}_{1}=\\operatorname{sign}(\\alpha_{1})\\bm{w}_{1}, roman_lim start_POSTSUBSCRIPT italic_t ‚Üí ‚àû end_POSTSUBSCRIPT bold_italic_v start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = divide start_ARG italic_Œ± start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG start_ARG | italic_Œ± start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT | end_ARG bold_italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = roman_sign ( italic_Œ± start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) bold_italic_w start_POSTSUBSC"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E24", "title": "ùíô = ùëº ‚Äã ùíõ + ùú∫ . \\bm{x}=\\bm{U}\\bm{z}+\\bm{\\varepsilon}. bold_italic_x = bold_italic_U bold_italic_z + bold_italic_Œµ . (2.1.24)", "snippet": "ùíô = ùëº ‚Äã ùíõ + ùú∫ . \\bm{x}=\\bm{U}\\bm{z}+\\bm{\\varepsilon}. bold_italic_x = bold_italic_U bold_italic_z + bold_italic_Œµ . (2.1.24)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E25", "title": "min ùëº ~ , ùíõ ~ ‚Å° ùîº ‚Å° ‚Äñ ùíô ‚àí ùëº ~ ‚Äã ùíõ ~ ‚Äñ 2 2 . \\min_{\\tilde{\\bm{U}},\\tilde{\\bm{z}}}\\operatorname{\\mathbb{E}}\\|\\bm{x}-\\tilde{\\bm{U}}\\tilde{\\bm{z}}\\|_{2}^{2}. roman_min start_POSTSUBSCRIPT over~ start_ARG ", "snippet": "min ùëº ~ , ùíõ ~ ‚Å° ùîº ‚Å° ‚Äñ ùíô ‚àí ùëº ~ ‚Äã ùíõ ~ ‚Äñ 2 2 . \\min_{\\tilde{\\bm{U}},\\tilde{\\bm{z}}}\\operatorname{\\mathbb{E}}\\|\\bm{x}-\\tilde{\\bm{U}}\\tilde{\\bm{z}}\\|_{2}^{2}. roman_min start_POSTSUBSCRIPT over~ start_ARG bold_italic_U end_ARG , over~ start_ARG bold_italic_z end_ARG end_POSTSUBSCRIPT blackboard_E ‚à• bold_italic_x - over~ start_ARG bold_italic_U end_ARG over~ start_ARG bold_italic_z end_ARG ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . (2.1.25)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E26", "title": "min ùëº ~ , ùíõ ~ ‚Å° ùîº ‚Å° ‚Äñ ùíô ‚àí ùëº ~ ‚Äã ùíõ ~ ‚Äñ 2 2 = min ùëº ~ ‚Å° ùîº ‚Å° min ùíõ ~ ‚Äã ( ùíô ) ‚Å° ‚Äñ ùíô ‚àí ùëº ~ ‚Äã ùíõ ~ ‚Äã ( ùíô ) ‚Äñ 2 2 = min ùëº ~ ‚Å° ùîº ‚Å° ‚Äñ ùíô ‚àí ùëº ~ ‚Äã ùëº ~ ‚ä§ ‚Äã ùíô ‚Äñ 2 2 , \\min_{\\tilde{\\bm{U}},\\tilde{\\bm{z}}}\\operatornam", "snippet": "min ùëº ~ , ùíõ ~ ‚Å° ùîº ‚Å° ‚Äñ ùíô ‚àí ùëº ~ ‚Äã ùíõ ~ ‚Äñ 2 2 = min ùëº ~ ‚Å° ùîº ‚Å° min ùíõ ~ ‚Äã ( ùíô ) ‚Å° ‚Äñ ùíô ‚àí ùëº ~ ‚Äã ùíõ ~ ‚Äã ( ùíô ) ‚Äñ 2 2 = min ùëº ~ ‚Å° ùîº ‚Å° ‚Äñ ùíô ‚àí ùëº ~ ‚Äã ùëº ~ ‚ä§ ‚Äã ùíô ‚Äñ 2 2 , \\min_{\\tilde{\\bm{U}},\\tilde{\\bm{z}}}\\operatorname{\\mathbb{E}}\\|\\bm{x}-\\tilde{\\bm{U}}\\tilde{\\bm{z}}\\|_{2}^{2}=\\min_{\\tilde{\\bm{U}}}\\operatorname{\\mathbb{E}}\\min_{\\tilde{\\bm{z}}(\\bm{x})}\\|\\bm{x}-\\tilde{\\bm{U}}\\tilde{\\bm{z}}(\\bm{x})\\|_{2}^{2}=\\min_{\\tilde{\\bm{U}}}\\operatorname{\\mathbb{E}}\\|\\bm{x}-\\tilde{\\bm{U}}\\tilde{\\bm{U}}^{\\top}\\bm{x}\\|_{2}^{2}, roman_min start_POSTSUBSCRIPT over~ start_ARG bold_italic_U end_ARG , over~ start_ARG bold_italic_z "}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E29", "title": "ùîº ‚Å° [ ùíô ] = ùëº ‚Äã ùîº ‚Å° [ ùíõ ] + ùîº ‚Å° [ ùú∫ ] = ùüé , \\operatorname{\\mathbb{E}}[\\bm{x}]=\\bm{U}\\operatorname{\\mathbb{E}}[\\bm{z}]+\\operatorname{\\mathbb{E}}[\\bm{\\varepsilon}]=\\bm{0}, blackboard_E [ bold_italic_x ]", "snippet": "ùîº ‚Å° [ ùíô ] = ùëº ‚Äã ùîº ‚Å° [ ùíõ ] + ùîº ‚Å° [ ùú∫ ] = ùüé , \\operatorname{\\mathbb{E}}[\\bm{x}]=\\bm{U}\\operatorname{\\mathbb{E}}[\\bm{z}]+\\operatorname{\\mathbb{E}}[\\bm{\\varepsilon}]=\\bm{0}, blackboard_E [ bold_italic_x ] = bold_italic_U blackboard_E [ bold_italic_z ] + blackboard_E [ bold_italic_Œµ ] = bold_0 , (2.1.29)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E30", "title": "Cov ‚Å° [ ùíô ] = ùëº ‚Äã Cov ‚Å° [ ùíõ ] ‚Äã ùëº ‚ä§ + Cov ‚Å° [ ùú∫ ] = ùëº ‚Äã ùîº ‚Å° [ ùíõ ‚Äã ùíõ ‚ä§ ] ‚Äã ùëº ‚ä§ + Cov ‚Å° [ ùú∫ ] . \\operatorname{Cov}[\\bm{x}]=\\bm{U}\\operatorname{Cov}[\\bm{z}]\\bm{U}^{\\top}+\\operatorname{Cov}[\\bm{\\varepsilo", "snippet": "Cov ‚Å° [ ùíô ] = ùëº ‚Äã Cov ‚Å° [ ùíõ ] ‚Äã ùëº ‚ä§ + Cov ‚Å° [ ùú∫ ] = ùëº ‚Äã ùîº ‚Å° [ ùíõ ‚Äã ùíõ ‚ä§ ] ‚Äã ùëº ‚ä§ + Cov ‚Å° [ ùú∫ ] . \\operatorname{Cov}[\\bm{x}]=\\bm{U}\\operatorname{Cov}[\\bm{z}]\\bm{U}^{\\top}+\\operatorname{Cov}[\\bm{\\varepsilon}]=\\bm{U}\\operatorname{\\mathbb{E}}[\\bm{z}\\bm{z}^{\\top}]\\bm{U}^{\\top}+\\operatorname{Cov}[\\bm{\\varepsilon}]. roman_Cov [ bold_italic_x ] = bold_italic_U roman_Cov [ bold_italic_z ] bold_italic_U start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT + roman_Cov [ bold_italic_Œµ ] = bold_italic_U blackboard_E [ bold_italic_z bold_italic_z start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ] bold_italic_U start_POSTSUP"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E31", "title": "min ùëº ~ ‚Å° ùîº ‚Å° ‚Äñ ùíô ‚àí ùëº ~ ‚Äã ùëº ~ ‚ä§ ‚Äã ùíô ‚Äñ 2 2 . \\min_{\\tilde{\\bm{U}}}\\operatorname{\\mathbb{E}}\\|\\bm{x}-\\tilde{\\bm{U}}\\tilde{\\bm{U}}^{\\top}\\bm{x}\\|_{2}^{2}. roman_min start_POSTSUBSCRIPT over~ start_ARG bo", "snippet": "min ùëº ~ ‚Å° ùîº ‚Å° ‚Äñ ùíô ‚àí ùëº ~ ‚Äã ùëº ~ ‚ä§ ‚Äã ùíô ‚Äñ 2 2 . \\min_{\\tilde{\\bm{U}}}\\operatorname{\\mathbb{E}}\\|\\bm{x}-\\tilde{\\bm{U}}\\tilde{\\bm{U}}^{\\top}\\bm{x}\\|_{2}^{2}. roman_min start_POSTSUBSCRIPT over~ start_ARG bold_italic_U end_ARG end_POSTSUBSCRIPT blackboard_E ‚à• bold_italic_x - over~ start_ARG bold_italic_U end_ARG over~ start_ARG bold_italic_U end_ARG start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_x ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . (2.1.31)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E32", "title": "ùëº ‚ãÜ ‚Äã ( ùëº ‚ãÜ ) ‚ä§ = ùëº ‚Äã ùëº ‚ä§ \\bm{U}^{\\star}(\\bm{U}^{\\star})^{\\top}=\\bm{U}\\bm{U}^{\\top} bold_italic_U start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ( bold_italic_U start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIP", "snippet": "ùëº ‚ãÜ ‚Äã ( ùëº ‚ãÜ ) ‚ä§ = ùëº ‚Äã ùëº ‚ä§ \\bm{U}^{\\star}(\\bm{U}^{\\star})^{\\top}=\\bm{U}\\bm{U}^{\\top} bold_italic_U start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ( bold_italic_U start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT = bold_italic_U bold_italic_U start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT (2.1.32)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E33", "title": "ùíÆ = col ( ùëº ) = col ( ùëº ‚Äã ùëº ‚ä§ ) = col ( ùëº ‚ãÜ ‚Äã ( ùëº ‚ãÜ ) ‚ä§ ) = col ( ùëº ‚ãÜ ) . \\mathcal{S}=\\mathop{\\mathrm{col}}(\\bm{U})=\\mathop{\\mathrm{col}}(\\bm{U}\\bm{U}^{\\top})=\\mathop{\\mathrm{col}}(\\bm{U}^{\\star}(\\bm{", "snippet": "ùíÆ = col ( ùëº ) = col ( ùëº ‚Äã ùëº ‚ä§ ) = col ( ùëº ‚ãÜ ‚Äã ( ùëº ‚ãÜ ) ‚ä§ ) = col ( ùëº ‚ãÜ ) . \\mathcal{S}=\\mathop{\\mathrm{col}}(\\bm{U})=\\mathop{\\mathrm{col}}(\\bm{U}\\bm{U}^{\\top})=\\mathop{\\mathrm{col}}(\\bm{U}^{\\star}(\\bm{U}^{\\star})^{\\top})=\\mathop{\\mathrm{col}}(\\bm{U}^{\\star}). caligraphic_S = roman_col ( bold_italic_U ) = roman_col ( bold_italic_U bold_italic_U start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ) = roman_col ( bold_italic_U start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ( bold_italic_U start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ) = roman_col ( bold_italic_U s"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E34", "title": "ùíô = ùëº ‚Äã ùíõ + ùú∫ \\bm{x}=\\bm{U}\\bm{z}+\\bm{\\varepsilon} bold_italic_x = bold_italic_U bold_italic_z + bold_italic_Œµ (2.1.34)", "snippet": "ùíô = ùëº ‚Äã ùíõ + ùú∫ \\bm{x}=\\bm{U}\\bm{z}+\\bm{\\varepsilon} bold_italic_x = bold_italic_U bold_italic_z + bold_italic_Œµ (2.1.34)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E35", "title": "ùíÄ = ùë¥ ‚äô ùëø , \\bm{Y}=\\bm{M}\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\scalebox{0.8}{$\\scriptstyle\\odot$}}{\\", "snippet": "ùíÄ = ùë¥ ‚äô ùëø , \\bm{Y}=\\bm{M}\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\scalebox{0.8}{$\\scriptstyle\\odot$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\odot$}}$}}{\\raisebox{1.3pt}{$\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\scalebox{0.8}{$\\scriptstyle\\odot$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\odot$}}$}}{\\raisebox{0.75pt}{$\\scriptstyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\scalebox{0.8}{$\\scriptstyle\\odot$}}{\\scalebox{0.8}{$"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S1.E36", "title": "ùë¥ = [ ùüè ( D ‚àí 1 ) √ó 1 ùüé ( D ‚àí 1 ) √ó ( N ‚àí 1 ) 1 ùüè 1 √ó ( N ‚àí 1 ) ] . \\bm{M}=\\begin{bmatrix}\\bm{1}_{(D-1)\\times 1}&\\bm{0}_{(D-1)\\times(N-1)}\\\\ 1&\\bm{1}_{1\\times(N-1)}\\end{bmatrix}. bold_italic_M = [ sta", "snippet": "ùë¥ = [ ùüè ( D ‚àí 1 ) √ó 1 ùüé ( D ‚àí 1 ) √ó ( N ‚àí 1 ) 1 ùüè 1 √ó ( N ‚àí 1 ) ] . \\bm{M}=\\begin{bmatrix}\\bm{1}_{(D-1)\\times 1}&\\bm{0}_{(D-1)\\times(N-1)}\\\\ 1&\\bm{1}_{1\\times(N-1)}\\end{bmatrix}. bold_italic_M = [ start_ARG start_ROW start_CELL bold_1 start_POSTSUBSCRIPT ( italic_D - 1 ) √ó 1 end_POSTSUBSCRIPT end_CELL start_CELL bold_0 start_POSTSUBSCRIPT ( italic_D - 1 ) √ó ( italic_N - 1 ) end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL 1 end_CELL start_CELL bold_1 start_POSTSUBSCRIPT 1 √ó ( italic_N - 1 ) end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] . (2.1.36)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.Ex1", "title": "ùíõ = ‚Ñ∞ ‚Äã ( ùíô ) = ùëº ‚ä§ ‚Äã ùíô , ùíô ^ = ùíü ‚Äã ( ùíõ ) = ùëº ‚Äã ùíõ , \\bm{z}=\\mathcal{E}(\\bm{x})=\\bm{U}^{\\top}\\bm{x},\\quad\\hat{\\bm{x}}=\\mathcal{D}(\\bm{z})=\\bm{U}\\bm{z}, bold_italic_z = caligraphic_E ( bold_italic_x ) =", "snippet": "ùíõ = ‚Ñ∞ ‚Äã ( ùíô ) = ùëº ‚ä§ ‚Äã ùíô , ùíô ^ = ùíü ‚Äã ( ùíõ ) = ùëº ‚Äã ùíõ , \\bm{z}=\\mathcal{E}(\\bm{x})=\\bm{U}^{\\top}\\bm{x},\\quad\\hat{\\bm{x}}=\\mathcal{D}(\\bm{z})=\\bm{U}\\bm{z}, bold_italic_z = caligraphic_E ( bold_italic_x ) = bold_italic_U start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_x , over^ start_ARG bold_italic_x end_ARG = caligraphic_D ( bold_italic_z ) = bold_italic_U bold_italic_z ,"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E1", "title": "ùíô ^ = ùíü ‚Äã ( ùíõ ) = ( ‚àë k = 1 K œÄ k ‚Äã ( ùíõ ) ‚Äã ùëº k ) ‚Äã ùíõ , \\hat{\\bm{x}}=\\mathcal{D}(\\bm{z})=\\left(\\sum_{k=1}^{K}\\pi_{k}(\\bm{z})\\bm{U}_{k}\\right)\\bm{z}, over^ start_ARG bold_italic_x end_ARG = caligraphic", "snippet": "ùíô ^ = ùíü ‚Äã ( ùíõ ) = ( ‚àë k = 1 K œÄ k ‚Äã ( ùíõ ) ‚Äã ùëº k ) ‚Äã ùíõ , \\hat{\\bm{x}}=\\mathcal{D}(\\bm{z})=\\left(\\sum_{k=1}^{K}\\pi_{k}(\\bm{z})\\bm{U}_{k}\\right)\\bm{z}, over^ start_ARG bold_italic_x end_ARG = caligraphic_D ( bold_italic_z ) = ( ‚àë start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_œÄ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_italic_z ) bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) bold_italic_z , (2.2.1)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E2", "title": "ùíô = ùëº k ‚Äã ùíõ for some ‚Äã k ‚àà [ K ] , ùíõ ‚àà ‚Ñù d . \\bm{x}=\\bm{U}_{k}\\bm{z}\\quad\\text{for some}\\enspace k\\in[K],\\enspace\\bm{z}\\in\\mathbb{R}^{d}. bold_italic_x = bold_italic_U start_POSTSUBSCRIPT italic_k end", "snippet": "ùíô = ùëº k ‚Äã ùíõ for some ‚Äã k ‚àà [ K ] , ùíõ ‚àà ‚Ñù d . \\bm{x}=\\bm{U}_{k}\\bm{z}\\quad\\text{for some}\\enspace k\\in[K],\\enspace\\bm{z}\\in\\mathbb{R}^{d}. bold_italic_x = bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_z for some italic_k ‚àà [ italic_K ] , bold_italic_z ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT . (2.2.2)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E3", "title": "ùíô ‚àº ‚àë k = 1 K œÄ k ‚Äã ùí© ‚Äã ( ùüé , ùëº k ‚Äã ùëº k ‚ä§ ) , for some ‚Äã œÄ k ‚â• 0 , ‚àë k = 1 K œÄ k = 1 . \\bm{x}\\sim\\sum_{k=1}^{K}\\pi_{k}\\mathcal{N}(\\mathbf{0},\\bm{U}_{k}\\bm{U}_{k}^{\\top}),\\quad\\text{for some}\\enspace\\p", "snippet": "ùíô ‚àº ‚àë k = 1 K œÄ k ‚Äã ùí© ‚Äã ( ùüé , ùëº k ‚Äã ùëº k ‚ä§ ) , for some ‚Äã œÄ k ‚â• 0 , ‚àë k = 1 K œÄ k = 1 . \\bm{x}\\sim\\sum_{k=1}^{K}\\pi_{k}\\mathcal{N}(\\mathbf{0},\\bm{U}_{k}\\bm{U}_{k}^{\\top}),\\quad\\text{for some}\\enspace\\pi_{k}\\geq 0,\\enspace\\sum_{k=1}^{K}\\pi_{k}=1. bold_italic_x ‚àº ‚àë start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_œÄ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT caligraphic_N ( bold_0 , bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚ä§ end_POST"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E4", "title": "ùíô = ‚àë i = 1 n w i ‚Äã ùíô i , ùíô i ‚àº ùí© ‚Äã ( ùüé , ùëº i ‚Äã ùëº i ‚ä§ ) , \\bm{x}=\\sum_{i=1}^{n}w_{i}\\bm{x}_{i},\\quad\\bm{x}_{i}\\sim\\mathcal{N}(\\mathbf{0},\\bm{U}_{i}\\bm{U}_{i}^{\\top}), bold_italic_x = ‚àë start_POSTSUBSC", "snippet": "ùíô = ‚àë i = 1 n w i ‚Äã ùíô i , ùíô i ‚àº ùí© ‚Äã ( ùüé , ùëº i ‚Äã ùëº i ‚ä§ ) , \\bm{x}=\\sum_{i=1}^{n}w_{i}\\bm{x}_{i},\\quad\\bm{x}_{i}\\sim\\mathcal{N}(\\mathbf{0},\\bm{U}_{i}\\bm{U}_{i}^{\\top}), bold_italic_x = ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ‚àº caligraphic_N ( bold_0 , bold_italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_i en"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E5", "title": "ùíô = [ | ‚Ä¶ | ùëº 1 ‚Ä¶ ùëº K | ‚Ä¶ | ] ‚èü ùëº ‚Äã [ ùíõ 1 ‚ãÆ ùíõ K ] ‚èü ùíõ , ‚Äñ [ ‚Äñ ùíõ 1 ‚Äñ 2 ‚ãÆ ‚Äñ ùíõ K ‚Äñ 2 ] ‚Äñ 0 = 1 . \\bm{x}=\\underbrace{\\begin{bmatrix}|&\\ldots&|\\\\ \\bm{U}_{1}&\\ldots&\\bm{U}_{K}\\\\ |&\\ldots&|\\end{bmatrix}}_{\\b", "snippet": "ùíô = [ | ‚Ä¶ | ùëº 1 ‚Ä¶ ùëº K | ‚Ä¶ | ] ‚èü ùëº ‚Äã [ ùíõ 1 ‚ãÆ ùíõ K ] ‚èü ùíõ , ‚Äñ [ ‚Äñ ùíõ 1 ‚Äñ 2 ‚ãÆ ‚Äñ ùíõ K ‚Äñ 2 ] ‚Äñ 0 = 1 . \\bm{x}=\\underbrace{\\begin{bmatrix}|&\\ldots&|\\\\ \\bm{U}_{1}&\\ldots&\\bm{U}_{K}\\\\ |&\\ldots&|\\end{bmatrix}}_{\\bm{U}}\\underbrace{\\begin{bmatrix}\\bm{z}_{1}\\\\ \\vdots\\\\ \\bm{z}_{K}\\end{bmatrix}}_{\\bm{z}},\\quad\\left\\|\\begin{bmatrix}\\left\\|\\bm{z}_{1}\\right\\|_{2}\\\\ \\vdots\\\\ \\left\\|\\bm{z}_{K}\\right\\|_{2}\\end{bmatrix}\\right\\|_{0}=1. bold_italic_x = under‚èü start_ARG [ start_ARG start_ROW start_CELL | end_CELL start_CELL ‚Ä¶ end_CELL start_CELL | end_CELL end_ROW start_ROW start_CELL bold_italic_U start_POSTSUBSCRIPT 1 "}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E6", "title": "‚Äñ ùíõ ‚Äñ 0 = | { i ‚à£ z i ‚â† 0 } | , \\|\\bm{z}\\|_{0}=\\left\\lvert\\{i\\mid z_{i}\\neq 0\\}\\right\\rvert, ‚à• bold_italic_z ‚à• start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = | { italic_i ‚à£ italic_z start_POSTSUBSCRIPT ita", "snippet": "‚Äñ ùíõ ‚Äñ 0 = | { i ‚à£ z i ‚â† 0 } | , \\|\\bm{z}\\|_{0}=\\left\\lvert\\{i\\mid z_{i}\\neq 0\\}\\right\\rvert, ‚à• bold_italic_z ‚à• start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = | { italic_i ‚à£ italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ‚â† 0 } | , (2.2.6)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E7", "title": "ùíô = ùëº ‚Äã ùíõ + ùú∫ , ‚Äñ ùíõ ‚Äñ 0 ‚â™ d , \\bm{x}=\\bm{U}\\bm{z}+\\bm{\\varepsilon},\\quad\\|\\bm{z}\\|_{0}\\ll d, bold_italic_x = bold_italic_U bold_italic_z + bold_italic_Œµ , ‚à• bold_italic_z ‚à• start_POSTSUBSCRIPT 0 end_P", "snippet": "ùíô = ùëº ‚Äã ùíõ + ùú∫ , ‚Äñ ùíõ ‚Äñ 0 ‚â™ d , \\bm{x}=\\bm{U}\\bm{z}+\\bm{\\varepsilon},\\quad\\|\\bm{z}\\|_{0}\\ll d, bold_italic_x = bold_italic_U bold_italic_z + bold_italic_Œµ , ‚à• bold_italic_z ‚à• start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ‚â™ italic_d , (2.2.7)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E8", "title": "ùíô ‚Üí ‚Ñ∞ = ùëº ‚ä§ ùíõ ‚Üí ùíü = ùëº ùíô ^ . \\bm{x}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}=\\bm{U}^{\\top}\\hskip 5.69054pt}\\bm{z}\\xrightarrow{\\hskip 5.69054pt\\mathcal{D}=\\bm{U}\\hskip 5.69054pt}\\hat{\\bm{x}}. bold_italic", "snippet": "ùíô ‚Üí ‚Ñ∞ = ùëº ‚ä§ ùíõ ‚Üí ùíü = ùëº ùíô ^ . \\bm{x}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}=\\bm{U}^{\\top}\\hskip 5.69054pt}\\bm{z}\\xrightarrow{\\hskip 5.69054pt\\mathcal{D}=\\bm{U}\\hskip 5.69054pt}\\hat{\\bm{x}}. bold_italic_x start_ARROW start_OVERACCENT caligraphic_E = bold_italic_U start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT end_OVERACCENT ‚Üí end_ARROW bold_italic_z start_ARROW start_OVERACCENT caligraphic_D = bold_italic_U end_OVERACCENT ‚Üí end_ARROW over^ start_ARG bold_italic_x end_ARG . (2.2.8)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E9", "title": "ùíô = ùëº ‚Äã ùíõ + ùú∫ , \\bm{x}=\\bm{U}\\bm{z}+\\bm{\\varepsilon}, bold_italic_x = bold_italic_U bold_italic_z + bold_italic_Œµ , (2.2.9)", "snippet": "ùíô = ùëº ‚Äã ùíõ + ùú∫ , \\bm{x}=\\bm{U}\\bm{z}+\\bm{\\varepsilon}, bold_italic_x = bold_italic_U bold_italic_z + bold_italic_Œµ , (2.2.9)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.Ex2", "title": "z i ‚àº Bern ‚Äã ( Œ∏ ) ‚ãÖ ùí© ‚Äã ( 0 , 1 / Œ∏ ) . z_{i}\\sim\\mathrm{Bern}(\\theta)\\cdot\\mathcal{N}(0,1/\\theta). italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ‚àº roman_Bern ( italic_Œ∏ ) ‚ãÖ caligraphic_N (", "snippet": "z i ‚àº Bern ‚Äã ( Œ∏ ) ‚ãÖ ùí© ‚Äã ( 0 , 1 / Œ∏ ) . z_{i}\\sim\\mathrm{Bern}(\\theta)\\cdot\\mathcal{N}(0,1/\\theta). italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ‚àº roman_Bern ( italic_Œ∏ ) ‚ãÖ caligraphic_N ( 0 , 1 / italic_Œ∏ ) ."}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E10", "title": "ùëø ¬Ø = ( 1 N ‚Äã Œ∏ ‚Äã ùëø ‚Äã ùëø ‚ä§ ) ‚àí 1 2 ‚Äã ùëø , \\bar{\\bm{X}}=\\Big{(}\\frac{1}{N\\theta}\\bm{X}\\bm{X}^{\\top}\\Big{)}^{-\\frac{1}{2}}\\bm{X}, over¬Ø start_ARG bold_italic_X end_ARG = ( divide start_ARG 1 end_ARG start", "snippet": "ùëø ¬Ø = ( 1 N ‚Äã Œ∏ ‚Äã ùëø ‚Äã ùëø ‚ä§ ) ‚àí 1 2 ‚Äã ùëø , \\bar{\\bm{X}}=\\Big{(}\\frac{1}{N\\theta}\\bm{X}\\bm{X}^{\\top}\\Big{)}^{-\\frac{1}{2}}\\bm{X}, over¬Ø start_ARG bold_italic_X end_ARG = ( divide start_ARG 1 end_ARG start_ARG italic_N italic_Œ∏ end_ARG bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT - divide start_ARG 1 end_ARG start_ARG 2 end_ARG end_POSTSUPERSCRIPT bold_italic_X , (2.2.10)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E11", "title": "ùëø ¬Ø = ùëº o ‚Äã ùíÅ . \\bar{\\bm{X}}=\\bm{U}_{o}\\bm{Z}. over¬Ø start_ARG bold_italic_X end_ARG = bold_italic_U start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT bold_italic_Z . (2.2.11)", "snippet": "ùëø ¬Ø = ùëº o ‚Äã ùíÅ . \\bar{\\bm{X}}=\\bm{U}_{o}\\bm{Z}. over¬Ø start_ARG bold_italic_X end_ARG = bold_italic_U start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT bold_italic_Z . (2.2.11)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E12", "title": "ùíô i = ùëº ‚Äã ùíõ i + ùú∫ i , ‚àÄ i ‚àà [ N ] . \\bm{x}_{i}=\\bm{U}\\bm{z}_{i}+\\bm{\\varepsilon}_{i},\\ \\forall i\\in[N]. bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_U bold_italic_z start", "snippet": "ùíô i = ùëº ‚Äã ùíõ i + ùú∫ i , ‚àÄ i ‚àà [ N ] . \\bm{x}_{i}=\\bm{U}\\bm{z}_{i}+\\bm{\\varepsilon}_{i},\\ \\forall i\\in[N]. bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_U bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + bold_italic_Œµ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , ‚àÄ italic_i ‚àà [ italic_N ] . (2.2.12)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E13", "title": "ùë® ‚Äã ùíô i = ùë® ‚Äã ùëº ‚Äã ùíõ i + ùë® ‚Äã ùú∫ i \\bm{A}\\bm{x}_{i}=\\bm{A}\\bm{U}\\bm{z}_{i}+\\bm{A}\\bm{\\varepsilon}_{i} bold_italic_A bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_A bold_itali", "snippet": "ùë® ‚Äã ùíô i = ùë® ‚Äã ùëº ‚Äã ùíõ i + ùë® ‚Äã ùú∫ i \\bm{A}\\bm{x}_{i}=\\bm{A}\\bm{U}\\bm{z}_{i}+\\bm{A}\\bm{\\varepsilon}_{i} bold_italic_A bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_A bold_italic_U bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + bold_italic_A bold_italic_Œµ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT (2.2.13)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E14", "title": "arg ‚Äã max ùíõ ‚àà ùïä n ‚Å° ‚Äñ ùíõ ‚Äñ 4 ‚áî arg ‚Äã min ùíõ ‚àà ùïä n ‚Å° ‚Äñ ùíõ ‚Äñ 0 . \\operatorname*{arg\\ max}_{\\bm{z}\\in\\mathbb{S}^{n}}\\|\\bm{z}\\|_{4}\\quad\\Leftrightarrow\\quad\\operatorname*{arg\\ min}_{\\bm{z}\\in\\mathbb{S}^{n}}\\", "snippet": "arg ‚Äã max ùíõ ‚àà ùïä n ‚Å° ‚Äñ ùíõ ‚Äñ 4 ‚áî arg ‚Äã min ùíõ ‚àà ùïä n ‚Å° ‚Äñ ùíõ ‚Äñ 0 . \\operatorname*{arg\\ max}_{\\bm{z}\\in\\mathbb{S}^{n}}\\|\\bm{z}\\|_{4}\\quad\\Leftrightarrow\\quad\\operatorname*{arg\\ min}_{\\bm{z}\\in\\mathbb{S}^{n}}\\|\\bm{z}\\|_{0}. start_OPERATOR roman_arg roman_max end_OPERATOR start_POSTSUBSCRIPT bold_italic_z ‚àà blackboard_S start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ‚à• bold_italic_z ‚à• start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT ‚áî start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT bold_italic_z ‚àà blackboard_S start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT end_POSTSUB"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E15", "title": "max ùë® ~ ‚àà ùñÆ ‚Äã ( D ) ‚Å° 1 4 ‚Äã ‚Äñ ùë® ~ ‚Äã ùëø ‚Äñ 4 4 = 1 4 ‚Äã ‚àë i = 1 N ‚Äñ ùë® ~ ‚Äã ùíô i ‚Äñ 4 4 \\max_{\\tilde{\\bm{A}}\\in\\mathsf{O}(D)}\\,\\frac{1}{4}\\left\\|\\tilde{\\bm{A}}\\bm{X}\\right\\|_{4}^{4}=\\frac{1}{4}\\sum_{i=1}^{N}\\", "snippet": "max ùë® ~ ‚àà ùñÆ ‚Äã ( D ) ‚Å° 1 4 ‚Äã ‚Äñ ùë® ~ ‚Äã ùëø ‚Äñ 4 4 = 1 4 ‚Äã ‚àë i = 1 N ‚Äñ ùë® ~ ‚Äã ùíô i ‚Äñ 4 4 \\max_{\\tilde{\\bm{A}}\\in\\mathsf{O}(D)}\\,\\frac{1}{4}\\left\\|\\tilde{\\bm{A}}\\bm{X}\\right\\|_{4}^{4}=\\frac{1}{4}\\sum_{i=1}^{N}\\left\\|\\tilde{\\bm{A}}\\bm{x}_{i}\\right\\|_{4}^{4} roman_max start_POSTSUBSCRIPT over~ start_ARG bold_italic_A end_ARG ‚àà sansserif_O ( italic_D ) end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG 4 end_ARG ‚à• over~ start_ARG bold_italic_A end_ARG bold_italic_X ‚à• start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT = divide start_ARG 1 end_ARG start_ARG 4 end_ARG ‚àë st"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.Ex3", "title": "arg ‚Äã min ùíõ ‚àà ùïä n ‚Å° ‚Äñ ùíõ ‚Äñ 1 ‚áî arg ‚Äã min ùíõ ‚àà ùïä n ‚Å° ‚Äñ ùíõ ‚Äñ 0 , \\operatorname*{arg\\ min}_{\\bm{z}\\in\\mathbb{S}^{n}}\\|\\bm{z}\\|_{1}\\quad\\Leftrightarrow\\quad\\operatorname*{arg\\ min}_{\\bm{z}\\in\\mathbb{S}^{n}}\\", "snippet": "arg ‚Äã min ùíõ ‚àà ùïä n ‚Å° ‚Äñ ùíõ ‚Äñ 1 ‚áî arg ‚Äã min ùíõ ‚àà ùïä n ‚Å° ‚Äñ ùíõ ‚Äñ 0 , \\operatorname*{arg\\ min}_{\\bm{z}\\in\\mathbb{S}^{n}}\\|\\bm{z}\\|_{1}\\quad\\Leftrightarrow\\quad\\operatorname*{arg\\ min}_{\\bm{z}\\in\\mathbb{S}^{n}}\\|\\bm{z}\\|_{0}, start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT bold_italic_z ‚àà blackboard_S start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ‚à• bold_italic_z ‚à• start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ‚áî start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT bold_italic_z ‚àà blackboard_S start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT end_POSTSUB"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E16", "title": "min ‚àí 1 4 ‚Äã ‚Äñ ùë® ~ ‚Äã ùëø ‚Äñ 4 4 subject to ùë® ~ ‚ä§ ‚Äã ùë® ~ = ùë∞ . \\min\\,-\\frac{1}{4}\\left\\|\\tilde{\\bm{A}}\\bm{X}\\right\\|_{4}^{4}\\quad\\mbox{subject to}\\quad\\tilde{\\bm{A}}^{\\top}\\tilde{\\bm{A}}=\\bm{I}. roman_min -", "snippet": "min ‚àí 1 4 ‚Äã ‚Äñ ùë® ~ ‚Äã ùëø ‚Äñ 4 4 subject to ùë® ~ ‚ä§ ‚Äã ùë® ~ = ùë∞ . \\min\\,-\\frac{1}{4}\\left\\|\\tilde{\\bm{A}}\\bm{X}\\right\\|_{4}^{4}\\quad\\mbox{subject to}\\quad\\tilde{\\bm{A}}^{\\top}\\tilde{\\bm{A}}=\\bm{I}. roman_min - divide start_ARG 1 end_ARG start_ARG 4 end_ARG ‚à• over~ start_ARG bold_italic_A end_ARG bold_italic_X ‚à• start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT subject to over~ start_ARG bold_italic_A end_ARG start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT over~ start_ARG bold_italic_A end_ARG = bold_italic_I . (2.2.16)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E17", "title": "ùë® ‚ãÜ = ùí´ O ‚Äã ( D ) ‚Äã [ ( ùë® ‚ãÜ ‚Äã ùëø ) ‚äô 3 ‚Äã ùëø ‚ä§ ] , \\bm{A}^{\\star}=\\mathcal{P}_{\\mathrm{O}(D)}[({\\bm{A}^{\\star}\\bm{X}})^{\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\di", "snippet": "ùë® ‚ãÜ = ùí´ O ‚Äã ( D ) ‚Äã [ ( ùë® ‚ãÜ ‚Äã ùëø ) ‚äô 3 ‚Äã ùëø ‚ä§ ] , \\bm{A}^{\\star}=\\mathcal{P}_{\\mathrm{O}(D)}[({\\bm{A}^{\\star}\\bm{X}})^{\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\scalebox{0.8}{$\\scriptstyle\\odot$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\odot$}}$}}{\\raisebox{1.3pt}{$\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\scalebox{0.8}{$\\scriptstyle\\odot$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\odot$}}$}}{\\raisebox{0.75pt}{$\\scriptstyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E18", "title": "ùë® t + 1 = ùí´ O ‚Äã ( D ) ‚Äã [ ( ùë® t ‚Äã ùëø ) ‚äô 3 ‚Äã ùëø ‚ä§ ] . \\bm{A}_{t+1}=\\mathcal{P}_{\\mathrm{O}(D)}[({\\bm{A}_{t}\\bm{X}})^{\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\disp", "snippet": "ùë® t + 1 = ùí´ O ‚Äã ( D ) ‚Äã [ ( ùë® t ‚Äã ùëø ) ‚äô 3 ‚Äã ùëø ‚ä§ ] . \\bm{A}_{t+1}=\\mathcal{P}_{\\mathrm{O}(D)}[({\\bm{A}_{t}\\bm{X}})^{\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\scalebox{0.8}{$\\scriptstyle\\odot$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\odot$}}$}}{\\raisebox{1.3pt}{$\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\scalebox{0.8}{$\\scriptstyle\\odot$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\odot$}}$}}{\\raisebox{0.75pt}{$\\scriptstyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E19", "title": "min ‚àí 1 4 ‚Äã ‚Äñ ùíí ‚ä§ ‚Äã ùëø ‚Äñ 4 4 subject to ùíí ‚ä§ ‚Äã ùíí = 1 . \\min\\,-\\frac{1}{4}\\left\\|\\bm{q}^{\\top}\\bm{X}\\right\\|_{4}^{4}\\quad\\mbox{subject to}\\quad\\bm{q}^{\\top}\\bm{q}=1. roman_min - divide start_ARG 1 end_AR", "snippet": "min ‚àí 1 4 ‚Äã ‚Äñ ùíí ‚ä§ ‚Äã ùëø ‚Äñ 4 4 subject to ùíí ‚ä§ ‚Äã ùíí = 1 . \\min\\,-\\frac{1}{4}\\left\\|\\bm{q}^{\\top}\\bm{X}\\right\\|_{4}^{4}\\quad\\mbox{subject to}\\quad\\bm{q}^{\\top}\\bm{q}=1. roman_min - divide start_ARG 1 end_ARG start_ARG 4 end_ARG ‚à• bold_italic_q start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_X ‚à• start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT subject to bold_italic_q start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_q = 1 . (2.2.19)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.Ex4", "title": "Œ¥ ‚Äã ùë® t + 1 = ùí´ O ‚Äã ( D ) ‚Äã [ ( ùíÅ t ) ‚äô 3 ‚Äã ùíÅ t ‚ä§ ] . \\delta\\bm{A}_{t+1}=\\mathcal{P}_{\\mathrm{O}(D)}[(\\bm{Z}_{t})^{\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\disp", "snippet": "Œ¥ ‚Äã ùë® t + 1 = ùí´ O ‚Äã ( D ) ‚Äã [ ( ùíÅ t ) ‚äô 3 ‚Äã ùíÅ t ‚ä§ ] . \\delta\\bm{A}_{t+1}=\\mathcal{P}_{\\mathrm{O}(D)}[(\\bm{Z}_{t})^{\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\scalebox{0.8}{$\\scriptstyle\\odot$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\odot$}}$}}{\\raisebox{1.3pt}{$\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\scalebox{0.8}{$\\scriptstyle\\odot$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\odot$}}$}}{\\raisebox{0.75pt}{$\\scriptstyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.Ex5", "title": "ùíÅ ‚üµ ùíÅ t + 1 = Œ¥ ‚Äã ùë® t + 1 ‚Äã Œ¥ ‚Äã ùë® t ‚Äã ‚Ä¶ ‚Äã Œ¥ ‚Äã ùë® 1 ‚èü forward constructed layers ‚Äã ùëø . \\bm{Z}\\;\\longleftarrow\\;\\bm{Z}_{t+1}=\\underbrace{\\delta\\bm{A}_{t+1}\\delta\\bm{A}_{t}\\ldots\\delta\\bm{A}_{1}}_{\\color[", "snippet": "ùíÅ ‚üµ ùíÅ t + 1 = Œ¥ ‚Äã ùë® t + 1 ‚Äã Œ¥ ‚Äã ùë® t ‚Äã ‚Ä¶ ‚Äã Œ¥ ‚Äã ùë® 1 ‚èü forward constructed layers ‚Äã ùëø . \\bm{Z}\\;\\longleftarrow\\;\\bm{Z}_{t+1}=\\underbrace{\\delta\\bm{A}_{t+1}\\delta\\bm{A}_{t}\\ldots\\delta\\bm{A}_{1}}_{\\color[rgb]{1,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{1,0,0}\\text{forward constructed layers}}\\bm{X}. bold_italic_Z ‚üµ bold_italic_Z start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT = under‚èü start_ARG italic_Œ¥ bold_italic_A start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT italic_Œ¥ bold_italic_A start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ‚Ä¶ italic_Œ¥ bold_italic_A start_POSTSUBSCRIPT 1 end_POSTS"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E20", "title": "kurt ( X ) = ùîº ‚Å° X 4 ‚àí 3 ‚Äã ( ùîº ‚Å° X 2 ) 2 . \\mathop{\\mathrm{kurt}}(X)=\\operatorname{\\mathbb{E}}{X^{4}}-3(\\operatorname{\\mathbb{E}}{X^{2}})^{2}. roman_kurt ( italic_X ) = blackboard_E italic_X start_POS", "snippet": "kurt ( X ) = ùîº ‚Å° X 4 ‚àí 3 ‚Äã ( ùîº ‚Å° X 2 ) 2 . \\mathop{\\mathrm{kurt}}(X)=\\operatorname{\\mathbb{E}}{X^{4}}-3(\\operatorname{\\mathbb{E}}{X^{2}})^{2}. roman_kurt ( italic_X ) = blackboard_E italic_X start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT - 3 ( blackboard_E italic_X start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . (2.2.20)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E21", "title": "kurt ( ùíô ) = 1 N ‚Äã ‚Äñ ùíô ‚Äñ 4 4 ‚àí 3 N 2 ‚Äã ‚Äñ ùíô ‚Äñ 2 4 . \\mathop{\\mathrm{kurt}}(\\bm{x})=\\frac{1}{N}\\|\\bm{x}\\|_{4}^{4}-\\frac{3}{N^{2}}\\|\\bm{x}\\|_{2}^{4}. roman_kurt ( bold_italic_x ) = divide start_ARG 1 end", "snippet": "kurt ( ùíô ) = 1 N ‚Äã ‚Äñ ùíô ‚Äñ 4 4 ‚àí 3 N 2 ‚Äã ‚Äñ ùíô ‚Äñ 2 4 . \\mathop{\\mathrm{kurt}}(\\bm{x})=\\frac{1}{N}\\|\\bm{x}\\|_{4}^{4}-\\frac{3}{N^{2}}\\|\\bm{x}\\|_{2}^{4}. roman_kurt ( bold_italic_x ) = divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ‚à• bold_italic_x ‚à• start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT - divide start_ARG 3 end_ARG start_ARG italic_N start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ‚à• bold_italic_x ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT . (2.2.21)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E22", "title": "max ùëΩ ‚ä§ ‚Äã ùëΩ = ùë∞ ‚Äã kurt ( ùëΩ ‚ä§ ‚Äã ùëø ) . \\max_{\\bm{V}^{\\top}\\bm{V}=\\bm{I}}\\mathop{\\mathrm{kurt}}(\\bm{V}^{\\top}\\bm{X}). roman_max start_POSTSUBSCRIPT bold_italic_V start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRI", "snippet": "max ùëΩ ‚ä§ ‚Äã ùëΩ = ùë∞ ‚Äã kurt ( ùëΩ ‚ä§ ‚Äã ùëø ) . \\max_{\\bm{V}^{\\top}\\bm{V}=\\bm{I}}\\mathop{\\mathrm{kurt}}(\\bm{V}^{\\top}\\bm{X}). roman_max start_POSTSUBSCRIPT bold_italic_V start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_V = bold_italic_I end_POSTSUBSCRIPT roman_kurt ( bold_italic_V start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_X ) . (2.2.22)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E23", "title": "max ‚Äñ ùíó ‚Äñ 2 2 = 1 ‚Äã kurt ( ùëø ‚ä§ ‚Äã ùíó ) . \\max_{\\|\\bm{v}\\|_{2}^{2}=1}\\,\\mathop{\\mathrm{kurt}}(\\bm{X}^{\\top}\\bm{v}). roman_max start_POSTSUBSCRIPT ‚à• bold_italic_v ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT", "snippet": "max ‚Äñ ùíó ‚Äñ 2 2 = 1 ‚Äã kurt ( ùëø ‚ä§ ‚Äã ùíó ) . \\max_{\\|\\bm{v}\\|_{2}^{2}=1}\\,\\mathop{\\mathrm{kurt}}(\\bm{X}^{\\top}\\bm{v}). roman_max start_POSTSUBSCRIPT ‚à• bold_italic_v ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 1 end_POSTSUBSCRIPT roman_kurt ( bold_italic_X start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_v ) . (2.2.23)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.Ex6", "title": "max ‚Äñ ùíò ‚Äñ 2 2 = 1 ‚Å° kurt ‚Äã ( ùíÅ ‚ä§ ‚Äã ùíò ) . \\max_{\\|\\bm{w}\\|_{2}^{2}=1}\\,\\mathrm{kurt}(\\bm{Z}^{\\top}\\bm{w}). roman_max start_POSTSUBSCRIPT ‚à• bold_italic_w ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_", "snippet": "max ‚Äñ ùíò ‚Äñ 2 2 = 1 ‚Å° kurt ‚Äã ( ùíÅ ‚ä§ ‚Äã ùíò ) . \\max_{\\|\\bm{w}\\|_{2}^{2}=1}\\,\\mathrm{kurt}(\\bm{Z}^{\\top}\\bm{w}). roman_max start_POSTSUBSCRIPT ‚à• bold_italic_w ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 1 end_POSTSUBSCRIPT roman_kurt ( bold_italic_Z start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_w ) ."}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E24", "title": "max ‚Äñ ùíò ‚Äñ 2 2 = 1 ‚Äã ‚àë i = 1 d kurt ‚Äã ( z i ) ‚Äã w i 4 . \\max_{\\|\\bm{w}\\|_{2}^{2}=1}\\,\\sum_{i=1}^{d}\\mathrm{kurt}(z_{i})w_{i}^{4}. roman_max start_POSTSUBSCRIPT ‚à• bold_italic_w ‚à• start_POSTSUBSCRIPT 2 e", "snippet": "max ‚Äñ ùíò ‚Äñ 2 2 = 1 ‚Äã ‚àë i = 1 d kurt ‚Äã ( z i ) ‚Äã w i 4 . \\max_{\\|\\bm{w}\\|_{2}^{2}=1}\\,\\sum_{i=1}^{d}\\mathrm{kurt}(z_{i})w_{i}^{4}. roman_max start_POSTSUBSCRIPT ‚à• bold_italic_w ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 1 end_POSTSUBSCRIPT ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT roman_kurt ( italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT . (2.2.24)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E25", "title": "kurt ( ùëø ‚ä§ ‚Äã ùíñ ) ‚âà 1 N ‚Äã ‚Äñ ùëø ‚ä§ ‚Äã ùíñ ‚Äñ 4 4 ‚àí 3 ‚Äã ‚Äñ ùíñ ‚Äñ 2 4 . \\mathop{\\mathrm{kurt}}(\\bm{X}^{\\top}\\bm{u})\\approx\\tfrac{1}{N}\\|\\bm{X}^{\\top}\\bm{u}\\|_{4}^{4}-3\\|\\bm{u}\\|_{2}^{4}. roman_kurt ( bold_italic_X", "snippet": "kurt ( ùëø ‚ä§ ‚Äã ùíñ ) ‚âà 1 N ‚Äã ‚Äñ ùëø ‚ä§ ‚Äã ùíñ ‚Äñ 4 4 ‚àí 3 ‚Äã ‚Äñ ùíñ ‚Äñ 2 4 . \\mathop{\\mathrm{kurt}}(\\bm{X}^{\\top}\\bm{u})\\approx\\tfrac{1}{N}\\|\\bm{X}^{\\top}\\bm{u}\\|_{4}^{4}-3\\|\\bm{u}\\|_{2}^{4}. roman_kurt ( bold_italic_X start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_u ) ‚âà divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ‚à• bold_italic_X start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_u ‚à• start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT - 3 ‚à• bold_italic_u ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT . (2.2.25)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.Ex7", "title": "‚àá ùíñ ‚Äã kurt ( ùëø ‚ä§ ‚Äã ùíñ ) ‚âà 4 N ‚Äã ùëø ‚Äã ( ùëø ‚ä§ ‚Äã ùíñ ) ‚äô 3 ‚àí 12 ‚Äã ‚Äñ ùíñ ‚Äñ 2 2 ‚Äã ùíñ . \\nabla_{\\bm{u}}\\mathop{\\mathrm{kurt}}(\\bm{X}^{\\top}\\bm{u})\\approx\\tfrac{4}{N}\\bm{X}(\\bm{X}^{\\top}\\bm{u})^{\\mathbin{\\mathchoice", "snippet": "‚àá ùíñ ‚Äã kurt ( ùëø ‚ä§ ‚Äã ùíñ ) ‚âà 4 N ‚Äã ùëø ‚Äã ( ùëø ‚ä§ ‚Äã ùíñ ) ‚äô 3 ‚àí 12 ‚Äã ‚Äñ ùíñ ‚Äñ 2 2 ‚Äã ùíñ . \\nabla_{\\bm{u}}\\mathop{\\mathrm{kurt}}(\\bm{X}^{\\top}\\bm{u})\\approx\\tfrac{4}{N}\\bm{X}(\\bm{X}^{\\top}\\bm{u})^{\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\scalebox{0.8}{$\\scriptstyle\\odot$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\odot$}}$}}{\\raisebox{1.3pt}{$\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\scalebox{0.8}{$\\scriptstyle\\odot$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\odot$}}$}}{\\raisebox{0.75"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S2.E29", "title": "ùíó + = 1 N ‚Äã ùëø ‚Äã ( ùëø ‚ä§ ‚Äã ùíñ ) ‚äô 3 ‚àí 3 ‚Äã ùíñ , ùíñ + = ùíó + / ‚Äñ ùíó + ‚Äñ 2 . \\begin{split}\\bm{v}^{+}&=\\tfrac{1}{N}\\bm{X}(\\bm{X}^{\\top}\\bm{u})^{\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\sca", "snippet": "ùíó + = 1 N ‚Äã ùëø ‚Äã ( ùëø ‚ä§ ‚Äã ùíñ ) ‚äô 3 ‚àí 3 ‚Äã ùíñ , ùíñ + = ùíó + / ‚Äñ ùíó + ‚Äñ 2 . \\begin{split}\\bm{v}^{+}&=\\tfrac{1}{N}\\bm{X}(\\bm{X}^{\\top}\\bm{u})^{\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\scalebox{0.8}{$\\scriptstyle\\odot$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\odot$}}$}}{\\raisebox{1.3pt}{$\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\scalebox{0.8}{$\\scriptstyle\\odot$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\odot$}}$}}{\\raisebox{0.75pt}{$\\scriptstyle\\mathchoice{\\scalebox{0.8}{$\\di"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.E1", "title": "ùíô = ùë® ‚Äã ùíõ + ùú∫ , ‚Äñ ùíõ ‚Äñ 0 = d ‚â™ m . \\bm{x}=\\bm{A}\\bm{z}+\\bm{\\varepsilon},\\quad\\|\\bm{z}\\|_{0}=d\\ll m. bold_italic_x = bold_italic_A bold_italic_z + bold_italic_Œµ , ‚à• bold_italic_z ‚à• start_POSTSUBSCRIPT 0", "snippet": "ùíô = ùë® ‚Äã ùíõ + ùú∫ , ‚Äñ ùíõ ‚Äñ 0 = d ‚â™ m . \\bm{x}=\\bm{A}\\bm{z}+\\bm{\\varepsilon},\\quad\\|\\bm{z}\\|_{0}=d\\ll m. bold_italic_x = bold_italic_A bold_italic_z + bold_italic_Œµ , ‚à• bold_italic_z ‚à• start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = italic_d ‚â™ italic_m . (2.3.1)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.E2", "title": "ùíô ‚Üí ‚Ñ∞ ùíõ ‚Üí ùíü = ùë® ùíô ^ . \\bm{x}\\xrightarrow{\\hskip 11.38109pt\\mathcal{E}\\hskip 11.38109pt}\\bm{z}\\xrightarrow{\\hskip 5.69054pt\\mathcal{D}=\\bm{A}\\hskip 5.69054pt}\\hat{\\bm{x}}. bold_italic_x start_ARROW sta", "snippet": "ùíô ‚Üí ‚Ñ∞ ùíõ ‚Üí ùíü = ùë® ùíô ^ . \\bm{x}\\xrightarrow{\\hskip 11.38109pt\\mathcal{E}\\hskip 11.38109pt}\\bm{z}\\xrightarrow{\\hskip 5.69054pt\\mathcal{D}=\\bm{A}\\hskip 5.69054pt}\\hat{\\bm{x}}. bold_italic_x start_ARROW start_OVERACCENT caligraphic_E end_OVERACCENT ‚Üí end_ARROW bold_italic_z start_ARROW start_OVERACCENT caligraphic_D = bold_italic_A end_OVERACCENT ‚Üí end_ARROW over^ start_ARG bold_italic_x end_ARG . (2.3.2)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.E3", "title": "ùíô i = ùë® ‚Äã ùíõ i + ùú∫ i , ‚àÄ i ‚àà [ N ] \\bm{x}_{i}=\\bm{A}\\bm{z}_{i}+\\bm{\\varepsilon}_{i},\\qquad\\forall i\\in[N] bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_A bold_italic_z star", "snippet": "ùíô i = ùë® ‚Äã ùíõ i + ùú∫ i , ‚àÄ i ‚àà [ N ] \\bm{x}_{i}=\\bm{A}\\bm{z}_{i}+\\bm{\\varepsilon}_{i},\\qquad\\forall i\\in[N] bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_A bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + bold_italic_Œµ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , ‚àÄ italic_i ‚àà [ italic_N ] (2.3.3)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.E4", "title": "ùëø = ùë® ‚Äã ùíÅ + ùë¨ . \\bm{X}=\\bm{A}\\bm{Z}+\\bm{E}. bold_italic_X = bold_italic_A bold_italic_Z + bold_italic_E . (2.3.4)", "snippet": "ùëø = ùë® ‚Äã ùíÅ + ùë¨ . \\bm{X}=\\bm{A}\\bm{Z}+\\bm{E}. bold_italic_X = bold_italic_A bold_italic_Z + bold_italic_E . (2.3.4)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.E5", "title": "min ùíÅ ‚àà ‚Ñù d √ó N ‚Å° { ‚Äñ ùëø ‚àí ùë® ‚Äã ùíÅ ‚Äñ F 2 + Œª ‚Äã ‚Äñ ùíÅ ‚Äñ 1 } , \\min_{\\bm{Z}\\in\\mathbb{R}^{d\\times N}}\\left\\{\\|\\bm{X}-\\bm{A}\\bm{Z}\\|_{F}^{2}+\\lambda\\|\\bm{Z}\\|_{1}\\right\\}, roman_min start_POSTSUBSCRIPT bold_i", "snippet": "min ùíÅ ‚àà ‚Ñù d √ó N ‚Å° { ‚Äñ ùëø ‚àí ùë® ‚Äã ùíÅ ‚Äñ F 2 + Œª ‚Äã ‚Äñ ùíÅ ‚Äñ 1 } , \\min_{\\bm{Z}\\in\\mathbb{R}^{d\\times N}}\\left\\{\\|\\bm{X}-\\bm{A}\\bm{Z}\\|_{F}^{2}+\\lambda\\|\\bm{Z}\\|_{1}\\right\\}, roman_min start_POSTSUBSCRIPT bold_italic_Z ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d √ó italic_N end_POSTSUPERSCRIPT end_POSTSUBSCRIPT { ‚à• bold_italic_X - bold_italic_A bold_italic_Z ‚à• start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_Œª ‚à• bold_italic_Z ‚à• start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT } , (2.3.5)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.E6", "title": "ùíÅ t + 1 ‚Üê ùíÅ t + Œ∑ ‚Äã ‚àá f ‚Äã ( ùíÅ t ) . \\bm{Z}_{t+1}\\leftarrow\\bm{Z}_{t}+\\eta\\nabla f(\\bm{Z}_{t}). bold_italic_Z start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT ‚Üê bold_italic_Z start_POSTSUBSCRIPT ital", "snippet": "ùíÅ t + 1 ‚Üê ùíÅ t + Œ∑ ‚Äã ‚àá f ‚Äã ( ùíÅ t ) . \\bm{Z}_{t+1}\\leftarrow\\bm{Z}_{t}+\\eta\\nabla f(\\bm{Z}_{t}). bold_italic_Z start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT ‚Üê bold_italic_Z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_Œ∑ ‚àá italic_f ( bold_italic_Z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) . (2.3.6)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.E7", "title": "ùíÅ t + 1 ‚Üê ùíÅ t + Œ∑ ‚Äã ‚àÇ f ‚Äã ( ùíÅ t ) . \\bm{Z}_{t+1}\\leftarrow\\bm{Z}_{t}+\\eta\\partial f(\\bm{Z}_{t}). bold_italic_Z start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT ‚Üê bold_italic_Z start_POSTSUBSCRIPT it", "snippet": "ùíÅ t + 1 ‚Üê ùíÅ t + Œ∑ ‚Äã ‚àÇ f ‚Äã ( ùíÅ t ) . \\bm{Z}_{t+1}\\leftarrow\\bm{Z}_{t}+\\eta\\partial f(\\bm{Z}_{t}). bold_italic_Z start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT ‚Üê bold_italic_Z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_Œ∑ ‚àÇ italic_f ( bold_italic_Z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) . (2.3.7)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.E12", "title": "ùíÅ t + 1 = nonlinearity ‚Äã ( ùíÅ t + linear ‚ä§ ‚Äã ( linear ‚Äã ( ùíÅ t ) + bias ) ) . \\bm{Z}_{t+1}=\\texttt{nonlinearity}(\\bm{Z}_{t}+\\texttt{linear}^{\\top}(\\texttt{linear}(\\bm{Z}_{t})+\\texttt{bias})). bold_itali", "snippet": "ùíÅ t + 1 = nonlinearity ‚Äã ( ùíÅ t + linear ‚ä§ ‚Äã ( linear ‚Äã ( ùíÅ t ) + bias ) ) . \\bm{Z}_{t+1}=\\texttt{nonlinearity}(\\bm{Z}_{t}+\\texttt{linear}^{\\top}(\\texttt{linear}(\\bm{Z}_{t})+\\texttt{bias})). bold_italic_Z start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT = nonlinearity ( bold_italic_Z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + linear start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ( linear ( bold_italic_Z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) + bias ) ) . (2.3.12)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.E13", "title": "ùíÅ t + 1 = ùíÅ t + linear 1 ‚ä§ ( nonlinearity ( linear 2 ( ùíÅ t ) + bias 1 ) + bias 2 , \\bm{Z}_{t+1}=\\bm{Z}_{t}+\\texttt{linear}_{1}^{\\top}(\\texttt{nonlinearity}(\\texttt{linear}_{2}(\\bm{Z}_{t})+\\texttt{bias", "snippet": "ùíÅ t + 1 = ùíÅ t + linear 1 ‚ä§ ( nonlinearity ( linear 2 ( ùíÅ t ) + bias 1 ) + bias 2 , \\bm{Z}_{t+1}=\\bm{Z}_{t}+\\texttt{linear}_{1}^{\\top}(\\texttt{nonlinearity}(\\texttt{linear}_{2}(\\bm{Z}_{t})+\\texttt{bias}_{1})+\\texttt{bias}_{2}, bold_italic_Z start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT = bold_italic_Z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + linear start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ( nonlinearity ( linear start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) + bias start_POSTSUBSCRI"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.E14", "title": "ùëø = ùë® ‚Äã ùíÅ + ùë¨ , \\bm{X}=\\bm{A}\\bm{Z}+\\bm{E}, bold_italic_X = bold_italic_A bold_italic_Z + bold_italic_E , (2.3.14)", "snippet": "ùëø = ùë® ‚Äã ùíÅ + ùë¨ , \\bm{X}=\\bm{A}\\bm{Z}+\\bm{E}, bold_italic_X = bold_italic_A bold_italic_Z + bold_italic_E , (2.3.14)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.E15", "title": "min ùë® ~ , ùíÅ ~ ‚Å° { ‚Äñ ùëø ‚àí ùë® ~ ‚Äã ùíÅ ~ ‚Äñ F 2 + Œª ‚Äã ‚Äñ ùíÅ ~ ‚Äñ 1 } . \\min_{\\tilde{\\bm{A}},\\tilde{\\bm{Z}}}\\left\\{\\|\\bm{X}-\\tilde{\\bm{A}}\\tilde{\\bm{Z}}\\|_{F}^{2}+\\lambda\\|\\tilde{\\bm{Z}}\\|_{1}\\right\\}. roman_min ", "snippet": "min ùë® ~ , ùíÅ ~ ‚Å° { ‚Äñ ùëø ‚àí ùë® ~ ‚Äã ùíÅ ~ ‚Äñ F 2 + Œª ‚Äã ‚Äñ ùíÅ ~ ‚Äñ 1 } . \\min_{\\tilde{\\bm{A}},\\tilde{\\bm{Z}}}\\left\\{\\|\\bm{X}-\\tilde{\\bm{A}}\\tilde{\\bm{Z}}\\|_{F}^{2}+\\lambda\\|\\tilde{\\bm{Z}}\\|_{1}\\right\\}. roman_min start_POSTSUBSCRIPT over~ start_ARG bold_italic_A end_ARG , over~ start_ARG bold_italic_Z end_ARG end_POSTSUBSCRIPT { ‚à• bold_italic_X - over~ start_ARG bold_italic_A end_ARG over~ start_ARG bold_italic_Z end_ARG ‚à• start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_Œª ‚à• over~ start_ARG bold_italic_Z end_ARG ‚à• start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT } "}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.E16", "title": "‚Äñ ùëø ‚àí ùë® ~ ‚Äã ùíÅ ~ ‚Äñ F 2 + c ‚Äã Œª ‚Äã ‚Äñ ùíÅ ~ ‚Äñ 1 . \\|\\bm{X}-\\tilde{\\bm{A}}\\tilde{\\bm{Z}}\\|_{F}^{2}+c\\lambda\\|\\tilde{\\bm{Z}}\\|_{1}. ‚à• bold_italic_X - over~ start_ARG bold_italic_A end_ARG over~ start_ARG bold", "snippet": "‚Äñ ùëø ‚àí ùë® ~ ‚Äã ùíÅ ~ ‚Äñ F 2 + c ‚Äã Œª ‚Äã ‚Äñ ùíÅ ~ ‚Äñ 1 . \\|\\bm{X}-\\tilde{\\bm{A}}\\tilde{\\bm{Z}}\\|_{F}^{2}+c\\lambda\\|\\tilde{\\bm{Z}}\\|_{1}. ‚à• bold_italic_X - over~ start_ARG bold_italic_A end_ARG over~ start_ARG bold_italic_Z end_ARG ‚à• start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_c italic_Œª ‚à• over~ start_ARG bold_italic_Z end_ARG ‚à• start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT . (2.3.16)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.E17", "title": "min ùíÅ ~ , ùë® ~ : ‚Äñ ùë® ~ j ‚Äñ 2 ‚â§ 1 ‚Å° { ‚Äñ ùëø ‚àí ùë® ~ ‚Äã ùíÅ ~ ‚Äñ F 2 + Œª ‚Äã ‚Äñ ùíÅ ~ ‚Äñ 1 } . \\min_{\\tilde{\\bm{Z}},\\tilde{\\bm{A}}\\,:\\,\\|\\tilde{\\bm{A}}_{j}\\|_{2}\\leq 1}\\left\\{\\|\\bm{X}-\\tilde{\\bm{A}}\\tilde{\\bm{Z}}\\|_{F", "snippet": "min ùíÅ ~ , ùë® ~ : ‚Äñ ùë® ~ j ‚Äñ 2 ‚â§ 1 ‚Å° { ‚Äñ ùëø ‚àí ùë® ~ ‚Äã ùíÅ ~ ‚Äñ F 2 + Œª ‚Äã ‚Äñ ùíÅ ~ ‚Äñ 1 } . \\min_{\\tilde{\\bm{Z}},\\tilde{\\bm{A}}\\,:\\,\\|\\tilde{\\bm{A}}_{j}\\|_{2}\\leq 1}\\left\\{\\|\\bm{X}-\\tilde{\\bm{A}}\\tilde{\\bm{Z}}\\|_{F}^{2}+\\lambda\\|\\tilde{\\bm{Z}}\\|_{1}\\right\\}. roman_min start_POSTSUBSCRIPT over~ start_ARG bold_italic_Z end_ARG , over~ start_ARG bold_italic_A end_ARG : ‚à• over~ start_ARG bold_italic_A end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ‚â§ 1 end_POSTSUBSCRIPT { ‚à• bold_italic_X - over~ start_ARG bold_italic_A end_ARG over~ start_ARG bold_italic_Z end_AR"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.Ex1", "title": "ùëø , ùíÅ 1 ‚Üí f ‚Äã ( ùë® 1 , ‚ãÖ ) ùíÅ 2 ‚Üí f ‚Äã ( ùë® 2 , ‚ãÖ ) ùíÅ 3 ‚Üí f ‚Äã ( ùë® 3 , ‚ãÖ ) ‚ãØ ‚Äã ùíÅ L ‚Üí f ‚Äã ( ùë® L , ‚ãÖ ) ùíÅ L + 1 ‚âà ùíÅ . \\bm{X},\\bm{Z}^{1}\\xrightarrow{\\hskip 2.84526ptf(\\bm{A}^{1},\\,\\cdot\\,)\\hskip 2.84526pt}\\bm{", "snippet": "ùëø , ùíÅ 1 ‚Üí f ‚Äã ( ùë® 1 , ‚ãÖ ) ùíÅ 2 ‚Üí f ‚Äã ( ùë® 2 , ‚ãÖ ) ùíÅ 3 ‚Üí f ‚Äã ( ùë® 3 , ‚ãÖ ) ‚ãØ ‚Äã ùíÅ L ‚Üí f ‚Äã ( ùë® L , ‚ãÖ ) ùíÅ L + 1 ‚âà ùíÅ . \\bm{X},\\bm{Z}^{1}\\xrightarrow{\\hskip 2.84526ptf(\\bm{A}^{1},\\,\\cdot\\,)\\hskip 2.84526pt}\\bm{Z}^{2}\\xrightarrow{\\hskip 2.84526ptf(\\bm{A}^{2},\\,\\cdot\\,)\\hskip 2.84526pt}\\bm{Z}^{3}\\xrightarrow{\\hskip 2.84526ptf(\\bm{A}^{3},\\,\\cdot\\,)\\hskip 2.84526pt}\\cdots\\bm{Z}^{L}\\xrightarrow{\\hskip 2.84526ptf(\\bm{A}^{L},\\,\\cdot\\,)\\hskip 2.84526pt}\\bm{Z}^{L+1}\\approx\\bm{Z}. bold_italic_X , bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_ARROW start_OVERACCENT italic_f ( bold_italic_A start_"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.E23", "title": "min { ùë® ‚Ñì } ‚Å° ‚Äñ ùíÅ L ‚Äã ( ùë® 1 , ‚Ä¶ , ùë® L ) ‚àí ùíÅ ‚Äñ 2 2 . \\min_{\\{\\bm{A}^{\\ell}\\}}\\big{\\|}\\bm{Z}^{L}(\\bm{A}^{1},\\ldots,\\bm{A}^{L})-\\bm{Z}\\big{\\|}_{2}^{2}. roman_min start_POSTSUBSCRIPT { bold_italic_A start", "snippet": "min { ùë® ‚Ñì } ‚Å° ‚Äñ ùíÅ L ‚Äã ( ùë® 1 , ‚Ä¶ , ùë® L ) ‚àí ùíÅ ‚Äñ 2 2 . \\min_{\\{\\bm{A}^{\\ell}\\}}\\big{\\|}\\bm{Z}^{L}(\\bm{A}^{1},\\ldots,\\bm{A}^{L})-\\bm{Z}\\big{\\|}_{2}^{2}. roman_min start_POSTSUBSCRIPT { bold_italic_A start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT } end_POSTSUBSCRIPT ‚à• bold_italic_Z start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT ( bold_italic_A start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , ‚Ä¶ , bold_italic_A start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT ) - bold_italic_Z ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . (2.3.23)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.E24", "title": "min f , g ‚à• ùëØ ‚àí g ( f ( ùëØ ) ) ) ‚à• F 2 + Œª ‚à• f ( ùëØ ) ‚à• 1 , \\min_{f,g}\\|\\bm{H}-g(f(\\bm{H})))\\|_{F}^{2}+\\lambda\\|f(\\bm{H})\\|_{1}, roman_min start_POSTSUBSCRIPT italic_f , italic_g end_POSTSUBSCRIPT ‚à• bol", "snippet": "min f , g ‚à• ùëØ ‚àí g ( f ( ùëØ ) ) ) ‚à• F 2 + Œª ‚à• f ( ùëØ ) ‚à• 1 , \\min_{f,g}\\|\\bm{H}-g(f(\\bm{H})))\\|_{F}^{2}+\\lambda\\|f(\\bm{H})\\|_{1}, roman_min start_POSTSUBSCRIPT italic_f , italic_g end_POSTSUBSCRIPT ‚à• bold_italic_H - italic_g ( italic_f ( bold_italic_H ) ) ) ‚à• start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_Œª ‚à• italic_f ( bold_italic_H ) ‚à• start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , (2.3.24)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.E25", "title": "ùíÅ 2 = f ‚Äã ( ùë® 1 , ùëø ) = S Œ∑ ‚Äã Œª ‚Äã ( ùíÅ 1 ‚àí 2 ‚Äã Œ∑ ‚Äã ( ùë® 1 ) ‚ä§ ‚Äã ( ùë® 1 ‚Äã ùíÅ 1 ‚àí ùëø ) ) . \\bm{Z}^{2}=f(\\bm{A}^{1},\\bm{X})=S_{\\eta\\lambda}\\left(\\bm{Z}^{1}-2\\eta(\\bm{A}^{1})^{\\top}(\\bm{A}^{1}\\bm{Z}^{1}-\\bm{X}", "snippet": "ùíÅ 2 = f ‚Äã ( ùë® 1 , ùëø ) = S Œ∑ ‚Äã Œª ‚Äã ( ùíÅ 1 ‚àí 2 ‚Äã Œ∑ ‚Äã ( ùë® 1 ) ‚ä§ ‚Äã ( ùë® 1 ‚Äã ùíÅ 1 ‚àí ùëø ) ) . \\bm{Z}^{2}=f(\\bm{A}^{1},\\bm{X})=S_{\\eta\\lambda}\\left(\\bm{Z}^{1}-2\\eta(\\bm{A}^{1})^{\\top}(\\bm{A}^{1}\\bm{Z}^{1}-\\bm{X})\\right). bold_italic_Z start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = italic_f ( bold_italic_A start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , bold_italic_X ) = italic_S start_POSTSUBSCRIPT italic_Œ∑ italic_Œª end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT - 2 italic_Œ∑ ( bold_italic_A start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRI"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.E26", "title": "ùíÅ 2 = f ‚Äã ( ùë® 1 , ùëø ) = max ‚Å° { ùíÅ 1 ‚àí 2 ‚Äã Œ∑ ‚Äã ( ùë® 1 ) ‚ä§ ‚Äã ( ùë® 1 ‚Äã ùíÅ 1 ‚àí ùëø ) ‚àí Œª ‚Äã Œ∑ ‚Äã ùüè , 0 } , \\bm{Z}^{2}=f(\\bm{A}^{1},\\bm{X})=\\max\\left\\{\\bm{Z}^{1}-2\\eta(\\bm{A}^{1})^{\\top}(\\bm{A}^{1}\\bm{Z}^{1}-\\bm{", "snippet": "ùíÅ 2 = f ‚Äã ( ùë® 1 , ùëø ) = max ‚Å° { ùíÅ 1 ‚àí 2 ‚Äã Œ∑ ‚Äã ( ùë® 1 ) ‚ä§ ‚Äã ( ùë® 1 ‚Äã ùíÅ 1 ‚àí ùëø ) ‚àí Œª ‚Äã Œ∑ ‚Äã ùüè , 0 } , \\bm{Z}^{2}=f(\\bm{A}^{1},\\bm{X})=\\max\\left\\{\\bm{Z}^{1}-2\\eta(\\bm{A}^{1})^{\\top}(\\bm{A}^{1}\\bm{Z}^{1}-\\bm{X})-\\lambda\\eta\\mathbf{1},0\\right\\}, bold_italic_Z start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = italic_f ( bold_italic_A start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , bold_italic_X ) = roman_max { bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT - 2 italic_Œ∑ ( bold_italic_A start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ( bold_italic_A start_POS"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S3.E27", "title": "ùíÅ 2 = f ‚Äã ( ùë® 1 , ùëø ) = max ‚Å° { 2 ‚Äã Œ∑ ‚Äã ( ùë® 1 ) ‚ä§ + ( ùíÅ 1 ‚àí 2 ‚Äã Œ∑ ‚Äã ( ùë® 1 ) ‚ä§ ‚Äã ùë® 1 ‚Äã ùíÅ 1 ‚àí Œª ‚Äã Œ∑ ‚Äã ùüè ) , 0 } . \\bm{Z}^{2}=f(\\bm{A}^{1},\\bm{X})=\\max\\left\\{2\\eta(\\bm{A}^{1})^{\\top}+\\left(\\bm{Z}^{1}-2\\e", "snippet": "ùíÅ 2 = f ‚Äã ( ùë® 1 , ùëø ) = max ‚Å° { 2 ‚Äã Œ∑ ‚Äã ( ùë® 1 ) ‚ä§ + ( ùíÅ 1 ‚àí 2 ‚Äã Œ∑ ‚Äã ( ùë® 1 ) ‚ä§ ‚Äã ùë® 1 ‚Äã ùíÅ 1 ‚àí Œª ‚Äã Œ∑ ‚Äã ùüè ) , 0 } . \\bm{Z}^{2}=f(\\bm{A}^{1},\\bm{X})=\\max\\left\\{2\\eta(\\bm{A}^{1})^{\\top}+\\left(\\bm{Z}^{1}-2\\eta(\\bm{A}^{1})^{\\top}\\bm{A}^{1}\\bm{Z}^{1}-\\lambda\\eta\\mathbf{1}\\right),0\\right\\}. bold_italic_Z start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = italic_f ( bold_italic_A start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , bold_italic_X ) = roman_max { 2 italic_Œ∑ ( bold_italic_A start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT + ( bold_italic_Z start_POSTSUPERSCRIPT "}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S4.E1", "title": "ùíñ t + 1 = ùë® ‚Äã ùíñ t ‚Äñ ùë® ‚Äã ùíñ t ‚Äñ 2 , \\bm{u}_{t+1}=\\frac{\\bm{A}\\bm{u}_{t}}{\\|\\bm{A}\\bm{u}_{t}\\|_{2}}, bold_italic_u start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT = divide start_ARG bold_italic_A bold", "snippet": "ùíñ t + 1 = ùë® ‚Äã ùíñ t ‚Äñ ùë® ‚Äã ùíñ t ‚Äñ 2 , \\bm{u}_{t+1}=\\frac{\\bm{A}\\bm{u}_{t}}{\\|\\bm{A}\\bm{u}_{t}\\|_{2}}, bold_italic_u start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT = divide start_ARG bold_italic_A bold_italic_u start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG ‚à• bold_italic_A bold_italic_u start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_ARG , (2.4.1)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.E1", "title": "max ‚Äñ ùíñ ‚Äñ 2 2 = 1 ‚Å° f ‚Äã ( ùíñ ) . \\max_{\\|\\bm{u}\\|_{2}^{2}=1}\\,f(\\bm{u}). roman_max start_POSTSUBSCRIPT ‚à• bold_italic_u ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCR", "snippet": "max ‚Äñ ùíñ ‚Äñ 2 2 = 1 ‚Å° f ‚Äã ( ùíñ ) . \\max_{\\|\\bm{u}\\|_{2}^{2}=1}\\,f(\\bm{u}). roman_max start_POSTSUBSCRIPT ‚à• bold_italic_u ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 1 end_POSTSUBSCRIPT italic_f ( bold_italic_u ) . (2.5.1)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.Ex1", "title": "U ‚à© ‚Ñ≥ = F ‚àí 1 ‚Äã ( { 0 } ) U\\cap\\mathcal{M}=F^{-1}(\\{0\\}) italic_U ‚à© caligraphic_M = italic_F start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( { 0 } )", "snippet": "U ‚à© ‚Ñ≥ = F ‚àí 1 ‚Äã ( { 0 } ) U\\cap\\mathcal{M}=F^{-1}(\\{0\\}) italic_U ‚à© caligraphic_M = italic_F start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( { 0 } )"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.Ex2", "title": "T ùíñ ‚Äã ‚Ñ≥ = Ker ‚Å° ( D ‚Äã F ùíñ ) . T_{\\bm{u}}\\mathcal{M}=\\operatorname{Ker}(DF_{\\bm{u}}). italic_T start_POSTSUBSCRIPT bold_italic_u end_POSTSUBSCRIPT caligraphic_M = roman_Ker ( italic_D italic_F start_PO", "snippet": "T ùíñ ‚Äã ‚Ñ≥ = Ker ‚Å° ( D ‚Äã F ùíñ ) . T_{\\bm{u}}\\mathcal{M}=\\operatorname{Ker}(DF_{\\bm{u}}). italic_T start_POSTSUBSCRIPT bold_italic_u end_POSTSUBSCRIPT caligraphic_M = roman_Ker ( italic_D italic_F start_POSTSUBSCRIPT bold_italic_u end_POSTSUBSCRIPT ) ."}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.Ex3", "title": "T ùíñ ‚Äã ùïä d ‚àí 1 = { ùíó ‚àà ‚Ñù d ‚à£ ‚ü® ùíó , ùíñ ‚ü© = 0 } , T_{\\bm{u}}\\mathbb{S}^{d-1}=\\{\\bm{v}\\in\\mathbb{R}^{d}\\mid\\langle\\bm{v},\\bm{u}\\rangle=0\\}, italic_T start_POSTSUBSCRIPT bold_italic_u end_POSTSUBSCRIPT blac", "snippet": "T ùíñ ‚Äã ùïä d ‚àí 1 = { ùíó ‚àà ‚Ñù d ‚à£ ‚ü® ùíó , ùíñ ‚ü© = 0 } , T_{\\bm{u}}\\mathbb{S}^{d-1}=\\{\\bm{v}\\in\\mathbb{R}^{d}\\mid\\langle\\bm{v},\\bm{u}\\rangle=0\\}, italic_T start_POSTSUBSCRIPT bold_italic_u end_POSTSUBSCRIPT blackboard_S start_POSTSUPERSCRIPT italic_d - 1 end_POSTSUPERSCRIPT = { bold_italic_v ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ‚à£ ‚ü® bold_italic_v , bold_italic_u ‚ü© = 0 } ,"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.E2", "title": "grad ‚Äã f ‚Äã ( ùíñ ) = ùë∑ ùíñ ‚üÇ ‚Äã ‚àá f \\mathrm{grad}\\,f(\\bm{u})=\\bm{P}_{\\bm{u}}^{\\perp}\\nabla f roman_grad italic_f ( bold_italic_u ) = bold_italic_P start_POSTSUBSCRIPT bold_italic_u end_POSTSUBSCRIPT start_", "snippet": "grad ‚Äã f ‚Äã ( ùíñ ) = ùë∑ ùíñ ‚üÇ ‚Äã ‚àá f \\mathrm{grad}\\,f(\\bm{u})=\\bm{P}_{\\bm{u}}^{\\perp}\\nabla f roman_grad italic_f ( bold_italic_u ) = bold_italic_P start_POSTSUBSCRIPT bold_italic_u end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚üÇ end_POSTSUPERSCRIPT ‚àá italic_f (2.5.2)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.Ex4", "title": "grad ‚Äã f ‚Äã ( ùíñ ) = ùüé . \\mathrm{grad}\\,f(\\bm{u})=\\mathbf{0}. roman_grad italic_f ( bold_italic_u ) = bold_0 .", "snippet": "grad ‚Äã f ‚Äã ( ùíñ ) = ùüé . \\mathrm{grad}\\,f(\\bm{u})=\\mathbf{0}. roman_grad italic_f ( bold_italic_u ) = bold_0 ."}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.Ex5", "title": "proj ùïä d ‚àí 1 ‚Äã ( ùíó ) ‚âê min ‚Äñ ùíñ ‚Äñ 2 2 = 1 ‚Å° ‚Äñ ùíñ ‚àí ùíó ‚Äñ 2 = ùíó ‚Äñ ùíó ‚Äñ 2 , \\mathrm{proj}_{\\mathbb{S}^{d-1}}(\\bm{v})\\doteq\\min_{\\|\\bm{u}\\|_{2}^{2}=1}\\,\\|\\bm{u}-\\bm{v}\\|_{2}=\\frac{\\bm{v}}{\\|\\bm{v}\\|_{2}}, rom", "snippet": "proj ùïä d ‚àí 1 ‚Äã ( ùíó ) ‚âê min ‚Äñ ùíñ ‚Äñ 2 2 = 1 ‚Å° ‚Äñ ùíñ ‚àí ùíó ‚Äñ 2 = ùíó ‚Äñ ùíó ‚Äñ 2 , \\mathrm{proj}_{\\mathbb{S}^{d-1}}(\\bm{v})\\doteq\\min_{\\|\\bm{u}\\|_{2}^{2}=1}\\,\\|\\bm{u}-\\bm{v}\\|_{2}=\\frac{\\bm{v}}{\\|\\bm{v}\\|_{2}}, roman_proj start_POSTSUBSCRIPT blackboard_S start_POSTSUPERSCRIPT italic_d - 1 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_v ) ‚âê roman_min start_POSTSUBSCRIPT ‚à• bold_italic_u ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 1 end_POSTSUBSCRIPT ‚à• bold_italic_u - bold_italic_v ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = divide start_ARG bold_italic_v end_A"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.E3", "title": "Hess ‚Äã f ‚Äã ( ùíñ ) = ùë∑ ùíñ ‚üÇ ‚Äã ( ‚àá 2 f ‚Äã ( ùíñ ) ‚àí ‚ü® ‚àá f ‚Äã ( ùíñ ) , ùíñ ‚ü© ‚Äã ùë∞ ) ‚Äã ùë∑ ùíñ ‚üÇ . \\mathrm{Hess}\\,f(\\bm{u})=\\bm{P}_{\\bm{u}}^{\\perp}\\left(\\nabla^{2}f(\\bm{u})-\\langle\\nabla f(\\bm{u}),\\bm{u}\\rangle\\bm{I}\\r", "snippet": "Hess ‚Äã f ‚Äã ( ùíñ ) = ùë∑ ùíñ ‚üÇ ‚Äã ( ‚àá 2 f ‚Äã ( ùíñ ) ‚àí ‚ü® ‚àá f ‚Äã ( ùíñ ) , ùíñ ‚ü© ‚Äã ùë∞ ) ‚Äã ùë∑ ùíñ ‚üÇ . \\mathrm{Hess}\\,f(\\bm{u})=\\bm{P}_{\\bm{u}}^{\\perp}\\left(\\nabla^{2}f(\\bm{u})-\\langle\\nabla f(\\bm{u}),\\bm{u}\\rangle\\bm{I}\\right)\\bm{P}_{\\bm{u}}^{\\perp}. roman_Hess italic_f ( bold_italic_u ) = bold_italic_P start_POSTSUBSCRIPT bold_italic_u end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚üÇ end_POSTSUPERSCRIPT ( ‚àá start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_f ( bold_italic_u ) - ‚ü® ‚àá italic_f ( bold_italic_u ) , bold_italic_u ‚ü© bold_italic_I ) bold_italic_P start_POSTSUBSCRIPT bold_italic_u end_POSTSUBSCRIPT start_POSTSU"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.E4", "title": "( ‚àë i = 1 d kurt ( z i ) ‚Äã w i 4 ) ‚Äã ùíò = kurt ( ùíõ ) ‚äô ùíò ‚äô 3 , \\left(\\sum_{i=1}^{d}\\mathop{\\mathrm{kurt}}(z_{i})w_{i}^{4}\\right)\\bm{w}=\\mathop{\\mathrm{kurt}}(\\bm{z})\\mathbin{\\mathchoice{\\raisebox{1.3pt", "snippet": "( ‚àë i = 1 d kurt ( z i ) ‚Äã w i 4 ) ‚Äã ùíò = kurt ( ùíõ ) ‚äô ùíò ‚äô 3 , \\left(\\sum_{i=1}^{d}\\mathop{\\mathrm{kurt}}(z_{i})w_{i}^{4}\\right)\\bm{w}=\\mathop{\\mathrm{kurt}}(\\bm{z})\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\scalebox{0.8}{$\\scriptstyle\\odot$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\odot$}}$}}{\\raisebox{1.3pt}{$\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\scalebox{0.8}{$\\scriptstyle\\odot$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\odot$}}$}}{\\raisebox{0.75pt}{$\\scriptstyl"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.E5", "title": "ùíò S = ‚àë i ‚àà S ¬± 1 kurt ( z i ) ‚Äã ‚àë j ‚àà S 1 kurt ( z j ) ‚Äã ùíÜ i \\bm{w}_{S}=\\sum_{i\\in S}\\pm\\sqrt{\\frac{1}{\\mathop{\\mathrm{kurt}}(z_{i})\\sum_{j\\in S}\\frac{1}{\\mathop{\\mathrm{kurt}}(z_{j})}}}\\bm{e}_{i} bo", "snippet": "ùíò S = ‚àë i ‚àà S ¬± 1 kurt ( z i ) ‚Äã ‚àë j ‚àà S 1 kurt ( z j ) ‚Äã ùíÜ i \\bm{w}_{S}=\\sum_{i\\in S}\\pm\\sqrt{\\frac{1}{\\mathop{\\mathrm{kurt}}(z_{i})\\sum_{j\\in S}\\frac{1}{\\mathop{\\mathrm{kurt}}(z_{j})}}}\\bm{e}_{i} bold_italic_w start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT = ‚àë start_POSTSUBSCRIPT italic_i ‚àà italic_S end_POSTSUBSCRIPT ¬± square-root start_ARG divide start_ARG 1 end_ARG start_ARG roman_kurt ( italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ‚àë start_POSTSUBSCRIPT italic_j ‚àà italic_S end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG roman_kurt ( italic_z start_POSTSUBSCRIPT italic_j"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.E6", "title": "max ùë∏ ‚ä§ ‚Äã ùë∏ = ùë∞ ‚Å° f ‚Äã ( ùë∏ ) . \\max_{\\bm{Q}^{\\top}\\bm{Q}=\\bm{I}}\\,f(\\bm{Q}). roman_max start_POSTSUBSCRIPT bold_italic_Q start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_Q = bold_italic_I end_PO", "snippet": "max ùë∏ ‚ä§ ‚Äã ùë∏ = ùë∞ ‚Å° f ‚Äã ( ùë∏ ) . \\max_{\\bm{Q}^{\\top}\\bm{Q}=\\bm{I}}\\,f(\\bm{Q}). roman_max start_POSTSUBSCRIPT bold_italic_Q start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_Q = bold_italic_I end_POSTSUBSCRIPT italic_f ( bold_italic_Q ) . (2.5.6)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.Ex6", "title": "T ùë∏ ‚Äã ùñÆ ‚Äã ( d ) = { ùë∏ ‚Äã ùõÄ ‚àà ‚Ñù d √ó d ‚à£ ùõÄ ‚ä§ = ‚àí ùõÄ } , T_{\\bm{Q}}\\mathsf{O}(d)=\\{\\bm{Q}\\bm{\\Omega}\\in\\mathbb{R}^{d\\times d}\\mid\\bm{\\Omega}^{\\top}=-\\bm{\\Omega}\\}, italic_T start_POSTSUBSCRIPT bold_italic_", "snippet": "T ùë∏ ‚Äã ùñÆ ‚Äã ( d ) = { ùë∏ ‚Äã ùõÄ ‚àà ‚Ñù d √ó d ‚à£ ùõÄ ‚ä§ = ‚àí ùõÄ } , T_{\\bm{Q}}\\mathsf{O}(d)=\\{\\bm{Q}\\bm{\\Omega}\\in\\mathbb{R}^{d\\times d}\\mid\\bm{\\Omega}^{\\top}=-\\bm{\\Omega}\\}, italic_T start_POSTSUBSCRIPT bold_italic_Q end_POSTSUBSCRIPT sansserif_O ( italic_d ) = { bold_italic_Q bold_Œ© ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d √ó italic_d end_POSTSUPERSCRIPT ‚à£ bold_Œ© start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT = - bold_Œ© } ,"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.Ex7", "title": "ùí´ T ùë∏ ‚Äã ùñÆ ‚Äã ( d ) ‚Äã ( ùö´ ) = ùë∏ ‚Äã skew ‚Å° ( ùë∏ ‚ä§ ‚Äã ùö´ ) , \\mathcal{P}_{T_{\\bm{Q}}\\mathsf{O}(d)}(\\bm{\\Delta})=\\bm{Q}\\operatorname{skew}(\\bm{Q}^{\\top}\\bm{\\Delta}), caligraphic_P start_POSTSUBSCRIPT italic_T ", "snippet": "ùí´ T ùë∏ ‚Äã ùñÆ ‚Äã ( d ) ‚Äã ( ùö´ ) = ùë∏ ‚Äã skew ‚Å° ( ùë∏ ‚ä§ ‚Äã ùö´ ) , \\mathcal{P}_{T_{\\bm{Q}}\\mathsf{O}(d)}(\\bm{\\Delta})=\\bm{Q}\\operatorname{skew}(\\bm{Q}^{\\top}\\bm{\\Delta}), caligraphic_P start_POSTSUBSCRIPT italic_T start_POSTSUBSCRIPT bold_italic_Q end_POSTSUBSCRIPT sansserif_O ( italic_d ) end_POSTSUBSCRIPT ( bold_Œî ) = bold_italic_Q roman_skew ( bold_italic_Q start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_Œî ) ,"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.E7", "title": "grad ‚Äã f ‚Äã ( ùë∏ ) = ùí´ T ùë∏ ‚Äã ùñÆ ‚Äã ( d ) ‚Äã ( ‚àá f ‚Äã ( ùë∏ ) ) \\mathrm{grad}\\,f(\\bm{Q})=\\mathcal{P}_{T_{\\bm{Q}}\\mathsf{O}(d)}\\left(\\nabla f(\\bm{Q})\\right) roman_grad italic_f ( bold_italic_Q ) = caligraphic_P", "snippet": "grad ‚Äã f ‚Äã ( ùë∏ ) = ùí´ T ùë∏ ‚Äã ùñÆ ‚Äã ( d ) ‚Äã ( ‚àá f ‚Äã ( ùë∏ ) ) \\mathrm{grad}\\,f(\\bm{Q})=\\mathcal{P}_{T_{\\bm{Q}}\\mathsf{O}(d)}\\left(\\nabla f(\\bm{Q})\\right) roman_grad italic_f ( bold_italic_Q ) = caligraphic_P start_POSTSUBSCRIPT italic_T start_POSTSUBSCRIPT bold_italic_Q end_POSTSUBSCRIPT sansserif_O ( italic_d ) end_POSTSUBSCRIPT ( ‚àá italic_f ( bold_italic_Q ) ) (2.5.7)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.Ex8", "title": "grad ‚Äã f ‚Äã ( ùë∏ ) = ùüé . \\mathrm{grad}\\,f(\\bm{Q})=\\mathbf{0}. roman_grad italic_f ( bold_italic_Q ) = bold_0 .", "snippet": "grad ‚Äã f ‚Äã ( ùë∏ ) = ùüé . \\mathrm{grad}\\,f(\\bm{Q})=\\mathbf{0}. roman_grad italic_f ( bold_italic_Q ) = bold_0 ."}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.E8", "title": "Hess ‚Äã f ‚Äã ( ùë∏ ) = ùí´ T ùë∏ ‚Äã ùñÆ ‚Äã ( d ) ‚Äã ( ‚àá 2 f ‚Äã ( ùë∏ ) ‚àí sym ‚Å° ( ùë∏ ‚ä§ ‚Äã ‚àá f ‚Äã ( ùë∏ ) ) ‚äó ùë∞ ) ‚Äã ùí´ T ùë∏ ‚Äã ùñÆ ‚Äã ( d ) , \\mathrm{Hess}\\,f(\\bm{Q})=\\mathcal{P}_{T_{\\bm{Q}}\\mathsf{O}(d)}\\left(\\nabla^{2}f(\\bm{Q})", "snippet": "Hess ‚Äã f ‚Äã ( ùë∏ ) = ùí´ T ùë∏ ‚Äã ùñÆ ‚Äã ( d ) ‚Äã ( ‚àá 2 f ‚Äã ( ùë∏ ) ‚àí sym ‚Å° ( ùë∏ ‚ä§ ‚Äã ‚àá f ‚Äã ( ùë∏ ) ) ‚äó ùë∞ ) ‚Äã ùí´ T ùë∏ ‚Äã ùñÆ ‚Äã ( d ) , \\mathrm{Hess}\\,f(\\bm{Q})=\\mathcal{P}_{T_{\\bm{Q}}\\mathsf{O}(d)}\\left(\\nabla^{2}f(\\bm{Q})-\\operatorname{sym}(\\bm{Q}^{\\top}\\nabla f(\\bm{Q}))\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\otimes$}}{\\scalebox{0.8}{$\\textstyle\\otimes$}}{\\scalebox{0.8}{$\\scriptstyle\\otimes$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\otimes$}}$}}{\\raisebox{1.3pt}{$\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\otimes$}}{\\scalebox{0.8}{$\\textstyle\\otimes$}}{\\scalebox{0.8}{"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.Ex9", "title": "Hess ‚Äã f ‚Äã ( ùë∏ ) ‚™Ø ùüé . \\mathrm{Hess}\\,f(\\bm{Q})\\preceq\\mathbf{0}. roman_Hess italic_f ( bold_italic_Q ) ‚™Ø bold_0 .", "snippet": "Hess ‚Äã f ‚Äã ( ùë∏ ) ‚™Ø ùüé . \\mathrm{Hess}\\,f(\\bm{Q})\\preceq\\mathbf{0}. roman_Hess italic_f ( bold_italic_Q ) ‚™Ø bold_0 ."}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.Ex10", "title": "( ùë© ‚ä§ ‚äó ùë® ) ‚Äã vec ‚Å° ( ùëø ) = vec ‚Å° ( ùë® ‚Äã ùëø ‚Äã ùë© ) , (\\bm{B}^{\\top}\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\otimes$}}{\\scalebox{0.8}{$\\textstyle\\otime", "snippet": "( ùë© ‚ä§ ‚äó ùë® ) ‚Äã vec ‚Å° ( ùëø ) = vec ‚Å° ( ùë® ‚Äã ùëø ‚Äã ùë© ) , (\\bm{B}^{\\top}\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\otimes$}}{\\scalebox{0.8}{$\\textstyle\\otimes$}}{\\scalebox{0.8}{$\\scriptstyle\\otimes$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\otimes$}}$}}{\\raisebox{1.3pt}{$\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\otimes$}}{\\scalebox{0.8}{$\\textstyle\\otimes$}}{\\scalebox{0.8}{$\\scriptstyle\\otimes$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\otimes$}}$}}{\\raisebox{0.75pt}{$\\scriptstyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\otimes$}}{\\scalebox{0.8}{$\\textstyle\\otim"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.E9", "title": "proj ùñÆ ‚Äã ( d ) ‚Äã ( ùëø ) ‚âê min ùë∏ ‚àà ùñÆ ‚Äã ( d ) ‚Å° ‚Äñ ùë∏ ‚àí ùëø ‚Äñ F 2 . \\mathrm{proj}_{\\mathsf{O}(d)}(\\bm{X})\\doteq\\min_{\\bm{Q}\\in\\mathsf{O}(d)}\\,\\|\\bm{Q}-\\bm{X}\\|_{F}^{2}. roman_proj start_POSTSUBSCRIPT sansser", "snippet": "proj ùñÆ ‚Äã ( d ) ‚Äã ( ùëø ) ‚âê min ùë∏ ‚àà ùñÆ ‚Äã ( d ) ‚Å° ‚Äñ ùë∏ ‚àí ùëø ‚Äñ F 2 . \\mathrm{proj}_{\\mathsf{O}(d)}(\\bm{X})\\doteq\\min_{\\bm{Q}\\in\\mathsf{O}(d)}\\,\\|\\bm{Q}-\\bm{X}\\|_{F}^{2}. roman_proj start_POSTSUBSCRIPT sansserif_O ( italic_d ) end_POSTSUBSCRIPT ( bold_italic_X ) ‚âê roman_min start_POSTSUBSCRIPT bold_italic_Q ‚àà sansserif_O ( italic_d ) end_POSTSUBSCRIPT ‚à• bold_italic_Q - bold_italic_X ‚à• start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . (2.5.9)"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.Ex11", "title": "proj ùñÆ ‚Äã ( d ) ‚Äã ( ùëø ) = ùëº ‚Äã ùëΩ ‚ä§ , \\mathrm{proj}_{\\mathsf{O}(d)}(\\bm{X})=\\bm{U}\\bm{V}^{\\top}, roman_proj start_POSTSUBSCRIPT sansserif_O ( italic_d ) end_POSTSUBSCRIPT ( bold_italic_X ) = bold_italic_", "snippet": "proj ùñÆ ‚Äã ( d ) ‚Äã ( ùëø ) = ùëº ‚Äã ùëΩ ‚ä§ , \\mathrm{proj}_{\\mathsf{O}(d)}(\\bm{X})=\\bm{U}\\bm{V}^{\\top}, roman_proj start_POSTSUBSCRIPT sansserif_O ( italic_d ) end_POSTSUBSCRIPT ( bold_italic_X ) = bold_italic_U bold_italic_V start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ,"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#S5.Ex14", "title": "ùëº ‚Äã ùëΩ ‚ä§ = proj ùñÆ ‚Äã ( d ) ‚Äã ( ùëø ) . \\bm{U}\\bm{V}^{\\top}=\\mathrm{proj}_{\\mathsf{O}(d)}(\\bm{X}). bold_italic_U bold_italic_V start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT = roman_proj start_POSTSUBSCRIPT s", "snippet": "ùëº ‚Äã ùëΩ ‚ä§ = proj ùñÆ ‚Äã ( d ) ‚Äã ( ùëø ) . \\bm{U}\\bm{V}^{\\top}=\\mathrm{proj}_{\\mathsf{O}(d)}(\\bm{X}). bold_italic_U bold_italic_V start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT = roman_proj start_POSTSUBSCRIPT sansserif_O ( italic_d ) end_POSTSUBSCRIPT ( bold_italic_X ) ."}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#A2.S3.EGx1", "title": "min { ùíõ ~ i } i = 1 N ‚Å° 1 N ‚Äã ‚àë i = 1 N ‚Äñ ùíô i ‚àí ùëº ~ ‚Äã ùíõ ~ i ‚Äñ 2 2 \\displaystyle\\min_{\\{\\tilde{\\bm{z}}_{i}\\}_{i=1}^{N}}\\frac{1}{N}\\sum_{i=1}^{N}\\|\\bm{x}_{i}-\\tilde{\\bm{U}}\\tilde{\\bm{z}}_{i}\\|_{2}^{2} r", "snippet": "min { ùíõ ~ i } i = 1 N ‚Å° 1 N ‚Äã ‚àë i = 1 N ‚Äñ ùíô i ‚àí ùëº ~ ‚Äã ùíõ ~ i ‚Äñ 2 2 \\displaystyle\\min_{\\{\\tilde{\\bm{z}}_{i}\\}_{i=1}^{N}}\\frac{1}{N}\\sum_{i=1}^{N}\\|\\bm{x}_{i}-\\tilde{\\bm{U}}\\tilde{\\bm{z}}_{i}\\|_{2}^{2} roman_min start_POSTSUBSCRIPT { over~ start_ARG bold_italic_z end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ‚à• bo"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#A2.S3.EGx2", "title": "arg ‚Äã min ùëº ~ ‚Å° 1 N ‚Äã ‚àë i = 1 N ‚Äñ ùíô i ‚àí ùëº ~ ‚Äã ùëº ~ ‚ä§ ‚Äã ùíô i ‚Äñ 2 2 \\displaystyle\\operatorname*{arg\\ min}_{\\tilde{\\bm{U}}}\\frac{1}{N}\\sum_{i=1}^{N}\\|\\bm{x}_{i}-\\tilde{\\bm{U}}\\tilde{\\bm{U}}^{\\top}\\bm{x}_{i", "snippet": "arg ‚Äã min ùëº ~ ‚Å° 1 N ‚Äã ‚àë i = 1 N ‚Äñ ùíô i ‚àí ùëº ~ ‚Äã ùëº ~ ‚ä§ ‚Äã ùíô i ‚Äñ 2 2 \\displaystyle\\operatorname*{arg\\ min}_{\\tilde{\\bm{U}}}\\frac{1}{N}\\sum_{i=1}^{N}\\|\\bm{x}_{i}-\\tilde{\\bm{U}}\\tilde{\\bm{U}}^{\\top}\\bm{x}_{i}\\|_{2}^{2} start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT over~ start_ARG bold_italic_U end_ARG end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ‚à• bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over~ start_ARG bold_italic_U end_ARG ove"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#A2.S3.EGx3", "title": "arg ‚Äã min ùëº ~ ‚Å° ùîº ‚Å° ‚Äñ ùíô ‚àí ùëº ~ ‚Äã ùëº ~ ‚ä§ ‚Äã ùíô ‚Äñ 2 2 \\displaystyle\\operatorname*{arg\\ min}_{\\tilde{\\bm{U}}}\\operatorname{\\mathbb{E}}\\|\\bm{x}-\\tilde{\\bm{U}}\\tilde{\\bm{U}}^{\\top}\\bm{x}\\|_{2}^{2} start_OPERAT", "snippet": "arg ‚Äã min ùëº ~ ‚Å° ùîº ‚Å° ‚Äñ ùíô ‚àí ùëº ~ ‚Äã ùëº ~ ‚ä§ ‚Äã ùíô ‚Äñ 2 2 \\displaystyle\\operatorname*{arg\\ min}_{\\tilde{\\bm{U}}}\\operatorname{\\mathbb{E}}\\|\\bm{x}-\\tilde{\\bm{U}}\\tilde{\\bm{U}}^{\\top}\\bm{x}\\|_{2}^{2} start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT over~ start_ARG bold_italic_U end_ARG end_POSTSUBSCRIPT blackboard_E ‚à• bold_italic_x - over~ start_ARG bold_italic_U end_ARG over~ start_ARG bold_italic_U end_ARG start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_x ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = arg ‚Äã max ùëº ~ ‚Å° ùîº ‚Å° ‚Äñ ùëº ~ ‚ä§ ‚Äã ùíô ‚Äñ 2"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#A2.S3.EGx4", "title": "ùëø ‚Äã ( ùëø ‚ä§ ‚Äã ùíñ ) ‚äô 3 = ‚ü® ùíñ , ùëø ‚Äã ( ùëø ‚ä§ ‚Äã ùíñ ) ‚äô 3 ‚ü© ‚èü Œª ‚Äã ùíñ , \\displaystyle\\bm{X}(\\bm{X}^{\\top}\\bm{u})^{\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot", "snippet": "ùëø ‚Äã ( ùëø ‚ä§ ‚Äã ùíñ ) ‚äô 3 = ‚ü® ùíñ , ùëø ‚Äã ( ùëø ‚ä§ ‚Äã ùíñ ) ‚äô 3 ‚ü© ‚èü Œª ‚Äã ùíñ , \\displaystyle\\bm{X}(\\bm{X}^{\\top}\\bm{u})^{\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\scalebox{0.8}{$\\scriptstyle\\odot$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\odot$}}$}}{\\raisebox{1.3pt}{$\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\scalebox{0.8}{$\\scriptstyle\\odot$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\odot$}}$}}{\\raisebox{0.75pt}{$\\scriptstyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#A2.S3.EGx5", "title": "1 N ‚Äã ùëø ‚Äã ( ùëø ‚ä§ ‚Äã ùíñ ) ‚äô 3 ‚àí 3 ‚Äã ùíñ = ( Œª N ‚àí 3 ) ‚Äã ùíñ , \\displaystyle\\frac{1}{N}\\bm{X}(\\bm{X}^{\\top}\\bm{u})^{\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle", "snippet": "1 N ‚Äã ùëø ‚Äã ( ùëø ‚ä§ ‚Äã ùíñ ) ‚äô 3 ‚àí 3 ‚Äã ùíñ = ( Œª N ‚àí 3 ) ‚Äã ùíñ , \\displaystyle\\frac{1}{N}\\bm{X}(\\bm{X}^{\\top}\\bm{u})^{\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\scalebox{0.8}{$\\scriptstyle\\odot$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\odot$}}$}}{\\raisebox{1.3pt}{$\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\scalebox{0.8}{$\\scriptstyle\\odot$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\odot$}}$}}{\\raisebox{0.75pt}{$\\scriptstyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scale"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#A2.S3.EGx6", "title": "1 N ‚Äã ùëø ‚Äã ( ùëø ‚ä§ ‚Äã ùíñ ) ‚äô 3 ‚àí 3 ‚Äã ùíñ ‚Äñ 1 N ‚Äã ùëø ‚Äã ( ùëø ‚ä§ ‚Äã ùíñ ) ‚äô 3 ‚àí 3 ‚Äã ùíñ ‚Äñ 2 = ùíñ . \\displaystyle\\frac{\\frac{1}{N}\\bm{X}(\\bm{X}^{\\top}\\bm{u})^{\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoi", "snippet": "1 N ‚Äã ùëø ‚Äã ( ùëø ‚ä§ ‚Äã ùíñ ) ‚äô 3 ‚àí 3 ‚Äã ùíñ ‚Äñ 1 N ‚Äã ùëø ‚Äã ( ùëø ‚ä§ ‚Äã ùíñ ) ‚äô 3 ‚àí 3 ‚Äã ùíñ ‚Äñ 2 = ùíñ . \\displaystyle\\frac{\\frac{1}{N}\\bm{X}(\\bm{X}^{\\top}\\bm{u})^{\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\scalebox{0.8}{$\\scriptstyle\\odot$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\odot$}}$}}{\\raisebox{1.3pt}{$\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\scalebox{0.8}{$\\scriptstyle\\odot$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\odot$}}$}}{\\raisebox{0.75pt}{$\\scriptstyle\\mathchoice{\\scalebox{0."}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#A2.S3.EGx7", "title": "ùíÅ 1 \\displaystyle\\bm{Z}_{1} bold_italic_Z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ‚àº \\displaystyle\\sim ‚àº ùí© ‚Å° ( ùüé , ùë∞ ) , \\displaystyle\\operatorname{\\mathcal{N}}(\\bm{0},\\bm{I}), caligraphic_N ( bold_0 ,", "snippet": "ùíÅ 1 \\displaystyle\\bm{Z}_{1} bold_italic_Z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ‚àº \\displaystyle\\sim ‚àº ùí© ‚Å° ( ùüé , ùë∞ ) , \\displaystyle\\operatorname{\\mathcal{N}}(\\bm{0},\\bm{I}), caligraphic_N ( bold_0 , bold_italic_I ) , (2.3.8) ùíÅ t + 1 \\displaystyle\\bm{Z}_{t+1} bold_italic_Z start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT = \\displaystyle= = S Œ∑ ‚Äã Œª ‚Äã ( ùíÅ t ‚àí 2 ‚Äã Œ∑ ‚Äã ùë® ‚ä§ ‚Äã ( ùë® ‚Äã ùíÅ t ‚àí ùëø ) ) , ‚àÄ t ‚â• 1 , \\displaystyle S_{\\eta\\lambda}\\left(\\bm{Z}_{t}-2\\eta\\bm{A}^{\\top}(\\bm{A}\\bm{Z}_{t}-\\bm{X})\\right),\\quad\\forall t\\geq 1, italic_S start_POSTSUBSCRIPT italic_Œ∑ italic_Œª end_POSTSUBSCRIPT ( bold_it"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#A2.S3.EGx8", "title": "S Œ± ‚Äã ( x ) \\displaystyle S_{\\alpha}(x) italic_S start_POSTSUBSCRIPT italic_Œ± end_POSTSUBSCRIPT ( italic_x ) ‚âê { x ‚àí Œ± , x ‚â• Œ± , 0 , ‚àí Œ± < x < Œ± , x + Œ± , x ‚â§ ‚àí Œ± \\displaystyle\\doteq\\begin{cases}x-\\al", "snippet": "S Œ± ‚Äã ( x ) \\displaystyle S_{\\alpha}(x) italic_S start_POSTSUBSCRIPT italic_Œ± end_POSTSUBSCRIPT ( italic_x ) ‚âê { x ‚àí Œ± , x ‚â• Œ± , 0 , ‚àí Œ± < x < Œ± , x + Œ± , x ‚â§ ‚àí Œ± \\displaystyle\\doteq\\begin{cases}x-\\alpha,&x\\geq\\alpha,\\\\ 0,&-\\alpha<x<\\alpha,\\\\ x+\\alpha,&x\\leq-\\alpha\\end{cases} ‚âê { start_ROW start_CELL italic_x - italic_Œ± , end_CELL start_CELL italic_x ‚â• italic_Œ± , end_CELL end_ROW start_ROW start_CELL 0 , end_CELL start_CELL - italic_Œ± < italic_x < italic_Œ± , end_CELL end_ROW start_ROW start_CELL italic_x + italic_Œ± , end_CELL start_CELL italic_x ‚â§ - italic_Œ± end_CELL end_ROW (2.3.10) = sign ‚Å° "}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#A2.S3.EGx9", "title": "ùíÅ ‚Ñì + 1 = S Œ∑ ‚Äã Œª ‚Äã ( ùíÅ ‚Ñì ‚àí 2 ‚Äã Œ∑ ‚Äã ùë® + ‚ä§ ‚Äã ( ùë® + ‚Äã ùíÅ ‚Ñì ‚àí ùëø ) ) , ùíÅ 1 = ùüé , ‚àÄ ‚Ñì ‚àà [ L ] \\displaystyle\\bm{Z}^{\\ell+1}=S_{\\eta\\lambda}\\left(\\bm{Z}^{\\ell}-2\\eta\\bm{A}_{+}^{\\top}(\\bm{A}_{+}\\bm{Z}^{\\ell}-\\", "snippet": "ùíÅ ‚Ñì + 1 = S Œ∑ ‚Äã Œª ‚Äã ( ùíÅ ‚Ñì ‚àí 2 ‚Äã Œ∑ ‚Äã ùë® + ‚ä§ ‚Äã ( ùë® + ‚Äã ùíÅ ‚Ñì ‚àí ùëø ) ) , ùíÅ 1 = ùüé , ‚àÄ ‚Ñì ‚àà [ L ] \\displaystyle\\bm{Z}^{\\ell+1}=S_{\\eta\\lambda}\\left(\\bm{Z}^{\\ell}-2\\eta\\bm{A}_{+}^{\\top}(\\bm{A}_{+}\\bm{Z}^{\\ell}-\\bm{X})\\right),\\quad\\bm{Z}^{1}=\\mathbf{0},\\quad\\forall\\ell\\in[L] bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì + 1 end_POSTSUPERSCRIPT = italic_S start_POSTSUBSCRIPT italic_Œ∑ italic_Œª end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT - 2 italic_Œ∑ bold_italic_A start_POSTSUBSCRIPT + end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ( bold_italic_A start_PO"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#A2.S3.EGx10", "title": "ùíÅ ‚Ñì + 1 = S Œ∑ ‚Äã Œª ‚Äã ( ùíÅ ‚Ñì ‚àí 2 ‚Äã Œ∑ ‚Äã ( ùë® ‚Ñì ) ‚ä§ ‚Äã ( ùë® ‚Ñì ‚Äã ùíÅ ‚Ñì ‚àí ùëø ) ) , ‚àÄ ‚Ñì ‚àà [ L ] . \\displaystyle\\bm{Z}^{\\ell+1}=S_{\\eta\\lambda}\\left(\\bm{Z}^{\\ell}-2\\eta(\\bm{A}^{\\ell})^{\\top}(\\bm{A}^{\\ell}\\bm{Z}^{\\el", "snippet": "ùíÅ ‚Ñì + 1 = S Œ∑ ‚Äã Œª ‚Äã ( ùíÅ ‚Ñì ‚àí 2 ‚Äã Œ∑ ‚Äã ( ùë® ‚Ñì ) ‚ä§ ‚Äã ( ùë® ‚Ñì ‚Äã ùíÅ ‚Ñì ‚àí ùëø ) ) , ‚àÄ ‚Ñì ‚àà [ L ] . \\displaystyle\\bm{Z}^{\\ell+1}=S_{\\eta\\lambda}\\left(\\bm{Z}^{\\ell}-2\\eta(\\bm{A}^{\\ell})^{\\top}(\\bm{A}^{\\ell}\\bm{Z}^{\\ell}-\\bm{X})\\right),\\quad\\forall\\ell\\in[L]. bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì + 1 end_POSTSUPERSCRIPT = italic_S start_POSTSUBSCRIPT italic_Œ∑ italic_Œª end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT - 2 italic_Œ∑ ( bold_italic_A start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ( bold_italic_A start_POSTSUPERS"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#A2.S3.EGx11", "title": "ùíÅ 1 = ùüé , ( ùë® 1 ) j ‚àº i . i . d . ùí© ‚Å° ( ùüé , 1 D ‚Äã ùë∞ ) , ‚àÄ j ‚àà [ m ] , \\displaystyle\\bm{Z}^{1}=\\mathbf{0},\\quad(\\bm{A}_{1})_{j}\\stackrel{{\\scriptstyle\\mathrm{i.i.d.}}}{{\\sim}}\\operatorname{\\mathcal{N}}", "snippet": "ùíÅ 1 = ùüé , ( ùë® 1 ) j ‚àº i . i . d . ùí© ‚Å° ( ùüé , 1 D ‚Äã ùë∞ ) , ‚àÄ j ‚àà [ m ] , \\displaystyle\\bm{Z}^{1}=\\mathbf{0},\\quad(\\bm{A}_{1})_{j}\\stackrel{{\\scriptstyle\\mathrm{i.i.d.}}}{{\\sim}}\\operatorname{\\mathcal{N}}(\\bm{0},\\tfrac{1}{D}\\bm{I}),\\enspace\\forall j\\in[m], bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT = bold_0 , ( bold_italic_A start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_RELOP SUPERSCRIPTOP start_ARG ‚àº end_ARG start_ARG roman_i . roman_i . roman_d . end_ARG end_RELOP caligraphic_N ( bold_0 , divide start_ARG 1 end_ARG start_ARG italic_"}, {"page": "Chapter 2 Learning Linear and Independent Structures", "href": "Ch2.html#A2.S3.EGx12", "title": "( ùë∏ ‚ä§ ‚Äã ùëø ) ‚ä§ \\displaystyle\\left(\\bm{Q}^{\\top}\\bm{X}\\right)^{\\top} ( bold_italic_Q start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_X ) start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT = ùë∏ ‚ä§ ‚Äã ùëø , \\", "snippet": "( ùë∏ ‚ä§ ‚Äã ùëø ) ‚ä§ \\displaystyle\\left(\\bm{Q}^{\\top}\\bm{X}\\right)^{\\top} ( bold_italic_Q start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_X ) start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT = ùë∏ ‚ä§ ‚Äã ùëø , \\displaystyle=\\bm{Q}^{\\top}\\bm{X}, = bold_italic_Q start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_X , ùë∏ ‚ä§ ‚Äã ùëø \\displaystyle\\bm{Q}^{\\top}\\bm{X} bold_italic_Q start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_X ‚™∞ ùüé . \\displaystyle\\succeq\\mathbf{0}. ‚™∞ bold_0 ."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#top", "title": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "snippet": ""}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S1", "title": "3.1 Entropy Minimization and Compression", "snippet": "3.1 Entropy Minimization and Compression 3.1.1 Entropy and Coding Rate In Chapter 1 , we have mentioned that the goal of learning is to find the simplest way to generate a given set of data. Conceptually, the Kolmogorov complexity was intended to provide such a measure of complexity but it is not computable and not associated with any implementable scheme that can actually reproduce the data. Hence we need an alternative, computable, and realizable, measure of complexity. That leads us to the notion of entropy , introduced by Shannon in 1948 [ Sha48 ] . To illustrate the constructive nature of"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2", "title": "3.2 Compression via Denoising", "snippet": "3.2 Compression via Denoising In this section, we will describe a natural and computationally tractable way to learn a distribution p ‚Äã ( ùíô ) p(\\bm{x}) italic_p ( bold_italic_x ) by way of learning a parametric encoding of our distribution such that the representation has the minimum entropy or coding rate, then using this encoding to transform high-entropy samples from a standard Gaussian into low-entropy samples from the target distribution, as illustrated in Figure 3.2 . This presents a methodology that utilizes both approaches above in order to learn and sample from the distribution. Figur"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3", "title": "3.3 Compression via Lossy Coding", "snippet": "3.3 Compression via Lossy Coding Let us recap what we have covered so far. We have discussed how to fit a denoiser ùíô ¬Ø Œ∏ \\bar{\\bm{x}}_{\\theta} over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT using finite samples. We showed that this denoiser encodes a distribution in that it is directly connected to its log-density via Tweedie‚Äôs formula ( 3.2.20 ). Then, we used it to gradually transform a pure noise (high-entropy) distribution towards the learned distribution via iterative denoising . Thus, we have developed the first way of learning or pursuing a distribu"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4", "title": "3.4 Maximizing Information Gain", "snippet": "3.4 Maximizing Information Gain So far in this chapter, we have discussed how to identify a distribution with low-dimensional structures through the principle of compression. As we have seen from the previous two sections, computational compression can be realized through either the denoising operation or through clustering. Figure 3.17 illustrates this concept with our favorite example. Figure 3.17 : Identify a low-dimensional distribution with two subspaces (left) via denoising or clustering, starting from a generic random Gaussian distribution (right). Of course, the ultimate goal for ident"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S5", "title": "3.5 Summary and Notes", "snippet": "3.5 Summary and Notes The use of denoising and diffusion for sampling has a rich history. The first work which is clearly about a diffusion model is probably [ SWM+15 ] , but before this there are many works about denoising as a computational and statistical problem. The most relevant of these is probably [ Hyv05 ] , which explicitly uses the score function to denoise (as well as perform independent component analysis). The most popular follow-ups are basically co-occurring: [ HJA20 , SE19 ] . Since then, thousands of papers have built on diffusion models; we will revisit this topic in Chapter"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S6", "title": "3.6 Exercises and Extensions", "snippet": "3.6 Exercises and Extensions Exercise 3.1 . Please show that ( 3.2.4 ) is the optimal solution of Problem ( 3.2.3 ). Exercise 3.2 . Consider random vectors ùíô ‚àà ‚Ñù D \\bm{x}\\in\\mathbb{R}^{D} bold_italic_x ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT and ùíö ‚àà ‚Ñù d \\bm{y}\\in\\mathbb{R}^{d} bold_italic_y ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , such that the pair ( ùíô , ùíö ) ‚àà ‚Ñù D + d (\\bm{x},\\bm{y})\\in\\mathbb{R}^{D+d} ( bold_italic_x , bold_italic_y ) ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D + italic_d end_POSTSUPERSCRIPT is jointly Gaussian. This mea"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S1.SS1", "title": "3.1.1 Entropy and Coding Rate", "snippet": "3.1.1 Entropy and Coding Rate In Chapter 1 , we have mentioned that the goal of learning is to find the simplest way to generate a given set of data. Conceptually, the Kolmogorov complexity was intended to provide such a measure of complexity but it is not computable and not associated with any implementable scheme that can actually reproduce the data. Hence we need an alternative, computable, and realizable, measure of complexity. That leads us to the notion of entropy , introduced by Shannon in 1948 [ Sha48 ] . To illustrate the constructive nature of entropy, let us start with the simplest "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S1.SS2", "title": "3.1.2 Differential Entropy", "snippet": "3.1.2 Differential Entropy When the random variable ùíô ‚àà ‚Ñù D \\bm{x}\\in\\mathbb{R}^{D} bold_italic_x ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT is continuous and has a probability density p p italic_p , one may view that the limit of the above sum ( 3.1.1 ) is related to an integral: h ‚Äã ( ùíô ) ‚âê ùîº ‚Å° [ log ‚Å° 1 / p ‚Äã ( ùíô ) ] = ‚àí ‚à´ ‚Ñù D p ‚Äã ( ùùÉ ) ‚Äã log ‚Å° p ‚Äã ( ùùÉ ) ‚Äã d ùùÉ . h(\\bm{x})\\doteq\\operatorname{\\mathbb{E}}[\\log 1/p(\\bm{x})]=-\\int_{\\mathbb{R}^{D}}p(\\bm{\\xi})\\log p(\\bm{\\xi})\\mathrm{d}\\bm{\\xi}. italic_h ( bold_italic_x ) ‚âê blackboard_E [ roman_log 1 / italic_p ( bold_italic_"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S1.SS3", "title": "3.1.3 Minimizing Coding Rate", "snippet": "3.1.3 Minimizing Coding Rate Remember that the learning problem entails the recovery of a (potentially continuous) distribution p ‚Äã ( ùíô ) p(\\bm{x}) italic_p ( bold_italic_x ) from a set of samples { ùíô 1 , ‚Ä¶ , ùíô N } \\{\\bm{x}_{1},\\ldots,\\bm{x}_{N}\\} { bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT } drawn from the distribution. For ease of exposition, we write ùëø = [ ùíô 1 , ‚Ä¶ , ùíô N ] ‚àà ‚Ñù D √ó N \\bm{X}=[\\bm{x}_{1},\\ldots,\\bm{x}_{N}]\\in\\mathbb{R}^{D\\times N} bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIP"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.SS1", "title": "3.2.1 Diffusion and Denoising Processes", "snippet": "3.2.1 Diffusion and Denoising Processes We first want to find a procedure to decrease the entropy of a given very noisy sample into a lower-entropy sample from the data distribution. Here, we describe a potential approach‚Äîone of many, but perhaps the most natural way to attack this problem. First, we find a way to gradually increase the entropy of existing samples from the data distribution. Then, we find an approximate inverse of this process. But in general, the operation of increasing entropy does not have an inverse, as information from the original distribution may be destroyed. We will t"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.SS2", "title": "3.2.2 Learning and Sampling a Distribution via Iterative Denoising", "snippet": "3.2.2 Learning and Sampling a Distribution via Iterative Denoising Remember that at the end of Section 3.1.3 , we discussed a pair of desiderata for pursuing a distribution with low-dimensional structure. The first such desideratum is to start with a normal distribution, say with high entropy, and gradually reduce its entropy until it reaches the distribution of the data. We will call this procedure sampling since we are generating new samples. It is now time for us to discuss how to do this with the toolkit we have built up. We know how to denoise very noisy samples ùíô T \\bm{x}_{T} bold_italic"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS1", "title": "3.3.1 Necessity of Lossy Coding", "snippet": "3.3.1 Necessity of Lossy Coding We have previously, multiple times, discussed a difficulty: if we learn the distribution from finite samples in the end, and our function class of denoisers contains enough functions, how do we ensure that we sample from the true distribution (with low-dimensional supports), instead of any other distribution that may produce those finite samples with high probability? Let us reveal some of the conceptual and technical difficulties with some concrete examples. Example 3.4 (Volume, Dimension, and Entropy) . For the example shown on the top of Figure 3.8 , suppose "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS2", "title": "3.3.2 Rate Distortion and Data Geometry", "snippet": "3.3.2 Rate Distortion and Data Geometry Of course, among all encoding schemes that satisfy the above constraint, we would like to choose the one that minimizes the resulting coding rate. For a given random variable ùíô \\bm{x} bold_italic_x and a precision œµ \\epsilon italic_œµ , this rate is known as the rate distortion , denoted as ‚Ñõ œµ ‚Äã ( ùíô ) \\mathcal{R}_{\\epsilon}(\\bm{x}) caligraphic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_x ) . A deep theorem in information theory, originally proved by [ Sha59 ] , establishes that this rate can be expressed equivalently in purely probabi"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS3", "title": "3.3.3 Lossy Coding Rate for a Low-Dimensional Gaussian", "snippet": "3.3.3 Lossy Coding Rate for a Low-Dimensional Gaussian Now suppose we are given a set of data samples in ùëø = [ ùíô 1 , ‚Ä¶ , ùíô N ] \\bm{X}=[\\bm{x}_{1},\\ldots,\\bm{x}_{N}] bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ] from any distribution. 17 17 17 Or these data points could be viewed as an (empirical) distribution themselves. We would like to come up with a constructive scheme that can encode the data up to certain precision, say ùíô i ‚Ü¶ ùíô ^ i , subject to ‚Äñ ùíô i ‚àí ùíô ^ i ‚Äñ 2 ‚â§ œµ . \\bm{x}_{i}\\mapsto\\hat{\\bm{x"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS4", "title": "3.3.4 Clustering a Mixture of Low-Dimensional Gaussians", "snippet": "3.3.4 Clustering a Mixture of Low-Dimensional Gaussians As we have discussed before, the given dataset ùëø \\bm{X} bold_italic_X often has low-dimensional intrinsic structures. Hence, encoding it as a general Gaussian would be very redundant. If we can identify those intrinsic structures in ùëø \\bm{X} bold_italic_X , we could design much better coding schemes that give much lower coding rates. Or equivalently, the codes used to encode such ùëø \\bm{X} bold_italic_X can be compressed. We will see that compression gives a unifying computable way to identify such structures. In this section, we demonstra"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS1", "title": "3.4.1 Linear Discriminative Representations", "snippet": "3.4.1 Linear Discriminative Representations Suppose that ùíô ‚àà ‚Ñù D \\bm{x}\\in\\mathbb{R}^{D} bold_italic_x ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT is a random vector drawn from a mixture of K K italic_K (component) distributions ùíü = { ùíü k } k = 1 K \\mathcal{D}=\\{\\mathcal{D}_{k}\\}_{k=1}^{K} caligraphic_D = { caligraphic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT . Give a finite set of i.i.d. samples ùëø = [ ùíô 1 , ùíô 2 , ‚Ä¶ , ùíô N ] ‚àà ‚Ñù D √ó N \\bm{X}=[\\bm{x}_{1},\\bm{x}_{2"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS2", "title": "3.4.2 The Principle of Maximal Coding Rate Reduction", "snippet": "3.4.2 The Principle of Maximal Coding Rate Reduction Although the three properties‚Äî between-class discriminative , within-class compressible , and maximally diverse representation ‚Äîfor linear discriminative representations (LDRs) are all highly desired properties of the learned representation ùíõ \\bm{z} bold_italic_z , they are by no means easy to obtain: Are these properties compatible so that we can expect to achieve them all at once? If so, is there a simple but principled objective that can measure the goodness of the resulting representations in terms of all these properties? The key to the"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS3", "title": "3.4.3 Optimization Properties of Coding Rate Reduction", "snippet": "3.4.3 Optimization Properties of Coding Rate Reduction In this subsection, we study the optimization properties of the MCR 2 function by analyzing its optimal solutions and the structure of its optimization landscape. To get around the technical difficulty introduced by the neural networks, we consider a simplified version of Problem ( 3.4.12 ) as follows: max ùíÅ R œµ ( ùíÅ ) ‚àí R œµ c ( ùíÅ ) s . t . ‚à• ùíÅ k ‚à• F 2 = N k , k = 1 , ‚Ä¶ , K . \\displaystyle\\max_{\\bm{Z}}\\ R_{\\epsilon}(\\bm{Z})-R_{\\epsilon}^{c}(\\bm{Z})\\qquad\\mathrm{s.t.}\\quad\\|\\bm{Z}_{k}\\|_{F}^{2}=N_{k},\\ k=1,\\dots,K. roman_max start_POSTSUBSCR"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.SS2.SSS0.Px1", "title": "Step 1: different discretizations.", "snippet": "Step 1: different discretizations. The first step we do is motivated by the following point: we do not need to spend so many denoising iterations at large t t italic_t . If we look at Figure 3.5 , we observe that the first 200 200 200 or 300 300 300 iterations, out of the 500 500 500 iterations of the sampling process, are just spent contracting the noise towards the data distribution as a whole, before the remaining iterations push the samples towards a subspace. Given a fixed iteration count L L italic_L , this signals that we should spend more timesteps t ‚Ñì t_{\\ell} italic_t start_POSTSUBSC"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.SS2.SSS0.Px2", "title": "Step 2: different noise models.", "snippet": "Step 2: different noise models. The second step is to consider slightly different models compared to ( 3.2.1 ). The basic motivation for this is as follows. In practice, the noise distribution ùí© ‚Å° ( ùüé , t L 2 ‚Äã ùë∞ ) \\operatorname{\\mathcal{N}}(\\bm{0},t_{L}^{2}\\bm{I}) caligraphic_N ( bold_0 , italic_t start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) becomes an increasingly poor estimate of the true covariance in high dimensions, i.e., ( 3.2.65 ) becomes an increasingly worse approximation, especially with anisotropic high-dimensional data."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.SS2.SSS0.Px3", "title": "Step 3: optimizing training pipelines.", "snippet": "Step 3: optimizing training pipelines. If we use the procedure dictated by Section 3.2.1 to learn a separate denoiser ùíô ¬Ø ‚Äã ( t , ‚ãÖ ) \\bar{\\bm{x}}(t,\\cdot) over¬Ø start_ARG bold_italic_x end_ARG ( italic_t , ‚ãÖ ) for each time t t italic_t to be used in the sampling algorithm, we would have to learn L L italic_L separate denoisers! This is highly inefficient‚Äîthe usual case is that we have to train L L italic_L separate neural networks, taking up L L italic_L times the training time and storage memory, and then be locked into using these timesteps for sampling forever. Instead, we can train a sin"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.SS2.SSS0.Px4", "title": "(Optional) Step 4: changing the estimation target.", "snippet": "(Optional) Step 4: changing the estimation target. Note that it is common to instead reorient the whole denoising pipeline around noise predictors , i.e., estimates of ùîº ‚Å° [ ùíà ‚à£ ùíô t ] \\operatorname{\\mathbb{E}}[\\bm{g}\\mid\\bm{x}_{t}] blackboard_E [ bold_italic_g ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] . In practice, noise predictors are slightly easier to train because their output is (almost) always of comparable size to a Gaussian random variable, so training is more numerically stable. Note that by ( 3.2.69 ) we have ùíô t = Œ± t ‚Äã ùîº ‚Å° [ ùíô ‚à£ ùíô t ] + œÉ t ‚Äã ùîº ‚Å° [ ùíà ‚à£ ùíô t ]"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS4.SSS0.Px1", "title": "The clustering problem.", "snippet": "The clustering problem. Now for this specific family of distributions, how can we effectively and efficiently identify those low-dimensional components from a set of samples ùëø = [ ùíô 1 , ùíô 2 , ‚Ä¶ , ùíô N ] , \\bm{X}=\\left[\\bm{x}_{1},\\bm{x}_{2},\\ldots,\\bm{x}_{N}\\right], bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ] , (3.3.25) drawn from them? In other words, given the whole data set ùëø \\bm{X} bold_italic_X , we want to partition, or cluster, it into mu"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS4.SSS0.Px2", "title": "Clustering via lossy compression.", "snippet": "Clustering via lossy compression. The main difficulty in solving the above clustering problem is that we normally do not know the number of clusters K K italic_K , nor do we know the dimension of each component. There has been a long history for the study of this clustering problem. The textbook [ VMS16 ] gives a systematic and comprehensive coverage of different approaches to this problem. To find an effective approach to this problem, we first need to understand and clarify why we want to cluster. In other words, what exactly do we gain from clustering the data, compared with not to? How do "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS4.SSS0.Px3", "title": "Optimization strategies to cluster.", "snippet": "Optimization strategies to cluster. The remaining question is how we optimize the above coding rate objective to find the optimal clusters. There are three natural approaches to this objective: 1. We may start with the whole set ùëø \\bm{X} bold_italic_X as a single cluster (i.e. the lazy regime) and then search (say randomly) to partition it so that it would lead to a smaller coding rate. 2. Inversely, we may start with each sample ùíô i \\bm{x}_{i} bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT as its own cluster (i.e. the memorization regime) and search to merge clusters that would "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS0.SSS0.Px1", "title": "How to measure the goodness of representations.", "snippet": "How to measure the goodness of representations. One may view a given dataset as samples of a random vector ùíô \\bm{x} bold_italic_x with a certain distribution in a high-dimensional space, say ‚Ñù D \\mathbb{R}^{D} blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT . Typically, the distribution of ùíô \\bm{x} bold_italic_x has a much lower intrinsic dimension than the ambient space. Generally speaking, learning a representation refers to learning a continuous mapping, say f ‚Äã ( ‚ãÖ ) f(\\cdot) italic_f ( ‚ãÖ ) , that transforms ùíô \\bm{x} bold_italic_x to a so-called feature vector ùíõ \\bm{z} bold"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS1.SSS0.Px1", "title": "Encoding class information via cross entropy.", "snippet": "Encoding class information via cross entropy. Extensive studies have shown that for many practical datasets (e.g., images, audio, and natural languages), the (encoding) mapping from the data ùíô \\bm{x} bold_italic_x to its class label ùíö \\bm{y} bold_italic_y can be effectively modeled by training a deep network, 26 26 26 Here let us not worry about yet which network we should use here and why. The purpose here is to consider any empirically tested deep network. We will leave the justification of the network architectures to the next chapter. here denoted as f ‚Äã ( ùíô , Œ∏ ) : ùíô ‚Ü¶ ùíö f(\\bm{x},\\theta):"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS1.SSS0.Px2", "title": "Minimal discriminative features via information bottleneck.", "snippet": "Minimal discriminative features via information bottleneck. One popular approach to interpret the role of deep networks is to view outputs of intermediate layers of the network as selecting certain latent features ùíõ = f ‚Äã ( ùíô , Œ∏ ) ‚àà ‚Ñù d \\bm{z}=f(\\bm{x},\\theta)\\in\\mathbb{R}^{d} bold_italic_z = italic_f ( bold_italic_x , italic_Œ∏ ) ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT of the data that are discriminative among multiple classes. Learned representations ùíõ \\bm{z} bold_italic_z then facilitate the subsequent classification task for predicting the class label ùíö \\bm{y} bol"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS1.SSS0.Px3", "title": "Linear discriminative representations.", "snippet": "Linear discriminative representations. Whether the given data ùëø \\bm{X} bold_italic_X of a mixed distribution ùíü \\mathcal{D} caligraphic_D can be effectively classified or clustered depends on how separable (or discriminative) the component distributions ùíü k \\mathcal{D}_{k} caligraphic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT are (or can be made). One popular working assumption is that the distribution of each class has relatively low-dimensional intrinsic structures. Hence we may assume that the distribution ùíü k \\mathcal{D}_{k} caligraphic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIP"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS2.SSS0.Px1", "title": "Coding rate of features.", "snippet": "Coding rate of features. Notably, a practical challenge in evaluating the coding rate is that the underlying distribution of the feature representations ùíÅ \\bm{Z} bold_italic_Z is typically unknown. To address this, we may approximate the features ùíÅ = [ ùíõ 1 , ‚Ä¶ , ùíõ N ] \\bm{Z}=[\\bm{z}_{1},\\ldots,\\bm{z}_{N}] bold_italic_Z = [ bold_italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_z start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ] as samples drawn from a multivariate Gaussian distribution. Under this assumption, as discussed in Chapter 3.3.3 , the compactness of the features ùíÅ \\bm"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS3.SSS0.Px1", "title": "Regularized MCR 2 .", "snippet": "Regularized MCR 2 . The above theorem characterizes properties of the global optima of the rate reduction objectives. What about other optima, such as local ones? Due to the constraints of the Frobenius norm, it is a difficult task to analyze Problem ( 3.4.15 ) from an optimization-theoretic perspective. Therefore, we consider the Lagrangian formulation of ( 3.4.15 ). This can be viewed as a tight relaxation or even an equivalent problem of ( 3.4.15 ) whose optimal solutions agree under specific settings of the regularization parameter; see [ WLP+24 , Proposition 1] . Specifically, the formula"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#p1", "title": "‚Äú We compress to learn, and we learn to compress .‚Äù ‚Äî High-dimensional Data Analysis, Wright and Ma, 2022", "snippet": "‚Äú We compress to learn, and we learn to compress .‚Äù ‚Äî High-dimensional Data Analysis, Wright and Ma, 2022"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#p2", "title": "In Chapter 2 , we have shown how to learn simple classes of distributions whose supports are assumed to be either a single or a mixture of low-dimensional subspaces or low-rank Gaussians. For further ", "snippet": "In Chapter 2 , we have shown how to learn simple classes of distributions whose supports are assumed to be either a single or a mixture of low-dimensional subspaces or low-rank Gaussians. For further simplicity, the different (hidden) linear or Gaussian modes are assumed to be orthogonal or independent 1 1 1 Or can be easily reduced to such idealistic cases. , as illustrated in Figure 2.4 . As we have shown, for such special distributions, one can derive rather simple and effective learning algorithms with correctness and efficiency guarantees. The geometric and statistical interpretation of o"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#p3", "title": "In practice, both linearity and independence are rather idealistic assumptions that distributions of real-world high-dimensional data rarely satisfy. The only thing that we may assume is that the intr", "snippet": "In practice, both linearity and independence are rather idealistic assumptions that distributions of real-world high-dimensional data rarely satisfy. The only thing that we may assume is that the intrinsic dimension of the distribution is very low compared to the dimension of the ambient space in which the data are embedded. Hence, in this chapter, we show how to learn a more general class of low-dimensional distributions in a high-dimensional space that is not necessarily (piecewise) linear."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#p4", "title": "It is typical that the distribution of real data often contains multiple components or modes, say corresponding to different classes of objects in the case of images. These modes might not be statisti", "snippet": "It is typical that the distribution of real data often contains multiple components or modes, say corresponding to different classes of objects in the case of images. These modes might not be statistically independent and they may even have different intrinsic dimensions. It is also typical that we have access to only a finite number of samples of the distribution. Therefore, in general, we may assume our data are distributed on a mixture of (nonlinear) low-dimensional submanifolds in a high-dimensional space. Figure 3.1 illustrates an example of such a distribution."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#p5", "title": "To learn such a distribution under such conditions, there are several fundamental questions that we need to address: ‚Ä¢ What is a general approach to learn a general low-dimensional distribution in a h", "snippet": "To learn such a distribution under such conditions, there are several fundamental questions that we need to address: ‚Ä¢ What is a general approach to learn a general low-dimensional distribution in a high-dimensional space and represent the learned distribution? ‚Ä¢ How do we measure the complexity of the resulting representation so that we can effectively exploit the low dimensionality to learn? ‚Ä¢ How do we make the learning process computationally tractable and even scalable, as the ambient dimension is usually high and the number of samples typically large? As we will see, the fundamental idea"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S0.I1.i1.p1", "title": "What is a general approach to learn a general low-dimensional distribution in a high-dimensional space and represent the learned distribution?", "snippet": "What is a general approach to learn a general low-dimensional distribution in a high-dimensional space and represent the learned distribution?"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S0.I1.i2.p1", "title": "How do we measure the complexity of the resulting representation so that we can effectively exploit the low dimensionality to learn?", "snippet": "How do we measure the complexity of the resulting representation so that we can effectively exploit the low dimensionality to learn?"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S0.I1.i3.p1", "title": "How do we make the learning process computationally tractable and even scalable, as the ambient dimension is usually high and the number of samples typically large?", "snippet": "How do we make the learning process computationally tractable and even scalable, as the ambient dimension is usually high and the number of samples typically large?"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#p6", "title": "Due to its theoretical and practical significance, we will study in greater depth how this general framework of learning low-dimensional distributions via compression substantiates when the distributi", "snippet": "Due to its theoretical and practical significance, we will study in greater depth how this general framework of learning low-dimensional distributions via compression substantiates when the distribution of interest can be well-modeled or approximated by a mixture of low-dimensional subspaces or low-rank Gaussians."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S1.SS1.p1", "title": "In Chapter 1 , we have mentioned that the goal of learning is to find the simplest way to generate a given set of data. Conceptually, the Kolmogorov complexity was intended to provide such a measure o", "snippet": "In Chapter 1 , we have mentioned that the goal of learning is to find the simplest way to generate a given set of data. Conceptually, the Kolmogorov complexity was intended to provide such a measure of complexity but it is not computable and not associated with any implementable scheme that can actually reproduce the data. Hence we need an alternative, computable, and realizable, measure of complexity. That leads us to the notion of entropy , introduced by Shannon in 1948 [ Sha48 ] ."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S1.SS1.p2", "title": "To illustrate the constructive nature of entropy, let us start with the simplest case. Suppose that we have a discrete random variable that takes N N italic_N distinct values, or tokens , { ùíô 1 , ‚Ä¶ , ", "snippet": "To illustrate the constructive nature of entropy, let us start with the simplest case. Suppose that we have a discrete random variable that takes N N italic_N distinct values, or tokens , { ùíô 1 , ‚Ä¶ , ùíô N } \\{\\bm{x}_{1},\\ldots,\\bm{x}_{N}\\} { bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT } with equal probability 1 / N 1/N 1 / italic_N . Then we could encode each token ùíô i \\bm{x}_{i} bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT using the log 2 ‚Å° N \\log_{2}N roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S1.SS2.p1", "title": "When the random variable ùíô ‚àà ‚Ñù D \\bm{x}\\in\\mathbb{R}^{D} bold_italic_x ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT is continuous and has a probability density p p italic_p , one ", "snippet": "When the random variable ùíô ‚àà ‚Ñù D \\bm{x}\\in\\mathbb{R}^{D} bold_italic_x ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT is continuous and has a probability density p p italic_p , one may view that the limit of the above sum ( 3.1.1 ) is related to an integral: h ‚Äã ( ùíô ) ‚âê ùîº ‚Å° [ log ‚Å° 1 / p ‚Äã ( ùíô ) ] = ‚àí ‚à´ ‚Ñù D p ‚Äã ( ùùÉ ) ‚Äã log ‚Å° p ‚Äã ( ùùÉ ) ‚Äã d ùùÉ . h(\\bm{x})\\doteq\\operatorname{\\mathbb{E}}[\\log 1/p(\\bm{x})]=-\\int_{\\mathbb{R}^{D}}p(\\bm{\\xi})\\log p(\\bm{\\xi})\\mathrm{d}\\bm{\\xi}. italic_h ( bold_italic_x ) ‚âê blackboard_E [ roman_log 1 / italic_p ( bold_italic_x ) ] = - ‚à´ start_POSTSUBSC"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample1.p1", "title": "Through direct calculation, it is possible to show that the entropy of a Gaussian distribution x ‚àº ùí© ‚Äã ( Œº , œÉ 2 ) x\\sim\\mathcal{N}(\\mu,\\sigma^{2}) italic_x ‚àº caligraphic_N ( italic_Œº , italic_œÉ start", "snippet": "Through direct calculation, it is possible to show that the entropy of a Gaussian distribution x ‚àº ùí© ‚Äã ( Œº , œÉ 2 ) x\\sim\\mathcal{N}(\\mu,\\sigma^{2}) italic_x ‚àº caligraphic_N ( italic_Œº , italic_œÉ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) is given by: h ‚Äã ( x ) = 1 2 ‚Äã log ‚Å° ( 2 ‚Äã œÄ ‚Äã œÉ 2 ) + 1 2 . h(x)=\\frac{1}{2}\\log(2\\pi\\sigma^{2})+\\frac{1}{2}. italic_h ( italic_x ) = divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_log ( 2 italic_œÄ italic_œÉ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) + divide start_ARG 1 end_ARG start_ARG 2 end_ARG . (3.1.3) It is also known that the Gaussian distr"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S1.SS2.p2", "title": "Similar to the entropy for a discrete distribution, we would like the differential entropy to be associated with the coding rate of some realizable coding scheme. For example, as above, we may discret", "snippet": "Similar to the entropy for a discrete distribution, we would like the differential entropy to be associated with the coding rate of some realizable coding scheme. For example, as above, we may discretize the domain of the distribution with a grid of size œµ > 0 \\epsilon>0 italic_œµ > 0 . The coding rate of the resulting discrete distribution can be viewed as an approximation to the differential entropy [ CT91 ] ."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S1.SS2.p3", "title": "Be aware that there are some caveats associated with the definition of differential entropy. For a distribution in a high-dimensional space, when its support becomes degenerate (low-dimensional), its ", "snippet": "Be aware that there are some caveats associated with the definition of differential entropy. For a distribution in a high-dimensional space, when its support becomes degenerate (low-dimensional), its differential entropy diverges to ‚àí ‚àû -\\infty - ‚àû . This fact is proved in Theorem B.1 (we also recall the maximum entropy characterization of the Gaussian distribution mentioned above in Theorem B.1 ) but even in the simple explicit case of Gaussian distributions ( 3.1.4 ), when the covariance ùö∫ \\bm{\\Sigma} bold_Œ£ is singular, we can see that log ‚Äã det ( ùö∫ ) = ‚àí ‚àû \\log\\det(\\bm{\\Sigma})=-\\infty rom"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S1.SS3.p1", "title": "Remember that the learning problem entails the recovery of a (potentially continuous) distribution p ‚Äã ( ùíô ) p(\\bm{x}) italic_p ( bold_italic_x ) from a set of samples { ùíô 1 , ‚Ä¶ , ùíô N } \\{\\bm{x}_{1},\\", "snippet": "Remember that the learning problem entails the recovery of a (potentially continuous) distribution p ‚Äã ( ùíô ) p(\\bm{x}) italic_p ( bold_italic_x ) from a set of samples { ùíô 1 , ‚Ä¶ , ùíô N } \\{\\bm{x}_{1},\\ldots,\\bm{x}_{N}\\} { bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT } drawn from the distribution. For ease of exposition, we write ùëø = [ ùíô 1 , ‚Ä¶ , ùíô N ] ‚àà ‚Ñù D √ó N \\bm{X}=[\\bm{x}_{1},\\ldots,\\bm{x}_{N}]\\in\\mathbb{R}^{D\\times N} bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_x start_P"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S1.SS3.p2", "title": "Now given the samples alone without knowing what p ‚Äã ( ùíô ) p(\\bm{x}) italic_p ( bold_italic_x ) is, in theory they could be interpreted as samples from any generic distribution. In particular, they co", "snippet": "Now given the samples alone without knowing what p ‚Äã ( ùíô ) p(\\bm{x}) italic_p ( bold_italic_x ) is, in theory they could be interpreted as samples from any generic distribution. In particular, they could be interpreted as any of the following cases: 1. as samples from the empirical distribution p ùëø p^{\\bm{X}} italic_p start_POSTSUPERSCRIPT bold_italic_X end_POSTSUPERSCRIPT itself, which assigns 1 / N 1/N 1 / italic_N probability each of the N N italic_N samples ùíô i , i = 1 , ‚Ä¶ , N \\bm{x}_{i},i=1,\\ldots,N bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_i = 1 , ‚Ä¶ , italic_N"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S1.I1.i1.p1", "title": "as samples from the empirical distribution p ùëø p^{\\bm{X}} italic_p start_POSTSUPERSCRIPT bold_italic_X end_POSTSUPERSCRIPT itself, which assigns 1 / N 1/N 1 / italic_N probability each of the N N ital", "snippet": "as samples from the empirical distribution p ùëø p^{\\bm{X}} italic_p start_POSTSUPERSCRIPT bold_italic_X end_POSTSUPERSCRIPT itself, which assigns 1 / N 1/N 1 / italic_N probability each of the N N italic_N samples ùíô i , i = 1 , ‚Ä¶ , N \\bm{x}_{i},i=1,\\ldots,N bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_i = 1 , ‚Ä¶ , italic_N ."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S1.I1.i2.p1", "title": "as samples from a standard normal distribution ùíô n ‚àº p n ‚âê ùí© ‚Äã ( ùüé , œÉ 2 ‚Äã ùë∞ ) \\bm{x}^{n}\\sim p^{n}\\doteq\\mathcal{N}(\\bm{0},\\sigma^{2}\\bm{I}) bold_italic_x start_POSTSUPERSCRIPT italic_n end_POSTSUPER", "snippet": "as samples from a standard normal distribution ùíô n ‚àº p n ‚âê ùí© ‚Äã ( ùüé , œÉ 2 ‚Äã ùë∞ ) \\bm{x}^{n}\\sim p^{n}\\doteq\\mathcal{N}(\\bm{0},\\sigma^{2}\\bm{I}) bold_italic_x start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ‚àº italic_p start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ‚âê caligraphic_N ( bold_0 , italic_œÉ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) with a variance œÉ 2 \\sigma^{2} italic_œÉ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT large enough (say larger than the sample norms);"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S1.I1.i3.p1", "title": "as samples from a normal distribution ùíô e ‚àº p e ‚âê ùí© ‚Äã ( ùüé , ùö∫ ^ ) \\bm{x}^{e}\\sim p^{e}\\doteq\\mathcal{N}(\\bm{0},\\hat{\\bm{\\Sigma}}) bold_italic_x start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT ‚àº ita", "snippet": "as samples from a normal distribution ùíô e ‚àº p e ‚âê ùí© ‚Äã ( ùüé , ùö∫ ^ ) \\bm{x}^{e}\\sim p^{e}\\doteq\\mathcal{N}(\\bm{0},\\hat{\\bm{\\Sigma}}) bold_italic_x start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT ‚àº italic_p start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT ‚âê caligraphic_N ( bold_0 , over^ start_ARG bold_Œ£ end_ARG ) with a covariance ùö∫ ^ = 1 N ‚Äã ùëø ‚Äã ùëø T \\hat{\\bm{\\Sigma}}=\\frac{1}{N}\\bm{X}\\bm{X}^{T} over^ start_ARG bold_Œ£ end_ARG = divide start_ARG 1 end_ARG start_ARG italic_N end_ARG bold_italic_X bold_italic_X start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT being the empirical covariance of"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S1.I1.i4.p1", "title": "as samples from a distribution ùíô ^ ‚àº q ^ ‚Äã ( ùíô ) \\hat{\\bm{x}}\\sim\\hat{q}(\\bm{x}) over^ start_ARG bold_italic_x end_ARG ‚àº over^ start_ARG italic_q end_ARG ( bold_italic_x ) that closely approximates th", "snippet": "as samples from a distribution ùíô ^ ‚àº q ^ ‚Äã ( ùíô ) \\hat{\\bm{x}}\\sim\\hat{q}(\\bm{x}) over^ start_ARG bold_italic_x end_ARG ‚àº over^ start_ARG italic_q end_ARG ( bold_italic_x ) that closely approximates the ground truth distribution p p italic_p ."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmtheorem1.p1", "title": "Let p ‚Äã ( ùê± ) , q ‚Äã ( ùê± ) p(\\bm{x}),q(\\bm{x}) italic_p ( bold_italic_x ) , italic_q ( bold_italic_x ) be two probability density functions (that have the same support). Then ùñ™ùñ´ ‚Å° ( p ‚à• q ) ‚â• 0 \\operat", "snippet": "Let p ‚Äã ( ùê± ) , q ‚Äã ( ùê± ) p(\\bm{x}),q(\\bm{x}) italic_p ( bold_italic_x ) , italic_q ( bold_italic_x ) be two probability density functions (that have the same support). Then ùñ™ùñ´ ‚Å° ( p ‚à• q ) ‚â• 0 \\operatorname{\\mathsf{KL}}(p\\;\\|\\;q)\\geq 0 sansserif_KL ( italic_p ‚à• italic_q ) ‚â• 0 , where the inequality becomes equality if and only if p = q p=q italic_p = italic_q . 4 4 4 Technically, this equality should be taken to mean ‚Äúalmost everywhere‚Äù, i.e., except possibly on a set of zero measure (volume), since this set would not impact the value of any integral."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S1.SS3.p3", "title": "‚àí ùñ™ùñ´ ‚Å° ( p ‚à• q ) \\displaystyle-\\operatorname{\\mathsf{KL}}(p\\;\\|\\;q) - sansserif_KL ( italic_p ‚à• italic_q ) = \\displaystyle= = ‚àí ‚à´ ‚Ñù D p ‚Äã ( ùùÉ ) ‚Äã log ‚Å° p ‚Äã ( ùùÉ ) q ‚Äã ( ùùÉ ) ‚Äã d ‚Äã ùùÉ = ‚à´ ‚Ñù D p ‚Äã ( ùùÉ ) ‚Äã ", "snippet": "‚àí ùñ™ùñ´ ‚Å° ( p ‚à• q ) \\displaystyle-\\operatorname{\\mathsf{KL}}(p\\;\\|\\;q) - sansserif_KL ( italic_p ‚à• italic_q ) = \\displaystyle= = ‚àí ‚à´ ‚Ñù D p ‚Äã ( ùùÉ ) ‚Äã log ‚Å° p ‚Äã ( ùùÉ ) q ‚Äã ( ùùÉ ) ‚Äã d ‚Äã ùùÉ = ‚à´ ‚Ñù D p ‚Äã ( ùùÉ ) ‚Äã log ‚Å° q ‚Äã ( ùùÉ ) p ‚Äã ( ùùÉ ) ‚Äã d ‚Äã ùùÉ \\displaystyle-\\int_{\\mathbb{R}^{D}}p(\\bm{\\xi})\\log\\frac{p(\\bm{\\xi})}{q(\\bm{\\xi})}\\mathrm{d}\\bm{\\xi}=\\int_{\\mathbb{R}^{D}}p(\\bm{\\xi})\\log\\frac{q(\\bm{\\xi})}{p(\\bm{\\xi})}\\mathrm{d}\\bm{\\xi} - ‚à´ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_p ( bold_italic_Œæ ) roman_log divide start_ARG italic_p ( bold_ital"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S1.SS3.p4", "title": "Hence, given a set of sampled data ùëø \\bm{X} bold_italic_X , to determine which case is better among p n p^{n} italic_p start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT , p e p^{e} italic_p start_POS", "snippet": "Hence, given a set of sampled data ùëø \\bm{X} bold_italic_X , to determine which case is better among p n p^{n} italic_p start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT , p e p^{e} italic_p start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT , and q ^ \\hat{q} over^ start_ARG italic_q end_ARG , we may compare their coding rates for ùëø \\bm{X} bold_italic_X and see which one gives the lowest rate. We know from the above that the (theoretically achievable) coding rate for a distribution is closely related to its entropy. In general, we have: h ‚Äã ( ùíô n ) > h ‚Äã ( ùíô e ) > h ‚Äã ( ùíô ^ ) . h(\\bm{x}^{n})>h"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S1.SS3.p5", "title": "This observation gives us a general guideline on how we may be able to pursue a distribution p ‚Äã ( ùíô ) p(\\bm{x}) italic_p ( bold_italic_x ) which has a low-dimensional structure. It suggests two possi", "snippet": "This observation gives us a general guideline on how we may be able to pursue a distribution p ‚Äã ( ùíô ) p(\\bm{x}) italic_p ( bold_italic_x ) which has a low-dimensional structure. It suggests two possible approaches: 1. Starting with a general distribution (say a normal distribution) with high entropy, gradually transforming the distribution towards the (empirical) distribution of the data by reducing entropy. 2. Among a large family of (parametric or non-parametric) distributions with explicit coding schemes that encode the given data, progressively search for better coding schemes that give l"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S1.I2.i1.p1", "title": "Starting with a general distribution (say a normal distribution) with high entropy, gradually transforming the distribution towards the (empirical) distribution of the data by reducing entropy.", "snippet": "Starting with a general distribution (say a normal distribution) with high entropy, gradually transforming the distribution towards the (empirical) distribution of the data by reducing entropy."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S1.I2.i2.p1", "title": "Among a large family of (parametric or non-parametric) distributions with explicit coding schemes that encode the given data, progressively search for better coding schemes that give lower coding rate", "snippet": "Among a large family of (parametric or non-parametric) distributions with explicit coding schemes that encode the given data, progressively search for better coding schemes that give lower coding rates."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.p1", "title": "In this section, we will describe a natural and computationally tractable way to learn a distribution p ‚Äã ( ùíô ) p(\\bm{x}) italic_p ( bold_italic_x ) by way of learning a parametric encoding of our dis", "snippet": "In this section, we will describe a natural and computationally tractable way to learn a distribution p ‚Äã ( ùíô ) p(\\bm{x}) italic_p ( bold_italic_x ) by way of learning a parametric encoding of our distribution such that the representation has the minimum entropy or coding rate, then using this encoding to transform high-entropy samples from a standard Gaussian into low-entropy samples from the target distribution, as illustrated in Figure 3.2 . This presents a methodology that utilizes both approaches above in order to learn and sample from the distribution."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.SS1.p1", "title": "We first want to find a procedure to decrease the entropy of a given very noisy sample into a lower-entropy sample from the data distribution. Here, we describe a potential approach‚Äîone of many, but p", "snippet": "We first want to find a procedure to decrease the entropy of a given very noisy sample into a lower-entropy sample from the data distribution. Here, we describe a potential approach‚Äîone of many, but perhaps the most natural way to attack this problem. First, we find a way to gradually increase the entropy of existing samples from the data distribution. Then, we find an approximate inverse of this process. But in general, the operation of increasing entropy does not have an inverse, as information from the original distribution may be destroyed. We will thus tackle a special case where (1) the "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.SS1.p2", "title": "We will increase the entropy in arguably the simplest possible way, i.e., adding isotropic Gaussian noise . More precisely, given the random variable ùíô \\bm{x} bold_italic_x , we can consider the stoch", "snippet": "We will increase the entropy in arguably the simplest possible way, i.e., adding isotropic Gaussian noise . More precisely, given the random variable ùíô \\bm{x} bold_italic_x , we can consider the stochastic process ( ùíô t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ‚àà [ 0 , italic_T ] end_POSTSUBSCRIPT which adds gradual noise to it, i.e., ùíô t ‚âê ùíô + t ‚Äã ùíà , ‚àÄ t ‚àà [ 0 , T ] , \\bm{x}_{t}\\doteq\\bm{x}+t\\bm{g},\\qquad\\forall t\\in[0,T], bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ‚âê bold_italic_x +"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmtheorem2.p1", "title": "Suppose that ( ùê± t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ‚àà [ 0 , italic_T ] end_POSTSUBSCRIPT follows t", "snippet": "Suppose that ( ùê± t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ‚àà [ 0 , italic_T ] end_POSTSUBSCRIPT follows the model ( 3.2.1 ). For any t ‚àà ( 0 , T ] t\\in(0,T] italic_t ‚àà ( 0 , italic_T ] , the random variable ùê± t \\bm{x}_{t} bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT has differential entropy h ‚Äã ( ùê± t ) > ‚àí ‚àû h(\\bm{x}_{t})>-\\infty italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) > - ‚àû . Moreover, under certain technical conditions on ùê± \\bm{x} bold_italic_x , d "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.SS1.p3", "title": "The proof is elementary, but it is rather long, so we postpone it to Section B.2.1 . The main as-yet unstated implication of this result is that h ‚Äã ( ùíô t ) > h ‚Äã ( ùíô ) h(\\bm{x}_{t})>h(\\bm{x}) italic_", "snippet": "The proof is elementary, but it is rather long, so we postpone it to Section B.2.1 . The main as-yet unstated implication of this result is that h ‚Äã ( ùíô t ) > h ‚Äã ( ùíô ) h(\\bm{x}_{t})>h(\\bm{x}) italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) > italic_h ( bold_italic_x ) for every t > 0 t>0 italic_t > 0 . To see this, note that if h ‚Äã ( ùíô ) = ‚àí ‚àû h(\\bm{x})=-\\infty italic_h ( bold_italic_x ) = - ‚àû then h ‚Äã ( ùíô t ) > ‚àí ‚àû h(\\bm{x}_{t})>-\\infty italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) > - ‚àû for all t > 0 t>0 italic_t > 0 , and if h ‚Äã ( ùíô ) >"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.SS1.p4", "title": "The inverse operation to adding noise is known as denoising . It is a classical and well-studied topic in signal processing and system theory, such as the Wiener filter and the Kalman filter. The seve", "snippet": "The inverse operation to adding noise is known as denoising . It is a classical and well-studied topic in signal processing and system theory, such as the Wiener filter and the Kalman filter. The several problems discussed in Chapter 2 , such as PCA, ICA, and Dictionary Learning, are specific instances of the denoising problem. For a fixed t t italic_t and the additive Gaussian noise model ( 3.2.1 ), the denoising problem can be formulated as attempting to learn a function ùíô ¬Ø ‚àó ‚Äã ( t , ‚ãÖ ) \\bar{\\bm{x}}^{\\ast}(t,\\cdot) over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ‚àó end_POSTSUPER"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.SS1.p5", "title": "This expression justifies the notation ùíô ¬Ø \\bar{\\bm{x}} over¬Ø start_ARG bold_italic_x end_ARG , which is meant to compute a conditional expectation (i.e., conditional mean or conditional average). In ", "snippet": "This expression justifies the notation ùíô ¬Ø \\bar{\\bm{x}} over¬Ø start_ARG bold_italic_x end_ARG , which is meant to compute a conditional expectation (i.e., conditional mean or conditional average). In short, it attempts to remove the noise from the noisy input, outputting the best possible guess (in expectation and w.r.t. the ‚Ñì 2 \\ell^{2} roman_‚Ñì start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT -distance) of the (de-noised) original random variable."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample2.p1", "title": "In this example we compute the Bayes optimal denoiser for an incredibly important class of distributions, the Gaussian mixture model. To start, let us fix parameters for the distribution: mixture weig", "snippet": "In this example we compute the Bayes optimal denoiser for an incredibly important class of distributions, the Gaussian mixture model. To start, let us fix parameters for the distribution: mixture weights ùùÖ ‚àà ‚Ñù K \\bm{\\pi}\\in\\mathbb{R}^{K} bold_italic_œÄ ‚àà blackboard_R start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT , component means { ùùÅ k } k = 1 K ‚äÜ ‚Ñù D \\{\\bm{\\mu}_{k}\\}_{k=1}^{K}\\subseteq\\mathbb{R}^{D} { bold_italic_Œº start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT ‚äÜ blackboard_R start_POST"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.I1.i1.p1", "title": "First, an index (or label ) y ‚àà [ K ] y\\in[K] italic_y ‚àà [ italic_K ] is sampled such that y = k y=k italic_y = italic_k with probability œÄ k \\pi_{k} italic_œÄ start_POSTSUBSCRIPT italic_k end_POSTSUBS", "snippet": "First, an index (or label ) y ‚àà [ K ] y\\in[K] italic_y ‚àà [ italic_K ] is sampled such that y = k y=k italic_y = italic_k with probability œÄ k \\pi_{k} italic_œÄ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.I1.i2.p1", "title": "Second, ùíô \\bm{x} bold_italic_x is sampled from the normal distribution ùí© ‚Å° ( ùùÅ y , ùö∫ y ) \\operatorname{\\mathcal{N}}(\\bm{\\mu}_{y},\\bm{\\Sigma}_{y}) caligraphic_N ( bold_italic_Œº start_POSTSUBSCRIPT ital", "snippet": "Second, ùíô \\bm{x} bold_italic_x is sampled from the normal distribution ùí© ‚Å° ( ùùÅ y , ùö∫ y ) \\operatorname{\\mathcal{N}}(\\bm{\\mu}_{y},\\bm{\\Sigma}_{y}) caligraphic_N ( bold_italic_Œº start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT , bold_Œ£ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT ) ."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample2.p2", "title": "Conditioned on y y italic_y , the variables are jointly Gaussian: if we say that ùíô = ùùÅ y + ùö∫ y 1 / 2 ‚Äã ùíñ \\bm{x}=\\bm{\\mu}_{y}+\\bm{\\Sigma}_{y}^{1/2}\\bm{u} bold_italic_x = bold_italic_Œº start_POSTSUBSCRI", "snippet": "Conditioned on y y italic_y , the variables are jointly Gaussian: if we say that ùíô = ùùÅ y + ùö∫ y 1 / 2 ‚Äã ùíñ \\bm{x}=\\bm{\\mu}_{y}+\\bm{\\Sigma}_{y}^{1/2}\\bm{u} bold_italic_x = bold_italic_Œº start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT + bold_Œ£ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT bold_italic_u where ( ‚ãÖ ) 1 / 2 (\\cdot)^{1/2} ( ‚ãÖ ) start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT is the matrix square root and ùíñ ‚àº ùí© ‚Å° ( ùüé , ùë∞ ) \\bm{u}\\sim\\operatorname{\\mathcal{N}}(\\bm{0},\\bm{I}) bold_italic_u ‚àº caligraphic_N ( bold_0 , bold_italic_I ) independe"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample2.p3", "title": "To try to understand ( 3.2.17 ) intuitively, let us first set K = 1 K=1 italic_K = 1 (i.e., one Gaussian) such that ùíô ‚àº ùí© ‚Å° ( ùùÅ , ùö∫ ) \\bm{x}\\sim\\operatorname{\\mathcal{N}}(\\bm{\\mu},\\bm{\\Sigma}) bold_it", "snippet": "To try to understand ( 3.2.17 ) intuitively, let us first set K = 1 K=1 italic_K = 1 (i.e., one Gaussian) such that ùíô ‚àº ùí© ‚Å° ( ùùÅ , ùö∫ ) \\bm{x}\\sim\\operatorname{\\mathcal{N}}(\\bm{\\mu},\\bm{\\Sigma}) bold_italic_x ‚àº caligraphic_N ( bold_italic_Œº , bold_Œ£ ) . Let us then diagonalize ùö∫ = ùëΩ ‚Äã ùö≤ ‚Äã ùëΩ ‚ä§ \\bm{\\Sigma}=\\bm{V}\\bm{\\Lambda}\\bm{V}^{\\top} bold_Œ£ = bold_italic_V bold_Œõ bold_italic_V start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT . Then the Bayes optimal denoiser is ùíô ¬Ø ‚àó ‚Äã ( t , ùíô t ) = ùùÅ + ùö∫ ‚Äã ( ùö∫ + t 2 ‚Äã ùë∞ ) ‚àí 1 ‚Äã ( ùíô t ‚àí ùùÅ ) = ùùÅ + ùëΩ ‚Äã [ Œª 1 / ( Œª 1 + t 2 ) ‚ã± Œª D / ( Œª D + t 2 ) ] ‚Äã ùëΩ ‚ä§ ‚Äã ( ùíô t ‚àí ùùÅ )"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.I2.i1.p1", "title": "Translate the input ùíô t \\bm{x}_{t} bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT by ùùÅ \\bm{\\mu} bold_italic_Œº .", "snippet": "Translate the input ùíô t \\bm{x}_{t} bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT by ùùÅ \\bm{\\mu} bold_italic_Œº ."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.I2.i2.p1", "title": "Contract the (translated) input ùíô t ‚àí ùùÅ \\bm{x}_{t}-\\bm{\\mu} bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - bold_italic_Œº in each eigenvector direction by a quantity Œª i / ( Œª i + t 2 )", "snippet": "Contract the (translated) input ùíô t ‚àí ùùÅ \\bm{x}_{t}-\\bm{\\mu} bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - bold_italic_Œº in each eigenvector direction by a quantity Œª i / ( Œª i + t 2 ) \\lambda_{i}/(\\lambda_{i}+t^{2}) italic_Œª start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT / ( italic_Œª start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) . If the translated input is low-rank and some eigenvalues of ùö∫ \\bm{\\Sigma} bold_Œ£ are zero, these directions get immediately contracted to 0 by the denoiser, ensuring that the output of the con"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.I2.i3.p1", "title": "Translate the output back by ùùÅ \\bm{\\mu} bold_italic_Œº .", "snippet": "Translate the output back by ùùÅ \\bm{\\mu} bold_italic_Œº ."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample2.p4", "title": "This is the geometric interpretation of the denoiser of a single Gaussian. The overall denoiser of the Gaussian mixture model ( 3.2.17 ) uses K K italic_K such denoisers, weighting their output by the", "snippet": "This is the geometric interpretation of the denoiser of a single Gaussian. The overall denoiser of the Gaussian mixture model ( 3.2.17 ) uses K K italic_K such denoisers, weighting their output by the posterior probabilities ‚Ñô ‚Å° [ y = k ‚à£ ùíô t ] \\operatorname{\\mathbb{P}}[y=k\\mid\\bm{x}_{t}] blackboard_P [ italic_y = italic_k ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] . If the means of the Gaussians are well-separated, these posterior probabilities are very close to 0 or 1 1 1 near each mean or cluster. In this regime, the overall denoiser ( 3.2.17 ) has the same geometric i"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample2.p5", "title": "At first glance, such a contraction mapping ( 3.2.19 ) may appear similar to power iterations (see Section 2.1.2 ). However, the two are fundamentally different. Power iteration implements a contracti", "snippet": "At first glance, such a contraction mapping ( 3.2.19 ) may appear similar to power iterations (see Section 2.1.2 ). However, the two are fundamentally different. Power iteration implements a contraction mapping towards a subspace‚Äînamely the subspace spanned by the first principal component. In contrast, the iterates in ( 3.2.19 ) converge to the mean ùùÅ \\bm{\\mu} bold_italic_Œº of the underlying distribution, which is a single point. ‚ñ† \\blacksquare ‚ñ†"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.SS1.p6", "title": "Intuitively, and as we can see from Example 3.2 , the Bayes optimal denoiser ùíô ¬Ø ‚àó ‚Äã ( t , ‚ãÖ ) \\bar{\\bm{x}}^{\\ast}(t,\\cdot) over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ‚àó end_POSTSUPERS", "snippet": "Intuitively, and as we can see from Example 3.2 , the Bayes optimal denoiser ùíô ¬Ø ‚àó ‚Äã ( t , ‚ãÖ ) \\bar{\\bm{x}}^{\\ast}(t,\\cdot) over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT ( italic_t , ‚ãÖ ) should move its input ùíô t \\bm{x}_{t} bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT towards the modes of the distribution of ùíô \\bm{x} bold_italic_x . It turns out that, actually, we can quantify this by showing that the Bayes optimal denoiser takes a gradient ascent step on the (log-)density of ùíô t \\bm{x}_{t} bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTS"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmtheorem3.p1", "title": "Suppose that ( ùê± t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ‚àà [ 0 , italic_T ] end_POSTSUBSCRIPT obeys ( 3", "snippet": "Suppose that ( ùê± t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ‚àà [ 0 , italic_T ] end_POSTSUBSCRIPT obeys ( 3.2.1 ). Let p t p_{t} italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT be the density of ùê± t \\bm{x}_{t} bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT (as previously declared). Then ùîº ‚Å° [ ùíô ‚à£ ùíô t ] = ùíô t + t 2 ‚Äã ‚àá ùíô t log ‚Å° p t ‚Äã ( ùíô t ) . \\operatorname{\\mathbb{E}}[\\bm{x}\\mid\\bm{x}_{t}]=\\bm{x}_{t}+t^{2}\\nabla_{\\bm{x}_{t}}\\log p_{t}(\\bm{x}_{t}). blackboard_E [ bold_italic_x ‚à£ b"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.SS1.p7", "title": "For the proof let us suppose that ùíô \\bm{x} bold_italic_x has a density (even though the theorem is true without this assumption), and call this density p p italic_p . Let p 0 ‚à£ t p_{0\\mid t} italic_p ", "snippet": "For the proof let us suppose that ùíô \\bm{x} bold_italic_x has a density (even though the theorem is true without this assumption), and call this density p p italic_p . Let p 0 ‚à£ t p_{0\\mid t} italic_p start_POSTSUBSCRIPT 0 ‚à£ italic_t end_POSTSUBSCRIPT and p t ‚à£ 0 p_{t\\mid 0} italic_p start_POSTSUBSCRIPT italic_t ‚à£ 0 end_POSTSUBSCRIPT be the conditional densities of ùíô = ùíô 0 \\bm{x}=\\bm{x}_{0} bold_italic_x = bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT given ùíô t \\bm{x}_{t} bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and ùíô t \\bm{x}_{t} bold_italic_x start_POSTSUBSCRIPT it"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.SS1.p8", "title": "This result develops a connection between denoising and optimization: the Bayes-optimal denoiser takes a single step of gradient ascent on the perturbed data density p t p_{t} italic_p start_POSTSUBSC", "snippet": "This result develops a connection between denoising and optimization: the Bayes-optimal denoiser takes a single step of gradient ascent on the perturbed data density p t p_{t} italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , and the step size adaptively becomes smaller (i.e., taking more precise steps) as the perturbation to the data distribution grows smaller. The quantity ‚àá ùíô t log ‚Å° p t ‚Äã ( ùíô t ) \\nabla_{\\bm{x}_{t}}\\log p_{t}(\\bm{x}_{t}) ‚àá start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.SS1.p9", "title": "Similar to how one step of gradient descent is almost never sufficient to minimize an objective in practice when initializing far from the optimum, the output of the Bayes-optimal denoiser ùíô ¬Ø ‚àó ‚Äã ( t", "snippet": "Similar to how one step of gradient descent is almost never sufficient to minimize an objective in practice when initializing far from the optimum, the output of the Bayes-optimal denoiser ùíô ¬Ø ‚àó ‚Äã ( t , ‚ãÖ ) \\bar{\\bm{x}}^{\\ast}(t,\\cdot) over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT ( italic_t , ‚ãÖ ) is almost never contained in a high-probability region of the data distribution when t t italic_t is large, especially when the data have low-dimensional structures. We illustrate this point explicitly in the following example."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample3.p1", "title": "Let x x italic_x be uniform on the two-point set { ‚àí 1 , + 1 } \\{-1,+1\\} { - 1 , + 1 } and let ( ùíô t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSC", "snippet": "Let x x italic_x be uniform on the two-point set { ‚àí 1 , + 1 } \\{-1,+1\\} { - 1 , + 1 } and let ( ùíô t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ‚àà [ 0 , italic_T ] end_POSTSUBSCRIPT follow ( 3.2.1 ). This is precisely a degenerate Gaussian mixture model with priors equal to 1 2 \\frac{1}{2} divide start_ARG 1 end_ARG start_ARG 2 end_ARG , means { ‚àí 1 , + 1 } \\{-1,+1\\} { - 1 , + 1 } , and covariances both equal to 0 . For a fixed t > 0 t>0 italic_t > 0 we can use the calculation of the Bayes-optimal denois"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.SS1.p10", "title": "Therefore, if we want to denoise the very noisy sample ùíô T \\bm{x}_{T} bold_italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT (where‚Äîrecall‚Äî T T italic_T is the maximum time), we cannot just use ", "snippet": "Therefore, if we want to denoise the very noisy sample ùíô T \\bm{x}_{T} bold_italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT (where‚Äîrecall‚Äî T T italic_T is the maximum time), we cannot just use the denoiser once . Instead, we must use the denoiser many times, analogously to gradient descent with decaying step sizes , to converge to a stationary point ùíô ^ \\hat{\\bm{x}} over^ start_ARG bold_italic_x end_ARG . Namely, we shall use the denoiser to go from ùíô T \\bm{x}_{T} bold_italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT to ùíô ^ T ‚àí Œ¥ \\hat{\\bm{x}}_{T-\\delta} over^ start_ARG bold_itali"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.SS1.p11", "title": "More formally, we uniformly discretize [ 0 , T ] [0,T] [ 0 , italic_T ] into L + 1 L+1 italic_L + 1 timesteps 0 = t 0 < t 1 < ‚ãØ < t L = T 0=t_{0}<t_{1}<\\cdots<t_{L}=T 0 = italic_t start_POSTSUBSCRIPT ", "snippet": "More formally, we uniformly discretize [ 0 , T ] [0,T] [ 0 , italic_T ] into L + 1 L+1 italic_L + 1 timesteps 0 = t 0 < t 1 < ‚ãØ < t L = T 0=t_{0}<t_{1}<\\cdots<t_{L}=T 0 = italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT < italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT < ‚ãØ < italic_t start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT = italic_T , i.e., t ‚Ñì = ‚Ñì L ‚Äã T , ‚Ñì ‚àà { 0 , 1 , ‚Ä¶ , L } . t_{\\ell}=\\frac{\\ell}{L}T,\\qquad\\ell\\in\\{0,1,\\dots,L\\}. italic_t start_POSTSUBSCRIPT roman_‚Ñì end_POSTSUBSCRIPT = divide start_ARG roman_‚Ñì end_ARG start_ARG italic_L end_ARG italic_T , roman_‚Ñì ‚àà { 0 , 1 , ‚Ä¶ , i"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.SS1.p12", "title": "The above is intuition for why we expect the denoising process to converge. We visualize the convergence process in ‚Ñù 3 \\mathbb{R}^{3} blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT in Figur", "snippet": "The above is intuition for why we expect the denoising process to converge. We visualize the convergence process in ‚Ñù 3 \\mathbb{R}^{3} blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT in Figure 3.5 . We will develop some rigorous results about convergence later. For now, recall that we wanted to build a process to reduce the entropy. While we did do this in a roundabout way by inverting a process which adds entropy, it is now time to pay the piper and confirm that our iterative denoising process reduces the entropy."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmtheorem4.p1", "title": "Suppose that ( ùê± t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ‚àà [ 0 , italic_T ] end_POSTSUBSCRIPT obeys ( 3", "snippet": "Suppose that ( ùê± t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ‚àà [ 0 , italic_T ] end_POSTSUBSCRIPT obeys ( 3.2.1 ). Then, under certain technical conditions on ùê± \\bm{x} bold_italic_x , for every s < t s<t italic_s < italic_t with s , t ‚àà ( 0 , T ] s,t\\in(0,T] italic_s , italic_t ‚àà ( 0 , italic_T ] , h ‚Äã ( ùîº ‚Å° [ ùíô s ‚à£ ùíô t ] ) < h ‚Äã ( ùíô t ) . h(\\operatorname{\\mathbb{E}}[\\bm{x}_{s}\\mid\\bm{x}_{t}])<h(\\bm{x}_{t}). italic_h ( blackboard_E [ bold_italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ‚à£ bold_i"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.SS1.p13", "title": "The full statement of the theorem, and the proof itself, requires some technicality, so it is postponed to Section B.2.2 .", "snippet": "The full statement of the theorem, and the proof itself, requires some technicality, so it is postponed to Section B.2.2 ."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.SS1.p14", "title": "The last thing we discuss here is that many times, we will not be able to compute ùíô ¬Ø ‚àó ‚Äã ( t , ‚ãÖ ) \\bar{\\bm{x}}^{\\ast}(t,\\cdot) over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ‚àó end_POSTS", "snippet": "The last thing we discuss here is that many times, we will not be able to compute ùíô ¬Ø ‚àó ‚Äã ( t , ‚ãÖ ) \\bar{\\bm{x}}^{\\ast}(t,\\cdot) over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT ( italic_t , ‚ãÖ ) for any t t italic_t , since we do not have the distribution p t p_{t} italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . But we can try to learn one from data . Recall that the denoiser ùíô ¬Ø ‚àó \\bar{\\bm{x}}^{\\ast} over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT is defined in ( 3.2.3 ) as minimizing the mean-squared error ùîº ‚Å° ‚Äñ ùíô ¬Ø"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.SS1.p15", "title": "What is a good architecture for this neural network ùíô ¬Ø Œ∏ ‚àó ‚Äã ( t , ‚ãÖ ) \\bar{\\bm{x}}_{\\theta^{\\ast}}(t,\\cdot) over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_Œ∏ start_POSTSUPERSCRIPT ‚àó", "snippet": "What is a good architecture for this neural network ùíô ¬Ø Œ∏ ‚àó ‚Äã ( t , ‚ãÖ ) \\bar{\\bm{x}}_{\\theta^{\\ast}}(t,\\cdot) over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_Œ∏ start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( italic_t , ‚ãÖ ) ? To answer this question, we will examine the ubiquitous case of a Gaussian mixture model , whose denoiser we computed in Example 3.2 . This model is relevant because it can approximate many types of distributions: in particular, given a distribution for ùíô \\bm{x} bold_italic_x , there is a Gaussian mixture model that can approximate it arbit"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.SS1.p16", "title": "In our case, we assume that ùíô \\bm{x} bold_italic_x is low-dimensional, which loosely translates into the requirement that ùíô \\bm{x} bold_italic_x is approximately distributed according to a mixture of ", "snippet": "In our case, we assume that ùíô \\bm{x} bold_italic_x is low-dimensional, which loosely translates into the requirement that ùíô \\bm{x} bold_italic_x is approximately distributed according to a mixture of low-rank Gaussians . Formally, we write ùíô ‚àº 1 K ‚Äã ‚àë k = 1 K ùí© ‚Å° ( ùüé , ùëº k ‚Äã ùëº k ‚ä§ ) \\bm{x}\\sim\\frac{1}{K}\\sum_{k=1}^{K}\\operatorname{\\mathcal{N}}(\\bm{0},\\bm{U}_{k}\\bm{U}_{k}^{\\top}) bold_italic_x ‚àº divide start_ARG 1 end_ARG start_ARG italic_K end_ARG ‚àë start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT caligraphic_N ( bold_0 , bold_italic_U start"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmremark1.p1", "title": "Connections between denoising a distribution and probabilistic PCA. Here, we would like to connect denoising a low-dimensional distribution to probabilistic PCA (see Section 2.1.3 for more details abo", "snippet": "Connections between denoising a distribution and probabilistic PCA. Here, we would like to connect denoising a low-dimensional distribution to probabilistic PCA (see Section 2.1.3 for more details about probabilistic PCA). Suppose that we consider K = 1 K=1 italic_K = 1 in ( 3.2.42 ), i.e., ùíô ‚àº ùí© ‚Å° ( ùüé , ùëº ‚Äã ùëº ‚ä§ ) \\bm{x}\\sim\\operatorname{\\mathcal{N}}(\\bm{0},\\bm{U}\\bm{U}^{\\top}) bold_italic_x ‚àº caligraphic_N ( bold_0 , bold_italic_U bold_italic_U start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ) , where ùëº ‚àà ùñÆ ‚Äã ( D , P ) ‚äÜ ‚Ñù D √ó P \\bm{U}\\in\\mathsf{O}(D,P)\\subseteq\\mathbb{R}^{D\\times P} bold_italic_"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.SS1.p17", "title": "Overall, the learned denoiser forms an (implicit parametric) encoding scheme of the given data, since it can be used to denoise/project onto the data distribution. Training a denoiser is equivalent to", "snippet": "Overall, the learned denoiser forms an (implicit parametric) encoding scheme of the given data, since it can be used to denoise/project onto the data distribution. Training a denoiser is equivalent to finding a better coding scheme, and this partially fulfills one of the desiderata (the second ) at the end of Section 3.1.3 . In the sequel, we will discuss how to fulfill the other (the first )."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.SS2.p1", "title": "Remember that at the end of Section 3.1.3 , we discussed a pair of desiderata for pursuing a distribution with low-dimensional structure. The first such desideratum is to start with a normal distribut", "snippet": "Remember that at the end of Section 3.1.3 , we discussed a pair of desiderata for pursuing a distribution with low-dimensional structure. The first such desideratum is to start with a normal distribution, say with high entropy, and gradually reduce its entropy until it reaches the distribution of the data. We will call this procedure sampling since we are generating new samples. It is now time for us to discuss how to do this with the toolkit we have built up."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.SS2.p2", "title": "We know how to denoise very noisy samples ùíô T \\bm{x}_{T} bold_italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT to attain approximations ùíô ^ \\hat{\\bm{x}} over^ start_ARG bold_italic_x end_ARG wh", "snippet": "We know how to denoise very noisy samples ùíô T \\bm{x}_{T} bold_italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT to attain approximations ùíô ^ \\hat{\\bm{x}} over^ start_ARG bold_italic_x end_ARG which have similar distributions to the target random variable ùíô \\bm{x} bold_italic_x . But the desideratum says that, to sample, we want to start with a template distribution with no influence from the distribution of ùíô \\bm{x} bold_italic_x , and use the denoiser to guide the iterates towards the distribution of ùíô \\bm{x} bold_italic_x . How can we do this? One way is motivated as follows: ùíô T T = ùíô"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.SS2.p3", "title": "So, discretizing [ 0 , T ] [0,T] [ 0 , italic_T ] into 0 = t 0 < t 1 < ‚ãØ < t L = T 0=t_{0}<t_{1}<\\cdots<t_{L}=T 0 = italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT < italic_t start_POSTSUBSCRIPT 1 en", "snippet": "So, discretizing [ 0 , T ] [0,T] [ 0 , italic_T ] into 0 = t 0 < t 1 < ‚ãØ < t L = T 0=t_{0}<t_{1}<\\cdots<t_{L}=T 0 = italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT < italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT < ‚ãØ < italic_t start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT = italic_T uniformly using t ‚Ñì = T ‚Äã ‚Ñì / L t_{\\ell}=T\\ell/L italic_t start_POSTSUBSCRIPT roman_‚Ñì end_POSTSUBSCRIPT = italic_T roman_‚Ñì / italic_L (as in the previous section), one possible way to sample from pure noise is: ‚Ä¢ Sample ùíô ^ T ‚àº ùí© ‚Å° ( ùüé , T 2 ‚Äã ùë∞ ) \\hat{\\bm{x}}_{T}\\sim\\operatorname{\\mathcal{N}}(\\bm{0},T^{2}\\bm{"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.I3.i1.p1", "title": "Sample ùíô ^ T ‚àº ùí© ‚Å° ( ùüé , T 2 ‚Äã ùë∞ ) \\hat{\\bm{x}}_{T}\\sim\\operatorname{\\mathcal{N}}(\\bm{0},T^{2}\\bm{I}) over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ‚àº caligraphic", "snippet": "Sample ùíô ^ T ‚àº ùí© ‚Å° ( ùüé , T 2 ‚Äã ùë∞ ) \\hat{\\bm{x}}_{T}\\sim\\operatorname{\\mathcal{N}}(\\bm{0},T^{2}\\bm{I}) over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ‚àº caligraphic_N ( bold_0 , italic_T start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) (i.i.d. of everything else)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.I3.i2.p1", "title": "Run the denoising iteration as in Section 3.2.1 , i.e., ùíô ^ t ‚Ñì ‚àí 1 = ( 1 ‚àí 1 ‚Ñì ) ‚ãÖ ùíô ^ t ‚Ñì + 1 ‚Ñì ‚ãÖ ùíô ¬Ø ‚àó ‚Äã ( t ‚Ñì , ùíô ^ t ‚Ñì ) . \\hat{\\bm{x}}_{t_{\\ell-1}}=\\left(1-\\frac{1}{\\ell}\\right)\\cdot\\hat{\\bm{x}}", "snippet": "Run the denoising iteration as in Section 3.2.1 , i.e., ùíô ^ t ‚Ñì ‚àí 1 = ( 1 ‚àí 1 ‚Ñì ) ‚ãÖ ùíô ^ t ‚Ñì + 1 ‚Ñì ‚ãÖ ùíô ¬Ø ‚àó ‚Äã ( t ‚Ñì , ùíô ^ t ‚Ñì ) . \\hat{\\bm{x}}_{t_{\\ell-1}}=\\left(1-\\frac{1}{\\ell}\\right)\\cdot\\hat{\\bm{x}}_{t_{\\ell}}+\\frac{1}{\\ell}\\cdot\\bar{\\bm{x}}^{\\ast}(t_{\\ell},\\hat{\\bm{x}}_{t_{\\ell}}). over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_‚Ñì - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = ( 1 - divide start_ARG 1 end_ARG start_ARG roman_‚Ñì end_ARG ) ‚ãÖ over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_‚Ñì end_POSTSUBSCRIPT e"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.I3.i3.p1", "title": "Output ùíô ^ = ùíô ^ 0 \\hat{\\bm{x}}=\\hat{\\bm{x}}_{0} over^ start_ARG bold_italic_x end_ARG = over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT .", "snippet": "Output ùíô ^ = ùíô ^ 0 \\hat{\\bm{x}}=\\hat{\\bm{x}}_{0} over^ start_ARG bold_italic_x end_ARG = over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.SS2.SSS0.Px1.p1", "title": "The first step we do is motivated by the following point: we do not need to spend so many denoising iterations at large t t italic_t . If we look at Figure 3.5 , we observe that the first 200 200 200 ", "snippet": "The first step we do is motivated by the following point: we do not need to spend so many denoising iterations at large t t italic_t . If we look at Figure 3.5 , we observe that the first 200 200 200 or 300 300 300 iterations, out of the 500 500 500 iterations of the sampling process, are just spent contracting the noise towards the data distribution as a whole, before the remaining iterations push the samples towards a subspace. Given a fixed iteration count L L italic_L , this signals that we should spend more timesteps t ‚Ñì t_{\\ell} italic_t start_POSTSUBSCRIPT roman_‚Ñì end_POSTSUBSCRIPT near"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.SS2.SSS0.Px2.p1", "title": "The second step is to consider slightly different models compared to ( 3.2.1 ). The basic motivation for this is as follows. In practice, the noise distribution ùí© ‚Å° ( ùüé , t L 2 ‚Äã ùë∞ ) \\operatorname{\\ma", "snippet": "The second step is to consider slightly different models compared to ( 3.2.1 ). The basic motivation for this is as follows. In practice, the noise distribution ùí© ‚Å° ( ùüé , t L 2 ‚Äã ùë∞ ) \\operatorname{\\mathcal{N}}(\\bm{0},t_{L}^{2}\\bm{I}) caligraphic_N ( bold_0 , italic_t start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) becomes an increasingly poor estimate of the true covariance in high dimensions, i.e., ( 3.2.65 ) becomes an increasingly worse approximation, especially with anisotropic high-dimensional data. The increased distance between "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.SS2.SSS0.Px2.p2", "title": "With this more general setup, Tweedie‚Äôs formula ( 3.2.20 ) becomes ùîº ‚Å° [ ùíô ‚à£ ùíô t ] = 1 Œ± t ‚Äã ( ùíô t + œÉ t 2 ‚Äã ‚àá log ‚Å° p t ‚Äã ( ùíô ) ) . \\operatorname{\\mathbb{E}}[\\bm{x}\\mid\\bm{x}_{t}]=\\frac{1}{\\alpha_{t}", "snippet": "With this more general setup, Tweedie‚Äôs formula ( 3.2.20 ) becomes ùîº ‚Å° [ ùíô ‚à£ ùíô t ] = 1 Œ± t ‚Äã ( ùíô t + œÉ t 2 ‚Äã ‚àá log ‚Å° p t ‚Äã ( ùíô ) ) . \\operatorname{\\mathbb{E}}[\\bm{x}\\mid\\bm{x}_{t}]=\\frac{1}{\\alpha_{t}}\\left(\\bm{x}_{t}+\\sigma_{t}^{2}\\nabla\\log p_{t}(\\bm{x})\\right). blackboard_E [ bold_italic_x ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] = divide start_ARG 1 end_ARG start_ARG italic_Œ± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_œÉ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSC"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.SS2.SSS0.Px3.p1", "title": "If we use the procedure dictated by Section 3.2.1 to learn a separate denoiser ùíô ¬Ø ‚Äã ( t , ‚ãÖ ) \\bar{\\bm{x}}(t,\\cdot) over¬Ø start_ARG bold_italic_x end_ARG ( italic_t , ‚ãÖ ) for each time t t italic_t t", "snippet": "If we use the procedure dictated by Section 3.2.1 to learn a separate denoiser ùíô ¬Ø ‚Äã ( t , ‚ãÖ ) \\bar{\\bm{x}}(t,\\cdot) over¬Ø start_ARG bold_italic_x end_ARG ( italic_t , ‚ãÖ ) for each time t t italic_t to be used in the sampling algorithm, we would have to learn L L italic_L separate denoisers! This is highly inefficient‚Äîthe usual case is that we have to train L L italic_L separate neural networks, taking up L L italic_L times the training time and storage memory, and then be locked into using these timesteps for sampling forever. Instead, we can train a single neural network to denoise across al"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.SS2.SSS0.Px4.p1", "title": "Note that it is common to instead reorient the whole denoising pipeline around noise predictors , i.e., estimates of ùîº ‚Å° [ ùíà ‚à£ ùíô t ] \\operatorname{\\mathbb{E}}[\\bm{g}\\mid\\bm{x}_{t}] blackboard_E [ bold", "snippet": "Note that it is common to instead reorient the whole denoising pipeline around noise predictors , i.e., estimates of ùîº ‚Å° [ ùíà ‚à£ ùíô t ] \\operatorname{\\mathbb{E}}[\\bm{g}\\mid\\bm{x}_{t}] blackboard_E [ bold_italic_g ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] . In practice, noise predictors are slightly easier to train because their output is (almost) always of comparable size to a Gaussian random variable, so training is more numerically stable. Note that by ( 3.2.69 ) we have ùíô t = Œ± t ‚Äã ùîº ‚Å° [ ùíô ‚à£ ùíô t ] + œÉ t ‚Äã ùîº ‚Å° [ ùíà ‚à£ ùíô t ] ‚üπ ùîº ‚Å° [ ùíà ‚à£ ùíô t ] = 1 œÉ t ‚Äã ( ùíô t ‚àí Œ± t ‚Äã ùîº ‚Å° [ ùíô "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.SS2.SSS0.Px4.p2", "title": "We have made lots of changes to our original platonic noising/denoising process. To assure ourselves that the new process still works in practice, we can compute numerical examples (such as Figure 3.7", "snippet": "We have made lots of changes to our original platonic noising/denoising process. To assure ourselves that the new process still works in practice, we can compute numerical examples (such as Figure 3.7 ). To assure ourselves that it is theoretically sound, we can prove a bound on the error rate for the sampling algorithm, which shows that the error rate is small. We will now furnish such a rate from the literature, which shows that the output distribution of the sampler converges in the so-called total variation (TV) distance to the true distribution. The TV distance is defined between two rand"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmtheorem5.p1", "title": "Suppose that ùîº ‚Å° ‚Äñ ùê± ‚Äñ 2 < ‚àû \\operatorname{\\mathbb{E}}\\|\\bm{x}\\|_{2}<\\infty blackboard_E ‚à• bold_italic_x ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT < ‚àû . If ùê± \\bm{x} bold_italic_x is denoised according", "snippet": "Suppose that ùîº ‚Å° ‚Äñ ùê± ‚Äñ 2 < ‚àû \\operatorname{\\mathbb{E}}\\|\\bm{x}\\|_{2}<\\infty blackboard_E ‚à• bold_italic_x ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT < ‚àû . If ùê± \\bm{x} bold_italic_x is denoised according to the VP process with an exponential discretization 7 7 7 The precise definition is rather lengthy in our notation and only defined up to various absolute constants, so we omit it here for brevity. Of course it is in the original paper [ LY24 ] . as in ( 3.2.67 ), the output ùê± ^ \\hat{\\bm{x}} over^ start_ARG bold_italic_x end_ARG of Algorithm 3.1 satisfies the total variation bound ùñ≥ùñµ ‚Å° ( ùíô , ùíô ^"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.SS2.SSS0.Px4.p3", "title": "The very high-level proof technique is, as discussed earlier, to bound the error at each step, distinguish the error sources (between discretization and denoiser error) and carefully ensure that the e", "snippet": "The very high-level proof technique is, as discussed earlier, to bound the error at each step, distinguish the error sources (between discretization and denoiser error) and carefully ensure that the errors do not accumulate too much (or even cancel out)."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.SS2.SSS0.Px4.p4", "title": "Note that, if L ‚Üí ‚àû L\\to\\infty italic_L ‚Üí ‚àû and we correctly learn the Bayes optimal denoiser ùíô ¬Ø = ùíô ¬Ø ‚àó \\bar{\\bm{x}}=\\bar{\\bm{x}}^{\\ast} over¬Ø start_ARG bold_italic_x end_ARG = over¬Ø start_ARG bold_", "snippet": "Note that, if L ‚Üí ‚àû L\\to\\infty italic_L ‚Üí ‚àû and we correctly learn the Bayes optimal denoiser ùíô ¬Ø = ùíô ¬Ø ‚àó \\bar{\\bm{x}}=\\bar{\\bm{x}}^{\\ast} over¬Ø start_ARG bold_italic_x end_ARG = over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT (such that the excess error is 0 ), then the sampling process in Algorithm 3.1 yields a perfect (in distribution) inverse of the noising process, since the error rate in Theorem 3.5 goes to 0 , 8 8 8 There are similar results for VE processes, though none are as sharp as this to our knowledge. as heuristically argued previously."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmremark2.p1", "title": "What if the data is low-dimensional, say supported on a low-rank subspace of the high dimensional space ‚Ñù D \\mathbb{R}^{D} blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT ? If the data", "snippet": "What if the data is low-dimensional, say supported on a low-rank subspace of the high dimensional space ‚Ñù D \\mathbb{R}^{D} blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT ? If the data distribution is compactly supported‚Äîsay if the data is normalized to the unit hypercube, which is often ensured as a pre-processing step for real data such as images‚Äîit is possible to do better. Namely, the authors of [ LY24 ] also define a measure of approximate intrinsic dimension using the asymptotics of the so-called covering number, which is extremely similar in intuition (if not in implemen"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmremark3.p1", "title": "Various other works define the reverse process as moving backward in the time index t t italic_t using an explicit difference equation, or differential equation in the limit L ‚Üí ‚àû L\\to\\infty italic_L ", "snippet": "Various other works define the reverse process as moving backward in the time index t t italic_t using an explicit difference equation, or differential equation in the limit L ‚Üí ‚àû L\\to\\infty italic_L ‚Üí ‚àû , or forward in time using the transformation ùíö t = ùíô T ‚àí t \\bm{y}_{t}=\\bm{x}_{T-t} bold_italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_x start_POSTSUBSCRIPT italic_T - italic_t end_POSTSUBSCRIPT , such that if t t italic_t increases then ùíö t \\bm{y}_{t} bold_italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT becomes closer to ùíô 0 \\bm{x}_{0} bold_italic_x start_POSTS"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmremark4.p1", "title": "The theory presented at the end of the last Section 3.2.1 seems to suggest (loosely speaking) that in practice, using a transformer-like network is a good choice for learning or approximating a denois", "snippet": "The theory presented at the end of the last Section 3.2.1 seems to suggest (loosely speaking) that in practice, using a transformer-like network is a good choice for learning or approximating a denoiser. This is reasonable, but what is the problem with using any old neural network (such as a multi-layer perceptron (MLP)) and just trying to scale it up to infinity? To observe the problem with this, let us look at another special case of the Gaussian mixture model studied in Example 3.2 . Namely, the empirical distribution is an instance of a degenerate Gaussian mixture model, with K = N K=N ita"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmremark4.p2", "title": "At a high level, a denoiser which memorizes all the training points, as in ( 3.2.81 ), corresponds to a parametric model of the distribution which has minimal coding rate, and achieves this by just co", "snippet": "At a high level, a denoiser which memorizes all the training points, as in ( 3.2.81 ), corresponds to a parametric model of the distribution which has minimal coding rate, and achieves this by just coding every sample separately. We will discuss this problem (and seeming paradox with our initial desiderata at the end of Section 3.1.3 ) from the perspective of information theory in the next section."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.p1", "title": "Let us recap what we have covered so far. We have discussed how to fit a denoiser ùíô ¬Ø Œ∏ \\bar{\\bm{x}}_{\\theta} over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT using", "snippet": "Let us recap what we have covered so far. We have discussed how to fit a denoiser ùíô ¬Ø Œ∏ \\bar{\\bm{x}}_{\\theta} over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT using finite samples. We showed that this denoiser encodes a distribution in that it is directly connected to its log-density via Tweedie‚Äôs formula ( 3.2.20 ). Then, we used it to gradually transform a pure noise (high-entropy) distribution towards the learned distribution via iterative denoising . Thus, we have developed the first way of learning or pursuing a distribution laid out at the end of Secti"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.p2", "title": "Nevertheless, in this methodology the encoding of the distribution is implicit in the denoiser‚Äôs functional form and parameters, if any. In fact, acute readers might have noticed that, for a general d", "snippet": "Nevertheless, in this methodology the encoding of the distribution is implicit in the denoiser‚Äôs functional form and parameters, if any. In fact, acute readers might have noticed that, for a general distribution, we have never explicitly specified what the functional form for the denoiser is. In practice, people typically model it by some deep neural network with an empirically designed architecture. In addition, although we know the above denoising process reduces the entropy, we do not know by how much, nor do we know the entropy of the intermediate and resulting distributions."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.p3", "title": "Recall that our general goal is to model data from a (continuous) distribution with a low-dimensional support. If our goal is to identify the ‚Äúsimplest‚Äù model that generates the data, one could consid", "snippet": "Recall that our general goal is to model data from a (continuous) distribution with a low-dimensional support. If our goal is to identify the ‚Äúsimplest‚Äù model that generates the data, one could consider three typical measures of parsimony: the dimension, the volume, or the (differential) entropy. Well, if one uses the dimension, then obviously the best model for a given dataset is the empirical distribution itself which is zero-dimensional. For all distributions with low-dimensional supports, the differential entropy is always negative infinity; the volume of their supports are always zero. So"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.p4", "title": "In the remainder of this chapter, we discuss a framework that allows us to alleviate the above technical difficulty by associating the learned distribution with an explicit computable encoding and dec", "snippet": "In the remainder of this chapter, we discuss a framework that allows us to alleviate the above technical difficulty by associating the learned distribution with an explicit computable encoding and decoding scheme, following the second approach suggested at the end of Section 3.1.3 . As we will see, such an approach essentially allows us to accurately approximate the entropy of the learned distributions in terms of a (lossy) coding length or coding rate associated with the coding scheme. With such a measure, not only can we accurately measure how much the entropy is reduced, hence information g"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS1.p1", "title": "We have previously, multiple times, discussed a difficulty: if we learn the distribution from finite samples in the end, and our function class of denoisers contains enough functions, how do we ensure", "snippet": "We have previously, multiple times, discussed a difficulty: if we learn the distribution from finite samples in the end, and our function class of denoisers contains enough functions, how do we ensure that we sample from the true distribution (with low-dimensional supports), instead of any other distribution that may produce those finite samples with high probability? Let us reveal some of the conceptual and technical difficulties with some concrete examples."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample4.p1", "title": "For the example shown on the top of Figure 3.8 , suppose we have taken some samples from a uniform distribution on a line (say in a 2D plane). The volume of the line or the sample sets is zero. Geomet", "snippet": "For the example shown on the top of Figure 3.8 , suppose we have taken some samples from a uniform distribution on a line (say in a 2D plane). The volume of the line or the sample sets is zero. Geometrically, the empirical distribution on the produced finite sample set is the minimum-dimension one which can produce the finite sample set. 9 9 9 A set of discrete samples are all of zero dimension whereas the supporting line is one dimension. But this is in seemingly contrast with yet another measure of complex: entropy. The (differential) entropy of the line is negative infinity but the (discret"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample5.p1", "title": "Consider the two sets of sampled data points shown in Figure 3.8 . Geometrically, they are essentially the same: each set consists of eight points and each point has occurred with equal frequency 1 / ", "snippet": "Consider the two sets of sampled data points shown in Figure 3.8 . Geometrically, they are essentially the same: each set consists of eight points and each point has occurred with equal frequency 1 / 8 1/8 1 / 8 th. The only difference is that for the second data set, some points are ‚Äúclose‚Äù enough to be viewed as having a higher density around their respective ‚Äúcluster.‚Äù Which one is more relevant to the true distribution that may have generated the samples? How can we reconcile such ambiguity in interpreting this kind of (empirical) distributions?"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample5.p2", "title": "‚ñ† \\blacksquare ‚ñ†", "snippet": "‚ñ† \\blacksquare ‚ñ†"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS1.p2", "title": "There is yet another technical difficulty associated with constructing an explicit encoding and decoding scheme for a data set. Given a sampled data set in ùëø = [ ùíô 1 , ‚Ä¶ , ùíô N ] \\bm{X}=[\\bm{x}_{1},\\ld", "snippet": "There is yet another technical difficulty associated with constructing an explicit encoding and decoding scheme for a data set. Given a sampled data set in ùëø = [ ùíô 1 , ‚Ä¶ , ùíô N ] \\bm{X}=[\\bm{x}_{1},\\ldots,\\bm{x}_{N}] bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ] , how to design a coding scheme that is implementable on machines with finite memory and computing resources? Note that even representing a general real number requires an infinite number of digits or bits. Therefore, one may wonder whether th"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample6.p1", "title": "Consider a discrete distribution ùëø = [ e , œÄ ] \\bm{X}=[e,\\pi] bold_italic_X = [ italic_e , italic_œÄ ] with equal probability 1 / 2 1/2 1 / 2 taking the values of the Euler number e ‚âà 2.71828 e\\approx ", "snippet": "Consider a discrete distribution ùëø = [ e , œÄ ] \\bm{X}=[e,\\pi] bold_italic_X = [ italic_e , italic_œÄ ] with equal probability 1 / 2 1/2 1 / 2 taking the values of the Euler number e ‚âà 2.71828 e\\approx 2.71828 italic_e ‚âà 2.71828 or the number œÄ ‚âà 3.14159 \\pi\\approx 3.14159 italic_œÄ ‚âà 3.14159 . The entropy of this distribution is H = 1 H=1 italic_H = 1 , which suggests that one may encode the two numbers by a one-bit digit 0 or 1 1 1 , respectively. But can you realize a decoding scheme for this code on a finite-state machine? The answer is actually no, as it takes infinitely many bits to describ"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS1.p3", "title": "Hence, it is generally impossible to have an encoding and decoding scheme that can precisely reproduce samples from an arbitrary real-valued distribution. 11 11 11 That is, if one wants to encode such", "snippet": "Hence, it is generally impossible to have an encoding and decoding scheme that can precisely reproduce samples from an arbitrary real-valued distribution. 11 11 11 That is, if one wants to encode such samples precisely, the only way is to memorize every single sample. But there would be little practical value to encode a distribution without being able to decode for samples drawn from the same distribution."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS1.p4", "title": "So to ensure that any encoding/decoding scheme is computable and implementable with finite memory and computational resources, we need to quantify the sample ùíô \\bm{x} bold_italic_x and encode it only ", "snippet": "So to ensure that any encoding/decoding scheme is computable and implementable with finite memory and computational resources, we need to quantify the sample ùíô \\bm{x} bold_italic_x and encode it only up to a certain precision, say œµ > 0 \\epsilon>0 italic_œµ > 0 . By doing so, in essence, we treat any two data points equivalent if their distance is less than œµ \\epsilon italic_œµ . More precisely, we would like to consider coding schemes ùíô ‚Ü¶ ùíô ^ \\bm{x}\\mapsto\\hat{\\bm{x}} bold_italic_x ‚Ü¶ over^ start_ARG bold_italic_x end_ARG (3.3.1) such that the expected error caused by the quantization is bounded"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS2.p1", "title": "Of course, among all encoding schemes that satisfy the above constraint, we would like to choose the one that minimizes the resulting coding rate. For a given random variable ùíô \\bm{x} bold_italic_x an", "snippet": "Of course, among all encoding schemes that satisfy the above constraint, we would like to choose the one that minimizes the resulting coding rate. For a given random variable ùíô \\bm{x} bold_italic_x and a precision œµ \\epsilon italic_œµ , this rate is known as the rate distortion , denoted as ‚Ñõ œµ ‚Äã ( ùíô ) \\mathcal{R}_{\\epsilon}(\\bm{x}) caligraphic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_x ) . A deep theorem in information theory, originally proved by [ Sha59 ] , establishes that this rate can be expressed equivalently in purely probabilistic terms as ‚Ñõ œµ ‚Äã ( ùíô ) = min p ‚Äã ( "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmremark5.p1", "title": "As it turns out, the rate distortion is an implementable approximation to the entropy of ùíô \\bm{x} bold_italic_x in the following sense. Assume that ùíô \\bm{x} bold_italic_x and ùíô ^ \\hat{\\bm{x}} over^ st", "snippet": "As it turns out, the rate distortion is an implementable approximation to the entropy of ùíô \\bm{x} bold_italic_x in the following sense. Assume that ùíô \\bm{x} bold_italic_x and ùíô ^ \\hat{\\bm{x}} over^ start_ARG bold_italic_x end_ARG are continuous random vectors. Then the mutual information can be written as I ‚Äã ( ùíô ; ùíô ^ ) = h ‚Äã ( ùíô ) ‚àí h ‚Äã ( ùíô ‚à£ ùíô ^ ) , I(\\bm{x};\\hat{\\bm{x}})=h(\\bm{x})-h(\\bm{x}\\mid\\hat{\\bm{x}}), italic_I ( bold_italic_x ; over^ start_ARG bold_italic_x end_ARG ) = italic_h ( bold_italic_x ) - italic_h ( bold_italic_x ‚à£ over^ start_ARG bold_italic_x end_ARG ) , (3.3.5) where h ‚Äã "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmremark5.p2", "title": "In fact, it is not necessary to assume that ùíô \\bm{x} bold_italic_x and ùíô ^ \\hat{\\bm{x}} over^ start_ARG bold_italic_x end_ARG are continuous to obtain the above type of conclusion. For example, if bot", "snippet": "In fact, it is not necessary to assume that ùíô \\bm{x} bold_italic_x and ùíô ^ \\hat{\\bm{x}} over^ start_ARG bold_italic_x end_ARG are continuous to obtain the above type of conclusion. For example, if both random vectors are instead discrete, we have after a suitable interpretation of the KL divergence for discrete-valued random vectors that I ‚Äã ( ùíô ; ùíô ^ ) = H ‚Äã ( ùíô ) ‚àí H ‚Äã ( ùíô ‚à£ ùíô ^ ) . I(\\bm{x};\\hat{\\bm{x}})=H(\\bm{x})-H(\\bm{x}\\mid\\hat{\\bm{x}}). italic_I ( bold_italic_x ; over^ start_ARG bold_italic_x end_ARG ) = italic_H ( bold_italic_x ) - italic_H ( bold_italic_x ‚à£ over^ start_ARG bold_italic"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmremark6.p1", "title": "Given a set of data points in ùëø = [ ùíô 1 , ‚Ä¶ , ùíô N ] \\bm{X}=[\\bm{x}_{1},\\ldots,\\bm{x}_{N}] bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_x start_POSTSUBSCRIP", "snippet": "Given a set of data points in ùëø = [ ùíô 1 , ‚Ä¶ , ùíô N ] \\bm{X}=[\\bm{x}_{1},\\ldots,\\bm{x}_{N}] bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ] , one can always interpret them as samples from a uniform discrete distribution with equal probability 1 / N 1/N 1 / italic_N on these N N italic_N vectors. The entropy for such a distribution is H ‚Äã ( ùëø ) = 1 N ‚Äã log 2 ‚Å° N H(\\bm{X})=\\frac{1}{N}\\log_{2}N italic_H ( bold_italic_X ) = divide start_ARG 1 end_ARG start_ARG italic_N end_ARG roman_log start_POSTSUBSCRIPT 2"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample7.p1", "title": "Sometimes, one may face an opposite situation when we want to fix the coding rate first and try to find a coding scheme that minimizes the distortion. For example, suppose that we only want to use a f", "snippet": "Sometimes, one may face an opposite situation when we want to fix the coding rate first and try to find a coding scheme that minimizes the distortion. For example, suppose that we only want to use a fixed number of codes for points sampled from a distribution, and we want to know how to design the codes such that the average or maximum distortion is minimized during the encoding/decoding scheme. For example, given a uniform distribution on a unit square, we wonder how precisely we can encode points drawn from this distribution, with say n n italic_n bits. This problem is equivalent to asking w"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample7.p2", "title": "‚ñ† \\blacksquare ‚ñ†", "snippet": "‚ñ† \\blacksquare ‚ñ†"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS2.p2", "title": "It turns out to be a notoriously hard problem to obtain closed-form expressions for the rate distortion function ( 3.3.3 ) for general distributions p ‚Äã ( ùíô ) p(\\bm{x}) italic_p ( bold_italic_x ) . Ho", "snippet": "It turns out to be a notoriously hard problem to obtain closed-form expressions for the rate distortion function ( 3.3.3 ) for general distributions p ‚Äã ( ùíô ) p(\\bm{x}) italic_p ( bold_italic_x ) . However, as Example 3.7 suggests, there are important special cases where the geometry of the support of the distribution p ‚Äã ( ùíô ) p(\\bm{x}) italic_p ( bold_italic_x ) can be linked to the rate distortion function, and hence to the optimal coding rate at distortion level œµ \\epsilon italic_œµ . In fact, this example can be generalized to any setting where the support of p ‚Äã ( ùíô ) p(\\bm{x}) italic_p ("}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmtheorem6.p1", "title": "Suppose that ùê± \\bm{x} bold_italic_x is a random variable such that its support K ‚âê Supp ‚Å° ( ùê± ) K\\doteq\\operatorname{Supp}(\\bm{x}) italic_K ‚âê roman_Supp ( bold_italic_x ) is a compact set. Define the ", "snippet": "Suppose that ùê± \\bm{x} bold_italic_x is a random variable such that its support K ‚âê Supp ‚Å° ( ùê± ) K\\doteq\\operatorname{Supp}(\\bm{x}) italic_K ‚âê roman_Supp ( bold_italic_x ) is a compact set. Define the covering number ùí© œµ ‚Äã ( K ) \\mathcal{N}_{\\epsilon}(K) caligraphic_N start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( italic_K ) as the minimum number of balls of radius œµ \\epsilon italic_œµ that can cover K K italic_K , i.e., ùí© œµ ‚Äã ( K ) ‚âê min ‚Å° { n ‚àà ‚Ñï : ‚àÉ ùíë 1 , ‚Ä¶ , ùíë n ‚àà K ‚Äã s.t. ‚Äã K ‚äÜ ‚ãÉ i = 1 n B œµ ‚Äã ( ùíë i ) } , \\mathcal{N}_{\\epsilon}(K)\\doteq\\min\\left\\{n\\in\\mathbb{N}\\colon\\exists\\bm{p}_{1},\\dot"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS2.p3", "title": "A proof of this theorem is beyond the scope of this book and we defer it to Section B.3 . ‚àé", "snippet": "A proof of this theorem is beyond the scope of this book and we defer it to Section B.3 . ‚àé"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS2.p4", "title": "The implication of Theorem 3.6 can be summarized as follows: for sufficiently accurate coding of the distribution of ùíô \\bm{x} bold_italic_x , the minimum rate distortion coding framework is completely", "snippet": "The implication of Theorem 3.6 can be summarized as follows: for sufficiently accurate coding of the distribution of ùíô \\bm{x} bold_italic_x , the minimum rate distortion coding framework is completely characterized by the sphere packing problem on the support of ùíô \\bm{x} bold_italic_x . The core of the proof of Theorem 3.6 can indeed be generalized to more complex distributions such as sufficiently incoherent mixtures of manifolds, but we leave this for a future study. So the rate distortion can be thought of as a ‚Äúprobability-aware‚Äù way to approximate the support of the distribution of ùíô \\bm{"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS2.p5", "title": "We now discuss another connection between this and the denoising-diffusion-entropy complexity hierarchy we discussed earlier in the Chapter.", "snippet": "We now discuss another connection between this and the denoising-diffusion-entropy complexity hierarchy we discussed earlier in the Chapter."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmremark7.p1", "title": "The key ingredient in the proof of the lower bound in Theorem 3.6 is an important result from information theory known as the Shannon lower bound for the rate distortion, named after Claude Shannon, w", "snippet": "The key ingredient in the proof of the lower bound in Theorem 3.6 is an important result from information theory known as the Shannon lower bound for the rate distortion, named after Claude Shannon, who first derived it in a special case [ Sha59 ] . It asserts the following estimate for the rate distortion function, for any random variable ùíô \\bm{x} bold_italic_x with a density p ‚Äã ( ùíô ) p(\\bm{x}) italic_p ( bold_italic_x ) and finite expected squared norm [ LZ94 ] : ‚Ñõ œµ ‚Äã ( ùíô ) ‚â• h ‚Äã ( ùíô ) ‚àí log 2 ‚Å° vol ‚Å° ( B œµ ) ‚àí C D , \\mathcal{R}_{\\epsilon}(\\bm{x})\\geq h(\\bm{x})-\\log_{2}\\operatorname{vol}(B"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmremark7.p2", "title": "The Shannon lower bound is the bridge between the coding rate, entropy minimization/denoising, and geometric sphere packing approaches for learning low-dimensional distributions. Notice that in the sp", "snippet": "The Shannon lower bound is the bridge between the coding rate, entropy minimization/denoising, and geometric sphere packing approaches for learning low-dimensional distributions. Notice that in the special case of a uniform density p ‚Äã ( ùíô ) p(\\bm{x}) italic_p ( bold_italic_x ) , ( 3.3.10 ) becomes ‚Ñõ œµ ‚Äã ( ùíô ) \\displaystyle\\mathcal{R}_{\\epsilon}(\\bm{x}) caligraphic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_x ) ‚â• ‚àí ‚à´ K 1 vol ‚Å° ( K ) ‚Äã log 2 ‚Å° 1 vol ‚Å° ( K ) ‚Äã d ‚Äã ùùÉ ‚àí log 2 ‚Å° vol ‚Å° ( B œµ ) ‚àí C d \\displaystyle\\geq-\\int_{K}\\frac{1}{\\operatorname{vol}(K)}\\log_{2}\\frac{1}{\\operat"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmremark7.p3", "title": "Thus, this finite rate distortion approach via sphere covering re-enables or generalizes all previous measures of complexity of the distribution, allowing us to differentiate between and rank differen", "snippet": "Thus, this finite rate distortion approach via sphere covering re-enables or generalizes all previous measures of complexity of the distribution, allowing us to differentiate between and rank different distributions in a unified way. These interrelated viewpoints are visualized in Figure 3.10 ."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS2.p6", "title": "For a general distribution at finite distortion levels, it is typically impossible to find its rate distortion function in an analytical form. One must often resort to numerical computation 16 16 16 I", "snippet": "For a general distribution at finite distortion levels, it is typically impossible to find its rate distortion function in an analytical form. One must often resort to numerical computation 16 16 16 Interested readers may refer to [ Bla72 ] for a classic algorithm that computes rate distortion function numerically for a discrete distribution. . Nevertheless, as we will see, in our context, we often need to know the rate distortion as an explicit function of a set of data points or their representations. This is because we want to use the coding rate as a measure of the goodness of the represen"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS3.p1", "title": "Now suppose we are given a set of data samples in ùëø = [ ùíô 1 , ‚Ä¶ , ùíô N ] \\bm{X}=[\\bm{x}_{1},\\ldots,\\bm{x}_{N}] bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_", "snippet": "Now suppose we are given a set of data samples in ùëø = [ ùíô 1 , ‚Ä¶ , ùíô N ] \\bm{X}=[\\bm{x}_{1},\\ldots,\\bm{x}_{N}] bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ] from any distribution. 17 17 17 Or these data points could be viewed as an (empirical) distribution themselves. We would like to come up with a constructive scheme that can encode the data up to certain precision, say ùíô i ‚Ü¶ ùíô ^ i , subject to ‚Äñ ùíô i ‚àí ùíô ^ i ‚Äñ 2 ‚â§ œµ . \\bm{x}_{i}\\mapsto\\hat{\\bm{x}}_{i},\\quad\\mbox{subject to}\\quad\\|\\bm{x}_{i}-\\hat{\\bm"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS3.p2", "title": "Without loss of generality, let us assume the mean of ùëø \\bm{X} bold_italic_X is zero, i.e., 1 N ‚Äã ‚àë i = 1 N ùíô i = ùüé \\frac{1}{N}\\sum_{i=1}^{N}\\bm{x}_{i}=\\bm{0} divide start_ARG 1 end_ARG start_ARG ital", "snippet": "Without loss of generality, let us assume the mean of ùëø \\bm{X} bold_italic_X is zero, i.e., 1 N ‚Äã ‚àë i = 1 N ùíô i = ùüé \\frac{1}{N}\\sum_{i=1}^{N}\\bm{x}_{i}=\\bm{0} divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_0 . Without any prior knowledge about the nature of the distribution behind ùëø \\bm{X} bold_italic_X , we may view ùëø \\bm{X} bold_italic_X as sampled from a Gaussian distribution ùí© ‚Äã ( ùüé , ùö∫ ) \\mathcal{N}(\\bm{0},{\\bm{\\"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS3.p3", "title": "We may view ùëø ^ = [ ùíô ^ 1 , ‚Ä¶ , ùíô ^ N ] \\hat{\\bm{X}}=[\\hat{\\bm{x}}_{1},\\ldots,\\hat{\\bm{x}}_{N}] over^ start_ARG bold_italic_X end_ARG = [ over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT 1 en", "snippet": "We may view ùëø ^ = [ ùíô ^ 1 , ‚Ä¶ , ùíô ^ N ] \\hat{\\bm{X}}=[\\hat{\\bm{x}}_{1},\\ldots,\\hat{\\bm{x}}_{N}] over^ start_ARG bold_italic_X end_ARG = [ over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ] as a noisy version of ùëø = [ ùíô 1 , ‚Ä¶ , ùíô N ] \\bm{X}=[\\bm{x}_{1},\\ldots,\\bm{x}_{N}] bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ] : ùíô ^ i = ùíô i + ùíò i , \\hat{\\bm{x}}_{i}=\\bm{x}_{i}+\\bm{w}_{i}, over^ "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS3.p4", "title": "To encode vectors that fall into the region spanned by ùíô ^ i \\hat{\\bm{x}}_{i} over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , we can cover the region with non-ov", "snippet": "To encode vectors that fall into the region spanned by ùíô ^ i \\hat{\\bm{x}}_{i} over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , we can cover the region with non-overlapping balls of radius œµ \\epsilon italic_œµ , as illustrated in Figure 3.11 . When the volume of the region spanned by ùíô ^ i \\hat{\\bm{x}}_{i} over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is significantly larger than the volume of the œµ \\epsilon italic_œµ -ball, the total number of balls that we need to cover the region is approximately equal to the ratio of"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample8.p1", "title": "Figure 3.11 shows an example of a 2D distribution with an ellipsoidal support ‚Äì approximating the support of a 2D Gaussian distribution. The region is covered by small balls of size œµ \\epsilon italic_", "snippet": "Figure 3.11 shows an example of a 2D distribution with an ellipsoidal support ‚Äì approximating the support of a 2D Gaussian distribution. The region is covered by small balls of size œµ \\epsilon italic_œµ . All the balls are numbered from 1 1 1 to say n n italic_n . Then given any vector ùíô \\bm{x} bold_italic_x in this region, we only need to determine to which œµ \\epsilon italic_œµ -ball center it is the closest, denoted as ball œµ ‚Å° ( ùíô ) \\operatorname{ball}_{\\epsilon}(\\bm{x}) roman_ball start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_x ) . To remember ùíô \\bm{x} bold_italic_x , we only "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS3.p5", "title": "From the above derivation, we know that the coding rate ‚Ñõ œµ ‚Äã ( ùëø ) \\mathcal{R}_{\\epsilon}(\\bm{X}) caligraphic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_X ) is (approximately) ach", "snippet": "From the above derivation, we know that the coding rate ‚Ñõ œµ ‚Äã ( ùëø ) \\mathcal{R}_{\\epsilon}(\\bm{X}) caligraphic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_X ) is (approximately) achievable with an explicit encoding (and decoding) scheme. It has two interesting properties: ‚Ä¢ First, one may notice that R œµ ‚Äã ( ùëø ) R_{\\epsilon}(\\bm{X}) italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_X ) closely resembles the rate distortion function of a Gaussian source [ CT91 ] . Indeed, when œµ \\epsilon italic_œµ is small, the above expression is a close approximation to t"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.I1.i1.p1", "title": "First, one may notice that R œµ ‚Äã ( ùëø ) R_{\\epsilon}(\\bm{X}) italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_X ) closely resembles the rate distortion function of a Gaussian sourc", "snippet": "First, one may notice that R œµ ‚Äã ( ùëø ) R_{\\epsilon}(\\bm{X}) italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_X ) closely resembles the rate distortion function of a Gaussian source [ CT91 ] . Indeed, when œµ \\epsilon italic_œµ is small, the above expression is a close approximation to the rate distortion of a Gaussian source, as pointed out by [ MDH+07 ] ."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.I1.i2.p1", "title": "Second, the same closed-form coding rate R œµ ‚Äã ( ùëø ) R_{\\epsilon}(\\bm{X}) italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_X ) can be derived as an approximation of ‚Ñõ œµ ‚Äã ( ùëø ) \\m", "snippet": "Second, the same closed-form coding rate R œµ ‚Äã ( ùëø ) R_{\\epsilon}(\\bm{X}) italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_X ) can be derived as an approximation of ‚Ñõ œµ ‚Äã ( ùëø ) \\mathcal{R}_{\\epsilon}(\\bm{X}) caligraphic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_X ) if the data ùëø \\bm{X} bold_italic_X are assumed to be from a linear subspace. This can be shown by properly quantifying the singular value decomposition (SVD) of ùëø = ùëº ‚Äã ùö∫ ‚Äã ùëΩ ‚ä§ \\bm{X}=\\bm{U}\\bm{\\Sigma}\\bm{V}^{\\top} bold_italic_X = bold_italic_U bold_Œ£ bold_italic_V start_POSTSUPERSCRIPT ‚ä§ en"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS4.p1", "title": "As we have discussed before, the given dataset ùëø \\bm{X} bold_italic_X often has low-dimensional intrinsic structures. Hence, encoding it as a general Gaussian would be very redundant. If we can identi", "snippet": "As we have discussed before, the given dataset ùëø \\bm{X} bold_italic_X often has low-dimensional intrinsic structures. Hence, encoding it as a general Gaussian would be very redundant. If we can identify those intrinsic structures in ùëø \\bm{X} bold_italic_X , we could design much better coding schemes that give much lower coding rates. Or equivalently, the codes used to encode such ùëø \\bm{X} bold_italic_X can be compressed. We will see that compression gives a unifying computable way to identify such structures. In this section, we demonstrate this important idea with the most basic family of low"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample9.p1", "title": "Figure 3.12 shows an example in which the data ùëø \\bm{X} bold_italic_X are distributed around two subspaces (or low-dimensional Gaussians). If they are viewed and coded together as one single Gaussian,", "snippet": "Figure 3.12 shows an example in which the data ùëø \\bm{X} bold_italic_X are distributed around two subspaces (or low-dimensional Gaussians). If they are viewed and coded together as one single Gaussian, the associated discrete (lossy) code book, represented by all the blue balls, is obviously very redundant. We can try to identify the locations of the two subspaces, denoted by S 1 S_{1} italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and S 2 S_{2} italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , and design a code book that only covers the two subspaces, i.e., the green balls. If we can correc"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS4.p2", "title": "So, more generally speaking, if the data are drawn from any mixture of subspaces or low-dimensional Gaussians, it would be desirable to identify those components and encode the data based on the intri", "snippet": "So, more generally speaking, if the data are drawn from any mixture of subspaces or low-dimensional Gaussians, it would be desirable to identify those components and encode the data based on the intrinsic dimensions of those components. It turns out that we do not lose much generality by assuming that the data are drawn from a mixture of low-dimensional Gaussians. This is because a mixture of Gaussians can closely approximate most general distributions [ BDS16 ] ."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS4.SSS0.Px1.p1", "title": "Now for this specific family of distributions, how can we effectively and efficiently identify those low-dimensional components from a set of samples ùëø = [ ùíô 1 , ùíô 2 , ‚Ä¶ , ùíô N ] , \\bm{X}=\\left[\\bm{x}_", "snippet": "Now for this specific family of distributions, how can we effectively and efficiently identify those low-dimensional components from a set of samples ùëø = [ ùíô 1 , ùíô 2 , ‚Ä¶ , ùíô N ] , \\bm{X}=\\left[\\bm{x}_{1},\\bm{x}_{2},\\ldots,\\bm{x}_{N}\\right], bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ] , (3.3.25) drawn from them? In other words, given the whole data set ùëø \\bm{X} bold_italic_X , we want to partition, or cluster, it into multiple, say K K italic_K"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS4.SSS0.Px2.p1", "title": "The main difficulty in solving the above clustering problem is that we normally do not know the number of clusters K K italic_K , nor do we know the dimension of each component. There has been a long ", "snippet": "The main difficulty in solving the above clustering problem is that we normally do not know the number of clusters K K italic_K , nor do we know the dimension of each component. There has been a long history for the study of this clustering problem. The textbook [ VMS16 ] gives a systematic and comprehensive coverage of different approaches to this problem. To find an effective approach to this problem, we first need to understand and clarify why we want to cluster. In other words, what exactly do we gain from clustering the data, compared with not to? How do we measure the gain? From the pers"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS4.SSS0.Px2.p2", "title": "For any given data set ùëø \\bm{X} bold_italic_X , there are already two obvious encoding schemes as the baseline. They represent two extreme ways to encode the data: ‚Ä¢ Simply view all the samples togeth", "snippet": "For any given data set ùëø \\bm{X} bold_italic_X , there are already two obvious encoding schemes as the baseline. They represent two extreme ways to encode the data: ‚Ä¢ Simply view all the samples together drawn as from one single Gaussian. The associated coding rate is, as derived before, given by: ‚Ñõ œµ ‚Äã ( ùëø ) ‚âà R œµ ‚Äã ( ùëø ) = 1 2 ‚Äã log ‚Äã det ( ùë∞ + D N ‚Äã œµ 2 ‚Äã ùëø ‚Äã ùëø ‚ä§ ) . \\mathcal{R}_{\\epsilon}(\\bm{X})\\approx R_{\\epsilon}(\\bm{X})=\\frac{1}{2}\\log\\det\\left(\\bm{I}+\\frac{D}{N\\epsilon^{2}}\\bm{X}\\bm{X}^{\\top}\\right). caligraphic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_X ) ‚âà itali"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.I2.i1.p1", "title": "Simply view all the samples together drawn as from one single Gaussian. The associated coding rate is, as derived before, given by: ‚Ñõ œµ ‚Äã ( ùëø ) ‚âà R œµ ‚Äã ( ùëø ) = 1 2 ‚Äã log ‚Äã det ( ùë∞ + D N ‚Äã œµ 2 ‚Äã ùëø ‚Äã ùëø ", "snippet": "Simply view all the samples together drawn as from one single Gaussian. The associated coding rate is, as derived before, given by: ‚Ñõ œµ ‚Äã ( ùëø ) ‚âà R œµ ‚Äã ( ùëø ) = 1 2 ‚Äã log ‚Äã det ( ùë∞ + D N ‚Äã œµ 2 ‚Äã ùëø ‚Äã ùëø ‚ä§ ) . \\mathcal{R}_{\\epsilon}(\\bm{X})\\approx R_{\\epsilon}(\\bm{X})=\\frac{1}{2}\\log\\det\\left(\\bm{I}+\\frac{D}{N\\epsilon^{2}}\\bm{X}\\bm{X}^{\\top}\\right). caligraphic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_X ) ‚âà italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_X ) = divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_log roman_det ( bold_italic_I + divide sta"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.I2.i2.p1", "title": "Simply memorize all the samples separately by assigning a different number to each sample. The coding rate would be: ‚Ñõ 0 ‚Äã ( ùëø ) = log ‚Å° ( N ) . \\mathcal{R}_{0}(\\bm{X})=\\log(N). caligraphic_R start_PO", "snippet": "Simply memorize all the samples separately by assigning a different number to each sample. The coding rate would be: ‚Ñõ 0 ‚Äã ( ùëø ) = log ‚Å° ( N ) . \\mathcal{R}_{0}(\\bm{X})=\\log(N). caligraphic_R start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( bold_italic_X ) = roman_log ( italic_N ) . (3.3.28)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS4.SSS0.Px2.p3", "title": "Note that either coding scheme can become the ‚Äúoptimal‚Äù solution for certain (extreme) choice of the quantization error œµ \\epsilon italic_œµ : 1. Lazy Regime : If we choose œµ \\epsilon italic_œµ to be ex", "snippet": "Note that either coding scheme can become the ‚Äúoptimal‚Äù solution for certain (extreme) choice of the quantization error œµ \\epsilon italic_œµ : 1. Lazy Regime : If we choose œµ \\epsilon italic_œµ to be extremely large, all samples in ùëø \\bm{X} bold_italic_X can be covered by a single ball. The rate is lim œµ ‚Üí ‚àû ‚Ñõ œµ ‚Üí 1 2 ‚Äã log ‚Äã det ( ùë∞ ) = 0 \\lim_{\\epsilon\\rightarrow\\infty}\\mathcal{R}_{\\epsilon}\\rightarrow\\frac{1}{2}\\log\\det(\\bm{I})=0 roman_lim start_POSTSUBSCRIPT italic_œµ ‚Üí ‚àû end_POSTSUBSCRIPT caligraphic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ‚Üí divide start_ARG 1 end_ARG start_ARG 2 en"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.I3.i1.p1", "title": "Lazy Regime : If we choose œµ \\epsilon italic_œµ to be extremely large, all samples in ùëø \\bm{X} bold_italic_X can be covered by a single ball. The rate is lim œµ ‚Üí ‚àû ‚Ñõ œµ ‚Üí 1 2 ‚Äã log ‚Äã det ( ùë∞ ) = 0 \\lim_", "snippet": "Lazy Regime : If we choose œµ \\epsilon italic_œµ to be extremely large, all samples in ùëø \\bm{X} bold_italic_X can be covered by a single ball. The rate is lim œµ ‚Üí ‚àû ‚Ñõ œµ ‚Üí 1 2 ‚Äã log ‚Äã det ( ùë∞ ) = 0 \\lim_{\\epsilon\\rightarrow\\infty}\\mathcal{R}_{\\epsilon}\\rightarrow\\frac{1}{2}\\log\\det(\\bm{I})=0 roman_lim start_POSTSUBSCRIPT italic_œµ ‚Üí ‚àû end_POSTSUBSCRIPT caligraphic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ‚Üí divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_log roman_det ( bold_italic_I ) = 0 ."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.I3.i2.p1", "title": "Memorization Regime : If œµ \\epsilon italic_œµ is extremely small, every sample in ùëø \\bm{X} bold_italic_X is covered by a different œµ \\epsilon italic_œµ -ball, hence the total is N N italic_N . The rate ", "snippet": "Memorization Regime : If œµ \\epsilon italic_œµ is extremely small, every sample in ùëø \\bm{X} bold_italic_X is covered by a different œµ \\epsilon italic_œµ -ball, hence the total is N N italic_N . The rate is lim œµ ‚Üí 0 ‚Ñõ œµ ‚Üí log ‚Å° ( N ) \\lim_{\\epsilon\\rightarrow 0}\\mathcal{R}_{\\epsilon}\\rightarrow\\log(N) roman_lim start_POSTSUBSCRIPT italic_œµ ‚Üí 0 end_POSTSUBSCRIPT caligraphic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ‚Üí roman_log ( italic_N ) ."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample10.p1", "title": "To see when the memorization regime is preferred or not, let us consider a number, say N N italic_N , of samples randomly distributed in a unit area on a 2D plane. 19 19 19 Say the points are drawn by", "snippet": "To see when the memorization regime is preferred or not, let us consider a number, say N N italic_N , of samples randomly distributed in a unit area on a 2D plane. 19 19 19 Say the points are drawn by a Poisson process with density N N italic_N points per unit area. Imagine we try to design a lossy coding scheme with a fixed quantization error œµ \\epsilon italic_œµ . This is equivalent to putting an œµ \\epsilon italic_œµ -disc around each sample, as shown in Figure 3.13 . When N N italic_N is small, the chance that all the discs overlap with each other is zero. A codebook of size N N italic_N is n"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS4.SSS0.Px2.p4", "title": "Both the lazy and memorization regimes are somewhat trivial and perhaps are of little theoretical or practical interest. Either scheme would be far from optimal when used to encode a large number of s", "snippet": "Both the lazy and memorization regimes are somewhat trivial and perhaps are of little theoretical or practical interest. Either scheme would be far from optimal when used to encode a large number of samples drawn from a distribution that has a compact and low-dimensional support . The interesting regime exists in between these two."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample11.p1", "title": "Figure 3.14 shows an example with noisy samples drawn from two lines and one plane in ‚Ñù 3 \\mathbb{R}^{3} blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT . As we notice from the plot (c) on th", "snippet": "Figure 3.14 shows an example with noisy samples drawn from two lines and one plane in ‚Ñù 3 \\mathbb{R}^{3} blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT . As we notice from the plot (c) on the right, the optimal coding rate decreases monotonically as we increase œµ \\epsilon italic_œµ , as anticipated from the property of the rate distortion function. The plots (a) and (b) show, when varying œµ \\epsilon italic_œµ from very small (near zero) to very large (towards infinite), the optimal number of clusters when the coding rate is minimal. We can clearly see the lazy regime and the memorizati"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS4.SSS0.Px2.p5", "title": "From the above discussion and examples, we see that, when the quantization error relative to the sample density 22 22 22 or the sample density relative to the quantization error is in a proper range, ", "snippet": "From the above discussion and examples, we see that, when the quantization error relative to the sample density 22 22 22 or the sample density relative to the quantization error is in a proper range, minimizing the lossy coding rate would allow us to uncover the underlying (low-dimensional) distribution of the sampled data. Hence, quantization, started as a choice of practicality, seems to be becoming necessary for learning a continuous distribution from its empirical distribution with finite samples. Although a rigorous theory for explaining this phenomenon remains elusive, here, for learning"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS4.SSS0.Px2.p6", "title": "Let us use the simple example shown in Figure 3.12 to illustrate the basic ideas. If one can partition all samples in ùëø \\bm{X} bold_italic_X into two clusters in ùëø 1 \\bm{X}_{1} bold_italic_X start_POS", "snippet": "Let us use the simple example shown in Figure 3.12 to illustrate the basic ideas. If one can partition all samples in ùëø \\bm{X} bold_italic_X into two clusters in ùëø 1 \\bm{X}_{1} bold_italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and ùëø 2 \\bm{X}_{2} bold_italic_X start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , with N 1 N_{1} italic_N start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and N 2 N_{2} italic_N start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT samples respectively, then the associated coding rate would be 23 23 23 We here ignore some overhead bits needed to encode the membership for each sample, say via the "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS4.SSS0.Px3.p1", "title": "The remaining question is how we optimize the above coding rate objective to find the optimal clusters. There are three natural approaches to this objective: 1. We may start with the whole set ùëø \\bm{X", "snippet": "The remaining question is how we optimize the above coding rate objective to find the optimal clusters. There are three natural approaches to this objective: 1. We may start with the whole set ùëø \\bm{X} bold_italic_X as a single cluster (i.e. the lazy regime) and then search (say randomly) to partition it so that it would lead to a smaller coding rate. 2. Inversely, we may start with each sample ùíô i \\bm{x}_{i} bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT as its own cluster (i.e. the memorization regime) and search to merge clusters that would result in a smaller coding rate. 3. "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.I4.i1.p1", "title": "We may start with the whole set ùëø \\bm{X} bold_italic_X as a single cluster (i.e. the lazy regime) and then search (say randomly) to partition it so that it would lead to a smaller coding rate.", "snippet": "We may start with the whole set ùëø \\bm{X} bold_italic_X as a single cluster (i.e. the lazy regime) and then search (say randomly) to partition it so that it would lead to a smaller coding rate."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.I4.i2.p1", "title": "Inversely, we may start with each sample ùíô i \\bm{x}_{i} bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT as its own cluster (i.e. the memorization regime) and search to merge clusters that", "snippet": "Inversely, we may start with each sample ùíô i \\bm{x}_{i} bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT as its own cluster (i.e. the memorization regime) and search to merge clusters that would result in a smaller coding rate."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.I4.i3.p1", "title": "Alternatively, if we could represent (or approximate) the membership ùö∑ \\bm{\\Pi} bold_Œ† as some continuous parameters, we may use optimization methods such as gradient descent (GD).", "snippet": "Alternatively, if we could represent (or approximate) the membership ùö∑ \\bm{\\Pi} bold_Œ† as some continuous parameters, we may use optimization methods such as gradient descent (GD)."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS4.SSS0.Px3.p2", "title": "The second approach was originally suggested in the work of [ MDH+07a ] . It demonstrates the benefit of being able to evaluate the coding rate efficiently (say with an analytical form). With it, the ", "snippet": "The second approach was originally suggested in the work of [ MDH+07a ] . It demonstrates the benefit of being able to evaluate the coding rate efficiently (say with an analytical form). With it, the (low-dimensional) clusters of the data can be found rather efficiently and effectively via the principle of minimizing coding length (MCL). Note that for a cluster ùëø k \\bm{X}_{k} bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT with N k N_{k} italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT samples, the length of binary bits needed to encode all the samples in ùëø k \\bm{X}_{k} bol"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS4.SSS0.Px3.p3", "title": "Then, given any two separate clusters ùëø 1 \\bm{X}_{1} bold_italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and ùëø 2 \\bm{X}_{2} bold_italic_X start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , we can decide whet", "snippet": "Then, given any two separate clusters ùëø 1 \\bm{X}_{1} bold_italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and ùëø 2 \\bm{X}_{2} bold_italic_X start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , we can decide whether to merge them or not based on the difference between the two coding lengths: L ‚Äã ( ùëø k ‚à™ ùëø l ) ‚àí L c ‚Äã ( ùëø k , ùëø l ) L(\\bm{X}_{k}\\cup\\bm{X}_{l})-L^{c}(\\bm{X}_{k},\\bm{X}_{l}) italic_L ( bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ‚à™ bold_italic_X start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ) - italic_L start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_X start_P"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.SS4.SSS0.Px3.p4", "title": "Note that this algorithm is tractable as the total number of (pairwise) comparisons and merges is about O ‚Äã ( N 2 ‚Äã log ‚Å° N ) O(N^{2}\\log N) italic_O ( italic_N start_POSTSUPERSCRIPT 2 end_POSTSUPERSC", "snippet": "Note that this algorithm is tractable as the total number of (pairwise) comparisons and merges is about O ‚Äã ( N 2 ‚Äã log ‚Å° N ) O(N^{2}\\log N) italic_O ( italic_N start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT roman_log italic_N ) . However, due to its greedy nature, there is no theoretical guarantee that the process will converge to the globally optimal clustering solution. Nevertheless, as reported in [ MDH+07a ] , in practice, this seemingly simple algorithm works extremely well. The clustering results plotted in Figure 3.14 were actually computed by this algorithm."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample12.p1", "title": "The above measure of coding length and the associated clustering algorithm assume the data distribution is a mixture of (low-dimensional) Gaussians. Although this seems somewhat idealistic, the measur", "snippet": "The above measure of coding length and the associated clustering algorithm assume the data distribution is a mixture of (low-dimensional) Gaussians. Although this seems somewhat idealistic, the measure and algorithm can already be very useful and even powerful in scenarios when the model is (approximately) valid."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample12.p2", "title": "For example, a natural image typically consists of multiple regions with nearly homogeneous textures. If we take many small windows from each region, they should resemble samples drawn from a (low-dim", "snippet": "For example, a natural image typically consists of multiple regions with nearly homogeneous textures. If we take many small windows from each region, they should resemble samples drawn from a (low-dimensional) Gaussian, as illustrated in Figure 3.15 . Figure 3.16 shows the results of image segmentation based on applying the above clustering algorithm to the image patches directly. More technical details regarding customizing the algorithm to the image segmentation problem can be found in [ MRY+11 ] . ‚ñ† \\blacksquare ‚ñ†"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.p1", "title": "So far in this chapter, we have discussed how to identify a distribution with low-dimensional structures through the principle of compression. As we have seen from the previous two sections, computati", "snippet": "So far in this chapter, we have discussed how to identify a distribution with low-dimensional structures through the principle of compression. As we have seen from the previous two sections, computational compression can be realized through either the denoising operation or through clustering. Figure 3.17 illustrates this concept with our favorite example."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.p2", "title": "Of course, the ultimate goal for identifying a data distribution is to use it to facilitate certain subsequent tasks such as segmentation, classification, or generation (of images). Hence, how the res", "snippet": "Of course, the ultimate goal for identifying a data distribution is to use it to facilitate certain subsequent tasks such as segmentation, classification, or generation (of images). Hence, how the resulting distribution is ‚Äúrepresented‚Äù matters tremendously with respect to how information related to these subsequent tasks can be efficiently and effectively retrieved and utilized. This naturally raises a fundamental question: what makes a representation truly ‚Äúgood‚Äù for downstream use? In the following, we will explore the essential properties that a meaningful and useful representation should "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS0.SSS0.Px1.p1", "title": "One may view a given dataset as samples of a random vector ùíô \\bm{x} bold_italic_x with a certain distribution in a high-dimensional space, say ‚Ñù D \\mathbb{R}^{D} blackboard_R start_POSTSUPERSCRIPT ita", "snippet": "One may view a given dataset as samples of a random vector ùíô \\bm{x} bold_italic_x with a certain distribution in a high-dimensional space, say ‚Ñù D \\mathbb{R}^{D} blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT . Typically, the distribution of ùíô \\bm{x} bold_italic_x has a much lower intrinsic dimension than the ambient space. Generally speaking, learning a representation refers to learning a continuous mapping, say f ‚Äã ( ‚ãÖ ) f(\\cdot) italic_f ( ‚ãÖ ) , that transforms ùíô \\bm{x} bold_italic_x to a so-called feature vector ùíõ \\bm{z} bold_italic_z in another (typically lower-dimensiona"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS0.SSS0.Px1.p1.p1", "title": "What is a principled and effective measure for the goodness of representations?", "snippet": "What is a principled and effective measure for the goodness of representations?"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS0.SSS0.Px1.p2", "title": "Conceptually, the quality of a representation ùíõ \\bm{z} bold_italic_z depends on how well it identifies the most relevant and sufficient information of ùíô \\bm{x} bold_italic_x for subsequent tasks and h", "snippet": "Conceptually, the quality of a representation ùíõ \\bm{z} bold_italic_z depends on how well it identifies the most relevant and sufficient information of ùíô \\bm{x} bold_italic_x for subsequent tasks and how efficiently it represents this information. For a long time, it was believed and argued that the ‚Äúsufficiency‚Äù or ‚Äúgoodness‚Äù of a learned feature representation should be defined in terms of a specific task. For example, ùíõ \\bm{z} bold_italic_z just needs to be sufficient for predicting the class label ùíö \\bm{y} bold_italic_y in a classification problem. Below, let us start with the classic probl"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS1.p1", "title": "Suppose that ùíô ‚àà ‚Ñù D \\bm{x}\\in\\mathbb{R}^{D} bold_italic_x ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT is a random vector drawn from a mixture of K K italic_K (component) distrib", "snippet": "Suppose that ùíô ‚àà ‚Ñù D \\bm{x}\\in\\mathbb{R}^{D} bold_italic_x ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT is a random vector drawn from a mixture of K K italic_K (component) distributions ùíü = { ùíü k } k = 1 K \\mathcal{D}=\\{\\mathcal{D}_{k}\\}_{k=1}^{K} caligraphic_D = { caligraphic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT . Give a finite set of i.i.d. samples ùëø = [ ùíô 1 , ùíô 2 , ‚Ä¶ , ùíô N ] ‚àà ‚Ñù D √ó N \\bm{X}=[\\bm{x}_{1},\\bm{x}_{2},\\ldots,\\bm{x}_{N}]\\in\\mathbb{R}^{D\\times N"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS1.SSS0.Px1.p1", "title": "Extensive studies have shown that for many practical datasets (e.g., images, audio, and natural languages), the (encoding) mapping from the data ùíô \\bm{x} bold_italic_x to its class label ùíö \\bm{y} bold", "snippet": "Extensive studies have shown that for many practical datasets (e.g., images, audio, and natural languages), the (encoding) mapping from the data ùíô \\bm{x} bold_italic_x to its class label ùíö \\bm{y} bold_italic_y can be effectively modeled by training a deep network, 26 26 26 Here let us not worry about yet which network we should use here and why. The purpose here is to consider any empirically tested deep network. We will leave the justification of the network architectures to the next chapter. here denoted as f ‚Äã ( ùíô , Œ∏ ) : ùíô ‚Ü¶ ùíö f(\\bm{x},\\theta):\\bm{x}\\mapsto\\bm{y} italic_f ( bold_italic_x ,"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS1.SSS0.Px1.p2", "title": "Despite its effectiveness and enormous popularity, there are two serious limitations with this approach: 1) It aims only to predict the labels ùíö \\bm{y} bold_italic_y even if they might be mislabeled. ", "snippet": "Despite its effectiveness and enormous popularity, there are two serious limitations with this approach: 1) It aims only to predict the labels ùíö \\bm{y} bold_italic_y even if they might be mislabeled. Empirical studies show that deep networks, used as a ‚Äúblack box,‚Äù can even fit random labels [ ZBH+17 ] . 2) With such an end-to-end data fitting, despite plenty of empirical efforts in trying to interpret the so-learned features, it is not clear to what extent the intermediate features learned by the network capture the intrinsic structures of the data that make meaningful classification possible"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS1.SSS0.Px2.p1", "title": "One popular approach to interpret the role of deep networks is to view outputs of intermediate layers of the network as selecting certain latent features ùíõ = f ‚Äã ( ùíô , Œ∏ ) ‚àà ‚Ñù d \\bm{z}=f(\\bm{x},\\theta", "snippet": "One popular approach to interpret the role of deep networks is to view outputs of intermediate layers of the network as selecting certain latent features ùíõ = f ‚Äã ( ùíô , Œ∏ ) ‚àà ‚Ñù d \\bm{z}=f(\\bm{x},\\theta)\\in\\mathbb{R}^{d} bold_italic_z = italic_f ( bold_italic_x , italic_Œ∏ ) ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT of the data that are discriminative among multiple classes. Learned representations ùíõ \\bm{z} bold_italic_z then facilitate the subsequent classification task for predicting the class label ùíö \\bm{y} bold_italic_y by optimizing a classifier g ‚Äã ( ùíõ ) g(\\bm{z}) it"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS1.SSS0.Px2.p2", "title": "Given one can overcome some caveats associated with this framework [ KTV18 ] , such as how to accurately evaluate mutual information with finite samples of degenerate distributions, this framework can", "snippet": "Given one can overcome some caveats associated with this framework [ KTV18 ] , such as how to accurately evaluate mutual information with finite samples of degenerate distributions, this framework can be helpful in explaining certain behaviors of deep networks. For example, recent work [ PHD20 ] indeed shows that the representations learned via the cross-entropy loss ( 3.4.2 ) exhibit a neural collapse phenomenon. That is, features of each class are mapped to a one-dimensional vector whereas all other information of the class is suppressed, as illustrated in Figure 3.18 ."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmremark8.p1", "title": "Neural collapse refers to a phenomenon observed in deep neural networks trained for classification, where the learned feature representations and classifier weights exhibit highly symmetric and struct", "snippet": "Neural collapse refers to a phenomenon observed in deep neural networks trained for classification, where the learned feature representations and classifier weights exhibit highly symmetric and structured behavior during the terminal phase of training [ PHD20 , ZDZ+21 ] . Specifically, within each class, features collapse to their class mean, and across classes, these means become maximally separated, forming a simplex equiangular configuration. The linear classifier aligns with the class mean up to rescaling. Additionally, the last-layer classifier converges to choosing whichever class has th"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS1.SSS0.Px2.p3", "title": "From the above example of classification, we see that the so-learned representation gives a very simple encoder that essentially maps each class of data to only one code word: the one-hot vector repre", "snippet": "From the above example of classification, we see that the so-learned representation gives a very simple encoder that essentially maps each class of data to only one code word: the one-hot vector representing each class. From the lossy compression perspective, such an encoder is too lossy to preserve information in the data distribution. Other information, such as that useful for tasks such as image generation, is severely lost in such a supervised learning process. To remedy this situation, we want to learn a different encoding scheme such that the resulting feature representation can capture "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS1.SSS0.Px3.p1", "title": "Whether the given data ùëø \\bm{X} bold_italic_X of a mixed distribution ùíü \\mathcal{D} caligraphic_D can be effectively classified or clustered depends on how separable (or discriminative) the component ", "snippet": "Whether the given data ùëø \\bm{X} bold_italic_X of a mixed distribution ùíü \\mathcal{D} caligraphic_D can be effectively classified or clustered depends on how separable (or discriminative) the component distributions ùíü k \\mathcal{D}_{k} caligraphic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT are (or can be made). One popular working assumption is that the distribution of each class has relatively low-dimensional intrinsic structures. Hence we may assume that the distribution ùíü k \\mathcal{D}_{k} caligraphic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT of each class has a support on a low-"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS1.SSS0.Px3.p2", "title": "Not only do we need to identify the low-dimensional distribution, but we also want to represent the distribution in a form that best facilitates subsequent tasks such as classification, clustering, an", "snippet": "Not only do we need to identify the low-dimensional distribution, but we also want to represent the distribution in a form that best facilitates subsequent tasks such as classification, clustering, and conditioned generation (as we will see in the future). To do so, we require our learned feature representations to have the following properties: 1. Within-Class Compressible: Features of samples from the same class should be strongly correlated in the sense that they belong to a low-dimensional linear subspace. 2. Between-Class Discriminative: Features of samples from different classes should b"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.I1.i1.p1", "title": "Within-Class Compressible: Features of samples from the same class should be strongly correlated in the sense that they belong to a low-dimensional linear subspace.", "snippet": "Within-Class Compressible: Features of samples from the same class should be strongly correlated in the sense that they belong to a low-dimensional linear subspace."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.I1.i2.p1", "title": "Between-Class Discriminative: Features of samples from different classes should be highly uncorrelated and belong to different low-dimensional linear subspaces.", "snippet": "Between-Class Discriminative: Features of samples from different classes should be highly uncorrelated and belong to different low-dimensional linear subspaces."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.I1.i3.p1", "title": "Maximally Diverse Representation: Dimension (or variance) of the features of each class should be as large as possible as long as they are incoherent to the other classes.", "snippet": "Maximally Diverse Representation: Dimension (or variance) of the features of each class should be as large as possible as long as they are incoherent to the other classes."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmremark9.p1", "title": "Linear discriminant analysis (LDA) [ HTF09 ] is a supervised dimensionality reduction technique that aims to find a linear projection of data that maximizes class separability. Specifically, given lab", "snippet": "Linear discriminant analysis (LDA) [ HTF09 ] is a supervised dimensionality reduction technique that aims to find a linear projection of data that maximizes class separability. Specifically, given labeled data, LDA seeks a linear transformation that projects high-dimensional inputs onto a lower-dimensional space where the classes are maximally separated. Note that PCA is an unsupervised method that projects data onto directions of maximum variance without considering class labels. While PCA focuses purely on preserving global variance structure, LDA explicitly exploits label information to enh"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS1.SSS0.Px3.p3", "title": "The third property is also important because we want the learned features to reveal all possible causes of why one class is different from all other classes. For example, to tell ‚Äúapple‚Äù from ‚Äúorange‚Äù", "snippet": "The third property is also important because we want the learned features to reveal all possible causes of why one class is different from all other classes. For example, to tell ‚Äúapple‚Äù from ‚Äúorange‚Äù, we care not only about color but also shape and the leaves. Ideally, the dimension of each subspace { ùíÆ k } \\{\\mathcal{S}_{k}\\} { caligraphic_S start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } should be equal to that of the corresponding submanifold ‚Ñ≥ k \\mathcal{M}_{k} caligraphic_M start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT . This property will be important if we would like the map f ‚Äã ( ùíô "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS1.SSS0.Px3.p4", "title": "In general, although the intrinsic structures of each class/cluster may be low-dimensional, they are by no means simply linear (or Gaussian) in their original representation ùíô \\bm{x} bold_italic_x and", "snippet": "In general, although the intrinsic structures of each class/cluster may be low-dimensional, they are by no means simply linear (or Gaussian) in their original representation ùíô \\bm{x} bold_italic_x and they need to be made linear first, through some nonlinear transformation. 27 27 27 We will discuss how this can be done explicitly in Chapter 5 . Therefore, overall, we use the nonlinear transformation f ‚Äã ( ùíô , Œ∏ ) f(\\bm{x},\\theta) italic_f ( bold_italic_x , italic_Œ∏ ) to seek a representation of the data such that the subspaces that represent all the classes are maximally incoherent linear subs"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS2.p1", "title": "Although the three properties‚Äî between-class discriminative , within-class compressible , and maximally diverse representation ‚Äîfor linear discriminative representations (LDRs) are all highly desired ", "snippet": "Although the three properties‚Äî between-class discriminative , within-class compressible , and maximally diverse representation ‚Äîfor linear discriminative representations (LDRs) are all highly desired properties of the learned representation ùíõ \\bm{z} bold_italic_z , they are by no means easy to obtain: Are these properties compatible so that we can expect to achieve them all at once? If so, is there a simple but principled objective that can measure the goodness of the resulting representations in terms of all these properties? The key to these questions is to find a principled ‚Äúmeasure of comp"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS2.p2", "title": "Without loss of generality, assume that the distribution ùíü \\mathcal{D} caligraphic_D of the random vector ùíô \\bm{x} bold_italic_x is supported on a mixture of distributions, i.e., ùíü = ‚à™ k = 1 K ùíü k \\ma", "snippet": "Without loss of generality, assume that the distribution ùíü \\mathcal{D} caligraphic_D of the random vector ùíô \\bm{x} bold_italic_x is supported on a mixture of distributions, i.e., ùíü = ‚à™ k = 1 K ùíü k \\mathcal{D}=\\cup_{k=1}^{K}\\mathcal{D}_{k} caligraphic_D = ‚à™ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT caligraphic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , where each ùíü k ‚äÇ ‚Ñù D \\mathcal{D}_{k}\\subset\\mathbb{R}^{D} caligraphic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ‚äÇ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPER"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS2.p3", "title": "On one hand, for learned features to be discriminative, features of different classes/clusters are preferred to be maximally incoherent to each other. Hence, they together should span a space of the l", "snippet": "On one hand, for learned features to be discriminative, features of different classes/clusters are preferred to be maximally incoherent to each other. Hence, they together should span a space of the largest possible volume (or dimension) and the coding rate of the whole set ùíÅ \\bm{Z} bold_italic_Z should be as large as possible. On the other hand, learned features of the same class/cluster should be highly correlated and coherent. Hence, each class/cluster should only span a space (or subspace) of a very small volume and the coding rate should be as small as possible. Now, we will introduce how"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS2.SSS0.Px1.p1", "title": "Notably, a practical challenge in evaluating the coding rate is that the underlying distribution of the feature representations ùíÅ \\bm{Z} bold_italic_Z is typically unknown. To address this, we may app", "snippet": "Notably, a practical challenge in evaluating the coding rate is that the underlying distribution of the feature representations ùíÅ \\bm{Z} bold_italic_Z is typically unknown. To address this, we may approximate the features ùíÅ = [ ùíõ 1 , ‚Ä¶ , ùíõ N ] \\bm{Z}=[\\bm{z}_{1},\\ldots,\\bm{z}_{N}] bold_italic_Z = [ bold_italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_z start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ] as samples drawn from a multivariate Gaussian distribution. Under this assumption, as discussed in Chapter 3.3.3 , the compactness of the features ùíÅ \\bm{Z} bold_italic_Z as a wh"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS2.SSS0.Px1.p2", "title": "On the other hand, we hope that a nonlinear transformation f ‚Äã ( ùíô ) f(\\bm{x}) italic_f ( bold_italic_x ) maps each class-specific submanifold ‚Ñ≥ k ‚äÇ ‚Ñù D \\mathcal{M}_{k}\\subset\\mathbb{R}^{D} caligraphi", "snippet": "On the other hand, we hope that a nonlinear transformation f ‚Äã ( ùíô ) f(\\bm{x}) italic_f ( bold_italic_x ) maps each class-specific submanifold ‚Ñ≥ k ‚äÇ ‚Ñù D \\mathcal{M}_{k}\\subset\\mathbb{R}^{D} caligraphic_M start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ‚äÇ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT to a maximally incoherent linear subspace ùíÆ k ‚äÇ ‚Ñù d \\mathcal{S}_{k}\\subset\\mathbb{R}^{d} caligraphic_S start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ‚äÇ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT such that the learned features ùíÅ \\bm{Z} bold_italic_Z lie in a unio"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS2.SSS0.Px1.p3", "title": "Therefore, a good representation ùíÅ \\bm{Z} bold_italic_Z of ùëø \\bm{X} bold_italic_X is the one that achieves a large difference between the coding rate for the whole and that for all the classes: Œî ‚Äã R ", "snippet": "Therefore, a good representation ùíÅ \\bm{Z} bold_italic_Z of ùëø \\bm{X} bold_italic_X is the one that achieves a large difference between the coding rate for the whole and that for all the classes: Œî ‚Äã R œµ ‚Äã ( ùíÅ ) ‚âê R œµ ‚Äã ( ùíÅ ) ‚àí R œµ c ‚Äã ( ùíÅ ) . \\Delta R_{\\epsilon}(\\bm{Z})\\doteq R_{\\epsilon}(\\bm{Z})-R_{\\epsilon}^{c}(\\bm{Z}). roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ) ‚âê italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ) - italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_ita"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS2.SSS0.Px1.p4", "title": "If we choose our feature mapping f ‚Äã ( ‚ãÖ ) f(\\cdot) italic_f ( ‚ãÖ ) to be a deep neural network f ‚Äã ( ‚ãÖ , Œ∏ ) f(\\cdot,\\theta) italic_f ( ‚ãÖ , italic_Œ∏ ) with network parameters Œ∏ \\theta italic_Œ∏ , the o", "snippet": "If we choose our feature mapping f ‚Äã ( ‚ãÖ ) f(\\cdot) italic_f ( ‚ãÖ ) to be a deep neural network f ‚Äã ( ‚ãÖ , Œ∏ ) f(\\cdot,\\theta) italic_f ( ‚ãÖ , italic_Œ∏ ) with network parameters Œ∏ \\theta italic_Œ∏ , the overall process of the feature representation and the resulting rate reduction can be illustrated by the following diagram: ùëø ‚Üí f ‚Äã ( ùíô , Œ∏ ) ùíÅ ‚Üí œµ Œî ‚Äã R œµ ‚Äã ( ùíÅ ) . \\bm{X}\\xrightarrow{\\hskip 5.69054ptf(\\bm{x},\\theta)\\hskip 5.69054pt}\\bm{Z}\\xrightarrow{\\hskip 5.69054pt\\epsilon\\hskip 5.69054pt}\\Delta R_{\\epsilon}(\\bm{Z}). bold_italic_X start_ARROW start_OVERACCENT italic_f ( bold_italic_x , italic_Œ∏"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS2.SSS0.Px1.p5", "title": "Once the representations are comparable, the goal becomes to learn a set of features ùíÅ = f ‚Äã ( ùëø , Œ∏ ) \\bm{Z}=f(\\bm{X},\\theta) bold_italic_Z = italic_f ( bold_italic_X , italic_Œ∏ ) such that they maxi", "snippet": "Once the representations are comparable, the goal becomes to learn a set of features ùíÅ = f ‚Äã ( ùëø , Œ∏ ) \\bm{Z}=f(\\bm{X},\\theta) bold_italic_Z = italic_f ( bold_italic_X , italic_Œ∏ ) such that they maximize the reduction between the coding rate of all features and that of the sum of features w.r.t. their classes: max Œ∏ \\displaystyle\\max_{\\theta} roman_max start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT Œî ‚Äã R œµ ‚Äã ( ùíÅ ) ‚âê R œµ ‚Äã ( ùíÅ ) ‚àí R œµ c ‚Äã ( ùíÅ ) , \\displaystyle\\;\\Delta R_{\\epsilon}\\big{(}\\bm{Z}\\big{)}\\doteq R_{\\epsilon}(\\bm{Z})-R_{\\epsilon}^{c}(\\bm{Z}), roman_Œî italic_R start_POSTSUBSCRIPT ital"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS2.SSS0.Px1.p6", "title": "Note that the above MCR 2 principle is designed for supervised learning problems, where the group memberships (or class labels) are known. However, this principle can be naturally extended to unsuperv", "snippet": "Note that the above MCR 2 principle is designed for supervised learning problems, where the group memberships (or class labels) are known. However, this principle can be naturally extended to unsupervised learning problems by introducing a membership matrix, which encodes the (potentially soft) assignment of each data point to latent groups or clusters. Specifically, let ùö∑ = { ùö∑ k } k = 1 K ‚äÇ ‚Ñù N √ó N \\bm{\\Pi}=\\{\\bm{\\Pi}_{k}\\}_{k=1}^{K}\\subset\\mathbb{R}^{N\\times N} bold_Œ† = { bold_Œ† start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUP"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS3.p1", "title": "In this subsection, we study the optimization properties of the MCR 2 function by analyzing its optimal solutions and the structure of its optimization landscape. To get around the technical difficult", "snippet": "In this subsection, we study the optimization properties of the MCR 2 function by analyzing its optimal solutions and the structure of its optimization landscape. To get around the technical difficulty introduced by the neural networks, we consider a simplified version of Problem ( 3.4.12 ) as follows: max ùíÅ R œµ ( ùíÅ ) ‚àí R œµ c ( ùíÅ ) s . t . ‚à• ùíÅ k ‚à• F 2 = N k , k = 1 , ‚Ä¶ , K . \\displaystyle\\max_{\\bm{Z}}\\ R_{\\epsilon}(\\bm{Z})-R_{\\epsilon}^{c}(\\bm{Z})\\qquad\\mathrm{s.t.}\\quad\\|\\bm{Z}_{k}\\|_{F}^{2}=N_{k},\\ k=1,\\dots,K. roman_max start_POSTSUBSCRIPT bold_italic_Z end_POSTSUBSCRIPT italic_R start_POST"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmtheorem7.p1", "title": "Suppose ùêô ‚àó = [ ùêô 1 ‚àó , ‚Ä¶ , ùêô K ‚àó ] \\bm{Z}^{\\ast}=[\\bm{Z}_{1}^{*},\\dots,\\bm{Z}_{K}^{*}] bold_italic_Z start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT = [ bold_italic_Z start_POSTSUBSCRIPT 1 end_POSTSUBSCR", "snippet": "Suppose ùêô ‚àó = [ ùêô 1 ‚àó , ‚Ä¶ , ùêô K ‚àó ] \\bm{Z}^{\\ast}=[\\bm{Z}_{1}^{*},\\dots,\\bm{Z}_{K}^{*}] bold_italic_Z start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT = [ bold_italic_Z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT , ‚Ä¶ , bold_italic_Z start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT ] is a global optimal solution of Problem ( 3.4.15 ). The following statements hold: ‚Ä¢ Between-Class Discriminative : As long as the ambient space is adequately large ( d ‚â• ‚àë k = 1 K d k d\\geq\\sum_{k=1}^{K}d_{k} italic_d ‚â• ‚àë start_POSTSUBSCR"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.I2.i1.p1", "title": "Between-Class Discriminative : As long as the ambient space is adequately large ( d ‚â• ‚àë k = 1 K d k d\\geq\\sum_{k=1}^{K}d_{k} italic_d ‚â• ‚àë start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTS", "snippet": "Between-Class Discriminative : As long as the ambient space is adequately large ( d ‚â• ‚àë k = 1 K d k d\\geq\\sum_{k=1}^{K}d_{k} italic_d ‚â• ‚àë start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ), the subspaces are all orthogonal to each other, i.e. , ( ùíÅ k ‚àó ) ‚ä§ ‚Äã ùíÅ l ‚àó = ùüé (\\bm{Z}_{k}^{*})^{\\top}\\bm{Z}_{l}^{*}=\\bm{0} ( bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_Z start_POST"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.I2.i2.p1", "title": "Maximally Diverse Representation : As long as the coding precision is adequately high, i.e., œµ 4 < c ‚ãÖ min k ‚Å° { N k N ‚Äã d 2 d k 2 } \\epsilon^{4}<c\\cdot\\min_{k}\\left\\{\\frac{N_{k}}{N}\\frac{d^{2}}{d_{k}", "snippet": "Maximally Diverse Representation : As long as the coding precision is adequately high, i.e., œµ 4 < c ‚ãÖ min k ‚Å° { N k N ‚Äã d 2 d k 2 } \\epsilon^{4}<c\\cdot\\min_{k}\\left\\{\\frac{N_{k}}{N}\\frac{d^{2}}{d_{k}^{2}}\\right\\} italic_œµ start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT < italic_c ‚ãÖ roman_min start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT { divide start_ARG italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG start_ARG italic_N end_ARG divide start_ARG italic_d start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POS"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS3.p2", "title": "This theorem indicates that the MCR 2 principle promotes embedding of data into multiple independent subspaces (as illustrated in Figure 3.22 ), with features distributed isotropically in each subspac", "snippet": "This theorem indicates that the MCR 2 principle promotes embedding of data into multiple independent subspaces (as illustrated in Figure 3.22 ), with features distributed isotropically in each subspace (except for possibly one dimension). Notably, this theorem also confirms that the features learned by the MCR 2 principle exhibit the desired low-dimensional discriminative properties discussed in Section 3.4.1 . In addition, among all such discriminative representations, it prefers the one with the highest dimensions in the ambient space. This is substantially different from the objective of in"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample13.p1", "title": "We here present how the MCR 2 objective helps learn better representations than the cross entropy ( 3.4.2 ) for image classification. Here we adopt the popular neural network architecture, the ResNet-", "snippet": "We here present how the MCR 2 objective helps learn better representations than the cross entropy ( 3.4.2 ) for image classification. Here we adopt the popular neural network architecture, the ResNet-18 [ HZR+16a ] , to model the feature mapping ùíõ = f ‚Äã ( ùíô , Œ∏ ) \\bm{z}=f(\\bm{x},\\theta) bold_italic_z = italic_f ( bold_italic_x , italic_Œ∏ ) . We optimize the neural network parameters Œ∏ \\theta italic_Œ∏ to maximize the coding rate reduction. We evaluate the performance with the CIFAR10 image classification dataset [ KH+09 ] ."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample13.p2", "title": "Figure 3.23(a) illustrates how the two rates and their difference (for both training and test data) evolves over epochs of training: After an initial phase, R œµ R_{\\epsilon} italic_R start_POSTSUBSCRI", "snippet": "Figure 3.23(a) illustrates how the two rates and their difference (for both training and test data) evolves over epochs of training: After an initial phase, R œµ R_{\\epsilon} italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT gradually increases while R œµ c R^{c}_{\\epsilon} italic_R start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT decreases, indicating that features ùíÅ \\bm{Z} bold_italic_Z are expanding as a whole while each class ùíÅ k \\bm{Z}_{k} bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT is being compressed. Figure 3.23(b) s"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS3.p3", "title": "However, there has been an apparent lack of justification of the network architectures used in the above experiments. It is yet unclear why the network adopted here (the ResNet-18) is suitable for rep", "snippet": "However, there has been an apparent lack of justification of the network architectures used in the above experiments. It is yet unclear why the network adopted here (the ResNet-18) is suitable for representing the map f ‚Äã ( ùíô , Œ∏ ) f(\\bm{x},\\theta) italic_f ( bold_italic_x , italic_Œ∏ ) , let alone for interpreting the layer operators and parameters Œ∏ \\theta italic_Œ∏ learned inside. In the next chapter, we will show how to derive network architectures and components entirely as a ‚Äúwhite box‚Äù from the desired objective (say the rate reduction) ."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS3.SSS0.Px1.p1", "title": "The above theorem characterizes properties of the global optima of the rate reduction objectives. What about other optima, such as local ones? Due to the constraints of the Frobenius norm, it is a dif", "snippet": "The above theorem characterizes properties of the global optima of the rate reduction objectives. What about other optima, such as local ones? Due to the constraints of the Frobenius norm, it is a difficult task to analyze Problem ( 3.4.15 ) from an optimization-theoretic perspective. Therefore, we consider the Lagrangian formulation of ( 3.4.15 ). This can be viewed as a tight relaxation or even an equivalent problem of ( 3.4.15 ) whose optimal solutions agree under specific settings of the regularization parameter; see [ WLP+24 , Proposition 1] . Specifically, the formulation we study, refer"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmtheorem8.p1", "title": "Let N k N_{k} italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT denote the number of training samples in the k k italic_k -th class for each k ‚àà { 1 , ‚Ä¶ , K } k\\in\\{1,\\dots,K\\} italic_k ‚àà { 1 , ", "snippet": "Let N k N_{k} italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT denote the number of training samples in the k k italic_k -th class for each k ‚àà { 1 , ‚Ä¶ , K } k\\in\\{1,\\dots,K\\} italic_k ‚àà { 1 , ‚Ä¶ , italic_K } , N max ‚âê max ‚Å° { N 1 , ‚Ä¶ , N K } N_{\\max}\\doteq\\max\\{N_{1},\\dots,N_{K}\\} italic_N start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT ‚âê roman_max { italic_N start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , italic_N start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT } , Œ± = d / ( N ‚Äã œµ 2 ) \\alpha=d/(N\\epsilon^{2}) italic_Œ± = italic_d / ( italic_N italic_œµ start_POSTSUPERSCRIPT 2 end_POSTSUPERS"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS3.SSS0.Px1.p2", "title": "This theorem explicitly characterizes the local and global optima of problem ( 3.4.16 ). Intuitively, this shows that the features represented by each local maximizer of Problem ( 3.4.16 ) are low-dim", "snippet": "This theorem explicitly characterizes the local and global optima of problem ( 3.4.16 ). Intuitively, this shows that the features represented by each local maximizer of Problem ( 3.4.16 ) are low-dimensional and discriminative. Although we have characterized the local and global optimal solutions in Theorem 3.8 , it remains unknown whether these solutions can be efficiently computed using GD to solve the problem ( 3.4.16 ), since GD may get stuck at other critical points such as a saddle point. Fortunately, [ SQW15 , LSJ+16 ] showed that if a function is twice continuously differentiable and "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmtheorem9.p1", "title": "Given a coding precision œµ > 0 \\epsilon>0 italic_œµ > 0 , if the regularization parameter satisfies ( 3.4.17 ), it holds that any critical point ùêô \\bm{Z} bold_italic_Z of the problem ( 3.4.16 ) is eith", "snippet": "Given a coding precision œµ > 0 \\epsilon>0 italic_œµ > 0 , if the regularization parameter satisfies ( 3.4.17 ), it holds that any critical point ùêô \\bm{Z} bold_italic_Z of the problem ( 3.4.16 ) is either a local maximizer or a strict saddle point."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.SS3.SSS0.Px1.p3", "title": "Together, the above two theorems show that the learned features associated with each local maximizer of the rate reduction objective‚Äînot just global maximizers‚Äîare structured as incoherent low-dimensi", "snippet": "Together, the above two theorems show that the learned features associated with each local maximizer of the rate reduction objective‚Äînot just global maximizers‚Äîare structured as incoherent low-dimensional subspaces. Furthermore, the (regularized) rate reduction objective ( 3.4.12 ) has a very benign landscape with only local maxima and strict saddles as critical points, as illustrated in Figure 3.25 . According to [ SQW15 , LSJ+16 ] , Theorems 3.8 and 3.9 imply that low-dimensional and discriminative representations (LDRs) can be efficiently found by applying (stochastic) gradient descent to t"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S5.p1", "title": "The use of denoising and diffusion for sampling has a rich history. The first work which is clearly about a diffusion model is probably [ SWM+15 ] , but before this there are many works about denoisin", "snippet": "The use of denoising and diffusion for sampling has a rich history. The first work which is clearly about a diffusion model is probably [ SWM+15 ] , but before this there are many works about denoising as a computational and statistical problem. The most relevant of these is probably [ Hyv05 ] , which explicitly uses the score function to denoise (as well as perform independent component analysis). The most popular follow-ups are basically co-occurring: [ HJA20 , SE19 ] . Since then, thousands of papers have built on diffusion models; we will revisit this topic in Chapter 5 ."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S5.p2", "title": "Many of these works use a different stochastic process than the simple linear combination ( 3.2.69 ). In fact, all works listed above emphasize the need to add independent Gaussian noise at the beginn", "snippet": "Many of these works use a different stochastic process than the simple linear combination ( 3.2.69 ). In fact, all works listed above emphasize the need to add independent Gaussian noise at the beginning of each step of the forward process. Theoretically-minded work actually uses Brownian motion or stochastic differential equations to formulate the forward process [ SSK+21 ] . However, since linear combinations of Gaussians still result in Gaussians, the marginal distributions of such processes still take the form of ( 3.2.69 ). Most of our discussion requires only that the marginal distributi"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S5.p3", "title": "On top of the theoretical work [ LY24 ] covered in Section 1.3.1 , and the lineage of work that it builds on, which studies the sampling efficiency of diffusion models when the data has low-dimensiona", "snippet": "On top of the theoretical work [ LY24 ] covered in Section 1.3.1 , and the lineage of work that it builds on, which studies the sampling efficiency of diffusion models when the data has low-dimensional structure, there is a large body of work which studies the training efficiency of diffusion models when the data has low-dimensional structure. Specifically, [ CHZ+23 ] and [ WZZ+24 ] characterized the approximation and estimation error of denoisers when the data belongs to a mixture of low-rank Gaussians, showing that the number of training samples required to accurately learn the distribution "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexercise1.p1", "title": "Please show that ( 3.2.4 ) is the optimal solution of Problem ( 3.2.3 ).", "snippet": "Please show that ( 3.2.4 ) is the optimal solution of Problem ( 3.2.3 )."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexercise2.p1", "title": "Consider random vectors ùíô ‚àà ‚Ñù D \\bm{x}\\in\\mathbb{R}^{D} bold_italic_x ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT and ùíö ‚àà ‚Ñù d \\bm{y}\\in\\mathbb{R}^{d} bold_italic_y ‚àà blackboard_R", "snippet": "Consider random vectors ùíô ‚àà ‚Ñù D \\bm{x}\\in\\mathbb{R}^{D} bold_italic_x ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT and ùíö ‚àà ‚Ñù d \\bm{y}\\in\\mathbb{R}^{d} bold_italic_y ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , such that the pair ( ùíô , ùíö ) ‚àà ‚Ñù D + d (\\bm{x},\\bm{y})\\in\\mathbb{R}^{D+d} ( bold_italic_x , bold_italic_y ) ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D + italic_d end_POSTSUPERSCRIPT is jointly Gaussian. This means that [ ùíô ùíö ] ‚àº ùí© ‚Äã ( [ ùùÅ ùíô ùùÅ ùíö ] , [ ùö∫ ùíô ùö∫ ùíô ‚Äã ùíö ùö∫ ùíô ‚Äã ùíö ‚ä§ ùö∫ ùíö ] ) , \\begin{bmatrix}\\bm{x}\\\\ \\bm{y}\\end{bmatrix}\\sim\\mathcal{N}\\l"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexercise2.p2", "title": "In this exercise, we will prove that the conditional distribution p ùíô ‚à£ ùíö p_{\\bm{x}\\mid\\bm{y}} italic_p start_POSTSUBSCRIPT bold_italic_x ‚à£ bold_italic_y end_POSTSUBSCRIPT is Gaussian: namely, p ùíô ‚à£ ùíö", "snippet": "In this exercise, we will prove that the conditional distribution p ùíô ‚à£ ùíö p_{\\bm{x}\\mid\\bm{y}} italic_p start_POSTSUBSCRIPT bold_italic_x ‚à£ bold_italic_y end_POSTSUBSCRIPT is Gaussian: namely, p ùíô ‚à£ ùíö ‚àº ùí© ‚Äã ( ùùÅ ùíô + ùö∫ ùíô ‚Äã ùíö ‚Äã ùö∫ ùíö ‚àí 1 ‚Äã ( ùíö ‚àí ùùÅ ùíö ) , ùö∫ ùíô ‚àí ùö∫ ùíô ‚Äã ùíö ‚Äã ùö∫ ùíö ‚àí 1 ‚Äã ùö∫ ùíô ‚Äã ùíö ‚ä§ ) . p_{\\bm{x}\\mid\\bm{y}}\\sim\\mathcal{N}\\left(\\bm{\\mu}_{\\bm{x}}+\\bm{\\Sigma}_{\\bm{x}\\bm{y}}\\bm{\\Sigma}_{\\bm{y}}^{-1}(\\bm{y}-\\bm{\\mu}_{\\bm{y}}),\\bm{\\Sigma}_{\\bm{x}}-\\bm{\\Sigma}_{\\bm{x}\\bm{y}}\\bm{\\Sigma}_{\\bm{y}}^{-1}\\bm{\\Sigma}_{\\bm{x}\\bm{y}}^{\\top}\\right). italic_p start_POSTSUBSCRIPT bold_italic_x ‚à£ bold_italic_y e"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexercise2.p3", "title": "1. Verify the following matrix identity for the covariance: [ ùö∫ ùíô ùö∫ ùíô ‚Äã ùíö ùö∫ ùíô ‚Äã ùíö ‚ä§ ùö∫ ùíö ] = [ ùë∞ D ùö∫ ùíô ‚Äã ùíö ‚Äã ùö∫ ùíö ‚àí 1 ùüé ùë∞ d ] ‚Äã [ ùö∫ ùíô ‚àí ùö∫ ùíô ‚Äã ùíö ‚Äã ùö∫ ùíö ‚àí 1 ‚Äã ùö∫ ùíô ‚Äã ùíö ‚ä§ ùüé ùüé ùö∫ ùíö ] ‚Äã [ ùë∞ D ùüé ùö∫ ùíö ‚àí 1 ‚Äã ùö∫ ùíô ‚Äã ", "snippet": "1. Verify the following matrix identity for the covariance: [ ùö∫ ùíô ùö∫ ùíô ‚Äã ùíö ùö∫ ùíô ‚Äã ùíö ‚ä§ ùö∫ ùíö ] = [ ùë∞ D ùö∫ ùíô ‚Äã ùíö ‚Äã ùö∫ ùíö ‚àí 1 ùüé ùë∞ d ] ‚Äã [ ùö∫ ùíô ‚àí ùö∫ ùíô ‚Äã ùíö ‚Äã ùö∫ ùíö ‚àí 1 ‚Äã ùö∫ ùíô ‚Äã ùíö ‚ä§ ùüé ùüé ùö∫ ùíö ] ‚Äã [ ùë∞ D ùüé ùö∫ ùíö ‚àí 1 ‚Äã ùö∫ ùíô ‚Äã ùíö ‚ä§ ùë∞ d ] . \\begin{bmatrix}\\bm{\\Sigma}_{\\bm{x}}&\\bm{\\Sigma}_{\\bm{x}\\bm{y}}\\\\ \\bm{\\Sigma}_{\\bm{x}\\bm{y}}^{\\top}&\\bm{\\Sigma}_{\\bm{y}}\\end{bmatrix}=\\begin{bmatrix}\\bm{I}_{D}&\\bm{\\Sigma}_{\\bm{x}\\bm{y}}\\bm{\\Sigma}_{\\bm{y}}^{-1}\\\\ \\mathbf{0}&\\bm{I}_{d}\\end{bmatrix}\\begin{bmatrix}\\bm{\\Sigma}_{\\bm{x}}-\\bm{\\Sigma}_{\\bm{x}\\bm{y}}\\bm{\\Sigma}_{\\bm{y}}^{-1}\\bm{\\Sigma}_{\\bm{x}\\bm{y}}^{\\top}&\\mathbf{0}\\\\ \\mathbf"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S6.I1.i1.p1", "title": "Verify the following matrix identity for the covariance: [ ùö∫ ùíô ùö∫ ùíô ‚Äã ùíö ùö∫ ùíô ‚Äã ùíö ‚ä§ ùö∫ ùíö ] = [ ùë∞ D ùö∫ ùíô ‚Äã ùíö ‚Äã ùö∫ ùíö ‚àí 1 ùüé ùë∞ d ] ‚Äã [ ùö∫ ùíô ‚àí ùö∫ ùíô ‚Äã ùíö ‚Äã ùö∫ ùíö ‚àí 1 ‚Äã ùö∫ ùíô ‚Äã ùíö ‚ä§ ùüé ùüé ùö∫ ùíö ] ‚Äã [ ùë∞ D ùüé ùö∫ ùíö ‚àí 1 ‚Äã ùö∫ ùíô ‚Äã ùíö ‚ä§", "snippet": "Verify the following matrix identity for the covariance: [ ùö∫ ùíô ùö∫ ùíô ‚Äã ùíö ùö∫ ùíô ‚Äã ùíö ‚ä§ ùö∫ ùíö ] = [ ùë∞ D ùö∫ ùíô ‚Äã ùíö ‚Äã ùö∫ ùíö ‚àí 1 ùüé ùë∞ d ] ‚Äã [ ùö∫ ùíô ‚àí ùö∫ ùíô ‚Äã ùíö ‚Äã ùö∫ ùíö ‚àí 1 ‚Äã ùö∫ ùíô ‚Äã ùíö ‚ä§ ùüé ùüé ùö∫ ùíö ] ‚Äã [ ùë∞ D ùüé ùö∫ ùíö ‚àí 1 ‚Äã ùö∫ ùíô ‚Äã ùíö ‚ä§ ùë∞ d ] . \\begin{bmatrix}\\bm{\\Sigma}_{\\bm{x}}&\\bm{\\Sigma}_{\\bm{x}\\bm{y}}\\\\ \\bm{\\Sigma}_{\\bm{x}\\bm{y}}^{\\top}&\\bm{\\Sigma}_{\\bm{y}}\\end{bmatrix}=\\begin{bmatrix}\\bm{I}_{D}&\\bm{\\Sigma}_{\\bm{x}\\bm{y}}\\bm{\\Sigma}_{\\bm{y}}^{-1}\\\\ \\mathbf{0}&\\bm{I}_{d}\\end{bmatrix}\\begin{bmatrix}\\bm{\\Sigma}_{\\bm{x}}-\\bm{\\Sigma}_{\\bm{x}\\bm{y}}\\bm{\\Sigma}_{\\bm{y}}^{-1}\\bm{\\Sigma}_{\\bm{x}\\bm{y}}^{\\top}&\\mathbf{0}\\\\ \\mathbf{0}"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S6.I1.i2.p1", "title": "Based on the previous identity, show that [ ùö∫ ùíô ùö∫ ùíô ‚Äã ùíö ùö∫ ùíô ‚Äã ùíö ‚ä§ ùö∫ ùíö ] ‚àí 1 = [ ùë∞ D ùüé ‚àí ùö∫ ùíö ‚àí 1 ‚Äã ùö∫ ùíô ‚Äã ùíö ‚ä§ ùë∞ d ] ‚Äã [ ( ùö∫ ùíô ‚àí ùö∫ ùíô ‚Äã ùíö ‚Äã ùö∫ ùíö ‚àí 1 ‚Äã ùö∫ ùíô ‚Äã ùíö ‚ä§ ) ‚àí 1 ùüé ùüé ùö∫ ùíö ‚àí 1 ] ‚Äã [ ùë∞ D ‚àí ùö∫ ùíô ‚Äã ùíö ‚Äã ùö∫ ùíö ", "snippet": "Based on the previous identity, show that [ ùö∫ ùíô ùö∫ ùíô ‚Äã ùíö ùö∫ ùíô ‚Äã ùíö ‚ä§ ùö∫ ùíö ] ‚àí 1 = [ ùë∞ D ùüé ‚àí ùö∫ ùíö ‚àí 1 ‚Äã ùö∫ ùíô ‚Äã ùíö ‚ä§ ùë∞ d ] ‚Äã [ ( ùö∫ ùíô ‚àí ùö∫ ùíô ‚Äã ùíö ‚Äã ùö∫ ùíö ‚àí 1 ‚Äã ùö∫ ùíô ‚Äã ùíö ‚ä§ ) ‚àí 1 ùüé ùüé ùö∫ ùíö ‚àí 1 ] ‚Äã [ ùë∞ D ‚àí ùö∫ ùíô ‚Äã ùíö ‚Äã ùö∫ ùíö ‚àí 1 ùüé ùë∞ d ] \\begin{bmatrix}\\bm{\\Sigma}_{\\bm{x}}&\\bm{\\Sigma}_{\\bm{x}\\bm{y}}\\\\ \\bm{\\Sigma}_{\\bm{x}\\bm{y}}^{\\top}&\\bm{\\Sigma}_{\\bm{y}}\\end{bmatrix}^{-1}=\\begin{bmatrix}\\bm{I}_{D}&\\mathbf{0}\\\\ -\\bm{\\Sigma}_{\\bm{y}}^{-1}\\bm{\\Sigma}_{\\bm{x}\\bm{y}}^{\\top}&\\bm{I}_{d}\\end{bmatrix}\\begin{bmatrix}\\left(\\bm{\\Sigma}_{\\bm{x}}-\\bm{\\Sigma}_{\\bm{x}\\bm{y}}\\bm{\\Sigma}_{\\bm{y}}^{-1}\\bm{\\Sigma}_{\\bm{x}\\bm{y}}^{\\top}\\r"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S6.I1.i3.p1", "title": "By dividing p ùíô , ùíö / p ùíö p_{\\bm{x},\\bm{y}}/p_{\\bm{y}} italic_p start_POSTSUBSCRIPT bold_italic_x , bold_italic_y end_POSTSUBSCRIPT / italic_p start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT , pro", "snippet": "By dividing p ùíô , ùíö / p ùíö p_{\\bm{x},\\bm{y}}/p_{\\bm{y}} italic_p start_POSTSUBSCRIPT bold_italic_x , bold_italic_y end_POSTSUBSCRIPT / italic_p start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT , prove Equation 3.6.1 . ( Hint: Using the previous identities, only minimal algebra should be necessary. For the normalizing constant, use Equation 3.6.3 to factor the determinant similarly. )"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexercise3.p1", "title": "Show the Sherman-Morrison-Woodbury identity, i.e., for matrices ùë® \\bm{A} bold_italic_A , ùë™ \\bm{C} bold_italic_C , ùëº \\bm{U} bold_italic_U , ùëΩ \\bm{V} bold_italic_V such that ùë® \\bm{A} bold_italic_A , ùë™ \\", "snippet": "Show the Sherman-Morrison-Woodbury identity, i.e., for matrices ùë® \\bm{A} bold_italic_A , ùë™ \\bm{C} bold_italic_C , ùëº \\bm{U} bold_italic_U , ùëΩ \\bm{V} bold_italic_V such that ùë® \\bm{A} bold_italic_A , ùë™ \\bm{C} bold_italic_C , and ùë® + ùëº ‚Äã ùë™ ‚Äã ùëΩ \\bm{A}+\\bm{U}\\bm{C}\\bm{V} bold_italic_A + bold_italic_U bold_italic_C bold_italic_V are invertible, ( ùë® + ùëº ‚Äã ùë™ ‚Äã ùëΩ ) ‚àí 1 = ùë® ‚àí 1 ‚àí ùë® ‚àí 1 ‚Äã ùëº ‚Äã ( ùë™ ‚àí 1 + ùëΩ ‚Äã ùë® ‚àí 1 ‚Äã ùëº ) ‚àí 1 ‚Äã ùëΩ ‚Äã ùë® ‚àí 1 (\\bm{A}+\\bm{U}\\bm{C}\\bm{V})^{-1}=\\bm{A}^{-1}-\\bm{A}^{-1}\\bm{U}(\\bm{C}^{-1}+\\bm{V}\\bm{A}^{-1}\\bm{U})^{-1}\\bm{V}\\bm{A}^{-1} ( bold_italic_A + bold_italic_U bold_italic_C bold_i"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexercise4.p1", "title": "Rederive the following, assuming ùíô t \\bm{x}_{t} bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT follows the generalized noise model ( 3.2.69 ). ‚Ä¢ Tweedie‚Äôs formula: ( 3.2.70 ). ‚Ä¢ The DDIM", "snippet": "Rederive the following, assuming ùíô t \\bm{x}_{t} bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT follows the generalized noise model ( 3.2.69 ). ‚Ä¢ Tweedie‚Äôs formula: ( 3.2.70 ). ‚Ä¢ The DDIM iteration: ( 3.2.71 ). ‚Ä¢ The Bayes optimal denoiser for a Gaussian mixture model: ( 3.2.72 )."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S6.I2.i1.p1", "title": "Tweedie‚Äôs formula: ( 3.2.70 ).", "snippet": "Tweedie‚Äôs formula: ( 3.2.70 )."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S6.I2.i2.p1", "title": "The DDIM iteration: ( 3.2.71 ).", "snippet": "The DDIM iteration: ( 3.2.71 )."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S6.I2.i3.p1", "title": "The Bayes optimal denoiser for a Gaussian mixture model: ( 3.2.72 ).", "snippet": "The Bayes optimal denoiser for a Gaussian mixture model: ( 3.2.72 )."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexercise5.p1", "title": "1. Implement the formulae derived in Exercise 3.4 , building a sampler for Gaussian mixtures. 2. Reproduce Figure 3.4 and Figure 3.7 . 3. We now introduce a separate process called Flow Matching (FM) ", "snippet": "1. Implement the formulae derived in Exercise 3.4 , building a sampler for Gaussian mixtures. 2. Reproduce Figure 3.4 and Figure 3.7 . 3. We now introduce a separate process called Flow Matching (FM) , as follows: Œ± t = 1 ‚àí t , œÉ t = t . \\alpha_{t}=1-t,\\qquad\\sigma_{t}=t. italic_Œ± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 1 - italic_t , italic_œÉ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_t . (3.6.7) Implement this process using the same framework, and test it for sampling in high dimensions. Which process seems to give better or more stable results?"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S6.I3.i1.p1", "title": "Implement the formulae derived in Exercise 3.4 , building a sampler for Gaussian mixtures.", "snippet": "Implement the formulae derived in Exercise 3.4 , building a sampler for Gaussian mixtures."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S6.I3.i2.p1", "title": "Reproduce Figure 3.4 and Figure 3.7 .", "snippet": "Reproduce Figure 3.4 and Figure 3.7 ."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S6.I3.i3.p1", "title": "We now introduce a separate process called Flow Matching (FM) , as follows: Œ± t = 1 ‚àí t , œÉ t = t . \\alpha_{t}=1-t,\\qquad\\sigma_{t}=t. italic_Œ± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 1 - ita", "snippet": "We now introduce a separate process called Flow Matching (FM) , as follows: Œ± t = 1 ‚àí t , œÉ t = t . \\alpha_{t}=1-t,\\qquad\\sigma_{t}=t. italic_Œ± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 1 - italic_t , italic_œÉ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_t . (3.6.7) Implement this process using the same framework, and test it for sampling in high dimensions. Which process seems to give better or more stable results?"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexercise6.p1", "title": "Please show the following properties of the log ‚Äã det ( ‚ãÖ ) \\log\\det(\\cdot) roman_log roman_det ( ‚ãÖ ) function. 1. Show that f ‚Äã ( ùëø ) = log ‚Äã det ( ùëø ) \\displaystyle f(\\bm{X})=\\log\\det\\left(\\bm{X}\\ri", "snippet": "Please show the following properties of the log ‚Äã det ( ‚ãÖ ) \\log\\det(\\cdot) roman_log roman_det ( ‚ãÖ ) function. 1. Show that f ‚Äã ( ùëø ) = log ‚Äã det ( ùëø ) \\displaystyle f(\\bm{X})=\\log\\det\\left(\\bm{X}\\right) italic_f ( bold_italic_X ) = roman_log roman_det ( bold_italic_X ) is a concave function. ( Hint: The function f ‚Äã ( ùíô ) f(\\bm{x}) italic_f ( bold_italic_x ) is convex if and only if the function f ‚Äã ( ùíô + t ‚Äã ùíâ ) f(\\bm{x}+t\\bm{h}) italic_f ( bold_italic_x + italic_t bold_italic_h ) for all ùíô \\bm{x} bold_italic_x and ùíâ \\bm{h} bold_italic_h .) 2. Show that: log ‚Äã det ( ùë∞ + ùëø ‚ä§ ‚Äã ùëø ) = log ‚Äã de"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S6.I4.i1.p1", "title": "Show that f ‚Äã ( ùëø ) = log ‚Äã det ( ùëø ) \\displaystyle f(\\bm{X})=\\log\\det\\left(\\bm{X}\\right) italic_f ( bold_italic_X ) = roman_log roman_det ( bold_italic_X ) is a concave function. ( Hint: The function", "snippet": "Show that f ‚Äã ( ùëø ) = log ‚Äã det ( ùëø ) \\displaystyle f(\\bm{X})=\\log\\det\\left(\\bm{X}\\right) italic_f ( bold_italic_X ) = roman_log roman_det ( bold_italic_X ) is a concave function. ( Hint: The function f ‚Äã ( ùíô ) f(\\bm{x}) italic_f ( bold_italic_x ) is convex if and only if the function f ‚Äã ( ùíô + t ‚Äã ùíâ ) f(\\bm{x}+t\\bm{h}) italic_f ( bold_italic_x + italic_t bold_italic_h ) for all ùíô \\bm{x} bold_italic_x and ùíâ \\bm{h} bold_italic_h .)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S6.I4.i2.p1", "title": "Show that: log ‚Äã det ( ùë∞ + ùëø ‚ä§ ‚Äã ùëø ) = log ‚Äã det ( ùë∞ + ùëø ‚Äã ùëø ‚ä§ ) \\displaystyle\\log\\det(\\bm{I}+\\bm{X}^{\\top}\\bm{X})=\\log\\det(\\bm{I}+\\bm{X}\\bm{X}^{\\top}) roman_log roman_det ( bold_italic_I + bold_itali", "snippet": "Show that: log ‚Äã det ( ùë∞ + ùëø ‚ä§ ‚Äã ùëø ) = log ‚Äã det ( ùë∞ + ùëø ‚Äã ùëø ‚ä§ ) \\displaystyle\\log\\det(\\bm{I}+\\bm{X}^{\\top}\\bm{X})=\\log\\det(\\bm{I}+\\bm{X}\\bm{X}^{\\top}) roman_log roman_det ( bold_italic_I + bold_italic_X start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_X ) = roman_log roman_det ( bold_italic_I + bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT )"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S6.I4.i3.p1", "title": "Let ùë® ‚àà ‚Ñù n √ó n \\bm{A}\\in\\mathbb{R}^{n\\times n} bold_italic_A ‚àà blackboard_R start_POSTSUPERSCRIPT italic_n √ó italic_n end_POSTSUPERSCRIPT be a positive definite matrix. Please show that: log ‚Äã det ( ", "snippet": "Let ùë® ‚àà ‚Ñù n √ó n \\bm{A}\\in\\mathbb{R}^{n\\times n} bold_italic_A ‚àà blackboard_R start_POSTSUPERSCRIPT italic_n √ó italic_n end_POSTSUPERSCRIPT be a positive definite matrix. Please show that: log ‚Äã det ( ùë® ) = ‚àë i = 1 n log ‚Å° ( Œª i ) , \\displaystyle\\log\\det\\left(\\bm{A}\\right)=\\sum_{i=1}^{n}\\log(\\lambda_{i}), roman_log roman_det ( bold_italic_A ) = ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT roman_log ( italic_Œª start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) , (3.6.8) where Œª 1 , Œª 2 , ‚Ä¶ , Œª n \\lambda_{1},\\lambda_{2},\\dots,\\lambda_{n} it"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample1", "title": "Example 3.1 (Entropy of Gaussian Distributions) .", "snippet": "Example 3.1 (Entropy of Gaussian Distributions) . Through direct calculation, it is possible to show that the entropy of a Gaussian distribution x ‚àº ùí© ‚Äã ( Œº , œÉ 2 ) x\\sim\\mathcal{N}(\\mu,\\sigma^{2}) italic_x ‚àº caligraphic_N ( italic_Œº , italic_œÉ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) is given by: h ‚Äã ( x ) = 1 2 ‚Äã log ‚Å° ( 2 ‚Äã œÄ ‚Äã œÉ 2 ) + 1 2 . h(x)=\\frac{1}{2}\\log(2\\pi\\sigma^{2})+\\frac{1}{2}. italic_h ( italic_x ) = divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_log ( 2 italic_œÄ italic_œÉ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) + divide start_ARG 1 end_ARG start_ARG 2 end_ARG "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmtheorem1", "title": "Theorem 3.1 (Information Inequality) .", "snippet": "Theorem 3.1 (Information Inequality) . Let p ‚Äã ( ùê± ) , q ‚Äã ( ùê± ) p(\\bm{x}),q(\\bm{x}) italic_p ( bold_italic_x ) , italic_q ( bold_italic_x ) be two probability density functions (that have the same support). Then ùñ™ùñ´ ‚Å° ( p ‚à• q ) ‚â• 0 \\operatorname{\\mathsf{KL}}(p\\;\\|\\;q)\\geq 0 sansserif_KL ( italic_p ‚à• italic_q ) ‚â• 0 , where the inequality becomes equality if and only if p = q p=q italic_p = italic_q . 4 4 4 Technically, this equality should be taken to mean ‚Äúalmost everywhere‚Äù, i.e., except possibly on a set of zero measure (volume), since this set would not impact the value of any integral."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmtheorem2", "title": "Theorem 3.2 (Simplified Version of Theorem B.2 ) .", "snippet": "Theorem 3.2 (Simplified Version of Theorem B.2 ) . Suppose that ( ùê± t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ‚àà [ 0 , italic_T ] end_POSTSUBSCRIPT follows the model ( 3.2.1 ). For any t ‚àà ( 0 , T ] t\\in(0,T] italic_t ‚àà ( 0 , italic_T ] , the random variable ùê± t \\bm{x}_{t} bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT has differential entropy h ‚Äã ( ùê± t ) > ‚àí ‚àû h(\\bm{x}_{t})>-\\infty italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) > - ‚àû . Moreover, under certain "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample2", "title": "Example 3.2 (Denoising Gaussian Noise from a Mixture of Gaussians) .", "snippet": "Example 3.2 (Denoising Gaussian Noise from a Mixture of Gaussians) . In this example we compute the Bayes optimal denoiser for an incredibly important class of distributions, the Gaussian mixture model. To start, let us fix parameters for the distribution: mixture weights ùùÖ ‚àà ‚Ñù K \\bm{\\pi}\\in\\mathbb{R}^{K} bold_italic_œÄ ‚àà blackboard_R start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT , component means { ùùÅ k } k = 1 K ‚äÜ ‚Ñù D \\{\\bm{\\mu}_{k}\\}_{k=1}^{K}\\subseteq\\mathbb{R}^{D} { bold_italic_Œº start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_P"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmtheorem3", "title": "Theorem 3.3 (Tweedie‚Äôs Formula) .", "snippet": "Theorem 3.3 (Tweedie‚Äôs Formula) . Suppose that ( ùê± t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ‚àà [ 0 , italic_T ] end_POSTSUBSCRIPT obeys ( 3.2.1 ). Let p t p_{t} italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT be the density of ùê± t \\bm{x}_{t} bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT (as previously declared). Then ùîº ‚Å° [ ùíô ‚à£ ùíô t ] = ùíô t + t 2 ‚Äã ‚àá ùíô t log ‚Å° p t ‚Äã ( ùíô t ) . \\operatorname{\\mathbb{E}}[\\bm{x}\\mid\\bm{x}_{t}]=\\bm{x}_{t}+t^{2}\\nabla_{\\bm{x}_{t}}\\log p_{t}(\\bm{x}_{t})"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample3", "title": "Example 3.3 (Denoising a Two-Point Mixture) .", "snippet": "Example 3.3 (Denoising a Two-Point Mixture) . Let x x italic_x be uniform on the two-point set { ‚àí 1 , + 1 } \\{-1,+1\\} { - 1 , + 1 } and let ( ùíô t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ‚àà [ 0 , italic_T ] end_POSTSUBSCRIPT follow ( 3.2.1 ). This is precisely a degenerate Gaussian mixture model with priors equal to 1 2 \\frac{1}{2} divide start_ARG 1 end_ARG start_ARG 2 end_ARG , means { ‚àí 1 , + 1 } \\{-1,+1\\} { - 1 , + 1 } , and covariances both equal to 0 . For a fixed t > 0 t>0 italic_t > 0 we can u"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmtheorem4", "title": "Theorem 3.4 (Simplified Version of Theorem B.3 ) .", "snippet": "Theorem 3.4 (Simplified Version of Theorem B.3 ) . Suppose that ( ùê± t ) t ‚àà [ 0 , T ] (\\bm{x}_{t})_{t\\in[0,T]} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_t ‚àà [ 0 , italic_T ] end_POSTSUBSCRIPT obeys ( 3.2.1 ). Then, under certain technical conditions on ùê± \\bm{x} bold_italic_x , for every s < t s<t italic_s < italic_t with s , t ‚àà ( 0 , T ] s,t\\in(0,T] italic_s , italic_t ‚àà ( 0 , italic_T ] , h ‚Äã ( ùîº ‚Å° [ ùíô s ‚à£ ùíô t ] ) < h ‚Äã ( ùíô t ) . h(\\operatorname{\\mathbb{E}}[\\bm{x}_{s}\\mid\\bm{x}_{t}])<h(\\bm{x}_{t}). italic_h ( blackboard_E [ bold_italic_x star"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmremark1", "title": "Remark 3.1 .", "snippet": "Remark 3.1 . Connections between denoising a distribution and probabilistic PCA. Here, we would like to connect denoising a low-dimensional distribution to probabilistic PCA (see Section 2.1.3 for more details about probabilistic PCA). Suppose that we consider K = 1 K=1 italic_K = 1 in ( 3.2.42 ), i.e., ùíô ‚àº ùí© ‚Å° ( ùüé , ùëº ‚Äã ùëº ‚ä§ ) \\bm{x}\\sim\\operatorname{\\mathcal{N}}(\\bm{0},\\bm{U}\\bm{U}^{\\top}) bold_italic_x ‚àº caligraphic_N ( bold_0 , bold_italic_U bold_italic_U start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ) , where ùëº ‚àà ùñÆ ‚Äã ( D , P ) ‚äÜ ‚Ñù D √ó P \\bm{U}\\in\\mathsf{O}(D,P)\\subseteq\\mathbb{R}^{D\\times P}"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmtheorem5", "title": "Theorem 3.5 ( [ LY24 ] Theorem 1, Simplified) .", "snippet": "Theorem 3.5 ( [ LY24 ] Theorem 1, Simplified) . Suppose that ùîº ‚Å° ‚Äñ ùê± ‚Äñ 2 < ‚àû \\operatorname{\\mathbb{E}}\\|\\bm{x}\\|_{2}<\\infty blackboard_E ‚à• bold_italic_x ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT < ‚àû . If ùê± \\bm{x} bold_italic_x is denoised according to the VP process with an exponential discretization 7 7 7 The precise definition is rather lengthy in our notation and only defined up to various absolute constants, so we omit it here for brevity. Of course it is in the original paper [ LY24 ] . as in ( 3.2.67 ), the output ùê± ^ \\hat{\\bm{x}} over^ start_ARG bold_italic_x end_ARG of Algorithm 3.1 sa"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmremark2", "title": "Remark 3.2 .", "snippet": "Remark 3.2 . What if the data is low-dimensional, say supported on a low-rank subspace of the high dimensional space ‚Ñù D \\mathbb{R}^{D} blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT ? If the data distribution is compactly supported‚Äîsay if the data is normalized to the unit hypercube, which is often ensured as a pre-processing step for real data such as images‚Äîit is possible to do better. Namely, the authors of [ LY24 ] also define a measure of approximate intrinsic dimension using the asymptotics of the so-called covering number, which is extremely similar in intuition (if no"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmremark3", "title": "Remark 3.3 .", "snippet": "Remark 3.3 . Various other works define the reverse process as moving backward in the time index t t italic_t using an explicit difference equation, or differential equation in the limit L ‚Üí ‚àû L\\to\\infty italic_L ‚Üí ‚àû , or forward in time using the transformation ùíö t = ùíô T ‚àí t \\bm{y}_{t}=\\bm{x}_{T-t} bold_italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_x start_POSTSUBSCRIPT italic_T - italic_t end_POSTSUBSCRIPT , such that if t t italic_t increases then ùíö t \\bm{y}_{t} bold_italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT becomes closer to ùíô 0 \\bm{x}_{0} bold_italic_"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmremark4", "title": "Remark 3.4 .", "snippet": "Remark 3.4 . The theory presented at the end of the last Section 3.2.1 seems to suggest (loosely speaking) that in practice, using a transformer-like network is a good choice for learning or approximating a denoiser. This is reasonable, but what is the problem with using any old neural network (such as a multi-layer perceptron (MLP)) and just trying to scale it up to infinity? To observe the problem with this, let us look at another special case of the Gaussian mixture model studied in Example 3.2 . Namely, the empirical distribution is an instance of a degenerate Gaussian mixture model, with "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample4", "title": "Example 3.4 (Volume, Dimension, and Entropy) .", "snippet": "Example 3.4 (Volume, Dimension, and Entropy) . For the example shown on the top of Figure 3.8 , suppose we have taken some samples from a uniform distribution on a line (say in a 2D plane). The volume of the line or the sample sets is zero. Geometrically, the empirical distribution on the produced finite sample set is the minimum-dimension one which can produce the finite sample set. 9 9 9 A set of discrete samples are all of zero dimension whereas the supporting line is one dimension. But this is in seemingly contrast with yet another measure of complex: entropy. The (differential) entropy of"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample5", "title": "Example 3.5 (Density) .", "snippet": "Example 3.5 (Density) . Consider the two sets of sampled data points shown in Figure 3.8 . Geometrically, they are essentially the same: each set consists of eight points and each point has occurred with equal frequency 1 / 8 1/8 1 / 8 th. The only difference is that for the second data set, some points are ‚Äúclose‚Äù enough to be viewed as having a higher density around their respective ‚Äúcluster.‚Äù Which one is more relevant to the true distribution that may have generated the samples? How can we reconcile such ambiguity in interpreting this kind of (empirical) distributions? Figure 3.8 : Eight p"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample6", "title": "Example 3.6 (Precision) .", "snippet": "Example 3.6 (Precision) . Consider a discrete distribution ùëø = [ e , œÄ ] \\bm{X}=[e,\\pi] bold_italic_X = [ italic_e , italic_œÄ ] with equal probability 1 / 2 1/2 1 / 2 taking the values of the Euler number e ‚âà 2.71828 e\\approx 2.71828 italic_e ‚âà 2.71828 or the number œÄ ‚âà 3.14159 \\pi\\approx 3.14159 italic_œÄ ‚âà 3.14159 . The entropy of this distribution is H = 1 H=1 italic_H = 1 , which suggests that one may encode the two numbers by a one-bit digit 0 or 1 1 1 , respectively. But can you realize a decoding scheme for this code on a finite-state machine? The answer is actually no, as it takes infin"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmremark5", "title": "Remark 3.5 .", "snippet": "Remark 3.5 . As it turns out, the rate distortion is an implementable approximation to the entropy of ùíô \\bm{x} bold_italic_x in the following sense. Assume that ùíô \\bm{x} bold_italic_x and ùíô ^ \\hat{\\bm{x}} over^ start_ARG bold_italic_x end_ARG are continuous random vectors. Then the mutual information can be written as I ‚Äã ( ùíô ; ùíô ^ ) = h ‚Äã ( ùíô ) ‚àí h ‚Äã ( ùíô ‚à£ ùíô ^ ) , I(\\bm{x};\\hat{\\bm{x}})=h(\\bm{x})-h(\\bm{x}\\mid\\hat{\\bm{x}}), italic_I ( bold_italic_x ; over^ start_ARG bold_italic_x end_ARG ) = italic_h ( bold_italic_x ) - italic_h ( bold_italic_x ‚à£ over^ start_ARG bold_italic_x end_ARG ) , (3.3."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmremark6", "title": "Remark 3.6 .", "snippet": "Remark 3.6 . Given a set of data points in ùëø = [ ùíô 1 , ‚Ä¶ , ùíô N ] \\bm{X}=[\\bm{x}_{1},\\ldots,\\bm{x}_{N}] bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ] , one can always interpret them as samples from a uniform discrete distribution with equal probability 1 / N 1/N 1 / italic_N on these N N italic_N vectors. The entropy for such a distribution is H ‚Äã ( ùëø ) = 1 N ‚Äã log 2 ‚Å° N H(\\bm{X})=\\frac{1}{N}\\log_{2}N italic_H ( bold_italic_X ) = divide start_ARG 1 end_ARG start_ARG italic_N end_ARG roman_log start_PO"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample7", "title": "Example 3.7 .", "snippet": "Example 3.7 . Sometimes, one may face an opposite situation when we want to fix the coding rate first and try to find a coding scheme that minimizes the distortion. For example, suppose that we only want to use a fixed number of codes for points sampled from a distribution, and we want to know how to design the codes such that the average or maximum distortion is minimized during the encoding/decoding scheme. For example, given a uniform distribution on a unit square, we wonder how precisely we can encode points drawn from this distribution, with say n n italic_n bits. This problem is equivale"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmtheorem6", "title": "Theorem 3.6 .", "snippet": "Theorem 3.6 . Suppose that ùê± \\bm{x} bold_italic_x is a random variable such that its support K ‚âê Supp ‚Å° ( ùê± ) K\\doteq\\operatorname{Supp}(\\bm{x}) italic_K ‚âê roman_Supp ( bold_italic_x ) is a compact set. Define the covering number ùí© œµ ‚Äã ( K ) \\mathcal{N}_{\\epsilon}(K) caligraphic_N start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( italic_K ) as the minimum number of balls of radius œµ \\epsilon italic_œµ that can cover K K italic_K , i.e., ùí© œµ ‚Äã ( K ) ‚âê min ‚Å° { n ‚àà ‚Ñï : ‚àÉ ùíë 1 , ‚Ä¶ , ùíë n ‚àà K ‚Äã s.t. ‚Äã K ‚äÜ ‚ãÉ i = 1 n B œµ ‚Äã ( ùíë i ) } , \\mathcal{N}_{\\epsilon}(K)\\doteq\\min\\left\\{n\\in\\mathbb{N}\\colon\\exists\\"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmremark7", "title": "Remark 3.7 .", "snippet": "Remark 3.7 . The key ingredient in the proof of the lower bound in Theorem 3.6 is an important result from information theory known as the Shannon lower bound for the rate distortion, named after Claude Shannon, who first derived it in a special case [ Sha59 ] . It asserts the following estimate for the rate distortion function, for any random variable ùíô \\bm{x} bold_italic_x with a density p ‚Äã ( ùíô ) p(\\bm{x}) italic_p ( bold_italic_x ) and finite expected squared norm [ LZ94 ] : ‚Ñõ œµ ‚Äã ( ùíô ) ‚â• h ‚Äã ( ùíô ) ‚àí log 2 ‚Å° vol ‚Å° ( B œµ ) ‚àí C D , \\mathcal{R}_{\\epsilon}(\\bm{x})\\geq h(\\bm{x})-\\log_{2}\\operat"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample8", "title": "Example 3.8 .", "snippet": "Example 3.8 . Figure 3.11 shows an example of a 2D distribution with an ellipsoidal support ‚Äì approximating the support of a 2D Gaussian distribution. The region is covered by small balls of size œµ \\epsilon italic_œµ . All the balls are numbered from 1 1 1 to say n n italic_n . Then given any vector ùíô \\bm{x} bold_italic_x in this region, we only need to determine to which œµ \\epsilon italic_œµ -ball center it is the closest, denoted as ball œµ ‚Å° ( ùíô ) \\operatorname{ball}_{\\epsilon}(\\bm{x}) roman_ball start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_x ) . To remember ùíô \\bm{x} bold_itali"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample9", "title": "Example 3.9 .", "snippet": "Example 3.9 . Figure 3.12 shows an example in which the data ùëø \\bm{X} bold_italic_X are distributed around two subspaces (or low-dimensional Gaussians). If they are viewed and coded together as one single Gaussian, the associated discrete (lossy) code book, represented by all the blue balls, is obviously very redundant. We can try to identify the locations of the two subspaces, denoted by S 1 S_{1} italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and S 2 S_{2} italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , and design a code book that only covers the two subspaces, i.e., the green balls. If"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample10", "title": "Example 3.10 .", "snippet": "Example 3.10 . To see when the memorization regime is preferred or not, let us consider a number, say N N italic_N , of samples randomly distributed in a unit area on a 2D plane. 19 19 19 Say the points are drawn by a Poisson process with density N N italic_N points per unit area. Imagine we try to design a lossy coding scheme with a fixed quantization error œµ \\epsilon italic_œµ . This is equivalent to putting an œµ \\epsilon italic_œµ -disc around each sample, as shown in Figure 3.13 . When N N italic_N is small, the chance that all the discs overlap with each other is zero. A codebook of size N "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample11", "title": "Example 3.11 .", "snippet": "Example 3.11 . Figure 3.14 : Top: 358 noisy samples drawn from two lines and one plane in ‚Ñù 3 \\mathbb{R}^{3} blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT . Bottom: the effect of varying œµ \\epsilon italic_œµ on the clustering result and the coding rate. The red line marks the variance œµ 0 \\epsilon_{0} italic_œµ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT of the Gaussian noise added to the samples. Figure 3.14 shows an example with noisy samples drawn from two lines and one plane in ‚Ñù 3 \\mathbb{R}^{3} blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT . As we notice from the plot (c)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample12", "title": "Example 3.12 (Image Segmentation) .", "snippet": "Example 3.12 (Image Segmentation) . The above measure of coding length and the associated clustering algorithm assume the data distribution is a mixture of (low-dimensional) Gaussians. Although this seems somewhat idealistic, the measure and algorithm can already be very useful and even powerful in scenarios when the model is (approximately) valid. For example, a natural image typically consists of multiple regions with nearly homogeneous textures. If we take many small windows from each region, they should resemble samples drawn from a (low-dimensional) Gaussian, as illustrated in Figure 3.15"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmremark8", "title": "Remark 3.8 .", "snippet": "Remark 3.8 . Neural collapse refers to a phenomenon observed in deep neural networks trained for classification, where the learned feature representations and classifier weights exhibit highly symmetric and structured behavior during the terminal phase of training [ PHD20 , ZDZ+21 ] . Specifically, within each class, features collapse to their class mean, and across classes, these means become maximally separated, forming a simplex equiangular configuration. The linear classifier aligns with the class mean up to rescaling. Additionally, the last-layer classifier converges to choosing whichever"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmremark9", "title": "Remark 3.9 .", "snippet": "Remark 3.9 . Linear discriminant analysis (LDA) [ HTF09 ] is a supervised dimensionality reduction technique that aims to find a linear projection of data that maximizes class separability. Specifically, given labeled data, LDA seeks a linear transformation that projects high-dimensional inputs onto a lower-dimensional space where the classes are maximally separated. Note that PCA is an unsupervised method that projects data onto directions of maximum variance without considering class labels. While PCA focuses purely on preserving global variance structure, LDA explicitly exploits label infor"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmtheorem7", "title": "Theorem 3.7 ( Characterization of Global Optimal Solutions).", "snippet": "Theorem 3.7 ( Characterization of Global Optimal Solutions). Suppose ùêô ‚àó = [ ùêô 1 ‚àó , ‚Ä¶ , ùêô K ‚àó ] \\bm{Z}^{\\ast}=[\\bm{Z}_{1}^{*},\\dots,\\bm{Z}_{K}^{*}] bold_italic_Z start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT = [ bold_italic_Z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT , ‚Ä¶ , bold_italic_Z start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT ] is a global optimal solution of Problem ( 3.4.15 ). The following statements hold: ‚Ä¢ Between-Class Discriminative : As long as the ambient space is adequately large ( d ‚â• ‚àë k = 1"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexample13", "title": "Example 3.13 (Classification of Images on CIFAR-10) .", "snippet": "Example 3.13 (Classification of Images on CIFAR-10) . We here present how the MCR 2 objective helps learn better representations than the cross entropy ( 3.4.2 ) for image classification. Here we adopt the popular neural network architecture, the ResNet-18 [ HZR+16a ] , to model the feature mapping ùíõ = f ‚Äã ( ùíô , Œ∏ ) \\bm{z}=f(\\bm{x},\\theta) bold_italic_z = italic_f ( bold_italic_x , italic_Œ∏ ) . We optimize the neural network parameters Œ∏ \\theta italic_Œ∏ to maximize the coding rate reduction. We evaluate the performance with the CIFAR10 image classification dataset [ KH+09 ] . (a) Evolution of "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmtheorem8", "title": "Theorem 3.8 ( Local and Global Optima).", "snippet": "Theorem 3.8 ( Local and Global Optima). Let N k N_{k} italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT denote the number of training samples in the k k italic_k -th class for each k ‚àà { 1 , ‚Ä¶ , K } k\\in\\{1,\\dots,K\\} italic_k ‚àà { 1 , ‚Ä¶ , italic_K } , N max ‚âê max ‚Å° { N 1 , ‚Ä¶ , N K } N_{\\max}\\doteq\\max\\{N_{1},\\dots,N_{K}\\} italic_N start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT ‚âê roman_max { italic_N start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , italic_N start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT } , Œ± = d / ( N ‚Äã œµ 2 ) \\alpha=d/(N\\epsilon^{2}) italic_Œ± = italic_d / ( italic_N italic_"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmtheorem9", "title": "Theorem 3.9 ( Benign Global Optimization Landscape).", "snippet": "Theorem 3.9 ( Benign Global Optimization Landscape). Given a coding precision œµ > 0 \\epsilon>0 italic_œµ > 0 , if the regularization parameter satisfies ( 3.4.17 ), it holds that any critical point ùêô \\bm{Z} bold_italic_Z of the problem ( 3.4.16 ) is either a local maximizer or a strict saddle point."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexercise1", "title": "Exercise 3.1 .", "snippet": "Exercise 3.1 . Please show that ( 3.2.4 ) is the optimal solution of Problem ( 3.2.3 )."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexercise2", "title": "Exercise 3.2 .", "snippet": "Exercise 3.2 . Consider random vectors ùíô ‚àà ‚Ñù D \\bm{x}\\in\\mathbb{R}^{D} bold_italic_x ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT and ùíö ‚àà ‚Ñù d \\bm{y}\\in\\mathbb{R}^{d} bold_italic_y ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , such that the pair ( ùíô , ùíö ) ‚àà ‚Ñù D + d (\\bm{x},\\bm{y})\\in\\mathbb{R}^{D+d} ( bold_italic_x , bold_italic_y ) ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D + italic_d end_POSTSUPERSCRIPT is jointly Gaussian. This means that [ ùíô ùíö ] ‚àº ùí© ‚Äã ( [ ùùÅ ùíô ùùÅ ùíö ] , [ ùö∫ ùíô ùö∫ ùíô ‚Äã ùíö ùö∫ ùíô ‚Äã ùíö ‚ä§ ùö∫ ùíö ] ) , \\begin{bmatrix}\\bm{x}\\\\ \\bm{y}\\end{bmatrix}\\s"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexercise3", "title": "Exercise 3.3 .", "snippet": "Exercise 3.3 . Show the Sherman-Morrison-Woodbury identity, i.e., for matrices ùë® \\bm{A} bold_italic_A , ùë™ \\bm{C} bold_italic_C , ùëº \\bm{U} bold_italic_U , ùëΩ \\bm{V} bold_italic_V such that ùë® \\bm{A} bold_italic_A , ùë™ \\bm{C} bold_italic_C , and ùë® + ùëº ‚Äã ùë™ ‚Äã ùëΩ \\bm{A}+\\bm{U}\\bm{C}\\bm{V} bold_italic_A + bold_italic_U bold_italic_C bold_italic_V are invertible, ( ùë® + ùëº ‚Äã ùë™ ‚Äã ùëΩ ) ‚àí 1 = ùë® ‚àí 1 ‚àí ùë® ‚àí 1 ‚Äã ùëº ‚Äã ( ùë™ ‚àí 1 + ùëΩ ‚Äã ùë® ‚àí 1 ‚Äã ùëº ) ‚àí 1 ‚Äã ùëΩ ‚Äã ùë® ‚àí 1 (\\bm{A}+\\bm{U}\\bm{C}\\bm{V})^{-1}=\\bm{A}^{-1}-\\bm{A}^{-1}\\bm{U}(\\bm{C}^{-1}+\\bm{V}\\bm{A}^{-1}\\bm{U})^{-1}\\bm{V}\\bm{A}^{-1} ( bold_italic_A + bold_italic_U bold_"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexercise4", "title": "Exercise 3.4 .", "snippet": "Exercise 3.4 . Rederive the following, assuming ùíô t \\bm{x}_{t} bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT follows the generalized noise model ( 3.2.69 ). ‚Ä¢ Tweedie‚Äôs formula: ( 3.2.70 ). ‚Ä¢ The DDIM iteration: ( 3.2.71 ). ‚Ä¢ The Bayes optimal denoiser for a Gaussian mixture model: ( 3.2.72 )."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexercise5", "title": "Exercise 3.5 .", "snippet": "Exercise 3.5 . 1. Implement the formulae derived in Exercise 3.4 , building a sampler for Gaussian mixtures. 2. Reproduce Figure 3.4 and Figure 3.7 . 3. We now introduce a separate process called Flow Matching (FM) , as follows: Œ± t = 1 ‚àí t , œÉ t = t . \\alpha_{t}=1-t,\\qquad\\sigma_{t}=t. italic_Œ± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 1 - italic_t , italic_œÉ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_t . (3.6.7) Implement this process using the same framework, and test it for sampling in high dimensions. Which process seems to give better or more stable results?"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#Thmexercise6", "title": "Exercise 3.6 .", "snippet": "Exercise 3.6 . Please show the following properties of the log ‚Äã det ( ‚ãÖ ) \\log\\det(\\cdot) roman_log roman_det ( ‚ãÖ ) function. 1. Show that f ‚Äã ( ùëø ) = log ‚Äã det ( ùëø ) \\displaystyle f(\\bm{X})=\\log\\det\\left(\\bm{X}\\right) italic_f ( bold_italic_X ) = roman_log roman_det ( bold_italic_X ) is a concave function. ( Hint: The function f ‚Äã ( ùíô ) f(\\bm{x}) italic_f ( bold_italic_x ) is convex if and only if the function f ‚Äã ( ùíô + t ‚Äã ùíâ ) f(\\bm{x}+t\\bm{h}) italic_f ( bold_italic_x + italic_t bold_italic_h ) for all ùíô \\bm{x} bold_italic_x and ùíâ \\bm{h} bold_italic_h .) 2. Show that: log ‚Äã det ( ùë∞ + ùëø ‚ä§ ‚Äã"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S1.E1", "title": "H ‚Äã ( ùíô ) ‚âê ùîº ‚Äã [ log ‚Å° 1 / p ‚Äã ( ùíô ) ] = ‚àí ‚àë i = 1 N p ‚Äã ( ùíô i ) ‚Äã log ‚Å° p ‚Äã ( ùíô i ) . H(\\bm{x})\\doteq\\mathbb{E}[\\log 1/p(\\bm{x})]=-\\sum_{i=1}^{N}p(\\bm{x}_{i})\\log p(\\bm{x}_{i}). italic_H ( bold_ital", "snippet": "H ‚Äã ( ùíô ) ‚âê ùîº ‚Äã [ log ‚Å° 1 / p ‚Äã ( ùíô ) ] = ‚àí ‚àë i = 1 N p ‚Äã ( ùíô i ) ‚Äã log ‚Å° p ‚Äã ( ùíô i ) . H(\\bm{x})\\doteq\\mathbb{E}[\\log 1/p(\\bm{x})]=-\\sum_{i=1}^{N}p(\\bm{x}_{i})\\log p(\\bm{x}_{i}). italic_H ( bold_italic_x ) ‚âê blackboard_E [ roman_log 1 / italic_p ( bold_italic_x ) ] = - ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT italic_p ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) roman_log italic_p ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) . (3.1.1)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S1.E2", "title": "h ‚Äã ( ùíô ) ‚âê ùîº ‚Å° [ log ‚Å° 1 / p ‚Äã ( ùíô ) ] = ‚àí ‚à´ ‚Ñù D p ‚Äã ( ùùÉ ) ‚Äã log ‚Å° p ‚Äã ( ùùÉ ) ‚Äã d ùùÉ . h(\\bm{x})\\doteq\\operatorname{\\mathbb{E}}[\\log 1/p(\\bm{x})]=-\\int_{\\mathbb{R}^{D}}p(\\bm{\\xi})\\log p(\\bm{\\xi})\\mathr", "snippet": "h ‚Äã ( ùíô ) ‚âê ùîº ‚Å° [ log ‚Å° 1 / p ‚Äã ( ùíô ) ] = ‚àí ‚à´ ‚Ñù D p ‚Äã ( ùùÉ ) ‚Äã log ‚Å° p ‚Äã ( ùùÉ ) ‚Äã d ùùÉ . h(\\bm{x})\\doteq\\operatorname{\\mathbb{E}}[\\log 1/p(\\bm{x})]=-\\int_{\\mathbb{R}^{D}}p(\\bm{\\xi})\\log p(\\bm{\\xi})\\mathrm{d}\\bm{\\xi}. italic_h ( bold_italic_x ) ‚âê blackboard_E [ roman_log 1 / italic_p ( bold_italic_x ) ] = - ‚à´ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_p ( bold_italic_Œæ ) roman_log italic_p ( bold_italic_Œæ ) roman_d bold_italic_Œæ . (3.1.2)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S1.E3", "title": "h ‚Äã ( x ) = 1 2 ‚Äã log ‚Å° ( 2 ‚Äã œÄ ‚Äã œÉ 2 ) + 1 2 . h(x)=\\frac{1}{2}\\log(2\\pi\\sigma^{2})+\\frac{1}{2}. italic_h ( italic_x ) = divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_log ( 2 italic_œÄ italic_œÉ", "snippet": "h ‚Äã ( x ) = 1 2 ‚Äã log ‚Å° ( 2 ‚Äã œÄ ‚Äã œÉ 2 ) + 1 2 . h(x)=\\frac{1}{2}\\log(2\\pi\\sigma^{2})+\\frac{1}{2}. italic_h ( italic_x ) = divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_log ( 2 italic_œÄ italic_œÉ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) + divide start_ARG 1 end_ARG start_ARG 2 end_ARG . (3.1.3)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S1.E4", "title": "h ‚Äã ( ùíô ) = D 2 ‚Äã ( 1 + log ‚Å° ( 2 ‚Äã œÄ ) ) + 1 2 ‚Äã log ‚Äã det ( ùö∫ ) . h(\\bm{x})=\\frac{D}{2}(1+\\log(2\\pi))+\\frac{1}{2}\\log\\det(\\bm{\\Sigma}). italic_h ( bold_italic_x ) = divide start_ARG italic_D end_ARG", "snippet": "h ‚Äã ( ùíô ) = D 2 ‚Äã ( 1 + log ‚Å° ( 2 ‚Äã œÄ ) ) + 1 2 ‚Äã log ‚Äã det ( ùö∫ ) . h(\\bm{x})=\\frac{D}{2}(1+\\log(2\\pi))+\\frac{1}{2}\\log\\det(\\bm{\\Sigma}). italic_h ( bold_italic_x ) = divide start_ARG italic_D end_ARG start_ARG 2 end_ARG ( 1 + roman_log ( 2 italic_œÄ ) ) + divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_log roman_det ( bold_Œ£ ) . (3.1.4)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S1.E5", "title": "1 N ‚Äã ‚àë i = 1 N ‚àí log ‚Å° q ‚Äã ( ùíô i ) ‚âà ‚àí ‚à´ ‚Ñù D p ‚Äã ( ùùÉ ) ‚Äã log ‚Å° q ‚Äã ( ùùÉ ) ‚Äã d ùùÉ \\frac{1}{N}\\sum_{i=1}^{N}-\\log q(\\bm{x}_{i})\\quad\\approx\\quad-\\int_{\\mathbb{R}^{D}}p(\\bm{\\xi})\\log q(\\bm{\\xi})\\mathrm{d}", "snippet": "1 N ‚Äã ‚àë i = 1 N ‚àí log ‚Å° q ‚Äã ( ùíô i ) ‚âà ‚àí ‚à´ ‚Ñù D p ‚Äã ( ùùÉ ) ‚Äã log ‚Å° q ‚Äã ( ùùÉ ) ‚Äã d ùùÉ \\frac{1}{N}\\sum_{i=1}^{N}-\\log q(\\bm{x}_{i})\\quad\\approx\\quad-\\int_{\\mathbb{R}^{D}}p(\\bm{\\xi})\\log q(\\bm{\\xi})\\mathrm{d}\\bm{\\xi} divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT - roman_log italic_q ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ‚âà - ‚à´ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_p ( bold_italic_Œæ ) roman_log it"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S1.E8", "title": "h ‚Äã ( ùíô n ) > h ‚Äã ( ùíô e ) > h ‚Äã ( ùíô ^ ) . h(\\bm{x}^{n})>h(\\bm{x}^{e})>h(\\hat{\\bm{x}}). italic_h ( bold_italic_x start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ) > italic_h ( bold_italic_x start_PO", "snippet": "h ‚Äã ( ùíô n ) > h ‚Äã ( ùíô e ) > h ‚Äã ( ùíô ^ ) . h(\\bm{x}^{n})>h(\\bm{x}^{e})>h(\\hat{\\bm{x}}). italic_h ( bold_italic_x start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ) > italic_h ( bold_italic_x start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT ) > italic_h ( over^ start_ARG bold_italic_x end_ARG ) . (3.1.8)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S1.E9", "title": "p ‚Äã ( ùíô n ) ‚Üí p ‚Äã ( ùíô e ) ‚Üí p ‚Äã ( ùíô ^ ) . p(\\bm{x}^{n})\\rightarrow p(\\bm{x}^{e})\\rightarrow p(\\hat{\\bm{x}}). italic_p ( bold_italic_x start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ) ‚Üí italic_p ( ", "snippet": "p ‚Äã ( ùíô n ) ‚Üí p ‚Äã ( ùíô e ) ‚Üí p ‚Äã ( ùíô ^ ) . p(\\bm{x}^{n})\\rightarrow p(\\bm{x}^{e})\\rightarrow p(\\hat{\\bm{x}}). italic_p ( bold_italic_x start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ) ‚Üí italic_p ( bold_italic_x start_POSTSUPERSCRIPT italic_e end_POSTSUPERSCRIPT ) ‚Üí italic_p ( over^ start_ARG bold_italic_x end_ARG ) . (3.1.9)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E1", "title": "ùíô t ‚âê ùíô + t ‚Äã ùíà , ‚àÄ t ‚àà [ 0 , T ] , \\bm{x}_{t}\\doteq\\bm{x}+t\\bm{g},\\qquad\\forall t\\in[0,T], bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ‚âê bold_italic_x + italic_t bold_italic_g , ‚àÄ it", "snippet": "ùíô t ‚âê ùíô + t ‚Äã ùíà , ‚àÄ t ‚àà [ 0 , T ] , \\bm{x}_{t}\\doteq\\bm{x}+t\\bm{g},\\qquad\\forall t\\in[0,T], bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ‚âê bold_italic_x + italic_t bold_italic_g , ‚àÄ italic_t ‚àà [ 0 , italic_T ] , (3.2.1)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E2", "title": "d d ‚Äã t ‚Äã h ‚Äã ( ùíô t ) > 0 , ‚àÄ t ‚àà ( 0 , T ] , \\frac{\\mathrm{d}}{\\mathrm{d}t}h(\\bm{x}_{t})>0,\\qquad\\forall t\\in(0,T], divide start_ARG roman_d end_ARG start_ARG roman_d italic_t end_ARG italic_h ( bold", "snippet": "d d ‚Äã t ‚Äã h ‚Äã ( ùíô t ) > 0 , ‚àÄ t ‚àà ( 0 , T ] , \\frac{\\mathrm{d}}{\\mathrm{d}t}h(\\bm{x}_{t})>0,\\qquad\\forall t\\in(0,T], divide start_ARG roman_d end_ARG start_ARG roman_d italic_t end_ARG italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) > 0 , ‚àÄ italic_t ‚àà ( 0 , italic_T ] , (3.2.2)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E3", "title": "ùíô ¬Ø ‚àó ‚Äã ( t , ‚ãÖ ) ‚àà arg ‚Äã min ùíô ¬Ø ‚Äã ( t , ‚ãÖ ) ‚Å° ùîº ùíô , ùíô t ‚Å° ‚Äñ ùíô ‚àí ùíô ¬Ø ‚Äã ( t , ùíô t ) ‚Äñ 2 2 . \\bar{\\bm{x}}^{\\ast}(t,\\cdot)\\in\\operatorname*{arg\\ min}_{\\bar{\\bm{x}}(t,\\cdot)}\\operatorname{\\mathbb{E}}_{\\b", "snippet": "ùíô ¬Ø ‚àó ‚Äã ( t , ‚ãÖ ) ‚àà arg ‚Äã min ùíô ¬Ø ‚Äã ( t , ‚ãÖ ) ‚Å° ùîº ùíô , ùíô t ‚Å° ‚Äñ ùíô ‚àí ùíô ¬Ø ‚Äã ( t , ùíô t ) ‚Äñ 2 2 . \\bar{\\bm{x}}^{\\ast}(t,\\cdot)\\in\\operatorname*{arg\\ min}_{\\bar{\\bm{x}}(t,\\cdot)}\\operatorname{\\mathbb{E}}_{\\bm{x},\\bm{x}_{t}}\\|\\bm{x}-\\bar{\\bm{x}}(t,\\bm{x}_{t})\\|_{2}^{2}. over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT ( italic_t , ‚ãÖ ) ‚àà start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT over¬Ø start_ARG bold_italic_x end_ARG ( italic_t , ‚ãÖ ) end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT bold_italic_x , bold_italic_x start_POSTSUBSCRIPT italic_t end"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E4", "title": "ùíô ¬Ø ‚àó ‚Äã ( t , ùùÉ ) ‚âê ùîº ‚Å° [ ùíô ‚à£ ùíô t = ùùÉ ] . \\bar{\\bm{x}}^{\\ast}(t,\\bm{\\xi})\\doteq\\operatorname{\\mathbb{E}}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi}]. over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ‚àó e", "snippet": "ùíô ¬Ø ‚àó ‚Äã ( t , ùùÉ ) ‚âê ùîº ‚Å° [ ùíô ‚à£ ùíô t = ùùÉ ] . \\bar{\\bm{x}}^{\\ast}(t,\\bm{\\xi})\\doteq\\operatorname{\\mathbb{E}}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi}]. over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT ( italic_t , bold_italic_Œæ ) ‚âê blackboard_E [ bold_italic_x ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_Œæ ] . (3.2.4)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E5", "title": "ùíô ‚àº ‚àë k = 1 K œÄ k ‚Äã ùí© ‚Å° ( ùùÅ k , ùö∫ k ) , \\bm{x}\\sim\\sum_{k=1}^{K}\\pi_{k}\\operatorname{\\mathcal{N}}(\\bm{\\mu}_{k},\\bm{\\Sigma}_{k}), bold_italic_x ‚àº ‚àë start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT st", "snippet": "ùíô ‚àº ‚àë k = 1 K œÄ k ‚Äã ùí© ‚Å° ( ùùÅ k , ùö∫ k ) , \\bm{x}\\sim\\sum_{k=1}^{K}\\pi_{k}\\operatorname{\\mathcal{N}}(\\bm{\\mu}_{k},\\bm{\\Sigma}_{k}), bold_italic_x ‚àº ‚àë start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_œÄ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT caligraphic_N ( bold_italic_Œº start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , bold_Œ£ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) , (3.2.5)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E6", "title": "ùíô t = ùíô + t ‚Äã ùíà ‚àº ‚àë k = 1 K œÄ k ‚Äã ùí© ‚Å° ( ùùÅ k , ùö∫ k + t 2 ‚Äã ùë∞ ) . \\bm{x}_{t}=\\bm{x}+t\\bm{g}\\sim\\sum_{k=1}^{K}\\pi_{k}\\operatorname{\\mathcal{N}}(\\bm{\\mu}_{k},\\bm{\\Sigma}_{k}+t^{2}\\bm{I}). bold_italic_x st", "snippet": "ùíô t = ùíô + t ‚Äã ùíà ‚àº ‚àë k = 1 K œÄ k ‚Äã ùí© ‚Å° ( ùùÅ k , ùö∫ k + t 2 ‚Äã ùë∞ ) . \\bm{x}_{t}=\\bm{x}+t\\bm{g}\\sim\\sum_{k=1}^{K}\\pi_{k}\\operatorname{\\mathcal{N}}(\\bm{\\mu}_{k},\\bm{\\Sigma}_{k}+t^{2}\\bm{I}). bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_x + italic_t bold_italic_g ‚àº ‚àë start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_œÄ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT caligraphic_N ( bold_italic_Œº start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , bold_Œ£ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + italic_t start_PO"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E7", "title": "p t ‚Äã ( ùíô t ) = ‚àë k = 1 K œÄ k ‚Äã œÜ ‚Äã ( ùíô t ; ùùÅ k , ùö∫ k + t 2 ‚Äã ùë∞ ) . p_{t}(\\bm{x}_{t})=\\sum_{k=1}^{K}\\pi_{k}\\varphi(\\bm{x}_{t};\\bm{\\mu}_{k},\\bm{\\Sigma}_{k}+t^{2}\\bm{I}). italic_p start_POSTSUBSCRIPT it", "snippet": "p t ‚Äã ( ùíô t ) = ‚àë k = 1 K œÄ k ‚Äã œÜ ‚Äã ( ùíô t ; ùùÅ k , ùö∫ k + t 2 ‚Äã ùë∞ ) . p_{t}(\\bm{x}_{t})=\\sum_{k=1}^{K}\\pi_{k}\\varphi(\\bm{x}_{t};\\bm{\\mu}_{k},\\bm{\\Sigma}_{k}+t^{2}\\bm{I}). italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = ‚àë start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_œÄ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_œÜ ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; bold_italic_Œº start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , bold_Œ£ star"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E8", "title": "[ ùíô ùíô t ] = [ ùùÅ y ùùÅ y ] + [ ùö∫ y 1 / 2 ùüé ùö∫ y 1 / 2 t ‚Äã ùë∞ ] ‚Äã [ ùíñ ùíà ] . \\begin{bmatrix}\\bm{x}\\\\ \\bm{x}_{t}\\end{bmatrix}=\\begin{bmatrix}\\bm{\\mu}_{y}\\\\ \\bm{\\mu}_{y}\\end{bmatrix}+\\begin{bmatrix}\\bm{\\Sigma}", "snippet": "[ ùíô ùíô t ] = [ ùùÅ y ùùÅ y ] + [ ùö∫ y 1 / 2 ùüé ùö∫ y 1 / 2 t ‚Äã ùë∞ ] ‚Äã [ ùíñ ùíà ] . \\begin{bmatrix}\\bm{x}\\\\ \\bm{x}_{t}\\end{bmatrix}=\\begin{bmatrix}\\bm{\\mu}_{y}\\\\ \\bm{\\mu}_{y}\\end{bmatrix}+\\begin{bmatrix}\\bm{\\Sigma}_{y}^{1/2}&\\bm{0}\\\\ \\bm{\\Sigma}_{y}^{1/2}&t\\bm{I}\\end{bmatrix}\\begin{bmatrix}\\bm{u}\\\\ \\bm{g}\\end{bmatrix}. [ start_ARG start_ROW start_CELL bold_italic_x end_CELL end_ROW start_ROW start_CELL bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] = [ start_ARG start_ROW start_CELL bold_italic_Œº start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT end_CELL end_ROW start_R"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E9", "title": "[ ùíô ùíô t ] ‚àº ùí© ‚Å° ( [ ùùÅ y ùùÅ y ] , [ ùö∫ y ùö∫ y ùö∫ y ùö∫ y + t 2 ‚Äã ùë∞ ] ) . \\begin{bmatrix}\\bm{x}\\\\ \\bm{x}_{t}\\end{bmatrix}\\sim\\operatorname{\\mathcal{N}}\\left(\\begin{bmatrix}\\bm{\\mu}_{y}\\\\ \\bm{\\mu}_{y}\\end{bmat", "snippet": "[ ùíô ùíô t ] ‚àº ùí© ‚Å° ( [ ùùÅ y ùùÅ y ] , [ ùö∫ y ùö∫ y ùö∫ y ùö∫ y + t 2 ‚Äã ùë∞ ] ) . \\begin{bmatrix}\\bm{x}\\\\ \\bm{x}_{t}\\end{bmatrix}\\sim\\operatorname{\\mathcal{N}}\\left(\\begin{bmatrix}\\bm{\\mu}_{y}\\\\ \\bm{\\mu}_{y}\\end{bmatrix},\\begin{bmatrix}\\bm{\\Sigma}_{y}&\\bm{\\Sigma}_{y}\\\\ \\bm{\\Sigma}_{y}&\\bm{\\Sigma}_{y}+t^{2}\\bm{I}\\end{bmatrix}\\right). [ start_ARG start_ROW start_CELL bold_italic_x end_CELL end_ROW start_ROW start_CELL bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] ‚àº caligraphic_N ( [ start_ARG start_ROW start_CELL bold_italic_Œº start_POSTSUBSCRIPT italic_y end_POSTSUBSCR"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E10", "title": "ùîº ‚Å° [ ùíô ‚à£ ùíô t , y ] = ùùÅ y + ùö∫ y ‚Äã ( ùö∫ y + t 2 ‚Äã ùë∞ ) ‚àí 1 ‚Äã ( ùíô t ‚àí ùùÅ y ) . \\operatorname{\\mathbb{E}}[\\bm{x}\\mid\\bm{x}_{t},y]=\\bm{\\mu}_{y}+\\bm{\\Sigma}_{y}(\\bm{\\Sigma}_{y}+t^{2}\\bm{I})^{-1}(\\bm{x}_{t}-\\b", "snippet": "ùîº ‚Å° [ ùíô ‚à£ ùíô t , y ] = ùùÅ y + ùö∫ y ‚Äã ( ùö∫ y + t 2 ‚Äã ùë∞ ) ‚àí 1 ‚Äã ( ùíô t ‚àí ùùÅ y ) . \\operatorname{\\mathbb{E}}[\\bm{x}\\mid\\bm{x}_{t},y]=\\bm{\\mu}_{y}+\\bm{\\Sigma}_{y}(\\bm{\\Sigma}_{y}+t^{2}\\bm{I})^{-1}(\\bm{x}_{t}-\\bm{\\mu}_{y}). blackboard_E [ bold_italic_x ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_y ] = bold_italic_Œº start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT + bold_Œ£ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT ( bold_Œ£ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) start_POSTSUPERSCRIPT - 1 end_POSTSU"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E16", "title": "ùîº ‚Å° [ ùíô ‚à£ ùíô t , y = k ] = ùùÅ k + ùö∫ k ‚Äã ( ùö∫ k + t 2 ‚Äã ùë∞ ) ‚àí 1 ‚Äã ( ùíô t ‚àí ùùÅ k ) . \\operatorname{\\mathbb{E}}[\\bm{x}\\mid\\bm{x}_{t},y=k]=\\bm{\\mu}_{k}+\\bm{\\Sigma}_{k}(\\bm{\\Sigma}_{k}+t^{2}\\bm{I})^{-1}(\\bm{x}_", "snippet": "ùîº ‚Å° [ ùíô ‚à£ ùíô t , y = k ] = ùùÅ k + ùö∫ k ‚Äã ( ùö∫ k + t 2 ‚Äã ùë∞ ) ‚àí 1 ‚Äã ( ùíô t ‚àí ùùÅ k ) . \\operatorname{\\mathbb{E}}[\\bm{x}\\mid\\bm{x}_{t},y=k]=\\bm{\\mu}_{k}+\\bm{\\Sigma}_{k}(\\bm{\\Sigma}_{k}+t^{2}\\bm{I})^{-1}(\\bm{x}_{t}-\\bm{\\mu}_{k}). blackboard_E [ bold_italic_x ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_y = italic_k ] = bold_italic_Œº start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + bold_Œ£ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_Œ£ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) start_POSTSUPERSCRI"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E17", "title": "ùíô ¬Ø ‚àó ‚Äã ( t , ùíô t ) = ‚àë k = 1 K œÄ k ‚Äã œÜ ‚Äã ( ùíô t ; ùùÅ k , ùö∫ k + t 2 ‚Äã ùë∞ ) ‚àë i = 1 K œÄ i ‚Äã œÜ ‚Äã ( ùíô t ; ùùÅ i , ùö∫ i + t 2 ‚Äã ùë∞ ) ‚ãÖ ( ùùÅ k + ùö∫ k ‚Äã ( ùö∫ k + t 2 ‚Äã ùë∞ ) ‚àí 1 ‚Äã ( ùíô t ‚àí ùùÅ k ) ) . \\bar{\\bm{x}}^{\\ast}(", "snippet": "ùíô ¬Ø ‚àó ‚Äã ( t , ùíô t ) = ‚àë k = 1 K œÄ k ‚Äã œÜ ‚Äã ( ùíô t ; ùùÅ k , ùö∫ k + t 2 ‚Äã ùë∞ ) ‚àë i = 1 K œÄ i ‚Äã œÜ ‚Äã ( ùíô t ; ùùÅ i , ùö∫ i + t 2 ‚Äã ùë∞ ) ‚ãÖ ( ùùÅ k + ùö∫ k ‚Äã ( ùö∫ k + t 2 ‚Äã ùë∞ ) ‚àí 1 ‚Äã ( ùíô t ‚àí ùùÅ k ) ) . \\bar{\\bm{x}}^{\\ast}(t,\\bm{x}_{t})=\\sum_{k=1}^{K}\\frac{\\pi_{k}\\varphi(\\bm{x}_{t};\\bm{\\mu}_{k},\\bm{\\Sigma}_{k}+t^{2}\\bm{I})}{\\sum_{i=1}^{K}\\pi_{i}\\varphi(\\bm{x}_{t};\\bm{\\mu}_{i},\\bm{\\Sigma}_{i}+t^{2}\\bm{I})}\\cdot\\left(\\bm{\\mu}_{k}+\\bm{\\Sigma}_{k}(\\bm{\\Sigma}_{k}+t^{2}\\bm{I})^{-1}(\\bm{x}_{t}-\\bm{\\mu}_{k})\\right). over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT ( italic_t , bold_italic_x"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E18", "title": "ùíô ¬Ø ‚àó ‚Äã ( t , ùíô t ) = ùùÅ + ùö∫ ‚Äã ( ùö∫ + t 2 ‚Äã ùë∞ ) ‚àí 1 ‚Äã ( ùíô t ‚àí ùùÅ ) = ùùÅ + ùëΩ ‚Äã [ Œª 1 / ( Œª 1 + t 2 ) ‚ã± Œª D / ( Œª D + t 2 ) ] ‚Äã ùëΩ ‚ä§ ‚Äã ( ùíô t ‚àí ùùÅ ) , \\bar{\\bm{x}}^{\\ast}(t,\\bm{x}_{t})=\\bm{\\mu}+\\bm{\\Sigma}(\\bm", "snippet": "ùíô ¬Ø ‚àó ‚Äã ( t , ùíô t ) = ùùÅ + ùö∫ ‚Äã ( ùö∫ + t 2 ‚Äã ùë∞ ) ‚àí 1 ‚Äã ( ùíô t ‚àí ùùÅ ) = ùùÅ + ùëΩ ‚Äã [ Œª 1 / ( Œª 1 + t 2 ) ‚ã± Œª D / ( Œª D + t 2 ) ] ‚Äã ùëΩ ‚ä§ ‚Äã ( ùíô t ‚àí ùùÅ ) , \\bar{\\bm{x}}^{\\ast}(t,\\bm{x}_{t})=\\bm{\\mu}+\\bm{\\Sigma}(\\bm{\\Sigma}+t^{2}\\bm{I})^{-1}(\\bm{x}_{t}-\\bm{\\mu})=\\bm{\\mu}+\\bm{V}\\begin{bmatrix}\\lambda_{1}/(\\lambda_{1}+t^{2})&&\\\\ &\\ddots&\\\\ &&\\lambda_{D}/(\\lambda_{D}+t^{2})\\end{bmatrix}\\bm{V}^{\\top}(\\bm{x}_{t}-\\bm{\\mu}), over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = bold_italic_Œº + bold_Œ£ ( bold_Œ£ +"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E19", "title": "‚Äñ ùíô ¬Ø ‚àó ‚Äã ( t , ùíô t ) ‚àí ùùÅ ‚Äñ 2 ‚â§ ‚Äñ ùíô t ‚àí ùùÅ ‚Äñ 2 . \\|\\bar{\\bm{x}}^{\\ast}(t,\\bm{x}_{t})-\\bm{\\mu}\\|_{2}\\leq\\|\\bm{x}_{t}-\\bm{\\mu}\\|_{2}. ‚à• over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ‚àó end_P", "snippet": "‚Äñ ùíô ¬Ø ‚àó ‚Äã ( t , ùíô t ) ‚àí ùùÅ ‚Äñ 2 ‚â§ ‚Äñ ùíô t ‚àí ùùÅ ‚Äñ 2 . \\|\\bar{\\bm{x}}^{\\ast}(t,\\bm{x}_{t})-\\bm{\\mu}\\|_{2}\\leq\\|\\bm{x}_{t}-\\bm{\\mu}\\|_{2}. ‚à• over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) - bold_italic_Œº ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ‚â§ ‚à• bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - bold_italic_Œº ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT . (3.2.19)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E20", "title": "ùîº ‚Å° [ ùíô ‚à£ ùíô t ] = ùíô t + t 2 ‚Äã ‚àá ùíô t log ‚Å° p t ‚Äã ( ùíô t ) . \\operatorname{\\mathbb{E}}[\\bm{x}\\mid\\bm{x}_{t}]=\\bm{x}_{t}+t^{2}\\nabla_{\\bm{x}_{t}}\\log p_{t}(\\bm{x}_{t}). blackboard_E [ bold_italic_x ‚à£ bold", "snippet": "ùîº ‚Å° [ ùíô ‚à£ ùíô t ] = ùíô t + t 2 ‚Äã ‚àá ùíô t log ‚Å° p t ‚Äã ( ùíô t ) . \\operatorname{\\mathbb{E}}[\\bm{x}\\mid\\bm{x}_{t}]=\\bm{x}_{t}+t^{2}\\nabla_{\\bm{x}_{t}}\\log p_{t}(\\bm{x}_{t}). blackboard_E [ bold_italic_x ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] = bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ‚àá start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUB"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E33", "title": "x ¬Ø ‚àó ‚Äã ( t , x t ) = œÜ ‚Äã ( x t ; + 1 , t 2 ) ‚àí œÜ ‚Äã ( x t ; ‚àí 1 , t 2 ) œÜ ‚Äã ( x t ; 1 , t 2 ) + œÜ ‚Äã ( x t ; ‚àí 1 , t 2 ) = tanh ‚Å° ( ‚àí x t t 2 ) . \\bar{x}^{\\ast}(t,x_{t})=\\frac{\\varphi(x_{t};+1,t^{2})-\\", "snippet": "x ¬Ø ‚àó ‚Äã ( t , x t ) = œÜ ‚Äã ( x t ; + 1 , t 2 ) ‚àí œÜ ‚Äã ( x t ; ‚àí 1 , t 2 ) œÜ ‚Äã ( x t ; 1 , t 2 ) + œÜ ‚Äã ( x t ; ‚àí 1 , t 2 ) = tanh ‚Å° ( ‚àí x t t 2 ) . \\bar{x}^{\\ast}(t,x_{t})=\\frac{\\varphi(x_{t};+1,t^{2})-\\varphi(x_{t};-1,t^{2})}{\\varphi(x_{t};1,t^{2})+\\varphi(x_{t};-1,t^{2})}=\\tanh\\left(-\\frac{x_{t}}{t^{2}}\\right). over¬Ø start_ARG italic_x end_ARG start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT ( italic_t , italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = divide start_ARG italic_œÜ ( italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; + 1 , italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPER"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E34", "title": "t ‚Ñì = ‚Ñì L ‚Äã T , ‚Ñì ‚àà { 0 , 1 , ‚Ä¶ , L } . t_{\\ell}=\\frac{\\ell}{L}T,\\qquad\\ell\\in\\{0,1,\\dots,L\\}. italic_t start_POSTSUBSCRIPT roman_‚Ñì end_POSTSUBSCRIPT = divide start_ARG roman_‚Ñì end_ARG start_ARG itali", "snippet": "t ‚Ñì = ‚Ñì L ‚Äã T , ‚Ñì ‚àà { 0 , 1 , ‚Ä¶ , L } . t_{\\ell}=\\frac{\\ell}{L}T,\\qquad\\ell\\in\\{0,1,\\dots,L\\}. italic_t start_POSTSUBSCRIPT roman_‚Ñì end_POSTSUBSCRIPT = divide start_ARG roman_‚Ñì end_ARG start_ARG italic_L end_ARG italic_T , roman_‚Ñì ‚àà { 0 , 1 , ‚Ä¶ , italic_L } . (3.2.34)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E40", "title": "h ‚Äã ( ùîº ‚Å° [ ùíô s ‚à£ ùíô t ] ) < h ‚Äã ( ùíô t ) . h(\\operatorname{\\mathbb{E}}[\\bm{x}_{s}\\mid\\bm{x}_{t}])<h(\\bm{x}_{t}). italic_h ( blackboard_E [ bold_italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ‚à£", "snippet": "h ‚Äã ( ùîº ‚Å° [ ùíô s ‚à£ ùíô t ] ) < h ‚Äã ( ùíô t ) . h(\\operatorname{\\mathbb{E}}[\\bm{x}_{s}\\mid\\bm{x}_{t}])<h(\\bm{x}_{t}). italic_h ( blackboard_E [ bold_italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] ) < italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) . (3.2.40)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E41", "title": "min Œ∏ ‚àà Œò ‚Å° ùîº ùíô , ùíô t ‚Å° ‚Äñ ùíô ¬Ø Œ∏ ‚Äã ( t , ùíô t ) ‚àí ùíô ‚Äñ 2 2 . \\min_{\\theta\\in\\Theta}\\operatorname{\\mathbb{E}}_{\\bm{x},\\bm{x}_{t}}\\|\\bar{\\bm{x}}_{\\theta}(t,\\bm{x}_{t})-\\bm{x}\\|_{2}^{2}. roman_min start_POS", "snippet": "min Œ∏ ‚àà Œò ‚Å° ùîº ùíô , ùíô t ‚Å° ‚Äñ ùíô ¬Ø Œ∏ ‚Äã ( t , ùíô t ) ‚àí ùíô ‚Äñ 2 2 . \\min_{\\theta\\in\\Theta}\\operatorname{\\mathbb{E}}_{\\bm{x},\\bm{x}_{t}}\\|\\bar{\\bm{x}}_{\\theta}(t,\\bm{x}_{t})-\\bm{x}\\|_{2}^{2}. roman_min start_POSTSUBSCRIPT italic_Œ∏ ‚àà roman_Œò end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT bold_italic_x , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ‚à• over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) - bold_italic_x ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT sta"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E42", "title": "ùíô ‚àº 1 K ‚Äã ‚àë k = 1 K ùí© ‚Å° ( ùüé , ùëº k ‚Äã ùëº k ‚ä§ ) \\bm{x}\\sim\\frac{1}{K}\\sum_{k=1}^{K}\\operatorname{\\mathcal{N}}(\\bm{0},\\bm{U}_{k}\\bm{U}_{k}^{\\top}) bold_italic_x ‚àº divide start_ARG 1 end_ARG start_ARG itali", "snippet": "ùíô ‚àº 1 K ‚Äã ‚àë k = 1 K ùí© ‚Å° ( ùüé , ùëº k ‚Äã ùëº k ‚ä§ ) \\bm{x}\\sim\\frac{1}{K}\\sum_{k=1}^{K}\\operatorname{\\mathcal{N}}(\\bm{0},\\bm{U}_{k}\\bm{U}_{k}^{\\top}) bold_italic_x ‚àº divide start_ARG 1 end_ARG start_ARG italic_K end_ARG ‚àë start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT caligraphic_N ( bold_0 , bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ) (3.2.42)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E43", "title": "ùíô ¬Ø ‚àó ‚Äã ( t , ùíô t ) = ‚àë k = 1 K œÜ ‚Äã ( ùíô t ; ùüé , ùëº k ‚Äã ùëº k ‚ä§ + t 2 ‚Äã ùë∞ ) ‚àë i = 1 K œÜ ‚Äã ( ùíô t ; ùüé , ùëº i ‚Äã ùëº i ‚ä§ + t 2 ‚Äã ùë∞ ) ‚ãÖ ( ùëº k ‚Äã ùëº k ‚ä§ ‚Äã ( ùëº k ‚Äã ùëº k ‚ä§ + t 2 ‚Äã ùë∞ ) ‚àí 1 ‚Äã ùíô t ) . \\bar{\\bm{x}}^{\\ast}(", "snippet": "ùíô ¬Ø ‚àó ‚Äã ( t , ùíô t ) = ‚àë k = 1 K œÜ ‚Äã ( ùíô t ; ùüé , ùëº k ‚Äã ùëº k ‚ä§ + t 2 ‚Äã ùë∞ ) ‚àë i = 1 K œÜ ‚Äã ( ùíô t ; ùüé , ùëº i ‚Äã ùëº i ‚ä§ + t 2 ‚Äã ùë∞ ) ‚ãÖ ( ùëº k ‚Äã ùëº k ‚ä§ ‚Äã ( ùëº k ‚Äã ùëº k ‚ä§ + t 2 ‚Äã ùë∞ ) ‚àí 1 ‚Äã ùíô t ) . \\bar{\\bm{x}}^{\\ast}(t,\\bm{x}_{t})=\\sum_{k=1}^{K}\\frac{\\varphi(\\bm{x}_{t};\\bm{0},\\bm{U}_{k}\\bm{U}_{k}^{\\top}+t^{2}\\bm{I})}{\\sum_{i=1}^{K}\\varphi(\\bm{x}_{t};\\bm{0},\\bm{U}_{i}\\bm{U}_{i}^{\\top}+t^{2}\\bm{I})}\\cdot\\left(\\bm{U}_{k}\\bm{U}_{k}^{\\top}(\\bm{U}_{k}\\bm{U}_{k}^{\\top}+t^{2}\\bm{I})^{-1}\\bm{x}_{t}\\right). over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT ( italic_t , bold_italic_x start"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E44", "title": "( ùë® + ùëº ‚Äã ùë™ ‚Äã ùëΩ ) ‚àí 1 = ùë® ‚àí 1 ‚àí ùë® ‚àí 1 ‚Äã ùëº ‚Äã ( ùë™ ‚àí 1 + ùëΩ ‚Äã ùë® ‚àí 1 ‚Äã ùëº ) ‚àí 1 ‚Äã ùëΩ ‚Äã ùë® ‚àí 1 . (\\bm{A}+\\bm{U}\\bm{C}\\bm{V})^{-1}=\\bm{A}^{-1}-\\bm{A}^{-1}\\bm{U}(\\bm{C}^{-1}+\\bm{V}\\bm{A}^{-1}\\bm{U})^{-1}\\bm{V}\\b", "snippet": "( ùë® + ùëº ‚Äã ùë™ ‚Äã ùëΩ ) ‚àí 1 = ùë® ‚àí 1 ‚àí ùë® ‚àí 1 ‚Äã ùëº ‚Äã ( ùë™ ‚àí 1 + ùëΩ ‚Äã ùë® ‚àí 1 ‚Äã ùëº ) ‚àí 1 ‚Äã ùëΩ ‚Äã ùë® ‚àí 1 . (\\bm{A}+\\bm{U}\\bm{C}\\bm{V})^{-1}=\\bm{A}^{-1}-\\bm{A}^{-1}\\bm{U}(\\bm{C}^{-1}+\\bm{V}\\bm{A}^{-1}\\bm{U})^{-1}\\bm{V}\\bm{A}^{-1}. ( bold_italic_A + bold_italic_U bold_italic_C bold_italic_V ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT = bold_italic_A start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT - bold_italic_A start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_italic_U ( bold_italic_C start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT + bold_italic_V bold_italic_A start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E56", "title": "ùíô ¬Ø ‚àó ‚Äã ( t , ùíô t ) = 1 1 + t 2 ‚Äã ‚àë k = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ‚Äñ ùëº k ‚ä§ ‚Äã ùíô t ‚Äñ 2 2 ) ‚àë i = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ‚Äñ ùëº i ‚ä§ ‚Äã ùíô t ‚Äñ 2 2 ) ‚Äã ùëº k ‚Äã ùëº k ‚ä§ ‚Äã ùíô t , \\bar{\\bm{x}}", "snippet": "ùíô ¬Ø ‚àó ‚Äã ( t , ùíô t ) = 1 1 + t 2 ‚Äã ‚àë k = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ‚Äñ ùëº k ‚ä§ ‚Äã ùíô t ‚Äñ 2 2 ) ‚àë i = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ‚Äñ ùëº i ‚ä§ ‚Äã ùíô t ‚Äñ 2 2 ) ‚Äã ùëº k ‚Äã ùëº k ‚ä§ ‚Äã ùíô t , \\bar{\\bm{x}}^{\\ast}(t,\\bm{x}_{t})=\\frac{1}{1+t^{2}}\\sum_{k=1}^{K}\\frac{\\exp\\left(\\frac{1}{2t^{2}(1+t^{2})}\\|\\bm{U}_{k}^{\\top}\\bm{x}_{t}\\|_{2}^{2}\\right)}{\\sum_{i=1}^{K}\\exp\\left(\\frac{1}{2t^{2}(1+t^{2})}\\|\\bm{U}_{i}^{\\top}\\bm{x}_{t}\\|_{2}^{2}\\right)}\\bm{U}_{k}\\bm{U}_{k}^{\\top}\\bm{x}_{t}, over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT ( italic_t , bold_italic_x start_POSTSUBS"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E59", "title": "min ùëΩ ‚àà ùñÆ ‚Äã ( D , P ) ‚Å° ùîº ùíô , ùíô t ‚Å° ‚Äñ ùíô ‚àí 1 1 + t 2 ‚Äã ùëΩ ‚Äã ùëΩ ‚ä§ ‚Äã ùíô t ‚Äñ 2 2 = ùîº ùíô , ùíà ‚Å° ‚Äñ ùíô ‚àí 1 1 + t 2 ‚Äã ùëΩ ‚Äã ùëΩ ‚ä§ ‚Äã ( ùíô + t ‚Äã ùíà ) ‚Äñ 2 2 , \\min_{\\bm{V}\\in\\mathsf{O}(D,P)}\\operatorname{\\mathbb{E}}_{\\bm{x}", "snippet": "min ùëΩ ‚àà ùñÆ ‚Äã ( D , P ) ‚Å° ùîº ùíô , ùíô t ‚Å° ‚Äñ ùíô ‚àí 1 1 + t 2 ‚Äã ùëΩ ‚Äã ùëΩ ‚ä§ ‚Äã ùíô t ‚Äñ 2 2 = ùîº ùíô , ùíà ‚Å° ‚Äñ ùíô ‚àí 1 1 + t 2 ‚Äã ùëΩ ‚Äã ùëΩ ‚ä§ ‚Äã ( ùíô + t ‚Äã ùíà ) ‚Äñ 2 2 , \\min_{\\bm{V}\\in\\mathsf{O}(D,P)}\\operatorname{\\mathbb{E}}_{\\bm{x},\\bm{x}_{t}}\\left\\|\\bm{x}-\\frac{1}{1+t^{2}}\\bm{V}\\bm{V}^{\\top}\\bm{x}_{t}\\right\\|_{2}^{2}=\\operatorname{\\mathbb{E}}_{\\bm{x},\\bm{g}}\\left\\|\\bm{x}-\\frac{1}{1+t^{2}}\\bm{V}\\bm{V}^{\\top}(\\bm{x}+t\\bm{g})\\right\\|_{2}^{2}, roman_min start_POSTSUBSCRIPT bold_italic_V ‚àà sansserif_O ( italic_D , italic_P ) end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT bold_italic_x , bold_italic_x start_POSTSUBSCRIPT ita"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E65", "title": "ùíô T T = ùíô + T ‚Äã ùíà T = ùíô T + ùíà ‚Üí ùíà ‚àº ùí© ‚Å° ( ùüé , ùë∞ ) . \\frac{\\bm{x}_{T}}{T}=\\frac{\\bm{x}+T\\bm{g}}{T}=\\frac{\\bm{x}}{T}+\\bm{g}\\to\\bm{g}\\sim\\operatorname{\\mathcal{N}}(\\bm{0},\\bm{I}). divide start_ARG bold_i", "snippet": "ùíô T T = ùíô + T ‚Äã ùíà T = ùíô T + ùíà ‚Üí ùíà ‚àº ùí© ‚Å° ( ùüé , ùë∞ ) . \\frac{\\bm{x}_{T}}{T}=\\frac{\\bm{x}+T\\bm{g}}{T}=\\frac{\\bm{x}}{T}+\\bm{g}\\to\\bm{g}\\sim\\operatorname{\\mathcal{N}}(\\bm{0},\\bm{I}). divide start_ARG bold_italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_ARG start_ARG italic_T end_ARG = divide start_ARG bold_italic_x + italic_T bold_italic_g end_ARG start_ARG italic_T end_ARG = divide start_ARG bold_italic_x end_ARG start_ARG italic_T end_ARG + bold_italic_g ‚Üí bold_italic_g ‚àº caligraphic_N ( bold_0 , bold_italic_I ) . (3.2.65)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E66", "title": "ùíô ^ t ‚Ñì ‚àí 1 = ( 1 ‚àí 1 ‚Ñì ) ‚ãÖ ùíô ^ t ‚Ñì + 1 ‚Ñì ‚ãÖ ùíô ¬Ø ‚àó ‚Äã ( t ‚Ñì , ùíô ^ t ‚Ñì ) . \\hat{\\bm{x}}_{t_{\\ell-1}}=\\left(1-\\frac{1}{\\ell}\\right)\\cdot\\hat{\\bm{x}}_{t_{\\ell}}+\\frac{1}{\\ell}\\cdot\\bar{\\bm{x}}^{\\ast}(t_{\\e", "snippet": "ùíô ^ t ‚Ñì ‚àí 1 = ( 1 ‚àí 1 ‚Ñì ) ‚ãÖ ùíô ^ t ‚Ñì + 1 ‚Ñì ‚ãÖ ùíô ¬Ø ‚àó ‚Äã ( t ‚Ñì , ùíô ^ t ‚Ñì ) . \\hat{\\bm{x}}_{t_{\\ell-1}}=\\left(1-\\frac{1}{\\ell}\\right)\\cdot\\hat{\\bm{x}}_{t_{\\ell}}+\\frac{1}{\\ell}\\cdot\\bar{\\bm{x}}^{\\ast}(t_{\\ell},\\hat{\\bm{x}}_{t_{\\ell}}). over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_‚Ñì - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = ( 1 - divide start_ARG 1 end_ARG start_ARG roman_‚Ñì end_ARG ) ‚ãÖ over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_‚Ñì end_POSTSUBSCRIPT end_POSTSUBSCRIPT + divide start_ARG 1 end_ARG start_ARG "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E67", "title": "t ‚Ñì = C 1 ‚Äã ( e C 2 ‚Äã ‚Ñì ‚àí 1 ) , ‚àÄ ‚Ñì ‚àà { 0 , 1 , ‚Ä¶ , L } t_{\\ell}=C_{1}(e^{C_{2}\\ell}-1),\\qquad\\forall\\ell\\in\\{0,1,\\dots,L\\} italic_t start_POSTSUBSCRIPT roman_‚Ñì end_POSTSUBSCRIPT = italic_C start_POST", "snippet": "t ‚Ñì = C 1 ‚Äã ( e C 2 ‚Äã ‚Ñì ‚àí 1 ) , ‚àÄ ‚Ñì ‚àà { 0 , 1 , ‚Ä¶ , L } t_{\\ell}=C_{1}(e^{C_{2}\\ell}-1),\\qquad\\forall\\ell\\in\\{0,1,\\dots,L\\} italic_t start_POSTSUBSCRIPT roman_‚Ñì end_POSTSUBSCRIPT = italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_e start_POSTSUPERSCRIPT italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT - 1 ) , ‚àÄ roman_‚Ñì ‚àà { 0 , 1 , ‚Ä¶ , italic_L } (3.2.67)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E68", "title": "ùíô ^ t ‚Ñì ‚àí 1 ‚âê t ‚Ñì ‚àí 1 t ‚Ñì ‚Äã ùíô ^ t ‚Ñì + ( 1 ‚àí t ‚Ñì ‚àí 1 t ‚Ñì ) ‚Äã ùíô ¬Ø ‚àó ‚Äã ( t ‚Ñì , ùíô ^ t ‚Ñì ) , \\hat{\\bm{x}}_{t_{\\ell-1}}\\doteq\\frac{t_{\\ell-1}}{t_{\\ell}}\\hat{\\bm{x}}_{t_{\\ell}}+\\left(1-\\frac{t_{\\ell-1}}{t_{\\", "snippet": "ùíô ^ t ‚Ñì ‚àí 1 ‚âê t ‚Ñì ‚àí 1 t ‚Ñì ‚Äã ùíô ^ t ‚Ñì + ( 1 ‚àí t ‚Ñì ‚àí 1 t ‚Ñì ) ‚Äã ùíô ¬Ø ‚àó ‚Äã ( t ‚Ñì , ùíô ^ t ‚Ñì ) , \\hat{\\bm{x}}_{t_{\\ell-1}}\\doteq\\frac{t_{\\ell-1}}{t_{\\ell}}\\hat{\\bm{x}}_{t_{\\ell}}+\\left(1-\\frac{t_{\\ell-1}}{t_{\\ell}}\\right)\\bar{\\bm{x}}^{\\ast}(t_{\\ell},\\hat{\\bm{x}}_{t_{\\ell}}), over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_‚Ñì - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ‚âê divide start_ARG italic_t start_POSTSUBSCRIPT roman_‚Ñì - 1 end_POSTSUBSCRIPT end_ARG start_ARG italic_t start_POSTSUBSCRIPT roman_‚Ñì end_POSTSUBSCRIPT end_ARG over^ start_ARG bold_italic_x end_ARG s"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E69", "title": "ùíô t ‚âê Œ± t ‚Äã ùíô + œÉ t ‚Äã ùíà , ‚àÄ t ‚àà [ 0 , T ] . \\bm{x}_{t}\\doteq\\alpha_{t}\\bm{x}+\\sigma_{t}\\bm{g},\\qquad\\forall t\\in[0,T]. bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ‚âê italic_Œ± start_POS", "snippet": "ùíô t ‚âê Œ± t ‚Äã ùíô + œÉ t ‚Äã ùíà , ‚àÄ t ‚àà [ 0 , T ] . \\bm{x}_{t}\\doteq\\alpha_{t}\\bm{x}+\\sigma_{t}\\bm{g},\\qquad\\forall t\\in[0,T]. bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ‚âê italic_Œ± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_x + italic_œÉ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_g , ‚àÄ italic_t ‚àà [ 0 , italic_T ] . (3.2.69)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E70", "title": "ùîº ‚Å° [ ùíô ‚à£ ùíô t ] = 1 Œ± t ‚Äã ( ùíô t + œÉ t 2 ‚Äã ‚àá log ‚Å° p t ‚Äã ( ùíô ) ) . \\operatorname{\\mathbb{E}}[\\bm{x}\\mid\\bm{x}_{t}]=\\frac{1}{\\alpha_{t}}\\left(\\bm{x}_{t}+\\sigma_{t}^{2}\\nabla\\log p_{t}(\\bm{x})\\right). bl", "snippet": "ùîº ‚Å° [ ùíô ‚à£ ùíô t ] = 1 Œ± t ‚Äã ( ùíô t + œÉ t 2 ‚Äã ‚àá log ‚Å° p t ‚Äã ( ùíô ) ) . \\operatorname{\\mathbb{E}}[\\bm{x}\\mid\\bm{x}_{t}]=\\frac{1}{\\alpha_{t}}\\left(\\bm{x}_{t}+\\sigma_{t}^{2}\\nabla\\log p_{t}(\\bm{x})\\right). blackboard_E [ bold_italic_x ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] = divide start_ARG 1 end_ARG start_ARG italic_Œ± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_œÉ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ‚àá roman_log italic_p start_POSTSUBSCRIPT"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E71", "title": "ùíô ^ t ‚Ñì ‚àí 1 = œÉ t ‚Ñì ‚àí 1 œÉ t ‚Ñì ‚Äã ùíô ^ t ‚Ñì + ( Œ± t ‚Ñì ‚àí 1 ‚àí œÉ t ‚Ñì ‚àí 1 œÉ t ‚Ñì ‚Äã Œ± t ‚Ñì ) ‚Äã ùíô ¬Ø ‚àó ‚Äã ( t ‚Ñì , ùíô ^ t ‚Ñì ) . \\hat{\\bm{x}}_{t_{\\ell-1}}=\\frac{\\sigma_{t_{\\ell-1}}}{\\sigma_{t_{\\ell}}}\\hat{\\bm{x}}_{t_{", "snippet": "ùíô ^ t ‚Ñì ‚àí 1 = œÉ t ‚Ñì ‚àí 1 œÉ t ‚Ñì ‚Äã ùíô ^ t ‚Ñì + ( Œ± t ‚Ñì ‚àí 1 ‚àí œÉ t ‚Ñì ‚àí 1 œÉ t ‚Ñì ‚Äã Œ± t ‚Ñì ) ‚Äã ùíô ¬Ø ‚àó ‚Äã ( t ‚Ñì , ùíô ^ t ‚Ñì ) . \\hat{\\bm{x}}_{t_{\\ell-1}}=\\frac{\\sigma_{t_{\\ell-1}}}{\\sigma_{t_{\\ell}}}\\hat{\\bm{x}}_{t_{\\ell}}+\\left(\\alpha_{t_{\\ell-1}}-\\frac{\\sigma_{t_{\\ell-1}}}{\\sigma_{t_{\\ell}}}\\alpha_{t_{\\ell}}\\right)\\bar{\\bm{x}}^{\\ast}(t_{\\ell},\\hat{\\bm{x}}_{t_{\\ell}}). over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_‚Ñì - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = divide start_ARG italic_œÉ start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_‚Ñì - 1 end_POSTSUBSCRIPT e"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E72", "title": "ùíô ¬Ø ‚àó ‚Äã ( t , ùíô t ) = ‚àë k = 1 K œÄ k ‚Äã œÜ ‚Äã ( ùíô t ; Œ± t ‚Äã ùùÅ k , Œ± t 2 ‚Äã ùö∫ k + œÉ t 2 ‚Äã ùë∞ ) ‚àë i = 1 K œÄ i ‚Äã œÜ ‚Äã ( ùíô t ; Œ± t ‚Äã ùùÅ i , Œ± t 2 ‚Äã ùö∫ i + œÉ t 2 ‚Äã ùë∞ ) ‚ãÖ ( ùùÅ k + Œ± t ‚Äã ùö∫ k ‚Äã ( Œ± t 2 ‚Äã ùö∫ k + œÉ t 2 ‚Äã ", "snippet": "ùíô ¬Ø ‚àó ‚Äã ( t , ùíô t ) = ‚àë k = 1 K œÄ k ‚Äã œÜ ‚Äã ( ùíô t ; Œ± t ‚Äã ùùÅ k , Œ± t 2 ‚Äã ùö∫ k + œÉ t 2 ‚Äã ùë∞ ) ‚àë i = 1 K œÄ i ‚Äã œÜ ‚Äã ( ùíô t ; Œ± t ‚Äã ùùÅ i , Œ± t 2 ‚Äã ùö∫ i + œÉ t 2 ‚Äã ùë∞ ) ‚ãÖ ( ùùÅ k + Œ± t ‚Äã ùö∫ k ‚Äã ( Œ± t 2 ‚Äã ùö∫ k + œÉ t 2 ‚Äã ùë∞ ) ‚àí 1 ‚Äã ( ùíô t ‚àí Œ± t ‚Äã ùùÅ k ) ) . \\bar{\\bm{x}}^{\\ast}(t,\\bm{x}_{t})=\\sum_{k=1}^{K}\\frac{\\pi_{k}\\varphi(\\bm{x}_{t};\\alpha_{t}\\bm{\\mu}_{k},\\alpha_{t}^{2}\\bm{\\Sigma}_{k}+\\sigma_{t}^{2}\\bm{I})}{\\sum_{i=1}^{K}\\pi_{i}\\varphi(\\bm{x}_{t};\\alpha_{t}\\bm{\\mu}_{i},\\alpha_{t}^{2}\\bm{\\Sigma}_{i}+\\sigma_{t}^{2}\\bm{I})}\\cdot\\left(\\bm{\\mu}_{k}+\\alpha_{t}\\bm{\\Sigma}_{k}(\\alpha_{t}^{2}\\bm{\\Sigma}_{k}+\\sigma_{t}^{2}\\"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.Ex1", "title": "ùíô ^ t ‚Ñì ‚àí 1 ‚âê œÉ t ‚Ñì ‚àí 1 œÉ t ‚Ñì ‚Äã ùíô ^ t ‚Ñì + ( Œ± t ‚Ñì ‚àí 1 ‚àí œÉ t ‚Ñì ‚àí 1 œÉ t ‚Ñì ‚Äã Œ± t ‚Ñì ) ‚Äã ùíô ¬Ø ‚Äã ( t ‚Ñì , ùíô ^ t ‚Ñì ) \\hat{\\bm{x}}_{t_{\\ell-1}}\\doteq\\frac{\\sigma_{t_{\\ell-1}}}{\\sigma_{t_{\\ell}}}\\hat{\\bm{x}}_{t_", "snippet": "ùíô ^ t ‚Ñì ‚àí 1 ‚âê œÉ t ‚Ñì ‚àí 1 œÉ t ‚Ñì ‚Äã ùíô ^ t ‚Ñì + ( Œ± t ‚Ñì ‚àí 1 ‚àí œÉ t ‚Ñì ‚àí 1 œÉ t ‚Ñì ‚Äã Œ± t ‚Ñì ) ‚Äã ùíô ¬Ø ‚Äã ( t ‚Ñì , ùíô ^ t ‚Ñì ) \\hat{\\bm{x}}_{t_{\\ell-1}}\\doteq\\frac{\\sigma_{t_{\\ell-1}}}{\\sigma_{t_{\\ell}}}\\hat{\\bm{x}}_{t_{\\ell}}+\\left(\\alpha_{t_{\\ell-1}}-\\frac{\\sigma_{t_{\\ell-1}}}{\\sigma_{t_{\\ell}}}\\alpha_{t_{\\ell}}\\right)\\bar{\\bm{x}}(t_{\\ell},\\hat{\\bm{x}}_{t_{\\ell}}) over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_‚Ñì - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ‚âê divide start_ARG italic_œÉ start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_‚Ñì - 1 end_POSTSUBSCRIPT end_POST"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E73", "title": "min Œ∏ ‚Å° ùîº t , ùíô , ùíô t ‚Å° ‚Äñ ùíô ¬Ø Œ∏ ‚Äã ( t , ùíô t ) ‚àí ùíô ‚Äñ 2 2 . \\min_{\\theta}\\operatorname{\\mathbb{E}}_{t,\\bm{x},\\bm{x}_{t}}\\|\\bar{\\bm{x}}_{\\theta}(t,\\bm{x}_{t})-\\bm{x}\\|_{2}^{2}. roman_min start_POSTSUBSCR", "snippet": "min Œ∏ ‚Å° ùîº t , ùíô , ùíô t ‚Å° ‚Äñ ùíô ¬Ø Œ∏ ‚Äã ( t , ùíô t ) ‚àí ùíô ‚Äñ 2 2 . \\min_{\\theta}\\operatorname{\\mathbb{E}}_{t,\\bm{x},\\bm{x}_{t}}\\|\\bar{\\bm{x}}_{\\theta}(t,\\bm{x}_{t})-\\bm{x}\\|_{2}^{2}. roman_min start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_t , bold_italic_x , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ‚à• over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) - bold_italic_x ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POS"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E74", "title": "min Œ∏ ‚Å° ùîº t ‚Å° w t ‚Äã ùîº ùíô , ùíô t ‚Å° ‚Äñ ùíô ¬Ø Œ∏ ‚Äã ( t , ùíô t ) ‚àí ùíô ‚Äñ 2 2 . \\min_{\\theta}\\operatorname{\\mathbb{E}}_{t}w_{t}\\operatorname{\\mathbb{E}}_{\\bm{x},\\bm{x}_{t}}\\|\\bar{\\bm{x}}_{\\theta}(t,\\bm{x}_{t})-\\bm{", "snippet": "min Œ∏ ‚Å° ùîº t ‚Å° w t ‚Äã ùîº ùíô , ùíô t ‚Å° ‚Äñ ùíô ¬Ø Œ∏ ‚Äã ( t , ùíô t ) ‚àí ùíô ‚Äñ 2 2 . \\min_{\\theta}\\operatorname{\\mathbb{E}}_{t}w_{t}\\operatorname{\\mathbb{E}}_{\\bm{x},\\bm{x}_{t}}\\|\\bar{\\bm{x}}_{\\theta}(t,\\bm{x}_{t})-\\bm{x}\\|_{2}^{2}. roman_min start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT bold_italic_x , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ‚à• over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIP"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E75", "title": "ùíô t = Œ± t ‚Äã ùîº ‚Å° [ ùíô ‚à£ ùíô t ] + œÉ t ‚Äã ùîº ‚Å° [ ùíà ‚à£ ùíô t ] ‚üπ ùîº ‚Å° [ ùíà ‚à£ ùíô t ] = 1 œÉ t ‚Äã ( ùíô t ‚àí Œ± t ‚Äã ùîº ‚Å° [ ùíô ‚à£ ùíô t ] ) , \\bm{x}_{t}=\\alpha_{t}\\operatorname{\\mathbb{E}}[\\bm{x}\\mid\\bm{x}_{t}]+\\sigma_{t}\\operat", "snippet": "ùíô t = Œ± t ‚Äã ùîº ‚Å° [ ùíô ‚à£ ùíô t ] + œÉ t ‚Äã ùîº ‚Å° [ ùíà ‚à£ ùíô t ] ‚üπ ùîº ‚Å° [ ùíà ‚à£ ùíô t ] = 1 œÉ t ‚Äã ( ùíô t ‚àí Œ± t ‚Äã ùîº ‚Å° [ ùíô ‚à£ ùíô t ] ) , \\bm{x}_{t}=\\alpha_{t}\\operatorname{\\mathbb{E}}[\\bm{x}\\mid\\bm{x}_{t}]+\\sigma_{t}\\operatorname{\\mathbb{E}}[\\bm{g}\\mid\\bm{x}_{t}]\\implies\\operatorname{\\mathbb{E}}[\\bm{g}\\mid\\bm{x}_{t}]=\\frac{1}{\\sigma_{t}}\\left(\\bm{x}_{t}-\\alpha_{t}\\operatorname{\\mathbb{E}}[\\bm{x}\\mid\\bm{x}_{t}]\\right), bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_Œ± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT blackboard_E [ bold_italic_x ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POST"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E76", "title": "ùíà ¬Ø ‚Äã ( t , ùíô t ) = 1 œÉ t ‚Äã ùíô t ‚àí Œ± t œÉ t ‚Äã ùíô ¬Ø ‚Äã ( t , ùíô t ) , \\bar{\\bm{g}}(t,\\bm{x}_{t})=\\frac{1}{\\sigma_{t}}\\bm{x}_{t}-\\frac{\\alpha_{t}}{\\sigma_{t}}\\bar{\\bm{x}}(t,\\bm{x}_{t}), over¬Ø start_ARG bold_", "snippet": "ùíà ¬Ø ‚Äã ( t , ùíô t ) = 1 œÉ t ‚Äã ùíô t ‚àí Œ± t œÉ t ‚Äã ùíô ¬Ø ‚Äã ( t , ùíô t ) , \\bar{\\bm{g}}(t,\\bm{x}_{t})=\\frac{1}{\\sigma_{t}}\\bm{x}_{t}-\\frac{\\alpha_{t}}{\\sigma_{t}}\\bar{\\bm{x}}(t,\\bm{x}_{t}), over¬Ø start_ARG bold_italic_g end_ARG ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = divide start_ARG 1 end_ARG start_ARG italic_œÉ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - divide start_ARG italic_Œ± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG italic_œÉ start_POSTSUBSCRIPT italic_t end_POSTSUBS"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E77", "title": "ùîº t ‚Å° w t ‚Äã ùîº ùíà , ùíô t ‚Å° ‚Äñ ùíà ‚àí ùíà ¬Ø ‚Äã ( t , ùíô t ) ‚Äñ 2 2 = ùîº t ‚Å° w t ‚Äã Œ± t 2 œÉ t 2 ‚Äã ùîº ùíô , ùíô t ‚Å° ‚Äñ ùíô ‚àí ùíô ¬Ø ‚Äã ( t , ùíô t ) ‚Äñ 2 2 . \\operatorname{\\mathbb{E}}_{t}w_{t}\\operatorname{\\mathbb{E}}_{\\bm{g},\\bm{x}", "snippet": "ùîº t ‚Å° w t ‚Äã ùîº ùíà , ùíô t ‚Å° ‚Äñ ùíà ‚àí ùíà ¬Ø ‚Äã ( t , ùíô t ) ‚Äñ 2 2 = ùîº t ‚Å° w t ‚Äã Œ± t 2 œÉ t 2 ‚Äã ùîº ùíô , ùíô t ‚Å° ‚Äñ ùíô ‚àí ùíô ¬Ø ‚Äã ( t , ùíô t ) ‚Äñ 2 2 . \\operatorname{\\mathbb{E}}_{t}w_{t}\\operatorname{\\mathbb{E}}_{\\bm{g},\\bm{x}_{t}}\\|\\bm{g}-\\bar{\\bm{g}}(t,\\bm{x}_{t})\\|_{2}^{2}=\\operatorname{\\mathbb{E}}_{t}w_{t}\\frac{\\alpha_{t}^{2}}{\\sigma_{t}^{2}}\\operatorname{\\mathbb{E}}_{\\bm{x},\\bm{x}_{t}}\\|\\bm{x}-\\bar{\\bm{x}}(t,\\bm{x}_{t})\\|_{2}^{2}. blackboard_E start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT bold_italic_g , bold_italic_x start_P"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E78", "title": "ùñ≥ùñµ ‚Å° ( ùíô , ùíö ) ‚âê sup A ‚äÜ ‚Ñù d | ‚Ñô ‚Å° [ ùíô ‚àà A ] ‚àí ‚Ñô ‚Å° [ ùíö ‚àà A ] | . \\operatorname{\\mathsf{TV}}(\\bm{x},\\bm{y})\\doteq\\sup_{A\\subseteq\\mathbb{R}^{d}}\\left\\lvert\\operatorname{\\mathbb{P}}[\\bm{x}\\in A]-\\operat", "snippet": "ùñ≥ùñµ ‚Å° ( ùíô , ùíö ) ‚âê sup A ‚äÜ ‚Ñù d | ‚Ñô ‚Å° [ ùíô ‚àà A ] ‚àí ‚Ñô ‚Å° [ ùíö ‚àà A ] | . \\operatorname{\\mathsf{TV}}(\\bm{x},\\bm{y})\\doteq\\sup_{A\\subseteq\\mathbb{R}^{d}}\\left\\lvert\\operatorname{\\mathbb{P}}[\\bm{x}\\in A]-\\operatorname{\\mathbb{P}}[\\bm{y}\\in A]\\right\\rvert. sansserif_TV ( bold_italic_x , bold_italic_y ) ‚âê roman_sup start_POSTSUBSCRIPT italic_A ‚äÜ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT end_POSTSUBSCRIPT | blackboard_P [ bold_italic_x ‚àà italic_A ] - blackboard_P [ bold_italic_y ‚àà italic_A ] | . (3.2.78)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E79", "title": "ùñ≥ùñµ ‚Å° ( ùíô , ùíô ^ ) = ùí™ ~ ‚Äã ( D L ‚èü discretization error + 1 L ‚Äã ‚àë ‚Ñì = 1 L Œ± t ‚Ñì œÉ t ‚Ñì 2 ‚Äã ùîº ùíô , ùíô t ‚Ñì ‚Å° ‚Äñ ùíô ¬Ø ‚àó ‚Äã ( t ‚Ñì , ùíô t ‚Ñì ) ‚àí ùíô ¬Ø ‚Äã ( t ‚Ñì , ùíô t ‚Ñì ) ‚Äñ 2 2 ‚èü average excess error of the denoiser ) \\", "snippet": "ùñ≥ùñµ ‚Å° ( ùíô , ùíô ^ ) = ùí™ ~ ‚Äã ( D L ‚èü discretization error + 1 L ‚Äã ‚àë ‚Ñì = 1 L Œ± t ‚Ñì œÉ t ‚Ñì 2 ‚Äã ùîº ùíô , ùíô t ‚Ñì ‚Å° ‚Äñ ùíô ¬Ø ‚àó ‚Äã ( t ‚Ñì , ùíô t ‚Ñì ) ‚àí ùíô ¬Ø ‚Äã ( t ‚Ñì , ùíô t ‚Ñì ) ‚Äñ 2 2 ‚èü average excess error of the denoiser ) \\operatorname{\\mathsf{TV}}(\\bm{x},\\hat{\\bm{x}})=\\tilde{\\mathcal{O}}\\left(\\underbrace{\\frac{D}{L}}_{\\text{discretization error}}+\\underbrace{\\sqrt{\\frac{1}{L}\\sum_{\\ell=1}^{L}\\frac{\\alpha_{t_{\\ell}}}{\\sigma_{t_{\\ell}}^{2}}\\operatorname{\\mathbb{E}}_{\\bm{x},\\bm{x}_{t_{\\ell}}}\\|\\bar{\\bm{x}}^{\\ast}(t_{\\ell},\\bm{x}_{t_{\\ell}})-\\bar{\\bm{x}}(t_{\\ell},\\bm{x}_{t_{\\ell}})\\|_{2}^{2}}}_{\\text{average excess err"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E80", "title": "ùí™ ~ ‚Äã ( approximate intrinsic dimension L ) \\tilde{\\mathcal{O}}\\left(\\frac{\\text{approximate intrinsic dimension}}{L}\\right) over~ start_ARG caligraphic_O end_ARG ( divide start_ARG approximate intrin", "snippet": "ùí™ ~ ‚Äã ( approximate intrinsic dimension L ) \\tilde{\\mathcal{O}}\\left(\\frac{\\text{approximate intrinsic dimension}}{L}\\right) over~ start_ARG caligraphic_O end_ARG ( divide start_ARG approximate intrinsic dimension end_ARG start_ARG italic_L end_ARG ) (3.2.80)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S2.E81", "title": "ùíô ¬Ø ‚ãÜ ‚Äã ( t , ùíô t ) = ‚àë i = 1 N e ‚àí ‚Äñ ùíô t ‚àí Œ± t ‚Äã ùíô i ‚Äñ 2 2 / ( 2 ‚Äã œÉ t 2 ) ‚àë j = 1 N e ‚àí ‚Äñ ùíô t ‚àí Œ± t ‚Äã ùíô j ‚Äñ 2 2 / ( 2 ‚Äã œÉ t 2 ) ‚Äã ùíô i . \\bar{\\bm{x}}^{\\star}(t,\\bm{x}_{t})=\\sum_{i=1}^{N}\\frac{e^{-\\|\\", "snippet": "ùíô ¬Ø ‚ãÜ ‚Äã ( t , ùíô t ) = ‚àë i = 1 N e ‚àí ‚Äñ ùíô t ‚àí Œ± t ‚Äã ùíô i ‚Äñ 2 2 / ( 2 ‚Äã œÉ t 2 ) ‚àë j = 1 N e ‚àí ‚Äñ ùíô t ‚àí Œ± t ‚Äã ùíô j ‚Äñ 2 2 / ( 2 ‚Äã œÉ t 2 ) ‚Äã ùíô i . \\bar{\\bm{x}}^{\\star}(t,\\bm{x}_{t})=\\sum_{i=1}^{N}\\frac{e^{-\\|\\bm{x}_{t}-\\alpha_{t}\\bm{x}_{i}\\|_{2}^{2}/(2\\sigma_{t}^{2})}}{\\sum_{j=1}^{N}e^{-\\|\\bm{x}_{t}-\\alpha_{t}\\bm{x}_{j}\\|_{2}^{2}/(2\\sigma_{t}^{2})}}\\bm{x}_{i}. over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N en"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E1", "title": "ùíô ‚Ü¶ ùíô ^ \\bm{x}\\mapsto\\hat{\\bm{x}} bold_italic_x ‚Ü¶ over^ start_ARG bold_italic_x end_ARG (3.3.1)", "snippet": "ùíô ‚Ü¶ ùíô ^ \\bm{x}\\mapsto\\hat{\\bm{x}} bold_italic_x ‚Ü¶ over^ start_ARG bold_italic_x end_ARG (3.3.1)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E2", "title": "ùîº ‚Å° [ d ‚Äã ( ùíô , ùíô ^ ) 2 ] ‚â§ œµ 2 . \\operatorname{\\mathbb{E}}[d(\\bm{x},\\hat{\\bm{x}})^{2}]\\leq\\epsilon^{2}. blackboard_E [ italic_d ( bold_italic_x , over^ start_ARG bold_italic_x end_ARG ) start_POSTSUP", "snippet": "ùîº ‚Å° [ d ‚Äã ( ùíô , ùíô ^ ) 2 ] ‚â§ œµ 2 . \\operatorname{\\mathbb{E}}[d(\\bm{x},\\hat{\\bm{x}})^{2}]\\leq\\epsilon^{2}. blackboard_E [ italic_d ( bold_italic_x , over^ start_ARG bold_italic_x end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] ‚â§ italic_œµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . (3.3.2)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E3", "title": "‚Ñõ œµ ‚Äã ( ùíô ) = min p ‚Äã ( ùíô ^ ‚à£ ùíô ) : ùîº ‚Å° [ ‚Äñ ùíô ‚àí ùíô ^ ‚Äñ 2 2 ] ‚â§ œµ 2 ‚Å° I ‚Äã ( ùíô ; ùíô ^ ) , \\mathcal{R}_{\\epsilon}(\\bm{x})=\\min_{p(\\hat{\\bm{x}}\\mid\\bm{x}):\\operatorname{\\mathbb{E}}[\\|\\bm{x}-\\hat{\\bm{x}}\\|_{", "snippet": "‚Ñõ œµ ‚Äã ( ùíô ) = min p ‚Äã ( ùíô ^ ‚à£ ùíô ) : ùîº ‚Å° [ ‚Äñ ùíô ‚àí ùíô ^ ‚Äñ 2 2 ] ‚â§ œµ 2 ‚Å° I ‚Äã ( ùíô ; ùíô ^ ) , \\mathcal{R}_{\\epsilon}(\\bm{x})=\\min_{p(\\hat{\\bm{x}}\\mid\\bm{x}):\\operatorname{\\mathbb{E}}[\\|\\bm{x}-\\hat{\\bm{x}}\\|_{2}^{2}]\\leq\\epsilon^{2}}I(\\bm{x};\\hat{\\bm{x}}), caligraphic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_x ) = roman_min start_POSTSUBSCRIPT italic_p ( over^ start_ARG bold_italic_x end_ARG ‚à£ bold_italic_x ) : blackboard_E [ ‚à• bold_italic_x - over^ start_ARG bold_italic_x end_ARG ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] ‚â§ italic_œµ st"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E4", "title": "I ‚Äã ( ùíô ; ùíô ^ ) = ùñ™ùñ´ ‚Å° ( p ‚Äã ( ùíô , ùíô ^ ) ‚à• p ‚Äã ( ùíô ) ‚Äã p ‚Äã ( ùíô ^ ) ) . I(\\bm{x};\\hat{\\bm{x}})=\\operatorname{\\mathsf{KL}}(p(\\bm{x},\\hat{\\bm{x}})\\;\\|\\;p(\\bm{x})p(\\hat{\\bm{x}})). italic_I ( bold_italic_x", "snippet": "I ‚Äã ( ùíô ; ùíô ^ ) = ùñ™ùñ´ ‚Å° ( p ‚Äã ( ùíô , ùíô ^ ) ‚à• p ‚Äã ( ùíô ) ‚Äã p ‚Äã ( ùíô ^ ) ) . I(\\bm{x};\\hat{\\bm{x}})=\\operatorname{\\mathsf{KL}}(p(\\bm{x},\\hat{\\bm{x}})\\;\\|\\;p(\\bm{x})p(\\hat{\\bm{x}})). italic_I ( bold_italic_x ; over^ start_ARG bold_italic_x end_ARG ) = sansserif_KL ( italic_p ( bold_italic_x , over^ start_ARG bold_italic_x end_ARG ) ‚à• italic_p ( bold_italic_x ) italic_p ( over^ start_ARG bold_italic_x end_ARG ) ) . (3.3.4)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E5", "title": "I ‚Äã ( ùíô ; ùíô ^ ) = h ‚Äã ( ùíô ) ‚àí h ‚Äã ( ùíô ‚à£ ùíô ^ ) , I(\\bm{x};\\hat{\\bm{x}})=h(\\bm{x})-h(\\bm{x}\\mid\\hat{\\bm{x}}), italic_I ( bold_italic_x ; over^ start_ARG bold_italic_x end_ARG ) = italic_h ( bold_italic_", "snippet": "I ‚Äã ( ùíô ; ùíô ^ ) = h ‚Äã ( ùíô ) ‚àí h ‚Äã ( ùíô ‚à£ ùíô ^ ) , I(\\bm{x};\\hat{\\bm{x}})=h(\\bm{x})-h(\\bm{x}\\mid\\hat{\\bm{x}}), italic_I ( bold_italic_x ; over^ start_ARG bold_italic_x end_ARG ) = italic_h ( bold_italic_x ) - italic_h ( bold_italic_x ‚à£ over^ start_ARG bold_italic_x end_ARG ) , (3.3.5)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E6", "title": "I ‚Äã ( ùíô ; ùíô ^ ) = H ‚Äã ( ùíô ) ‚àí H ‚Äã ( ùíô ‚à£ ùíô ^ ) . I(\\bm{x};\\hat{\\bm{x}})=H(\\bm{x})-H(\\bm{x}\\mid\\hat{\\bm{x}}). italic_I ( bold_italic_x ; over^ start_ARG bold_italic_x end_ARG ) = italic_H ( bold_italic_", "snippet": "I ‚Äã ( ùíô ; ùíô ^ ) = H ‚Äã ( ùíô ) ‚àí H ‚Äã ( ùíô ‚à£ ùíô ^ ) . I(\\bm{x};\\hat{\\bm{x}})=H(\\bm{x})-H(\\bm{x}\\mid\\hat{\\bm{x}}). italic_I ( bold_italic_x ; over^ start_ARG bold_italic_x end_ARG ) = italic_H ( bold_italic_x ) - italic_H ( bold_italic_x ‚à£ over^ start_ARG bold_italic_x end_ARG ) . (3.3.6)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E7", "title": "ùí© œµ ‚Äã ( K ) ‚âê min ‚Å° { n ‚àà ‚Ñï : ‚àÉ ùíë 1 , ‚Ä¶ , ùíë n ‚àà K ‚Äã s.t. ‚Äã K ‚äÜ ‚ãÉ i = 1 n B œµ ‚Äã ( ùíë i ) } , \\mathcal{N}_{\\epsilon}(K)\\doteq\\min\\left\\{n\\in\\mathbb{N}\\colon\\exists\\bm{p}_{1},\\dots,\\bm{p}_{n}\\in K\\ \\text{", "snippet": "ùí© œµ ‚Äã ( K ) ‚âê min ‚Å° { n ‚àà ‚Ñï : ‚àÉ ùíë 1 , ‚Ä¶ , ùíë n ‚àà K ‚Äã s.t. ‚Äã K ‚äÜ ‚ãÉ i = 1 n B œµ ‚Äã ( ùíë i ) } , \\mathcal{N}_{\\epsilon}(K)\\doteq\\min\\left\\{n\\in\\mathbb{N}\\colon\\exists\\bm{p}_{1},\\dots,\\bm{p}_{n}\\in K\\ \\text{s.t.}\\ K\\subseteq\\bigcup_{i=1}^{n}B_{\\epsilon}(\\bm{p}_{i})\\right\\}, caligraphic_N start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( italic_K ) ‚âê roman_min { italic_n ‚àà blackboard_N : ‚àÉ bold_italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_p start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ‚àà italic_K s.t. italic_K ‚äÜ ‚ãÉ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPER"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E8", "title": "‚Ñõ œµ ‚Äã ( ùíô ) ‚â§ log 2 ‚Å° ùí© œµ ‚Äã ( K ) . \\mathcal{R}_{\\epsilon}(\\bm{x})\\leq\\log_{2}\\mathcal{N}_{\\epsilon}(K). caligraphic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_x ) ‚â§ roman_log star", "snippet": "‚Ñõ œµ ‚Äã ( ùíô ) ‚â§ log 2 ‚Å° ùí© œµ ‚Äã ( K ) . \\mathcal{R}_{\\epsilon}(\\bm{x})\\leq\\log_{2}\\mathcal{N}_{\\epsilon}(K). caligraphic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_x ) ‚â§ roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT caligraphic_N start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( italic_K ) . (3.3.8)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E9", "title": "‚Ñõ œµ ‚Äã ( ùíô ) ‚â• log 2 ‚Å° ùí© œµ ‚Äã ( K ) ‚àí O ‚Äã ( D ) . \\mathcal{R}_{\\epsilon}(\\bm{x})\\geq\\log_{2}\\mathcal{N}_{\\epsilon}(K)-O(D). caligraphic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_x )", "snippet": "‚Ñõ œµ ‚Äã ( ùíô ) ‚â• log 2 ‚Å° ùí© œµ ‚Äã ( K ) ‚àí O ‚Äã ( D ) . \\mathcal{R}_{\\epsilon}(\\bm{x})\\geq\\log_{2}\\mathcal{N}_{\\epsilon}(K)-O(D). caligraphic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_x ) ‚â• roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT caligraphic_N start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( italic_K ) - italic_O ( italic_D ) . (3.3.9)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E10", "title": "‚Ñõ œµ ‚Äã ( ùíô ) ‚â• h ‚Äã ( ùíô ) ‚àí log 2 ‚Å° vol ‚Å° ( B œµ ) ‚àí C D , \\mathcal{R}_{\\epsilon}(\\bm{x})\\geq h(\\bm{x})-\\log_{2}\\operatorname{vol}(B_{\\epsilon})-C_{D}, caligraphic_R start_POSTSUBSCRIPT italic_œµ end_POST", "snippet": "‚Ñõ œµ ‚Äã ( ùíô ) ‚â• h ‚Äã ( ùíô ) ‚àí log 2 ‚Å° vol ‚Å° ( B œµ ) ‚àí C D , \\mathcal{R}_{\\epsilon}(\\bm{x})\\geq h(\\bm{x})-\\log_{2}\\operatorname{vol}(B_{\\epsilon})-C_{D}, caligraphic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_x ) ‚â• italic_h ( bold_italic_x ) - roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT roman_vol ( italic_B start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ) - italic_C start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT , (3.3.10)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E11", "title": "lim œµ ‚Üí 0 ‚Ñõ œµ ‚Äã ( ùíô ) ‚àí [ h ‚Äã ( ùíô ) ‚àí log 2 ‚Å° vol ‚Å° ( B œµ ) ‚àí C D ] = 0 . \\lim_{\\epsilon\\to 0}\\mathcal{R}_{\\epsilon}(\\bm{x})-\\left[h(\\bm{x})-\\log_{2}\\operatorname{vol}(B_{\\epsilon})-C_{D}\\right]=0. ro", "snippet": "lim œµ ‚Üí 0 ‚Ñõ œµ ‚Äã ( ùíô ) ‚àí [ h ‚Äã ( ùíô ) ‚àí log 2 ‚Å° vol ‚Å° ( B œµ ) ‚àí C D ] = 0 . \\lim_{\\epsilon\\to 0}\\mathcal{R}_{\\epsilon}(\\bm{x})-\\left[h(\\bm{x})-\\log_{2}\\operatorname{vol}(B_{\\epsilon})-C_{D}\\right]=0. roman_lim start_POSTSUBSCRIPT italic_œµ ‚Üí 0 end_POSTSUBSCRIPT caligraphic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_x ) - [ italic_h ( bold_italic_x ) - roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT roman_vol ( italic_B start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ) - italic_C start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT ] = 0 . (3.3.11)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E16", "title": "ùíô i ‚Ü¶ ùíô ^ i , subject to ‚Äñ ùíô i ‚àí ùíô ^ i ‚Äñ 2 ‚â§ œµ . \\bm{x}_{i}\\mapsto\\hat{\\bm{x}}_{i},\\quad\\mbox{subject to}\\quad\\|\\bm{x}_{i}-\\hat{\\bm{x}}_{i}\\|_{2}\\leq\\epsilon. bold_italic_x start_POSTSUBSCRIPT italic_", "snippet": "ùíô i ‚Ü¶ ùíô ^ i , subject to ‚Äñ ùíô i ‚àí ùíô ^ i ‚Äñ 2 ‚â§ œµ . \\bm{x}_{i}\\mapsto\\hat{\\bm{x}}_{i},\\quad\\mbox{subject to}\\quad\\|\\bm{x}_{i}-\\hat{\\bm{x}}_{i}\\|_{2}\\leq\\epsilon. bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ‚Ü¶ over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , subject to ‚à• bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ‚â§ italic_œµ . (3.3.16)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E17", "title": "ùö∫ = 1 N ‚Äã ùëø ‚Äã ùëø ‚ä§ . {\\bm{\\Sigma}}=\\frac{1}{N}\\bm{X}\\bm{X}^{\\top}. bold_Œ£ = divide start_ARG 1 end_ARG start_ARG italic_N end_ARG bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT", "snippet": "ùö∫ = 1 N ‚Äã ùëø ‚Äã ùëø ‚ä§ . {\\bm{\\Sigma}}=\\frac{1}{N}\\bm{X}\\bm{X}^{\\top}. bold_Œ£ = divide start_ARG 1 end_ARG start_ARG italic_N end_ARG bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT . (3.3.17)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E18", "title": "ùíô ^ i = ùíô i + ùíò i , \\hat{\\bm{x}}_{i}=\\bm{x}_{i}+\\bm{w}_{i}, over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSU", "snippet": "ùíô ^ i = ùíô i + ùíò i , \\hat{\\bm{x}}_{i}=\\bm{x}_{i}+\\bm{w}_{i}, over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + bold_italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , (3.3.18)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E19", "title": "ùö∫ ^ = ùîº ‚Äã [ ùíô ^ i ‚Äã ùíô ^ i ‚ä§ ] = œµ 2 D ‚Äã ùë∞ + 1 N ‚Äã ùëø ‚Äã ùëø ‚ä§ . \\hat{\\bm{\\Sigma}}=\\mathbb{E}\\left[\\hat{\\bm{x}}_{i}\\hat{\\bm{x}}_{i}^{\\top}\\right]=\\frac{\\epsilon^{2}}{D}\\bm{I}+\\frac{1}{N}\\bm{X}\\bm{X}^{\\top}", "snippet": "ùö∫ ^ = ùîº ‚Äã [ ùíô ^ i ‚Äã ùíô ^ i ‚ä§ ] = œµ 2 D ‚Äã ùë∞ + 1 N ‚Äã ùëø ‚Äã ùëø ‚ä§ . \\hat{\\bm{\\Sigma}}=\\mathbb{E}\\left[\\hat{\\bm{x}}_{i}\\hat{\\bm{x}}_{i}^{\\top}\\right]=\\frac{\\epsilon^{2}}{D}\\bm{I}+\\frac{1}{N}\\bm{X}\\bm{X}^{\\top}. over^ start_ARG bold_Œ£ end_ARG = blackboard_E [ over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ] = divide start_ARG italic_œµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_D end_ARG bold_italic_I + divide star"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E20", "title": "volume ‚Äã ( ùíô ^ i ) ‚àù det ( ùö∫ ^ ) = det ( œµ 2 D ‚Äã ùë∞ + 1 N ‚Äã ùëø ‚Äã ùëø ‚ä§ ) . \\mbox{volume}(\\hat{\\bm{x}}_{i})\\propto\\sqrt{\\det\\big{(}\\hat{\\bm{\\Sigma}}\\big{)}}=\\sqrt{\\det\\left(\\frac{\\epsilon^{2}}{D}\\bm{I}+\\fr", "snippet": "volume ‚Äã ( ùíô ^ i ) ‚àù det ( ùö∫ ^ ) = det ( œµ 2 D ‚Äã ùë∞ + 1 N ‚Äã ùëø ‚Äã ùëø ‚ä§ ) . \\mbox{volume}(\\hat{\\bm{x}}_{i})\\propto\\sqrt{\\det\\big{(}\\hat{\\bm{\\Sigma}}\\big{)}}=\\sqrt{\\det\\left(\\frac{\\epsilon^{2}}{D}\\bm{I}+\\frac{1}{N}\\bm{X}\\bm{X}^{\\top}\\right)}. volume ( over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ‚àù square-root start_ARG roman_det ( over^ start_ARG bold_Œ£ end_ARG ) end_ARG = square-root start_ARG roman_det ( divide start_ARG italic_œµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_D end_ARG bold_italic_I + divide start_ARG 1 end_ARG start_"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E21", "title": "volume ‚Äã ( ùíò i ) ‚àù det ( œµ 2 D ‚Äã ùë∞ ) . \\mbox{volume}(\\bm{w}_{i})\\propto\\sqrt{\\det\\left(\\frac{\\epsilon^{2}}{D}\\bm{I}\\right)}. volume ( bold_italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ‚àù s", "snippet": "volume ‚Äã ( ùíò i ) ‚àù det ( œµ 2 D ‚Äã ùë∞ ) . \\mbox{volume}(\\bm{w}_{i})\\propto\\sqrt{\\det\\left(\\frac{\\epsilon^{2}}{D}\\bm{I}\\right)}. volume ( bold_italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ‚àù square-root start_ARG roman_det ( divide start_ARG italic_œµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_D end_ARG bold_italic_I ) end_ARG . (3.3.21)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E22", "title": "# ‚Äã œµ ‚Äã -balls ‚âà volume ‚Äã ( ùíô ^ i ) volume ‚Äã ( ùíò i ) = det ( ùë∞ + D N ‚Äã œµ 2 ‚Äã ùëø ‚Äã ùëø ‚ä§ ) . \\#\\,\\epsilon\\mbox{-balls}\\approx\\frac{\\mbox{volume}(\\hat{\\bm{x}}_{i})}{\\mbox{volume}(\\bm{w}_{i})}=\\sqrt{\\det\\le", "snippet": "# ‚Äã œµ ‚Äã -balls ‚âà volume ‚Äã ( ùíô ^ i ) volume ‚Äã ( ùíò i ) = det ( ùë∞ + D N ‚Äã œµ 2 ‚Äã ùëø ‚Äã ùëø ‚ä§ ) . \\#\\,\\epsilon\\mbox{-balls}\\approx\\frac{\\mbox{volume}(\\hat{\\bm{x}}_{i})}{\\mbox{volume}(\\bm{w}_{i})}=\\sqrt{\\det\\left(\\bm{I}+\\frac{D}{N\\epsilon^{2}}\\bm{X}\\bm{X}^{\\top}\\right)}. # italic_œµ -balls ‚âà divide start_ARG volume ( over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG volume ( bold_italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG = square-root start_ARG roman_det ( bold_italic_I + divide start_ARG italic_D end_ARG start_ARG italic_N it"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E23", "title": "‚Ñõ œµ ‚Äã ( ùëø ) ‚âà log 2 ‚Å° ( # ‚Äã œµ ‚Äã -balls ) ‚âà R œµ ‚Äã ( ùëø ) ‚âê 1 2 ‚Äã log ‚Äã det ( ùë∞ + D N ‚Äã œµ 2 ‚Äã ùëø ‚Äã ùëø ‚ä§ ) . \\mathcal{R}_{\\epsilon}(\\bm{X})\\approx\\log_{2}(\\#\\,\\epsilon\\mbox{-balls})\\approx R_{\\epsilon}(\\bm{", "snippet": "‚Ñõ œµ ‚Äã ( ùëø ) ‚âà log 2 ‚Å° ( # ‚Äã œµ ‚Äã -balls ) ‚âà R œµ ‚Äã ( ùëø ) ‚âê 1 2 ‚Äã log ‚Äã det ( ùë∞ + D N ‚Äã œµ 2 ‚Äã ùëø ‚Äã ùëø ‚ä§ ) . \\mathcal{R}_{\\epsilon}(\\bm{X})\\approx\\log_{2}(\\#\\,\\epsilon\\mbox{-balls})\\approx R_{\\epsilon}(\\bm{X})\\doteq\\frac{1}{2}\\log\\det\\left(\\bm{I}+\\frac{D}{N\\epsilon^{2}}\\bm{X}\\bm{X}^{\\top}\\right). caligraphic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_X ) ‚âà roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( # italic_œµ -balls ) ‚âà italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_X ) ‚âê divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_log roman_det ( bold_ita"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E24", "title": "ùíô ‚ü∂ ball œµ ‚Å° ( ùíô ) ‚ü∂ ùíô ^ = center of ‚Äã ball œµ ‚Å° ( ùíô ) . \\bm{x}\\longrightarrow\\operatorname{ball}_{\\epsilon}(\\bm{x})\\longrightarrow\\hat{\\bm{x}}=\\mbox{center of}\\operatorname{ball}_{\\epsilon}(\\bm{x}). b", "snippet": "ùíô ‚ü∂ ball œµ ‚Å° ( ùíô ) ‚ü∂ ùíô ^ = center of ‚Äã ball œµ ‚Å° ( ùíô ) . \\bm{x}\\longrightarrow\\operatorname{ball}_{\\epsilon}(\\bm{x})\\longrightarrow\\hat{\\bm{x}}=\\mbox{center of}\\operatorname{ball}_{\\epsilon}(\\bm{x}). bold_italic_x ‚ü∂ roman_ball start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_x ) ‚ü∂ over^ start_ARG bold_italic_x end_ARG = center of roman_ball start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_x ) . (3.3.24)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E25", "title": "ùëø = [ ùíô 1 , ùíô 2 , ‚Ä¶ , ùíô N ] , \\bm{X}=\\left[\\bm{x}_{1},\\bm{x}_{2},\\ldots,\\bm{x}_{N}\\right], bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_italic_x start_POSTSUBSCRIPT 2", "snippet": "ùëø = [ ùíô 1 , ùíô 2 , ‚Ä¶ , ùíô N ] , \\bm{X}=\\left[\\bm{x}_{1},\\bm{x}_{2},\\ldots,\\bm{x}_{N}\\right], bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ] , (3.3.25)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E26", "title": "ùëø ‚Äã ùö∑ = [ ùëø 1 , ùëø 2 , ‚Ä¶ , ùëø K ] , \\bm{X}\\bm{\\Pi}=[\\bm{X}_{1},\\bm{X}_{2},\\dots,\\bm{X}_{K}], bold_italic_X bold_Œ† = [ bold_italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_italic_X start_POSTSUBS", "snippet": "ùëø ‚Äã ùö∑ = [ ùëø 1 , ùëø 2 , ‚Ä¶ , ùëø K ] , \\bm{X}\\bm{\\Pi}=[\\bm{X}_{1},\\bm{X}_{2},\\dots,\\bm{X}_{K}], bold_italic_X bold_Œ† = [ bold_italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_italic_X start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_X start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT ] , (3.3.26)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E27", "title": "‚Ñõ œµ ‚Äã ( ùëø ) ‚âà R œµ ‚Äã ( ùëø ) = 1 2 ‚Äã log ‚Äã det ( ùë∞ + D N ‚Äã œµ 2 ‚Äã ùëø ‚Äã ùëø ‚ä§ ) . \\mathcal{R}_{\\epsilon}(\\bm{X})\\approx R_{\\epsilon}(\\bm{X})=\\frac{1}{2}\\log\\det\\left(\\bm{I}+\\frac{D}{N\\epsilon^{2}}\\bm{X}\\bm{X}", "snippet": "‚Ñõ œµ ‚Äã ( ùëø ) ‚âà R œµ ‚Äã ( ùëø ) = 1 2 ‚Äã log ‚Äã det ( ùë∞ + D N ‚Äã œµ 2 ‚Äã ùëø ‚Äã ùëø ‚ä§ ) . \\mathcal{R}_{\\epsilon}(\\bm{X})\\approx R_{\\epsilon}(\\bm{X})=\\frac{1}{2}\\log\\det\\left(\\bm{I}+\\frac{D}{N\\epsilon^{2}}\\bm{X}\\bm{X}^{\\top}\\right). caligraphic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_X ) ‚âà italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_X ) = divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_log roman_det ( bold_italic_I + divide start_ARG italic_D end_ARG start_ARG italic_N italic_œµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG bold_italic_X bold_italic_X "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E28", "title": "‚Ñõ 0 ‚Äã ( ùëø ) = log ‚Å° ( N ) . \\mathcal{R}_{0}(\\bm{X})=\\log(N). caligraphic_R start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( bold_italic_X ) = roman_log ( italic_N ) . (3.3.28)", "snippet": "‚Ñõ 0 ‚Äã ( ùëø ) = log ‚Å° ( N ) . \\mathcal{R}_{0}(\\bm{X})=\\log(N). caligraphic_R start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( bold_italic_X ) = roman_log ( italic_N ) . (3.3.28)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E29", "title": "R œµ c ‚Äã ( ùëø ‚à£ ùö∑ ) = N 1 N ‚Äã R œµ ‚Äã ( ùëø 1 ) + N 2 N ‚Äã R œµ ‚Äã ( ùëø 2 ) , R_{\\epsilon}^{c}(\\bm{X}\\mid\\bm{\\Pi})=\\frac{N_{1}}{N}R_{\\epsilon}(\\bm{X}_{1})+\\frac{N_{2}}{N}R_{\\epsilon}(\\bm{X}_{2}), italic_R start", "snippet": "R œµ c ‚Äã ( ùëø ‚à£ ùö∑ ) = N 1 N ‚Äã R œµ ‚Äã ( ùëø 1 ) + N 2 N ‚Äã R œµ ‚Äã ( ùëø 2 ) , R_{\\epsilon}^{c}(\\bm{X}\\mid\\bm{\\Pi})=\\frac{N_{1}}{N}R_{\\epsilon}(\\bm{X}_{1})+\\frac{N_{2}}{N}R_{\\epsilon}(\\bm{X}_{2}), italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_X ‚à£ bold_Œ† ) = divide start_ARG italic_N start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG start_ARG italic_N end_ARG italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) + divide start_ARG italic_N start_POSTSUBSCRIPT 2 end_POSTSUBSC"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E30", "title": "R œµ c ‚Äã ( ùëø ‚à£ ùö∑ ) ‚â™ R œµ ‚Äã ( ùëø ) , R œµ c ‚Äã ( ùëø ‚à£ ùö∑ ) ‚â™ R 0 ‚Äã ( ùëø ) . R_{\\epsilon}^{c}(\\bm{X}\\mid\\bm{\\Pi})\\ll R_{\\epsilon}(\\bm{X}),\\quad R_{\\epsilon}^{c}(\\bm{X}\\mid\\bm{\\Pi})\\ll R_{0}(\\bm{X}). italic_R s", "snippet": "R œµ c ‚Äã ( ùëø ‚à£ ùö∑ ) ‚â™ R œµ ‚Äã ( ùëø ) , R œµ c ‚Äã ( ùëø ‚à£ ùö∑ ) ‚â™ R 0 ‚Äã ( ùëø ) . R_{\\epsilon}^{c}(\\bm{X}\\mid\\bm{\\Pi})\\ll R_{\\epsilon}(\\bm{X}),\\quad R_{\\epsilon}^{c}(\\bm{X}\\mid\\bm{\\Pi})\\ll R_{0}(\\bm{X}). italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_X ‚à£ bold_Œ† ) ‚â™ italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_X ) , italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_X ‚à£ bold_Œ† ) ‚â™ italic_R start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( bold_italic"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E31", "title": "min ùö∑ ‚Å° { R œµ c ‚Äã ( ùëø ‚à£ ùö∑ ) ‚âê ‚àë k = 1 K N k N ‚Äã R œµ ‚Äã ( ùëø k ) } . \\min_{\\bm{\\Pi}}\\left\\{R_{\\epsilon}^{c}(\\bm{X}\\mid\\bm{\\Pi})\\doteq\\sum_{k=1}^{K}\\frac{N_{k}}{N}R_{\\epsilon}(\\bm{X}_{k})\\right\\}. roman_m", "snippet": "min ùö∑ ‚Å° { R œµ c ‚Äã ( ùëø ‚à£ ùö∑ ) ‚âê ‚àë k = 1 K N k N ‚Äã R œµ ‚Äã ( ùëø k ) } . \\min_{\\bm{\\Pi}}\\left\\{R_{\\epsilon}^{c}(\\bm{X}\\mid\\bm{\\Pi})\\doteq\\sum_{k=1}^{K}\\frac{N_{k}}{N}R_{\\epsilon}(\\bm{X}_{k})\\right\\}. roman_min start_POSTSUBSCRIPT bold_Œ† end_POSTSUBSCRIPT { italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_X ‚à£ bold_Œ† ) ‚âê ‚àë start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT divide start_ARG italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG start_ARG italic_N end_ARG it"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E32", "title": "L ‚Äã ( ùëø k ) = N k ‚Äã R œµ ‚Äã ( ùëø k ) . L(\\bm{X}_{k})=N_{k}R_{\\epsilon}(\\bm{X}_{k}). italic_L ( bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) = italic_N start_POSTSUBSCRIPT italic_k end_P", "snippet": "L ‚Äã ( ùëø k ) = N k ‚Äã R œµ ‚Äã ( ùëø k ) . L(\\bm{X}_{k})=N_{k}R_{\\epsilon}(\\bm{X}_{k}). italic_L ( bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) = italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) . (3.3.32)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.Ex1", "title": "L c ‚Äã ( ùëø k , ùëø l ) = N k ‚Äã R œµ ‚Äã ( ùëø k ) + N l ‚Äã R œµ ‚Äã ( ùëø l ) ‚àí N k ‚Äã log ‚Å° N k N k + N l ‚àí N l ‚Äã log ‚Å° N l N k + N l . L^{c}(\\bm{X}_{k},\\bm{X}_{l})=N_{k}R_{\\epsilon}(\\bm{X}_{k})+N_{l}R_{\\epsilon}(\\", "snippet": "L c ‚Äã ( ùëø k , ùëø l ) = N k ‚Äã R œµ ‚Äã ( ùëø k ) + N l ‚Äã R œµ ‚Äã ( ùëø l ) ‚àí N k ‚Äã log ‚Å° N k N k + N l ‚àí N l ‚Äã log ‚Å° N l N k + N l . L^{c}(\\bm{X}_{k},\\bm{X}_{l})=N_{k}R_{\\epsilon}(\\bm{X}_{k})+N_{l}R_{\\epsilon}(\\bm{X}_{l})-N_{k}\\log\\frac{N_{k}}{N_{k}+N_{l}}-N_{l}\\log\\frac{N_{l}}{N_{k}+N_{l}}. italic_L start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , bold_italic_X start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ) = italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_X "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S3.E33", "title": "L ‚Äã ( ùëø k ‚à™ ùëø l ) ‚àí L c ‚Äã ( ùëø k , ùëø l ) L(\\bm{X}_{k}\\cup\\bm{X}_{l})-L^{c}(\\bm{X}_{k},\\bm{X}_{l}) italic_L ( bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ‚à™ bold_italic_X start_POSTSUBSC", "snippet": "L ‚Äã ( ùëø k ‚à™ ùëø l ) ‚àí L c ‚Äã ( ùëø k , ùëø l ) L(\\bm{X}_{k}\\cup\\bm{X}_{l})-L^{c}(\\bm{X}_{k},\\bm{X}_{l}) italic_L ( bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ‚à™ bold_italic_X start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ) - italic_L start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , bold_italic_X start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ) (3.3.33)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.E1", "title": "ùíô ‚àà ‚Ñù D ‚Üí f ‚Äã ( ùíô ) ùíõ ‚àà ‚Ñù d , \\bm{x}\\in\\mathbb{R}^{D}\\xrightarrow{\\hskip 5.69054ptf(\\bm{x})\\hskip 5.69054pt}\\bm{z}\\in\\mathbb{R}^{d}, bold_italic_x ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D end_POS", "snippet": "ùíô ‚àà ‚Ñù D ‚Üí f ‚Äã ( ùíô ) ùíõ ‚àà ‚Ñù d , \\bm{x}\\in\\mathbb{R}^{D}\\xrightarrow{\\hskip 5.69054ptf(\\bm{x})\\hskip 5.69054pt}\\bm{z}\\in\\mathbb{R}^{d}, bold_italic_x ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT start_ARROW start_OVERACCENT italic_f ( bold_italic_x ) end_OVERACCENT ‚Üí end_ARROW bold_italic_z ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , (3.4.1)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.Ex1", "title": "f ‚Äã ( ùíô , Œ∏ ) : ùíô ‚Ü¶ ùíö f(\\bm{x},\\theta):\\bm{x}\\mapsto\\bm{y} italic_f ( bold_italic_x , italic_Œ∏ ) : bold_italic_x ‚Ü¶ bold_italic_y", "snippet": "f ‚Äã ( ùíô , Œ∏ ) : ùíô ‚Ü¶ ùíö f(\\bm{x},\\theta):\\bm{x}\\mapsto\\bm{y} italic_f ( bold_italic_x , italic_Œ∏ ) : bold_italic_x ‚Ü¶ bold_italic_y"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.E2", "title": "min Œ∏ ‚àà Œò ‚àí ùîº ‚Äã [ ‚ü® ùíö , log ‚Å° ( f ‚Äã ( ùíô , Œ∏ ) ) ‚ü© ] ‚âà ‚àí 1 N ‚Äã ‚àë i = 1 N ‚ü® ùíö i , log ‚Å° ( f ‚Äã ( ùíô i , Œ∏ ) ) ‚ü© . \\min_{\\theta\\in\\Theta}\\;-\\mathbb{E}[\\langle\\bm{y},\\log(f(\\bm{x},\\theta))\\rangle]\\,\\approx-", "snippet": "min Œ∏ ‚àà Œò ‚àí ùîº ‚Äã [ ‚ü® ùíö , log ‚Å° ( f ‚Äã ( ùíô , Œ∏ ) ) ‚ü© ] ‚âà ‚àí 1 N ‚Äã ‚àë i = 1 N ‚ü® ùíö i , log ‚Å° ( f ‚Äã ( ùíô i , Œ∏ ) ) ‚ü© . \\min_{\\theta\\in\\Theta}\\;-\\mathbb{E}[\\langle\\bm{y},\\log(f(\\bm{x},\\theta))\\rangle]\\,\\approx-\\frac{1}{N}\\sum_{i=1}^{N}\\langle\\bm{y}_{i},\\log\\left(f(\\bm{x}_{i},\\theta)\\right)\\rangle. roman_min start_POSTSUBSCRIPT italic_Œ∏ ‚àà roman_Œò end_POSTSUBSCRIPT - blackboard_E [ ‚ü® bold_italic_y , roman_log ( italic_f ( bold_italic_x , italic_Œ∏ ) ) ‚ü© ] ‚âà - divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUP"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.E3", "title": "ùíô ‚Üí f ‚Äã ( ùíô , Œ∏ ) ùíõ ‚Üí g ‚Äã ( ùíõ ) ùíö . \\bm{x}\\xrightarrow{\\hskip 5.69054ptf(\\bm{x},\\theta)\\hskip 5.69054pt}\\bm{z}\\xrightarrow{\\hskip 5.69054ptg(\\bm{z})\\hskip 5.69054pt}\\bm{y}. bold_italic_x start_ARROW s", "snippet": "ùíô ‚Üí f ‚Äã ( ùíô , Œ∏ ) ùíõ ‚Üí g ‚Äã ( ùíõ ) ùíö . \\bm{x}\\xrightarrow{\\hskip 5.69054ptf(\\bm{x},\\theta)\\hskip 5.69054pt}\\bm{z}\\xrightarrow{\\hskip 5.69054ptg(\\bm{z})\\hskip 5.69054pt}\\bm{y}. bold_italic_x start_ARROW start_OVERACCENT italic_f ( bold_italic_x , italic_Œ∏ ) end_OVERACCENT ‚Üí end_ARROW bold_italic_z start_ARROW start_OVERACCENT italic_g ( bold_italic_z ) end_OVERACCENT ‚Üí end_ARROW bold_italic_y . (3.4.3)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.E4", "title": "I ‚Äã ( ùíô ; ùíõ ) = H ‚Äã ( ùíô ) ‚àí H ‚Äã ( ùíô ‚à£ ùíõ ) , I(\\bm{x};\\bm{z})=H(\\bm{x})-H(\\bm{x}\\mid\\bm{z}), italic_I ( bold_italic_x ; bold_italic_z ) = italic_H ( bold_italic_x ) - italic_H ( bold_italic_x ‚à£ bold_it", "snippet": "I ‚Äã ( ùíô ; ùíõ ) = H ‚Äã ( ùíô ) ‚àí H ‚Äã ( ùíô ‚à£ ùíõ ) , I(\\bm{x};\\bm{z})=H(\\bm{x})-H(\\bm{x}\\mid\\bm{z}), italic_I ( bold_italic_x ; bold_italic_z ) = italic_H ( bold_italic_x ) - italic_H ( bold_italic_x ‚à£ bold_italic_z ) , (3.4.4)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.E5", "title": "max Œ∏ ‚àà Œò ‚Å° IB ‚Äã ( ùíô , ùíö , ùíõ ) ‚âê I ‚Äã ( ùíõ ; ùíö ) ‚àí Œ≤ ‚Äã I ‚Äã ( ùíô ; ùíõ ) s . t . ùíõ = f ‚Äã ( ùíô , Œ∏ ) , \\max_{\\theta\\in\\Theta}\\;\\mbox{IB}(\\bm{x},\\bm{y},\\bm{z})\\doteq I(\\bm{z};\\bm{y})-\\beta I(\\bm{x};\\bm{z})\\qua", "snippet": "max Œ∏ ‚àà Œò ‚Å° IB ‚Äã ( ùíô , ùíö , ùíõ ) ‚âê I ‚Äã ( ùíõ ; ùíö ) ‚àí Œ≤ ‚Äã I ‚Äã ( ùíô ; ùíõ ) s . t . ùíõ = f ‚Äã ( ùíô , Œ∏ ) , \\max_{\\theta\\in\\Theta}\\;\\mbox{IB}(\\bm{x},\\bm{y},\\bm{z})\\doteq I(\\bm{z};\\bm{y})-\\beta I(\\bm{x};\\bm{z})\\quad\\ \\mathrm{s.t.}\\ \\bm{z}=f(\\bm{x},\\theta), roman_max start_POSTSUBSCRIPT italic_Œ∏ ‚àà roman_Œò end_POSTSUBSCRIPT IB ( bold_italic_x , bold_italic_y , bold_italic_z ) ‚âê italic_I ( bold_italic_z ; bold_italic_y ) - italic_Œ≤ italic_I ( bold_italic_x ; bold_italic_z ) roman_s . roman_t . bold_italic_z = italic_f ( bold_italic_x , italic_Œ∏ ) , (3.4.5)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.E6", "title": "ùíô ‚Üí f ‚Äã ( ùíô ) ùíõ , \\bm{x}\\xrightarrow{\\hskip 5.69054ptf(\\bm{x})\\hskip 5.69054pt}\\bm{z}, bold_italic_x start_ARROW start_OVERACCENT italic_f ( bold_italic_x ) end_OVERACCENT ‚Üí end_ARROW bold_italic_z , ", "snippet": "ùíô ‚Üí f ‚Äã ( ùíô ) ùíõ , \\bm{x}\\xrightarrow{\\hskip 5.69054ptf(\\bm{x})\\hskip 5.69054pt}\\bm{z}, bold_italic_x start_ARROW start_OVERACCENT italic_f ( bold_italic_x ) end_OVERACCENT ‚Üí end_ARROW bold_italic_z , (3.4.6)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.E7", "title": "R œµ ‚Äã ( ùíÅ ) = 1 2 ‚Äã log ‚Äã det ( ùë∞ + d N ‚Äã œµ 2 ‚Äã ùíÅ ‚Äã ùíÅ ‚ä§ ) . R_{\\epsilon}(\\bm{Z})=\\frac{1}{2}\\log\\det\\left(\\bm{I}+\\frac{d}{N\\epsilon^{2}}\\bm{Z}\\bm{Z}^{\\top}\\right). italic_R start_POSTSUBSCRIPT italic_", "snippet": "R œµ ‚Äã ( ùíÅ ) = 1 2 ‚Äã log ‚Äã det ( ùë∞ + d N ‚Äã œµ 2 ‚Äã ùíÅ ‚Äã ùíÅ ‚ä§ ) . R_{\\epsilon}(\\bm{Z})=\\frac{1}{2}\\log\\det\\left(\\bm{I}+\\frac{d}{N\\epsilon^{2}}\\bm{Z}\\bm{Z}^{\\top}\\right). italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ) = divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_log roman_det ( bold_italic_I + divide start_ARG italic_d end_ARG start_ARG italic_N italic_œµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG bold_italic_Z bold_italic_Z start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ) . (3.4.7)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.E9", "title": "R œµ c ‚Äã ( ùíÅ ) ‚âê ‚àë k = 1 K R œµ ‚Äã ( ùíÅ k ) , R_{\\epsilon}^{c}(\\bm{Z})\\doteq\\sum_{k=1}^{K}R_{\\epsilon}(\\bm{Z}_{k}), italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c e", "snippet": "R œµ c ‚Äã ( ùíÅ ) ‚âê ‚àë k = 1 K R œµ ‚Äã ( ùíÅ k ) , R_{\\epsilon}^{c}(\\bm{Z})\\doteq\\sum_{k=1}^{K}R_{\\epsilon}(\\bm{Z}_{k}), italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_Z ) ‚âê ‚àë start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) , (3.4.9)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.E10", "title": "Œî ‚Äã R œµ ‚Äã ( ùíÅ ) ‚âê R œµ ‚Äã ( ùíÅ ) ‚àí R œµ c ‚Äã ( ùíÅ ) . \\Delta R_{\\epsilon}(\\bm{Z})\\doteq R_{\\epsilon}(\\bm{Z})-R_{\\epsilon}^{c}(\\bm{Z}). roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_", "snippet": "Œî ‚Äã R œµ ‚Äã ( ùíÅ ) ‚âê R œµ ‚Äã ( ùíÅ ) ‚àí R œµ c ‚Äã ( ùíÅ ) . \\Delta R_{\\epsilon}(\\bm{Z})\\doteq R_{\\epsilon}(\\bm{Z})-R_{\\epsilon}^{c}(\\bm{Z}). roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ) ‚âê italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ) - italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_Z ) . (3.4.10)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.E11", "title": "ùëø ‚Üí f ‚Äã ( ùíô , Œ∏ ) ùíÅ ‚Üí œµ Œî ‚Äã R œµ ‚Äã ( ùíÅ ) . \\bm{X}\\xrightarrow{\\hskip 5.69054ptf(\\bm{x},\\theta)\\hskip 5.69054pt}\\bm{Z}\\xrightarrow{\\hskip 5.69054pt\\epsilon\\hskip 5.69054pt}\\Delta R_{\\epsilon}(\\bm{Z}). b", "snippet": "ùëø ‚Üí f ‚Äã ( ùíô , Œ∏ ) ùíÅ ‚Üí œµ Œî ‚Äã R œµ ‚Äã ( ùíÅ ) . \\bm{X}\\xrightarrow{\\hskip 5.69054ptf(\\bm{x},\\theta)\\hskip 5.69054pt}\\bm{Z}\\xrightarrow{\\hskip 5.69054pt\\epsilon\\hskip 5.69054pt}\\Delta R_{\\epsilon}(\\bm{Z}). bold_italic_X start_ARROW start_OVERACCENT italic_f ( bold_italic_x , italic_Œ∏ ) end_OVERACCENT ‚Üí end_ARROW bold_italic_Z start_ARROW start_OVERACCENT italic_œµ end_OVERACCENT ‚Üí end_ARROW roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ) . (3.4.11)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.E12X", "title": "max Œ∏ \\displaystyle\\max_{\\theta} roman_max start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT Œî ‚Äã R œµ ‚Äã ( ùíÅ ) ‚âê R œµ ‚Äã ( ùíÅ ) ‚àí R œµ c ‚Äã ( ùíÅ ) , \\displaystyle\\;\\Delta R_{\\epsilon}\\big{(}\\bm{Z}\\big{)}\\doteq R", "snippet": "max Œ∏ \\displaystyle\\max_{\\theta} roman_max start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT Œî ‚Äã R œµ ‚Äã ( ùíÅ ) ‚âê R œµ ‚Äã ( ùíÅ ) ‚àí R œµ c ‚Äã ( ùíÅ ) , \\displaystyle\\;\\Delta R_{\\epsilon}\\big{(}\\bm{Z}\\big{)}\\doteq R_{\\epsilon}(\\bm{Z})-R_{\\epsilon}^{c}(\\bm{Z}), roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ) ‚âê italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ) - italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_Z ) , (3.4.12)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.E12Xa", "title": "s.t. ùíÅ = f ‚Äã ( ùëø , Œ∏ ) , ‚Äñ ùíÅ k ‚Äñ F 2 = N k , k = 1 , ‚Ä¶ , K . \\displaystyle\\ \\ \\,\\bm{Z}=f(\\bm{X},\\theta),\\ \\|\\bm{Z}_{k}\\|_{F}^{2}=N_{k},\\ k=1,\\dots,K. bold_italic_Z = italic_f ( bold_italic_X , italic_", "snippet": "s.t. ùíÅ = f ‚Äã ( ùëø , Œ∏ ) , ‚Äñ ùíÅ k ‚Äñ F 2 = N k , k = 1 , ‚Ä¶ , K . \\displaystyle\\ \\ \\,\\bm{Z}=f(\\bm{X},\\theta),\\ \\|\\bm{Z}_{k}\\|_{F}^{2}=N_{k},\\ k=1,\\dots,K. bold_italic_Z = italic_f ( bold_italic_X , italic_Œ∏ ) , ‚à• bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ‚à• start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , italic_k = 1 , ‚Ä¶ , italic_K ."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S6.Ex1", "title": "[ ùíô ùíö ] ‚àº ùí© ‚Äã ( [ ùùÅ ùíô ùùÅ ùíö ] , [ ùö∫ ùíô ùö∫ ùíô ‚Äã ùíö ùö∫ ùíô ‚Äã ùíö ‚ä§ ùö∫ ùíö ] ) , \\begin{bmatrix}\\bm{x}\\\\ \\bm{y}\\end{bmatrix}\\sim\\mathcal{N}\\left(\\begin{bmatrix}\\bm{\\mu}_{\\bm{x}}\\\\ \\bm{\\mu}_{\\bm{y}}\\end{bmatrix},\\begin", "snippet": "[ ùíô ùíö ] ‚àº ùí© ‚Äã ( [ ùùÅ ùíô ùùÅ ùíö ] , [ ùö∫ ùíô ùö∫ ùíô ‚Äã ùíö ùö∫ ùíô ‚Äã ùíö ‚ä§ ùö∫ ùíö ] ) , \\begin{bmatrix}\\bm{x}\\\\ \\bm{y}\\end{bmatrix}\\sim\\mathcal{N}\\left(\\begin{bmatrix}\\bm{\\mu}_{\\bm{x}}\\\\ \\bm{\\mu}_{\\bm{y}}\\end{bmatrix},\\begin{bmatrix}\\bm{\\Sigma}_{\\bm{x}}&\\bm{\\Sigma}_{\\bm{x}\\bm{y}}\\\\ \\bm{\\Sigma}_{\\bm{x}\\bm{y}}^{\\top}&\\bm{\\Sigma}_{\\bm{y}}\\end{bmatrix}\\right), [ start_ARG start_ROW start_CELL bold_italic_x end_CELL end_ROW start_ROW start_CELL bold_italic_y end_CELL end_ROW end_ARG ] ‚àº caligraphic_N ( [ start_ARG start_ROW start_CELL bold_italic_Œº start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT end_CELL end_ROW start"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S6.Ex2", "title": "ùùÅ ùíô = ùîº ‚Äã [ ùíô ] , ùùÅ ùíö = ùîº ‚Äã [ ùíö ] , [ ùö∫ ùíô ùö∫ ùíô ‚Äã ùíö ùö∫ ùíô ‚Äã ùíö ‚ä§ ùö∫ ùíö ] = ùîº ‚Äã [ [ ùíô ‚àí ùîº ‚Äã [ ùíô ] ùíö ‚àí ùîº ‚Äã [ ùíö ] ] ‚Äã [ ùíô ‚àí ùîº ‚Äã [ ùíô ] ùíö ‚àí ùîº ‚Äã [ ùíö ] ] ‚ä§ ] \\bm{\\mu}_{\\bm{x}}=\\mathbb{E}[\\bm{x}],\\quad\\bm{\\mu}_{\\bm{", "snippet": "ùùÅ ùíô = ùîº ‚Äã [ ùíô ] , ùùÅ ùíö = ùîº ‚Äã [ ùíö ] , [ ùö∫ ùíô ùö∫ ùíô ‚Äã ùíö ùö∫ ùíô ‚Äã ùíö ‚ä§ ùö∫ ùíö ] = ùîº ‚Äã [ [ ùíô ‚àí ùîº ‚Äã [ ùíô ] ùíö ‚àí ùîº ‚Äã [ ùíö ] ] ‚Äã [ ùíô ‚àí ùîº ‚Äã [ ùíô ] ùíö ‚àí ùîº ‚Äã [ ùíö ] ] ‚ä§ ] \\bm{\\mu}_{\\bm{x}}=\\mathbb{E}[\\bm{x}],\\quad\\bm{\\mu}_{\\bm{y}}=\\mathbb{E}[\\bm{y}],\\quad\\begin{bmatrix}\\bm{\\Sigma}_{\\bm{x}}&\\bm{\\Sigma}_{\\bm{x}\\bm{y}}\\\\ \\bm{\\Sigma}_{\\bm{x}\\bm{y}}^{\\top}&\\bm{\\Sigma}_{\\bm{y}}\\end{bmatrix}=\\mathbb{E}\\left[\\begin{bmatrix}\\bm{x}-\\mathbb{E}[\\bm{x}]\\\\ \\bm{y}-\\mathbb{E}[\\bm{y}]\\end{bmatrix}\\begin{bmatrix}\\bm{x}-\\mathbb{E}[\\bm{x}]\\\\ \\bm{y}-\\mathbb{E}[\\bm{y}]\\end{bmatrix}^{\\top}\\right] bold_italic_Œº start_POSTSUBSCRIPT bold_italic_"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S6.E1", "title": "p ùíô ‚à£ ùíö ‚àº ùí© ‚Äã ( ùùÅ ùíô + ùö∫ ùíô ‚Äã ùíö ‚Äã ùö∫ ùíö ‚àí 1 ‚Äã ( ùíö ‚àí ùùÅ ùíö ) , ùö∫ ùíô ‚àí ùö∫ ùíô ‚Äã ùíö ‚Äã ùö∫ ùíö ‚àí 1 ‚Äã ùö∫ ùíô ‚Äã ùíö ‚ä§ ) . p_{\\bm{x}\\mid\\bm{y}}\\sim\\mathcal{N}\\left(\\bm{\\mu}_{\\bm{x}}+\\bm{\\Sigma}_{\\bm{x}\\bm{y}}\\bm{\\Sigma}_{\\bm{y}", "snippet": "p ùíô ‚à£ ùíö ‚àº ùí© ‚Äã ( ùùÅ ùíô + ùö∫ ùíô ‚Äã ùíö ‚Äã ùö∫ ùíö ‚àí 1 ‚Äã ( ùíö ‚àí ùùÅ ùíö ) , ùö∫ ùíô ‚àí ùö∫ ùíô ‚Äã ùíö ‚Äã ùö∫ ùíö ‚àí 1 ‚Äã ùö∫ ùíô ‚Äã ùíö ‚ä§ ) . p_{\\bm{x}\\mid\\bm{y}}\\sim\\mathcal{N}\\left(\\bm{\\mu}_{\\bm{x}}+\\bm{\\Sigma}_{\\bm{x}\\bm{y}}\\bm{\\Sigma}_{\\bm{y}}^{-1}(\\bm{y}-\\bm{\\mu}_{\\bm{y}}),\\bm{\\Sigma}_{\\bm{x}}-\\bm{\\Sigma}_{\\bm{x}\\bm{y}}\\bm{\\Sigma}_{\\bm{y}}^{-1}\\bm{\\Sigma}_{\\bm{x}\\bm{y}}^{\\top}\\right). italic_p start_POSTSUBSCRIPT bold_italic_x ‚à£ bold_italic_y end_POSTSUBSCRIPT ‚àº caligraphic_N ( bold_italic_Œº start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT + bold_Œ£ start_POSTSUBSCRIPT bold_italic_x bold_italic_y end_POSTSUBSCRIPT bold_Œ£ start_POST"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S6.E2", "title": "[ ùö∫ ùíô ùö∫ ùíô ‚Äã ùíö ùö∫ ùíô ‚Äã ùíö ‚ä§ ùö∫ ùíö ] = [ ùë∞ D ùö∫ ùíô ‚Äã ùíö ‚Äã ùö∫ ùíö ‚àí 1 ùüé ùë∞ d ] ‚Äã [ ùö∫ ùíô ‚àí ùö∫ ùíô ‚Äã ùíö ‚Äã ùö∫ ùíö ‚àí 1 ‚Äã ùö∫ ùíô ‚Äã ùíö ‚ä§ ùüé ùüé ùö∫ ùíö ] ‚Äã [ ùë∞ D ùüé ùö∫ ùíö ‚àí 1 ‚Äã ùö∫ ùíô ‚Äã ùíö ‚ä§ ùë∞ d ] . \\begin{bmatrix}\\bm{\\Sigma}_{\\bm{x}}&\\bm{\\Sigma}_", "snippet": "[ ùö∫ ùíô ùö∫ ùíô ‚Äã ùíö ùö∫ ùíô ‚Äã ùíö ‚ä§ ùö∫ ùíö ] = [ ùë∞ D ùö∫ ùíô ‚Äã ùíö ‚Äã ùö∫ ùíö ‚àí 1 ùüé ùë∞ d ] ‚Äã [ ùö∫ ùíô ‚àí ùö∫ ùíô ‚Äã ùíö ‚Äã ùö∫ ùíö ‚àí 1 ‚Äã ùö∫ ùíô ‚Äã ùíö ‚ä§ ùüé ùüé ùö∫ ùíö ] ‚Äã [ ùë∞ D ùüé ùö∫ ùíö ‚àí 1 ‚Äã ùö∫ ùíô ‚Äã ùíö ‚ä§ ùë∞ d ] . \\begin{bmatrix}\\bm{\\Sigma}_{\\bm{x}}&\\bm{\\Sigma}_{\\bm{x}\\bm{y}}\\\\ \\bm{\\Sigma}_{\\bm{x}\\bm{y}}^{\\top}&\\bm{\\Sigma}_{\\bm{y}}\\end{bmatrix}=\\begin{bmatrix}\\bm{I}_{D}&\\bm{\\Sigma}_{\\bm{x}\\bm{y}}\\bm{\\Sigma}_{\\bm{y}}^{-1}\\\\ \\mathbf{0}&\\bm{I}_{d}\\end{bmatrix}\\begin{bmatrix}\\bm{\\Sigma}_{\\bm{x}}-\\bm{\\Sigma}_{\\bm{x}\\bm{y}}\\bm{\\Sigma}_{\\bm{y}}^{-1}\\bm{\\Sigma}_{\\bm{x}\\bm{y}}^{\\top}&\\mathbf{0}\\\\ \\mathbf{0}&\\bm{\\Sigma}_{\\bm{y}}\\end{bmatrix}\\begin{bmatrix}\\bm{I}_{"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S6.E3", "title": "[ ùö∫ ùíô ùö∫ ùíô ‚Äã ùíö ùö∫ ùíô ‚Äã ùíö ‚ä§ ùö∫ ùíö ] ‚àí 1 = [ ùë∞ D ùüé ‚àí ùö∫ ùíö ‚àí 1 ‚Äã ùö∫ ùíô ‚Äã ùíö ‚ä§ ùë∞ d ] ‚Äã [ ( ùö∫ ùíô ‚àí ùö∫ ùíô ‚Äã ùíö ‚Äã ùö∫ ùíö ‚àí 1 ‚Äã ùö∫ ùíô ‚Äã ùíö ‚ä§ ) ‚àí 1 ùüé ùüé ùö∫ ùíö ‚àí 1 ] ‚Äã [ ùë∞ D ‚àí ùö∫ ùíô ‚Äã ùíö ‚Äã ùö∫ ùíö ‚àí 1 ùüé ùë∞ d ] \\begin{bmatrix}\\bm{\\Sigma}_{\\b", "snippet": "[ ùö∫ ùíô ùö∫ ùíô ‚Äã ùíö ùö∫ ùíô ‚Äã ùíö ‚ä§ ùö∫ ùíö ] ‚àí 1 = [ ùë∞ D ùüé ‚àí ùö∫ ùíö ‚àí 1 ‚Äã ùö∫ ùíô ‚Äã ùíö ‚ä§ ùë∞ d ] ‚Äã [ ( ùö∫ ùíô ‚àí ùö∫ ùíô ‚Äã ùíö ‚Äã ùö∫ ùíö ‚àí 1 ‚Äã ùö∫ ùíô ‚Äã ùíö ‚ä§ ) ‚àí 1 ùüé ùüé ùö∫ ùíö ‚àí 1 ] ‚Äã [ ùë∞ D ‚àí ùö∫ ùíô ‚Äã ùíö ‚Äã ùö∫ ùíö ‚àí 1 ùüé ùë∞ d ] \\begin{bmatrix}\\bm{\\Sigma}_{\\bm{x}}&\\bm{\\Sigma}_{\\bm{x}\\bm{y}}\\\\ \\bm{\\Sigma}_{\\bm{x}\\bm{y}}^{\\top}&\\bm{\\Sigma}_{\\bm{y}}\\end{bmatrix}^{-1}=\\begin{bmatrix}\\bm{I}_{D}&\\mathbf{0}\\\\ -\\bm{\\Sigma}_{\\bm{y}}^{-1}\\bm{\\Sigma}_{\\bm{x}\\bm{y}}^{\\top}&\\bm{I}_{d}\\end{bmatrix}\\begin{bmatrix}\\left(\\bm{\\Sigma}_{\\bm{x}}-\\bm{\\Sigma}_{\\bm{x}\\bm{y}}\\bm{\\Sigma}_{\\bm{y}}^{-1}\\bm{\\Sigma}_{\\bm{x}\\bm{y}}^{\\top}\\right)^{-1}&\\mathbf{0}\\\\ \\mathbf{0}&\\bm{\\Si"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S6.E6", "title": "( ùë® + ùëº ‚Äã ùë™ ‚Äã ùëΩ ) ‚àí 1 = ùë® ‚àí 1 ‚àí ùë® ‚àí 1 ‚Äã ùëº ‚Äã ( ùë™ ‚àí 1 + ùëΩ ‚Äã ùë® ‚àí 1 ‚Äã ùëº ) ‚àí 1 ‚Äã ùëΩ ‚Äã ùë® ‚àí 1 (\\bm{A}+\\bm{U}\\bm{C}\\bm{V})^{-1}=\\bm{A}^{-1}-\\bm{A}^{-1}\\bm{U}(\\bm{C}^{-1}+\\bm{V}\\bm{A}^{-1}\\bm{U})^{-1}\\bm{V}\\bm{", "snippet": "( ùë® + ùëº ‚Äã ùë™ ‚Äã ùëΩ ) ‚àí 1 = ùë® ‚àí 1 ‚àí ùë® ‚àí 1 ‚Äã ùëº ‚Äã ( ùë™ ‚àí 1 + ùëΩ ‚Äã ùë® ‚àí 1 ‚Äã ùëº ) ‚àí 1 ‚Äã ùëΩ ‚Äã ùë® ‚àí 1 (\\bm{A}+\\bm{U}\\bm{C}\\bm{V})^{-1}=\\bm{A}^{-1}-\\bm{A}^{-1}\\bm{U}(\\bm{C}^{-1}+\\bm{V}\\bm{A}^{-1}\\bm{U})^{-1}\\bm{V}\\bm{A}^{-1} ( bold_italic_A + bold_italic_U bold_italic_C bold_italic_V ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT = bold_italic_A start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT - bold_italic_A start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_italic_U ( bold_italic_C start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT + bold_italic_V bold_italic_A start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_ita"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S6.E7", "title": "Œ± t = 1 ‚àí t , œÉ t = t . \\alpha_{t}=1-t,\\qquad\\sigma_{t}=t. italic_Œ± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 1 - italic_t , italic_œÉ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_t .", "snippet": "Œ± t = 1 ‚àí t , œÉ t = t . \\alpha_{t}=1-t,\\qquad\\sigma_{t}=t. italic_Œ± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 1 - italic_t , italic_œÉ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_t . (3.6.7)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx13", "title": "ùñ™ùñ´ ‚Å° ( p ‚à• q ) \\displaystyle\\operatorname{\\mathsf{KL}}(p\\;\\|\\;q) sansserif_KL ( italic_p ‚à• italic_q ) ‚âê \\displaystyle\\doteq ‚âê ‚àí ‚à´ ‚Ñù D p ‚Äã ( ùùÉ ) ‚Äã log ‚Å° q ‚Äã ( ùùÉ ) ‚Äã d ùùÉ ‚àí ( ‚àí ‚à´ ‚Ñù D p ‚Äã ( ùùÉ ) ‚Äã log ‚Å° p ", "snippet": "ùñ™ùñ´ ‚Å° ( p ‚à• q ) \\displaystyle\\operatorname{\\mathsf{KL}}(p\\;\\|\\;q) sansserif_KL ( italic_p ‚à• italic_q ) ‚âê \\displaystyle\\doteq ‚âê ‚àí ‚à´ ‚Ñù D p ‚Äã ( ùùÉ ) ‚Äã log ‚Å° q ‚Äã ( ùùÉ ) ‚Äã d ùùÉ ‚àí ( ‚àí ‚à´ ‚Ñù D p ‚Äã ( ùùÉ ) ‚Äã log ‚Å° p ‚Äã ( ùùÉ ) ‚Äã d ùùÉ ) \\displaystyle-\\int_{\\mathbb{R}^{D}}p(\\bm{\\xi})\\log q(\\bm{\\xi})\\mathrm{d}\\bm{\\xi}-\\Big{(}-\\int_{\\mathbb{R}^{D}}p(\\bm{\\xi})\\log p(\\bm{\\xi})\\mathrm{d}\\bm{\\xi}\\Big{)} - ‚à´ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_p ( bold_italic_Œæ ) roman_log italic_q ( bold_italic_Œæ ) roman_d bold_italic_Œæ - ( - ‚à´ start_POSTSUBSCRIPT b"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx14", "title": "‚àí ùñ™ùñ´ ‚Å° ( p ‚à• q ) \\displaystyle-\\operatorname{\\mathsf{KL}}(p\\;\\|\\;q) - sansserif_KL ( italic_p ‚à• italic_q ) = \\displaystyle= = ‚àí ‚à´ ‚Ñù D p ‚Äã ( ùùÉ ) ‚Äã log ‚Å° p ‚Äã ( ùùÉ ) q ‚Äã ( ùùÉ ) ‚Äã d ‚Äã ùùÉ = ‚à´ ‚Ñù D p ‚Äã ( ùùÉ ) ‚Äã ", "snippet": "‚àí ùñ™ùñ´ ‚Å° ( p ‚à• q ) \\displaystyle-\\operatorname{\\mathsf{KL}}(p\\;\\|\\;q) - sansserif_KL ( italic_p ‚à• italic_q ) = \\displaystyle= = ‚àí ‚à´ ‚Ñù D p ‚Äã ( ùùÉ ) ‚Äã log ‚Å° p ‚Äã ( ùùÉ ) q ‚Äã ( ùùÉ ) ‚Äã d ‚Äã ùùÉ = ‚à´ ‚Ñù D p ‚Äã ( ùùÉ ) ‚Äã log ‚Å° q ‚Äã ( ùùÉ ) p ‚Äã ( ùùÉ ) ‚Äã d ‚Äã ùùÉ \\displaystyle-\\int_{\\mathbb{R}^{D}}p(\\bm{\\xi})\\log\\frac{p(\\bm{\\xi})}{q(\\bm{\\xi})}\\mathrm{d}\\bm{\\xi}=\\int_{\\mathbb{R}^{D}}p(\\bm{\\xi})\\log\\frac{q(\\bm{\\xi})}{p(\\bm{\\xi})}\\mathrm{d}\\bm{\\xi} - ‚à´ start_POSTSUBSCRIPT blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT end_POSTSUBSCRIPT italic_p ( bold_italic_Œæ ) roman_log divide start_ARG italic_p ( bold_ital"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx15", "title": "ùíô ¬Ø ‚àó ‚Äã ( t , ùíô t ) \\displaystyle\\bar{\\bm{x}}^{\\ast}(t,\\bm{x}_{t}) over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT ital", "snippet": "ùíô ¬Ø ‚àó ‚Äã ( t , ùíô t ) \\displaystyle\\bar{\\bm{x}}^{\\ast}(t,\\bm{x}_{t}) over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = ùîº ‚Å° [ ùíô ‚à£ ùíô t ] \\displaystyle=\\operatorname{\\mathbb{E}}[\\bm{x}\\mid\\bm{x}_{t}] = blackboard_E [ bold_italic_x ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] (3.2.11) = ùîº ‚Å° [ ùîº ‚Å° [ ùíô ‚à£ ùíô t , y ] ‚à£ ùíô t ] \\displaystyle=\\operatorname{\\mathbb{E}}[\\operatorname{\\mathbb{E}}[\\bm{x}\\mid\\bm{x}_{t},y]\\mid\\bm{x}_{t}] = blackboard_E [ blackboard_E [ bold_italic_x ‚à£ b"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx16", "title": "‚Ñô ‚Å° [ y = k ‚à£ ùíô t ] \\displaystyle\\operatorname{\\mathbb{P}}[y=k\\mid\\bm{x}_{t}] blackboard_P [ italic_y = italic_k ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] = p t ‚à£ y ‚Äã ( ùíô t ‚à£ k ", "snippet": "‚Ñô ‚Å° [ y = k ‚à£ ùíô t ] \\displaystyle\\operatorname{\\mathbb{P}}[y=k\\mid\\bm{x}_{t}] blackboard_P [ italic_y = italic_k ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] = p t ‚à£ y ‚Äã ( ùíô t ‚à£ k ) ‚Äã œÄ k p t ‚Äã ( ùíô t ) \\displaystyle=\\frac{p_{t\\mid y}(\\bm{x}_{t}\\mid k)\\pi_{k}}{p_{t}(\\bm{x}_{t})} = divide start_ARG italic_p start_POSTSUBSCRIPT italic_t ‚à£ italic_y end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ‚à£ italic_k ) italic_œÄ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG start_ARG italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_it"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx17", "title": "‚àá ùíô t log ‚Å° p t ‚Äã ( ùíô t ) \\displaystyle\\nabla_{\\bm{x}_{t}}\\log p_{t}(\\bm{x}_{t}) ‚àá start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_log italic_p", "snippet": "‚àá ùíô t log ‚Å° p t ‚Äã ( ùíô t ) \\displaystyle\\nabla_{\\bm{x}_{t}}\\log p_{t}(\\bm{x}_{t}) ‚àá start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = ‚àá ùíô t p t ‚Äã ( ùíô t ) p t ‚Äã ( ùíô t ) \\displaystyle=\\frac{\\nabla_{\\bm{x}_{t}}p_{t}(\\bm{x}_{t})}{p_{t}(\\bm{x}_{t})} = divide start_ARG ‚àá start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIP"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx18", "title": "ùíô ^ t ‚Ñì ‚àí 1 \\displaystyle\\hat{\\bm{x}}_{t_{\\ell-1}} over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_‚Ñì - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = ùîº ‚Å° [ ùíô t ‚Ñì ", "snippet": "ùíô ^ t ‚Ñì ‚àí 1 \\displaystyle\\hat{\\bm{x}}_{t_{\\ell-1}} over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_‚Ñì - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = ùîº ‚Å° [ ùíô t ‚Ñì ‚àí 1 ‚à£ ùíô t ‚Ñì = ùíô ^ t ‚Ñì ] \\displaystyle=\\operatorname{\\mathbb{E}}[\\bm{x}_{t_{\\ell-1}}\\mid\\bm{x}_{t_{\\ell}}=\\hat{\\bm{x}}_{t_{\\ell}}] = blackboard_E [ bold_italic_x start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_‚Ñì - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t start_POSTSUBSCRIPT roman_‚Ñì end_POSTSUBSCRIPT end_POSTSUBSCRIPT = over^ start_ARG bold_italic_x e"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx19", "title": "( ùëº k ‚Äã ùëº k ‚ä§ + t 2 ‚Äã ùë∞ ) ‚àí 1 \\displaystyle(\\bm{U}_{k}\\bm{U}_{k}^{\\top}+t^{2}\\bm{I})^{-1} ( bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_", "snippet": "( ùëº k ‚Äã ùëº k ‚ä§ + t 2 ‚Äã ùë∞ ) ‚àí 1 \\displaystyle(\\bm{U}_{k}\\bm{U}_{k}^{\\top}+t^{2}\\bm{I})^{-1} ( bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT = 1 t 2 ‚Äã ùë∞ ‚àí 1 t 4 ‚Äã ùëº k ‚Äã ( ùë∞ + 1 t 2 ‚Äã ùëº k ‚ä§ ‚Äã ùëº k ) ‚àí 1 ‚Äã ùëº k ‚ä§ \\displaystyle=\\frac{1}{t^{2}}\\bm{I}-\\frac{1}{t^{4}}\\bm{U}_{k}\\left(\\bm{I}+\\frac{1}{t^{2}}\\bm{U}_{k}^{\\top}\\bm{U}_{k}\\right)^{-1}\\bm{U}_{k}^{\\top} = divide st"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx20", "title": "œÜ ‚Äã ( ùíô t ; ùüé , ùëº k ‚Äã ùëº k ‚ä§ + t 2 ‚Äã ùë∞ ) ‚àë i = 1 K œÜ ‚Äã ( ùíô t ; ùüé , ùëº i ‚Äã ùëº i ‚ä§ + t 2 ‚Äã ùë∞ ) \\displaystyle\\frac{\\varphi(\\bm{x}_{t};\\bm{0},\\bm{U}_{k}\\bm{U}_{k}^{\\top}+t^{2}\\bm{I})}{\\sum_{i=1}^{K}\\varphi(\\", "snippet": "œÜ ‚Äã ( ùíô t ; ùüé , ùëº k ‚Äã ùëº k ‚ä§ + t 2 ‚Äã ùë∞ ) ‚àë i = 1 K œÜ ‚Äã ( ùíô t ; ùüé , ùëº i ‚Äã ùëº i ‚ä§ + t 2 ‚Äã ùë∞ ) \\displaystyle\\frac{\\varphi(\\bm{x}_{t};\\bm{0},\\bm{U}_{k}\\bm{U}_{k}^{\\top}+t^{2}\\bm{I})}{\\sum_{i=1}^{K}\\varphi(\\bm{x}_{t};\\bm{0},\\bm{U}_{i}\\bm{U}_{i}^{\\top}+t^{2}\\bm{I})} divide start_ARG italic_œÜ ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ; bold_0 , bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) end_"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx21", "title": "ùëº k ‚Äã ùëº k ‚ä§ ‚Äã ( ùëº k ‚Äã ùëº k ‚ä§ + t 2 ‚Äã ùë∞ ) ‚àí 1 ‚Äã ùíô t \\displaystyle\\bm{U}_{k}\\bm{U}_{k}^{\\top}(\\bm{U}_{k}\\bm{U}_{k}^{\\top}+t^{2}\\bm{I})^{-1}\\bm{x}_{t} bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSU", "snippet": "ùëº k ‚Äã ùëº k ‚ä§ ‚Äã ( ùëº k ‚Äã ùëº k ‚ä§ + t 2 ‚Äã ùë∞ ) ‚àí 1 ‚Äã ùíô t \\displaystyle\\bm{U}_{k}\\bm{U}_{k}^{\\top}(\\bm{U}_{k}\\bm{U}_{k}^{\\top}+t^{2}\\bm{I})^{-1}\\bm{x}_{t} bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ( bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bo"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx22", "title": "ùíô ¬Ø ‚àó ‚Äã ( t , ùíô t ) = 1 1 + t 2 ‚Äã ùëº ‚Äã ùëº ‚ä§ ‚Äã ùíô t . \\displaystyle\\bar{\\bm{x}}^{\\ast}(t,\\bm{x}_{t})=\\frac{1}{1+t^{2}}\\bm{U}\\bm{U}^{\\top}\\bm{x}_{t}. over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUPERSC", "snippet": "ùíô ¬Ø ‚àó ‚Äã ( t , ùíô t ) = 1 1 + t 2 ‚Äã ùëº ‚Äã ùëº ‚ä§ ‚Äã ùíô t . \\displaystyle\\bar{\\bm{x}}^{\\ast}(t,\\bm{x}_{t})=\\frac{1}{1+t^{2}}\\bm{U}\\bm{U}^{\\top}\\bm{x}_{t}. over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = divide start_ARG 1 end_ARG start_ARG 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG bold_italic_U bold_italic_U start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . (3.2.57)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx23", "title": "ùíô ¬Ø ‚Äã ( t , ùíô t ) = 1 1 + t 2 ‚Äã ùëΩ ‚Äã ùëΩ ‚ä§ ‚Äã ùíô t , \\displaystyle\\bar{\\bm{x}}(t,\\bm{x}_{t})=\\frac{1}{1+t^{2}}\\bm{V}\\bm{V}^{\\top}\\bm{x}_{t}, over¬Ø start_ARG bold_italic_x end_ARG ( italic_t , bold_italic_x", "snippet": "ùíô ¬Ø ‚Äã ( t , ùíô t ) = 1 1 + t 2 ‚Äã ùëΩ ‚Äã ùëΩ ‚ä§ ‚Äã ùíô t , \\displaystyle\\bar{\\bm{x}}(t,\\bm{x}_{t})=\\frac{1}{1+t^{2}}\\bm{V}\\bm{V}^{\\top}\\bm{x}_{t}, over¬Ø start_ARG bold_italic_x end_ARG ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = divide start_ARG 1 end_ARG start_ARG 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG bold_italic_V bold_italic_V start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , (3.2.58)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx24", "title": "ùîº ùíà ‚Å° ‚Äñ ùíô ‚àí 1 1 + t 2 ‚Äã ùëΩ ‚Äã ùëΩ ‚ä§ ‚Äã ( ùíô + t ‚Äã ùíà ) ‚Äñ 2 2 \\displaystyle\\operatorname{\\mathbb{E}}_{\\bm{g}}\\left\\|\\bm{x}-\\frac{1}{1+t^{2}}\\bm{V}\\bm{V}^{\\top}(\\bm{x}+t\\bm{g})\\right\\|_{2}^{2} blackboard_E sta", "snippet": "ùîº ùíà ‚Å° ‚Äñ ùíô ‚àí 1 1 + t 2 ‚Äã ùëΩ ‚Äã ùëΩ ‚ä§ ‚Äã ( ùíô + t ‚Äã ùíà ) ‚Äñ 2 2 \\displaystyle\\operatorname{\\mathbb{E}}_{\\bm{g}}\\left\\|\\bm{x}-\\frac{1}{1+t^{2}}\\bm{V}\\bm{V}^{\\top}(\\bm{x}+t\\bm{g})\\right\\|_{2}^{2} blackboard_E start_POSTSUBSCRIPT bold_italic_g end_POSTSUBSCRIPT ‚à• bold_italic_x - divide start_ARG 1 end_ARG start_ARG 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG bold_italic_V bold_italic_V start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ( bold_italic_x + italic_t bold_italic_g ) ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT (3.2.60) = \\displaystyle= = ‚Äñ "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx25", "title": "min ùëΩ ‚àà ùñÆ ‚Äã ( D , P ) ‚Å° ùîº ùíô ‚Å° ‚Äñ ùíô ‚àí 1 1 + t 2 ‚Äã ùëΩ ‚Äã ùëΩ ‚ä§ ‚Äã ùíô ‚Äñ 2 2 = ùîº ùíô ‚Å° ‚Äñ ùíô ‚Äñ 2 2 + ( 1 ( 1 + t 2 ) 2 ‚àí 2 1 + t 2 ) ‚Äã ùîº ùíô ‚Å° ‚Äñ ùëΩ ‚ä§ ‚Äã ùíô ‚Äñ 2 2 . \\displaystyle\\min_{\\bm{V}\\in\\mathsf{O}(D,P)}\\operatornam", "snippet": "min ùëΩ ‚àà ùñÆ ‚Äã ( D , P ) ‚Å° ùîº ùíô ‚Å° ‚Äñ ùíô ‚àí 1 1 + t 2 ‚Äã ùëΩ ‚Äã ùëΩ ‚ä§ ‚Äã ùíô ‚Äñ 2 2 = ùîº ùíô ‚Å° ‚Äñ ùíô ‚Äñ 2 2 + ( 1 ( 1 + t 2 ) 2 ‚àí 2 1 + t 2 ) ‚Äã ùîº ùíô ‚Å° ‚Äñ ùëΩ ‚ä§ ‚Äã ùíô ‚Äñ 2 2 . \\displaystyle\\min_{\\bm{V}\\in\\mathsf{O}(D,P)}\\operatorname{\\mathbb{E}}_{\\bm{x}}\\left\\|\\bm{x}-\\frac{1}{1+t^{2}}\\bm{V}\\bm{V}^{\\top}\\bm{x}\\right\\|_{2}^{2}=\\operatorname{\\mathbb{E}}_{\\bm{x}}\\|\\bm{x}\\|_{2}^{2}+\\left(\\frac{1}{(1+t^{2})^{2}}-\\frac{2}{1+t^{2}}\\right)\\operatorname{\\mathbb{E}}_{\\bm{x}}\\|\\bm{V}^{\\top}\\bm{x}\\|_{2}^{2}. roman_min start_POSTSUBSCRIPT bold_italic_V ‚àà sansserif_O ( italic_D , italic_P ) end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIP"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx26", "title": "max ùëΩ ‚àà ùñÆ ‚Äã ( D , P ) ‚Å° ùîº ùíô ‚Å° ‚Äñ ùëΩ ‚ä§ ‚Äã ùíô ‚Äñ 2 2 , \\displaystyle\\max_{\\bm{V}\\in\\mathsf{O}(D,P)}\\operatorname{\\mathbb{E}}_{\\bm{x}}\\|\\bm{V}^{\\top}\\bm{x}\\|_{2}^{2}, roman_max start_POSTSUBSCRIPT bold_italic", "snippet": "max ùëΩ ‚àà ùñÆ ‚Äã ( D , P ) ‚Å° ùîº ùíô ‚Å° ‚Äñ ùëΩ ‚ä§ ‚Äã ùíô ‚Äñ 2 2 , \\displaystyle\\max_{\\bm{V}\\in\\mathsf{O}(D,P)}\\operatorname{\\mathbb{E}}_{\\bm{x}}\\|\\bm{V}^{\\top}\\bm{x}\\|_{2}^{2}, roman_max start_POSTSUBSCRIPT bold_italic_V ‚àà sansserif_O ( italic_D , italic_P ) end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT ‚à• bold_italic_V start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_x ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , (3.2.64)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx27", "title": "‚Ñõ œµ ‚Äã ( ùíô ) \\displaystyle\\mathcal{R}_{\\epsilon}(\\bm{x}) caligraphic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_x ) ‚â• ‚àí ‚à´ K 1 vol ‚Å° ( K ) ‚Äã log 2 ‚Å° 1 vol ‚Å° ( K ) ‚Äã d ‚Äã ùùÉ ‚àí log 2 ‚Å° v", "snippet": "‚Ñõ œµ ‚Äã ( ùíô ) \\displaystyle\\mathcal{R}_{\\epsilon}(\\bm{x}) caligraphic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_x ) ‚â• ‚àí ‚à´ K 1 vol ‚Å° ( K ) ‚Äã log 2 ‚Å° 1 vol ‚Å° ( K ) ‚Äã d ‚Äã ùùÉ ‚àí log 2 ‚Å° vol ‚Å° ( B œµ ) ‚àí C d \\displaystyle\\geq-\\int_{K}\\frac{1}{\\operatorname{vol}(K)}\\log_{2}\\frac{1}{\\operatorname{vol}(K)}\\mathrm{d}\\bm{\\xi}-\\log_{2}\\operatorname{vol}(B_{\\epsilon})-C_{d} ‚â• - ‚à´ start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG roman_vol ( italic_K ) end_ARG roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG roman_vol ("}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx28", "title": "h ‚Äã ( ùíô , ùíà ) \\displaystyle h(\\bm{x},\\bm{g}) italic_h ( bold_italic_x , bold_italic_g ) = ‚àí ‚à´ p ‚Äã ( ùùÉ ) ‚Äã p ‚Äã ( ùú∏ ) ‚Äã log 2 ‚Å° p ‚Äã ( ùùÉ ) ‚Äã p ‚Äã ( ùú∏ ) ‚Äã d ùùÉ ‚Äã d ùú∏ \\displaystyle=-\\int p(\\bm{\\xi})p(\\bm{\\ga", "snippet": "h ‚Äã ( ùíô , ùíà ) \\displaystyle h(\\bm{x},\\bm{g}) italic_h ( bold_italic_x , bold_italic_g ) = ‚àí ‚à´ p ‚Äã ( ùùÉ ) ‚Äã p ‚Äã ( ùú∏ ) ‚Äã log 2 ‚Å° p ‚Äã ( ùùÉ ) ‚Äã p ‚Äã ( ùú∏ ) ‚Äã d ùùÉ ‚Äã d ùú∏ \\displaystyle=-\\int p(\\bm{\\xi})p(\\bm{\\gamma})\\log_{2}p(\\bm{\\xi})p(\\bm{\\gamma})\\mathrm{d}\\bm{\\xi}\\mathrm{d}\\bm{\\gamma} = - ‚à´ italic_p ( bold_italic_Œæ ) italic_p ( bold_italic_Œ≥ ) roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_p ( bold_italic_Œæ ) italic_p ( bold_italic_Œ≥ ) roman_d bold_italic_Œæ roman_d bold_italic_Œ≥ (3.3.14) = h ‚Äã ( ùíô ) + h ‚Äã ( œµ ‚Äã ùíà ) . \\displaystyle=h(\\bm{x})+h(\\epsilon\\bm{g}). = italic_h ( bold_italic_x ) + i"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx29", "title": "R œµ ‚Äã ( ùíÅ k ) = N k 2 ‚Äã N ‚Äã log ‚Äã det ( ùë∞ + d N k ‚Äã œµ 2 ‚Äã ùíÅ k ‚Äã ùíÅ k ‚ä§ ) \\displaystyle R_{\\epsilon}(\\bm{Z}_{k})=\\frac{N_{k}}{2N}\\log\\det\\left(\\bm{I}+\\frac{d}{N_{k}\\epsilon^{2}}\\bm{Z}_{k}\\bm{Z}_{k}^{\\to", "snippet": "R œµ ‚Äã ( ùíÅ k ) = N k 2 ‚Äã N ‚Äã log ‚Äã det ( ùë∞ + d N k ‚Äã œµ 2 ‚Äã ùíÅ k ‚Äã ùíÅ k ‚ä§ ) \\displaystyle R_{\\epsilon}(\\bm{Z}_{k})=\\frac{N_{k}}{2N}\\log\\det\\left(\\bm{I}+\\frac{d}{N_{k}\\epsilon^{2}}\\bm{Z}_{k}\\bm{Z}_{k}^{\\top}\\right) italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) = divide start_ARG italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG start_ARG 2 italic_N end_ARG roman_log roman_det ( bold_italic_I + divide start_ARG italic_d end_ARG start_ARG italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_œµ start_POST"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#S4.E12", "title": "max Œ∏ \\displaystyle\\max_{\\theta} roman_max start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT Œî ‚Äã R œµ ‚Äã ( ùíÅ ) ‚âê R œµ ‚Äã ( ùíÅ ) ‚àí R œµ c ‚Äã ( ùíÅ ) , \\displaystyle\\;\\Delta R_{\\epsilon}\\big{(}\\bm{Z}\\big{)}\\doteq R", "snippet": "max Œ∏ \\displaystyle\\max_{\\theta} roman_max start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT Œî ‚Äã R œµ ‚Äã ( ùíÅ ) ‚âê R œµ ‚Äã ( ùíÅ ) ‚àí R œµ c ‚Äã ( ùíÅ ) , \\displaystyle\\;\\Delta R_{\\epsilon}\\big{(}\\bm{Z}\\big{)}\\doteq R_{\\epsilon}(\\bm{Z})-R_{\\epsilon}^{c}(\\bm{Z}), roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ) ‚âê italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ) - italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_Z ) , (3.4.12) s.t. ùíÅ = f ‚Äã ( ùëø , Œ∏ ) , ‚Äñ ùíÅ k ‚Äñ F 2 = N k , k = 1 , ‚Ä¶ , K"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx30", "title": "R œµ c ‚Äã ( ùíÅ ‚à£ ùö∑ ) ‚âê ‚àë k = 1 K tr ‚Äã ( ùö∑ k ) 2 ‚Äã N ‚Äã log ‚Äã det ( ùë∞ + d tr ‚Äã ( ùö∑ k ) ‚Äã œµ 2 ‚Äã ùíÅ ‚Äã ùö∑ k ‚Äã ùíÅ ‚ä§ ) . \\displaystyle R_{\\epsilon}^{c}(\\bm{Z}\\mid\\bm{\\Pi})\\doteq\\sum_{k=1}^{K}\\frac{\\mathrm{tr}(\\bm{", "snippet": "R œµ c ‚Äã ( ùíÅ ‚à£ ùö∑ ) ‚âê ‚àë k = 1 K tr ‚Äã ( ùö∑ k ) 2 ‚Äã N ‚Äã log ‚Äã det ( ùë∞ + d tr ‚Äã ( ùö∑ k ) ‚Äã œµ 2 ‚Äã ùíÅ ‚Äã ùö∑ k ‚Äã ùíÅ ‚ä§ ) . \\displaystyle R_{\\epsilon}^{c}(\\bm{Z}\\mid\\bm{\\Pi})\\doteq\\sum_{k=1}^{K}\\frac{\\mathrm{tr}(\\bm{\\Pi}_{k})}{2N}\\log\\det\\left(\\bm{I}+\\frac{d}{\\mathrm{tr}(\\bm{\\Pi}_{k})\\epsilon^{2}}\\bm{Z}\\bm{\\Pi}_{k}\\bm{Z}^{\\top}\\right). italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_Z ‚à£ bold_Œ† ) ‚âê ‚àë start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT divide start_ARG roman_tr ( bold_Œ† sta"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx31", "title": "max ùö∑ , Œ∏ \\displaystyle\\max_{\\bm{\\Pi},\\theta} roman_max start_POSTSUBSCRIPT bold_Œ† , italic_Œ∏ end_POSTSUBSCRIPT Œî ‚Äã R œµ ‚Äã ( ùíÅ ‚à£ ùö∑ ) ‚âê R œµ ‚Äã ( ùíÅ ) ‚àí R œµ c ‚Äã ( ùíÅ ‚à£ ùö∑ ) \\displaystyle\\ \\Delta R_{\\epsilon}", "snippet": "max ùö∑ , Œ∏ \\displaystyle\\max_{\\bm{\\Pi},\\theta} roman_max start_POSTSUBSCRIPT bold_Œ† , italic_Œ∏ end_POSTSUBSCRIPT Œî ‚Äã R œµ ‚Äã ( ùíÅ ‚à£ ùö∑ ) ‚âê R œµ ‚Äã ( ùíÅ ) ‚àí R œµ c ‚Äã ( ùíÅ ‚à£ ùö∑ ) \\displaystyle\\ \\Delta R_{\\epsilon}\\big{(}\\bm{Z}\\mid\\bm{\\Pi})\\doteq R_{\\epsilon}(\\bm{Z})-R_{\\epsilon}^{c}(\\bm{Z}\\mid\\bm{\\Pi}) roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ‚à£ bold_Œ† ) ‚âê italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ) - italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_Z ‚à£ bold_Œ† ) s . t "}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx32", "title": "max ùíÅ R œµ ( ùíÅ ) ‚àí R œµ c ( ùíÅ ) s . t . ‚à• ùíÅ k ‚à• F 2 = N k , k = 1 , ‚Ä¶ , K . \\displaystyle\\max_{\\bm{Z}}\\ R_{\\epsilon}(\\bm{Z})-R_{\\epsilon}^{c}(\\bm{Z})\\qquad\\mathrm{s.t.}\\quad\\|\\bm{Z}_{k}\\|_{F}^{2}=N_{k},", "snippet": "max ùíÅ R œµ ( ùíÅ ) ‚àí R œµ c ( ùíÅ ) s . t . ‚à• ùíÅ k ‚à• F 2 = N k , k = 1 , ‚Ä¶ , K . \\displaystyle\\max_{\\bm{Z}}\\ R_{\\epsilon}(\\bm{Z})-R_{\\epsilon}^{c}(\\bm{Z})\\qquad\\mathrm{s.t.}\\quad\\|\\bm{Z}_{k}\\|_{F}^{2}=N_{k},\\ k=1,\\dots,K. roman_max start_POSTSUBSCRIPT bold_italic_Z end_POSTSUBSCRIPT italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ) - italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_Z ) roman_s . roman_t . ‚à• bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ‚à• start_POSTSUBSCRIPT italic_F end_POST"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx33", "title": "max ùíÅ ‚Å° R œµ ‚Äã ( ùíÅ ) ‚àí R œµ c ‚Äã ( ùíÅ ) ‚àí Œª 2 ‚Äã ‚Äñ ùíÅ ‚Äñ F 2 , \\displaystyle\\max_{\\bm{Z}}\\ R_{\\epsilon}(\\bm{Z})-R_{\\epsilon}^{c}(\\bm{Z})-\\frac{\\lambda}{2}\\|\\bm{Z}\\|_{F}^{2}, roman_max start_POSTSUBSCRIPT bol", "snippet": "max ùíÅ ‚Å° R œµ ‚Äã ( ùíÅ ) ‚àí R œµ c ‚Äã ( ùíÅ ) ‚àí Œª 2 ‚Äã ‚Äñ ùíÅ ‚Äñ F 2 , \\displaystyle\\max_{\\bm{Z}}\\ R_{\\epsilon}(\\bm{Z})-R_{\\epsilon}^{c}(\\bm{Z})-\\frac{\\lambda}{2}\\|\\bm{Z}\\|_{F}^{2}, roman_max start_POSTSUBSCRIPT bold_italic_Z end_POSTSUBSCRIPT italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ) - italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_Z ) - divide start_ARG italic_Œª end_ARG start_ARG 2 end_ARG ‚à• bold_italic_Z ‚à• start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , (3."}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx34", "title": "Œª ‚àà ( 0 , d ‚Äã ( N / N max ‚àí 1 ) N ‚Äã ( N / N max + 1 ) ‚Äã œµ 2 ] , \\displaystyle\\lambda\\in\\left(0,\\frac{d(\\sqrt{N/N_{\\max}}-1)}{N(\\sqrt{N/N_{\\max}}+1)\\epsilon^{2}}\\right], italic_Œª ‚àà ( 0 , divide start_A", "snippet": "Œª ‚àà ( 0 , d ‚Äã ( N / N max ‚àí 1 ) N ‚Äã ( N / N max + 1 ) ‚Äã œµ 2 ] , \\displaystyle\\lambda\\in\\left(0,\\frac{d(\\sqrt{N/N_{\\max}}-1)}{N(\\sqrt{N/N_{\\max}}+1)\\epsilon^{2}}\\right], italic_Œª ‚àà ( 0 , divide start_ARG italic_d ( square-root start_ARG italic_N / italic_N start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT end_ARG - 1 ) end_ARG start_ARG italic_N ( square-root start_ARG italic_N / italic_N start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT end_ARG + 1 ) italic_œµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ] , (3.4.17)"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx35", "title": "ùíÅ k ‚àó = ( Œ∑ k + Œ∑ k 2 ‚àí 4 ‚Äã Œª 2 ‚Äã N / N k 2 ‚Äã Œª ‚Äã Œ± k ) 1 / 2 ‚Äã ùëº k ‚Äã ùëΩ k ‚ä§ , \\displaystyle\\bm{Z}_{k}^{*}=\\left(\\frac{\\eta_{k}+\\sqrt{\\eta_{k}^{2}-4\\lambda^{2}N/N_{k}}}{2\\lambda\\alpha_{k}}\\right)^{1/2}", "snippet": "ùíÅ k ‚àó = ( Œ∑ k + Œ∑ k 2 ‚àí 4 ‚Äã Œª 2 ‚Äã N / N k 2 ‚Äã Œª ‚Äã Œ± k ) 1 / 2 ‚Äã ùëº k ‚Äã ùëΩ k ‚ä§ , \\displaystyle\\bm{Z}_{k}^{*}=\\left(\\frac{\\eta_{k}+\\sqrt{\\eta_{k}^{2}-4\\lambda^{2}N/N_{k}}}{2\\lambda\\alpha_{k}}\\right)^{1/2}\\bm{U}_{k}\\bm{V}_{k}^{\\top}, bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT = ( divide start_ARG italic_Œ∑ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + square-root start_ARG italic_Œ∑ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT - 4 italic_Œª start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_N"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx36", "title": "[ ùíô ‚àí ùùÅ ùíô ùíö ‚àí ùùÅ ùíö ] ‚ä§ ‚Äã [ ùö∫ ùíô ùö∫ ùíô ‚Äã ùíö ùö∫ ùíô ‚Äã ùíö ‚ä§ ùö∫ ùíö ] ‚àí 1 ‚Äã [ ùíô ‚àí ùùÅ ùíô ùíö ‚àí ùùÅ ùíö ] \\displaystyle\\begin{bmatrix}\\bm{x}-\\bm{\\mu}_{\\bm{x}}\\\\ \\bm{y}-\\bm{\\mu}_{\\bm{y}}\\end{bmatrix}^{\\top}\\begin{bmatrix}\\bm{\\S", "snippet": "[ ùíô ‚àí ùùÅ ùíô ùíö ‚àí ùùÅ ùíö ] ‚ä§ ‚Äã [ ùö∫ ùíô ùö∫ ùíô ‚Äã ùíö ùö∫ ùíô ‚Äã ùíö ‚ä§ ùö∫ ùíö ] ‚àí 1 ‚Äã [ ùíô ‚àí ùùÅ ùíô ùíö ‚àí ùùÅ ùíö ] \\displaystyle\\begin{bmatrix}\\bm{x}-\\bm{\\mu}_{\\bm{x}}\\\\ \\bm{y}-\\bm{\\mu}_{\\bm{y}}\\end{bmatrix}^{\\top}\\begin{bmatrix}\\bm{\\Sigma}_{\\bm{x}}&\\bm{\\Sigma}_{\\bm{x}\\bm{y}}\\\\ \\bm{\\Sigma}_{\\bm{x}\\bm{y}}^{\\top}&\\bm{\\Sigma}_{\\bm{y}}\\end{bmatrix}^{-1}\\begin{bmatrix}\\bm{x}-\\bm{\\mu}_{\\bm{x}}\\\\ \\bm{y}-\\bm{\\mu}_{\\bm{y}}\\end{bmatrix} [ start_ARG start_ROW start_CELL bold_italic_x - bold_italic_Œº start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL bold_italic_y - bold_italic_Œº start_POSTSUBSCRIPT b"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx37", "title": "f ‚Äã ( ùëø ) = log ‚Äã det ( ùëø ) \\displaystyle f(\\bm{X})=\\log\\det\\left(\\bm{X}\\right) italic_f ( bold_italic_X ) = roman_log roman_det ( bold_italic_X )", "snippet": "f ‚Äã ( ùëø ) = log ‚Äã det ( ùëø ) \\displaystyle f(\\bm{X})=\\log\\det\\left(\\bm{X}\\right) italic_f ( bold_italic_X ) = roman_log roman_det ( bold_italic_X )"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx38", "title": "log ‚Äã det ( ùë∞ + ùëø ‚ä§ ‚Äã ùëø ) = log ‚Äã det ( ùë∞ + ùëø ‚Äã ùëø ‚ä§ ) \\displaystyle\\log\\det(\\bm{I}+\\bm{X}^{\\top}\\bm{X})=\\log\\det(\\bm{I}+\\bm{X}\\bm{X}^{\\top}) roman_log roman_det ( bold_italic_I + bold_italic_X start_P", "snippet": "log ‚Äã det ( ùë∞ + ùëø ‚ä§ ‚Äã ùëø ) = log ‚Äã det ( ùë∞ + ùëø ‚Äã ùëø ‚ä§ ) \\displaystyle\\log\\det(\\bm{I}+\\bm{X}^{\\top}\\bm{X})=\\log\\det(\\bm{I}+\\bm{X}\\bm{X}^{\\top}) roman_log roman_det ( bold_italic_I + bold_italic_X start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_X ) = roman_log roman_det ( bold_italic_I + bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT )"}, {"page": "Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression", "href": "Ch3.html#A2.S3.EGx39", "title": "log ‚Äã det ( ùë® ) = ‚àë i = 1 n log ‚Å° ( Œª i ) , \\displaystyle\\log\\det\\left(\\bm{A}\\right)=\\sum_{i=1}^{n}\\log(\\lambda_{i}), roman_log roman_det ( bold_italic_A ) = ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POS", "snippet": "log ‚Äã det ( ùë® ) = ‚àë i = 1 n log ‚Å° ( Œª i ) , \\displaystyle\\log\\det\\left(\\bm{A}\\right)=\\sum_{i=1}^{n}\\log(\\lambda_{i}), roman_log roman_det ( bold_italic_A ) = ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT roman_log ( italic_Œª start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) , (3.6.8)"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#top", "title": "Chapter 4 Deep Representations from Unrolled Optimization", "snippet": ""}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1", "title": "4.1 White-Box Deep Networks via Unrolled Optimization", "snippet": "4.1 White-Box Deep Networks via Unrolled Optimization Now, if we agree that maximizing the rate reduction or information gain leads to the desired representation as discussed in Section 3.4 , the remaining question is how to construct and learn a (nonlinear) mapping from the data ùëø \\bm{X} bold_italic_X to the optimal representation ùíÅ ‚àó \\bm{Z}^{*} bold_italic_Z start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT . This involves designing a network architecture and learning algorithm that can effectively capture the underlying structures in the data and faithfully realize the optimal representation. 4.1"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2", "title": "4.2 White-Box Transformers from Unrolled Optimization", "snippet": "4.2 White-Box Transformers from Unrolled Optimization As we have seen in the previous section, we use the problem of classification to provide a rigorous interpretation for main architectural characteristics of popular deep networks such as the ResNet and the CNN: each layer of such networks can be viewed as to imitate a gradient step which increases the rate reduction (or information gain) objective. This perspective also leads to a somewhat surprising fact: the the parameters and operators of the layers of such a deep network, the ReduNet, can be computed in a purely forward fashion. Despite"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3", "title": "4.3 Variants of Deep Architectures by Design", "snippet": "4.3 Variants of Deep Architectures by Design So far, we wish that we have provided compelling evidence that the role of (popular) deep networks is to realize certain optimization algorithms for minimizing the coding rate (or maximizing the information gain) of the learned representations. However, readers who are familiar with optimization methods might have noticed that the above architectures (the ReduNet or the CRATE) correspond to rather basic optimization techniques. They may have plenty of room for improvement in efficiency or effectiveness. Moreover, if we believe the proposed theoretic"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S4", "title": "4.4 Summary and Notes", "snippet": "4.4 Summary and Notes The materials presented in this chapter are based on a series of recent works on this topic, including [ CYY+22 , WLP+24 , WLY+25 , WDL+25 , YBP+23 ] . These contributions encompass both theoretical advances and practical methodologies for constructing interpretable deep networks through unrolled optimization. Many of the key results and proofs discussed in this chapter are derived directly from, or inspired by, these foundational works. The idea of unrolling an optimization algorithm to construct a neural network traces back to the seminal work [ GL10 ] . In this work, t"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S5", "title": "4.5 Exercises and Extensions", "snippet": "4.5 Exercises and Extensions Exercise 4.1 . Let ùíÅ = [ ùíÅ 1 , ‚Ä¶ , ùíÅ K ] ‚àà ‚Ñù d √ó m \\bm{Z}=[\\bm{Z}_{1},\\dots,\\bm{Z}_{K}]\\in\\mathbb{R}^{d\\times m} bold_italic_Z = [ bold_italic_Z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_Z start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT ] ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d √ó italic_m end_POSTSUPERSCRIPT with ùíÅ k ‚àà ‚Ñù d √ó m k \\bm{Z}_{k}\\in\\mathbb{R}^{d\\times m_{k}} bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d √ó italic_m start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERS"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS1", "title": "4.1.1 Deep Networks from Unrolled Gradient Descent", "snippet": "4.1.1 Deep Networks from Unrolled Gradient Descent In the previous chapter, we presented the rate reduction objective ( 3.4.12 ) as a principled objective for learning linear discriminative representations of the data. We have, however, not specified the architecture of the feature mapping ùíõ = f ‚Äã ( ùíô , ùúΩ ) \\bm{z}=f(\\bm{x},\\bm{\\theta}) bold_italic_z = italic_f ( bold_italic_x , bold_italic_Œ∏ ) for extracting such representations from input data ùíô \\bm{x} bold_italic_x . A straightforward choice is to use a conventional deep network, such as ResNet, for implementing f ‚Äã ( ùíô , ùúΩ ) f(\\bm{x},\\bm{\\t"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS2", "title": "4.1.2 Convolutional Networks from Invariant Rate Reduction", "snippet": "4.1.2 Convolutional Networks from Invariant Rate Reduction In the previous section, we derived the layer-wise architecture of a deep network, the ReduNet, using unrolled optimization for the rate reduction objective. Specifically, the compression term R œµ c ‚Äã ( ùíÅ ‚à£ ùö∑ ) R^{c}_{\\epsilon}(\\bm{Z}\\mid\\bm{\\Pi}) italic_R start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ‚à£ bold_Œ† ) in ( 4.1.1 ) is designed to compress representations from the same class. However, this formulation does not account for possible domain transformation or defo"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.SS1", "title": "4.2.1 Unrolled Optimization for Sparse Rate Reduction", "snippet": "4.2.1 Unrolled Optimization for Sparse Rate Reduction We consider a general learning setup associated with real-world signals. Let ùëø = [ ùíô 1 , ‚Ä¶ , ùíô N ] ‚àà ‚Ñù D √ó N \\bm{X}=\\begin{bmatrix}\\bm{x}_{1},\\dots,\\bm{x}_{N}\\end{bmatrix}\\in\\mathbb{R}^{D\\times N} bold_italic_X = [ start_ARG start_ROW start_CELL bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D √ó italic_N end_POSTSUPERSCRIPT denote random variables representing our data source. In vision tasks, each"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.SS2", "title": "4.2.2 Overall White-Box Transformer Architecture: CRATE", "snippet": "4.2.2 Overall White-Box Transformer Architecture: CRATE We now design a white-box transformer architecture, named the Coding RATE Transformer ( crate ), by unrolling the above updates. By combining the above two steps ( 4.2.14 ) and ( 4.2.18 ): 1. Local compression of tokens within a sample towards a mixture-of-subspace structure, leading to the multi-head subspace self-attention block ‚Äì MSSA ; 2. Global sparsification of token sets across all samples through sparse coding, leading to the sparsification block ‚Äì ISTA ; we can get the following rate-reduction-based transformer layer, illustrated"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.SS1", "title": "4.3.1 Attention-Only Transformer Architecture", "snippet": "4.3.1 Attention-Only Transformer Architecture In this subsection, we propose a minimalistic transformer architecture consisting of interpretable layers based on the MSSA operator. To derive a fully interpretable transformer architecture with only necessary components, we contend that the goal of representation learning is to compress a set of noisy initial token representations towards a mixture of low-dimensional subspaces. Here, we assume that the initial token representations ùíÅ ( 1 ) \\bm{Z}^{(1)} bold_italic_Z start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT are sampled from a mixture of low"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.SS2", "title": "4.3.2 Linear-Time Attention: Token Statistics Transformer", "snippet": "4.3.2 Linear-Time Attention: Token Statistics Transformer In this subsection, we propose a new transformer attention operator whose computational complexity scales linearly with the number of tokens based on the coding rate reduction objective. Specifically, we derive a novel variational form of the MCR 2 objective and show that the architecture that results from unrolled gradient descent of this variational objective leads to a new attention module called Token Statistics Self-Attention ( TSSA ). TSSA has linear computational and memory complexity and radically departs from the typical attent"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS1.SSS0.Px1", "title": "Gradient Ascent for Coding Rate Reduction.", "snippet": "Gradient Ascent for Coding Rate Reduction. From the previous chapter, we see that to seek a linear discriminative representation (LDR), mathematically, we are essentially seeking a continuous mapping f ‚Äã ( ‚ãÖ ) : ùíô ‚Ü¶ ùíõ f(\\cdot):\\bm{x}\\mapsto\\bm{z} italic_f ( ‚ãÖ ) : bold_italic_x ‚Ü¶ bold_italic_z from the data ùëø = [ ùíô 1 , ‚Ä¶ , ùíô N ] ‚àà ‚Ñù D √ó N \\bm{X}=[\\bm{x}_{1},\\ldots,\\bm{x}_{N}]\\in\\mathbb{R}^{D\\times N} bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ] ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D √ó italic_N"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS1.SSS0.Px2", "title": "Gradient-Guided Feature Map Increment.", "snippet": "Gradient-Guided Feature Map Increment. Notice that in the above, the gradient ascent considers all the features ùíÅ ‚Ñì = [ ùíõ 1 ‚Ñì , ‚Ä¶ , ùíõ N ‚Ñì ] \\bm{Z}^{\\ell}=[\\bm{z}^{\\ell}_{1},\\dots,\\bm{z}^{\\ell}_{N}] bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT = [ bold_italic_z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ] as free variables. The increment ùíÅ ‚Ñì + 1 ‚àí ùíÅ ‚Ñì = Œ∑ ‚Äã ‚àÇ Œî ‚Äã R œµ ‚àÇ ùíÅ ‚Äã ( ùíÅ ‚Ñì ) \\bm{Z}^{\\ell+1}-\\bm{Z}^{\\ell}=\\eta"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS1.SSS0.Px3", "title": "Deep Network for Optimizing Rate Reduction.", "snippet": "Deep Network for Optimizing Rate Reduction. Notice that the increment is constructed to emulate the gradient ascent for the rate reduction Œî ‚Äã R œµ \\Delta R_{\\epsilon} roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT . Hence by transforming the features iteratively via the above process, we expect the rate reduction to increase, as we will see in the experimental section. This iterative process, once converged say after L L italic_L iterations, gives the desired feature map f ‚Äã ( ùíô , ùúΩ ) f(\\bm{x},\\bm{\\theta}) italic_f ( bold_italic_x , bold_italic_Œ∏ ) on the input ùíô = ùíõ ‚Äã ‚Ä¶ ‚Äã ‚Ä¶ ‚Äã"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS2.SSS0.Px1", "title": "1D Serial Data and Shift Invariance", "snippet": "1D Serial Data and Shift Invariance To classify one-dimensional data ùíô = [ x ‚Äã ( 0 ) , x ‚Äã ( 1 ) , ‚Ä¶ , x ‚Äã ( D ‚àí 1 ) ] ‚àà ‚Ñù D \\bm{x}=[x(0),x(1),\\ldots,x(D-1)]\\in\\mathbb{R}^{D} bold_italic_x = [ italic_x ( 0 ) , italic_x ( 1 ) , ‚Ä¶ , italic_x ( italic_D - 1 ) ] ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT invariant under shifting, we take ùîæ \\mathbb{G} blackboard_G to be the group of all circular shifts. Each observation ùíô i \\bm{x}_{i} bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT generates a family { ùíô i ‚àò ùî§ | ùî§ ‚àà ùîæ } \\{\\bm{x}_{i}\\circ\\mathfrak{g}\\,|\\,\\mathfrak"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS2.SSS0.Px2", "title": "A Fundamental Trade-off between Invariance and Sparsity.", "snippet": "A Fundamental Trade-off between Invariance and Sparsity. There is one problem though: In general, the set of all circular permutations of a vector ùíõ \\bm{z} bold_italic_z gives a full-rank matrix. That is, the d d italic_d ‚Äúaugmented‚Äù features associated with each sample (hence each class) typically already span the entire space ‚Ñù d \\mathbb{R}^{d} blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT . For instance, all shifted versions of a delta function Œ¥ ‚Äã ( d ) \\delta(d) italic_Œ¥ ( italic_d ) can generate any other signal as their (dense) weighted superposition. The MCR 2 objecti"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS2.SSS0.Px3", "title": "Overall Network Architecture and Comparison.", "snippet": "Overall Network Architecture and Comparison. Following the above derivation, we see that in order to find a linear discriminative representation (LDR) for multiple classes of signals/images that is invariant to translation, sparse coding, a multi-layer architecture with multi-channel convolutions, different nonlinear activation, and spectrum computing all become necessary components for achieving the objective effectively and efficiently. Figure 4.9 illustrates the overall process of learning such a representation via invariant rate reduction on the input sparse codes. Figure 4.9 : The overall"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.SS1.SSS0.Px1", "title": "Objective for Learning a Structured and Compact Representation.", "snippet": "Objective for Learning a Structured and Compact Representation. Following the framework of rate reduction Section 4.1 , we contend that the goal of representation learning is to find a feature mapping f : ùëø ‚àà ‚Ñù D √ó N ‚Üí ùíÅ ‚àà ‚Ñù d √ó N f\\colon\\bm{X}\\in\\mathbb{R}^{D\\times N}\\to\\bm{Z}\\in\\mathbb{R}^{d\\times N} italic_f : bold_italic_X ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D √ó italic_N end_POSTSUPERSCRIPT ‚Üí bold_italic_Z ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d √ó italic_N end_POSTSUPERSCRIPT which transforms input tokens { ùíô i } i = 1 N ‚äÇ ‚Ñù D \\{\\bm{x}_{i}\\}_{i=1}^{N}\\subset\\mathbb{R}^{D} { b"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.SS1.SSS0.Px2", "title": "Sparse Rate Reduction.", "snippet": "Sparse Rate Reduction. Note that the rate reduction objective ( 4.2.1 ) is invariant to arbitrary joint rotations of the representations and subspaces. In particular, optimizing the rate reduction objective may not naturally lead to axis-aligned (i.e., sparse ) representations. For instance, consider the three sets of learned representations in Figure 4.12 . The coding rate reduction increases from (a) to (b), but because it is invariant under rotations, remains the same from (b) to (c). Therefore, we would like to transform the representations (and their supporting subspaces) so that the repr"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.SS1.SSS0.Px3", "title": "White-Box Network Architecture via Unrolled Optimization.", "snippet": "White-Box Network Architecture via Unrolled Optimization. Although easy to state, each term in the above objective is computationally challenging to optimize [ WM22 ] . Hence it is natural to adopt an approximation approach that realizes the global transformation f f italic_f to optimize ( 4.2.4 ) through a concatenation of multiple, say L L italic_L , simple incremental and local operations f ‚Ñì f^{\\ell} italic_f start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT that push the representation distribution towards the desired parsimonious model distribution: f : ùëø = ùíÅ 0 ‚Üí f 0 ùíÅ 1 ‚Üí ‚ãØ ‚Üí ùíÅ ‚Ñì ‚Üí f ‚Ñì "}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.SS1.SSS0.Px4", "title": "Self-Attention as Gradient Descent on Coding Rate of Token Representations.", "snippet": "Self-Attention as Gradient Descent on Coding Rate of Token Representations. For the first step ( 4.2.7 ), the gradient of the coding rate ‚àá ùíÅ R œµ c \\nabla_{\\bm{Z}}R^{c}_{\\epsilon} ‚àá start_POSTSUBSCRIPT bold_italic_Z end_POSTSUBSCRIPT italic_R start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT is costly to compute, as it involves K K italic_K separate matrix inverses, one for each of the K K italic_K subspaces with basis ùëº k ‚Ñì \\bm{U}_{k}^{\\ell} bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERS"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.SS1.SSS0.Px5", "title": "MLP as Proximal Gradient Descent for Sparse Coding of Token Representations.", "snippet": "MLP as Proximal Gradient Descent for Sparse Coding of Token Representations. For the second step of alternating minimization, we need to minimize Œª ‚Äã ‚Äñ ùíÅ ‚Äñ 1 ‚àí R œµ ‚Äã ( ùíÅ ) \\lambda\\|\\bm{Z}\\|_{1}-R_{\\epsilon}(\\bm{Z}) italic_Œª ‚à• bold_italic_Z ‚à• start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT - italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ) . Note that the gradient ‚àá R œµ ‚Äã ( ùíÅ ) \\nabla R_{\\epsilon}(\\bm{Z}) ‚àá italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ) involves a matrix inverse, and thus naive proximal gradient (see Section A.1.3 ) to optimize this p"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.SS1.SSS0.Px1", "title": "Denoising Operator for Token Representations.", "snippet": "Denoising Operator for Token Representations. Now, we show that the MSSA operator (see ( 4.2.13 )) can incrementally denoise token representations generated from the above model. Spefically, we consider for each ‚Ñì = 1 , ‚Ä¶ , L \\ell=1,\\dots,L roman_‚Ñì = 1 , ‚Ä¶ , italic_L , ùíÅ ( ‚Ñì + 1 ) = ùíÅ ( ‚Ñì ) + Œ∑ ‚Äã ‚àë k = 1 K ùëº k ‚Äã ùëº k T ‚Äã ùíÅ ( ‚Ñì ) ‚Äã œÜ ‚Äã ( ùíÅ ( ‚Ñì ) T ‚Äã ùëº k ‚Äã ùëº k T ‚Äã ùíÅ ( ‚Ñì ) ) , \\displaystyle\\bm{Z}^{(\\ell+1)}=\\bm{Z}^{(\\ell)}+\\eta\\sum_{k=1}^{K}\\bm{U}_{k}\\bm{U}_{k}^{T}\\bm{Z}^{(\\ell)}\\varphi\\left(\\bm{Z}^{(\\ell)^{T}}\\bm{U}_{k}\\bm{U}_{k}^{T}\\bm{Z}^{(\\ell)}\\right), bold_italic_Z start_POSTSUPERSCRIPT ( ro"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.SS1.SSS0.Px2", "title": "Attention-Only Transformer.", "snippet": "Attention-Only Transformer. Now, we formally propose an attention-only transformer architecture. Specifically, by unrolling the iterative optimization steps ( 4.3.2 ) as layers of a deep network, we construct a transformer architecture in Figure 4.17 . Each layer of the proposed architecture only consists of the MSSA operator and a skip connection. For language tasks, we additionally incorporate LayerNorm before the MSSA operator to improve performance. The complete architecture is built by stacking such layers, along with essential task-specific pre-processing and post-processing steps, such "}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.SS2.SSS0.Px1", "title": "A New Variational Form for Coding Rates.", "snippet": "A New Variational Form for Coding Rates. To begin, we consider a general form of MCR 2 -like objectives based on concave functions of the spectrum of a matrix. Namely, for a given PSD matrix ùë¥ ‚àà ùñØùñ≤ùñ£ ‚Äã ( d ) \\bm{M}\\in\\mathsf{PSD}(d) bold_italic_M ‚àà sansserif_PSD ( italic_d ) and any scalar c ‚â• 0 c\\geq 0 italic_c ‚â• 0 we have that log ‚Äã det ( ùë∞ + c ‚Äã ùë¥ ) = ‚àë i = 1 d log ‚Å° ( 1 + c ‚Äã Œª i ‚Äã ( ùë¥ ) ) \\log\\det(\\bm{I}+c\\bm{M})=\\sum_{i=1}^{d}\\log(1+c\\lambda_{i}(\\bm{M})) roman_log roman_det ( bold_italic_I + italic_c bold_italic_M ) = ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCR"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.SS2.SSS0.Px2", "title": "Model interpretation.", "snippet": "Model interpretation. Given the proposed attention operator in ( 4.3.11 ), first recall that the rows of ùö∑ \\bm{\\Pi} bold_Œ† are non-negative and sum to 1 , so our operator takes a weighted average of K K italic_K ‚Äúattention head‚Äù-esque operators and then adds a residual connection. Using that ‚àë k = 1 K Œ† j ‚Äã k = 1 \\sum_{k=1}^{K}\\Pi_{jk}=1 ‚àë start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_Œ† start_POSTSUBSCRIPT italic_j italic_k end_POSTSUBSCRIPT = 1 , we can rewrite ( 4.3.11 ) as: ùíõ j + = ‚àë k = 1 K Œ† j ‚Äã k ‚Äã [ ùíõ j ‚Äã ‚àí œÑ n ‚Äã ùëº k ‚Äã ùë´ ‚Äã ( "}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.SS2.SSS0.Px3", "title": "Practical Implementation Details.", "snippet": "Practical Implementation Details. Having introduced our proposed attention operator, we now discuss further practical considerations. First, until this point in the presentation, we have avoided discussion of how tokens are ‚Äúgrouped‚Äù into various attention heads via the ùö∑ \\bm{\\Pi} bold_Œ† matrix, but clearly a means of constructing ùö∑ \\bm{\\Pi} bold_Œ† is needed to implement our method. Additionally, our variational form in Theorem 4.2 requires the ùëº \\bm{U} bold_italic_U matrices to be square and orthogonal, but one would ideally like to use smaller matrices (i.e., reduce the number of columns in "}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#p1", "title": "‚Äú What I cannot create, I do not understand .‚Äù ‚Äî Richard Feynman", "snippet": "‚Äú What I cannot create, I do not understand .‚Äù ‚Äî Richard Feynman"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#p2", "title": "In previous chapters, we have shown how to identify low-dimensional structures in high-dimensional spaces, mainly focusing on linear structures . For example, we introduced principal component analysi", "snippet": "In previous chapters, we have shown how to identify low-dimensional structures in high-dimensional spaces, mainly focusing on linear structures . For example, we introduced principal component analysis (PCA) to learn the linear denoiser ùëº ^ ‚ä§ \\hat{\\bm{U}}^{\\top} over^ start_ARG bold_italic_U end_ARG start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT when the observed data ùíô \\bm{x} bold_italic_x follow the statistical model ùíô = ùëº ‚Äã ùíõ + ùú∫ \\bm{x}=\\bm{U}\\bm{z}+\\bm{\\varepsilon} bold_italic_x = bold_italic_U bold_italic_z + bold_italic_Œµ . In this setting, the learned representations are linearly transform"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#p3", "title": "On the other hand, the linear model can be limited when dealing with real-world applications, especially when the input data ùíô \\bm{x} bold_italic_x is complex, such as speech and natural languages, im", "snippet": "On the other hand, the linear model can be limited when dealing with real-world applications, especially when the input data ùíô \\bm{x} bold_italic_x is complex, such as speech and natural languages, images and videos, and robotic motions. The low-dimensional distributions of such data are typically nonlinear. How to deal with nonlinearity has a long history across different disciplines such as control theory, signal processing, and pattern recognition. There have been considerable efforts that try to extend methods and solutions for linear models to handle nonlinearity, including early effort t"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#p4", "title": "More recently, deep neural networks have achieved remarkable success across a wide range of data and applications. A neural network f ‚Äã ( ‚ãÖ , ùúΩ ) : ùíô ‚Üí f 0 ùíõ 0 ‚Üí ‚ãØ ‚Üí ùíõ ‚Ñì ‚Üí f ‚Ñì ùíõ ‚Ñì + 1 ‚Üí ‚ãØ ‚Üí ùíõ L = ùíõ . ", "snippet": "More recently, deep neural networks have achieved remarkable success across a wide range of data and applications. A neural network f ‚Äã ( ‚ãÖ , ùúΩ ) : ùíô ‚Üí f 0 ùíõ 0 ‚Üí ‚ãØ ‚Üí ùíõ ‚Ñì ‚Üí f ‚Ñì ùíõ ‚Ñì + 1 ‚Üí ‚ãØ ‚Üí ùíõ L = ùíõ . f(\\cdot,\\bm{\\theta})\\colon\\bm{x}\\xrightarrow{\\hskip 2.84526ptf^{0}\\hskip 2.84526pt}\\bm{z}^{0}\\rightarrow\\cdots\\rightarrow\\bm{z}^{\\ell}\\xrightarrow{\\hskip 2.84526ptf^{\\ell}\\hskip 2.84526pt}\\bm{z}^{\\ell+1}\\rightarrow\\cdots\\to\\bm{z}^{L}=\\bm{z}. italic_f ( ‚ãÖ , bold_italic_Œ∏ ) : bold_italic_x start_ARROW start_OVERACCENT italic_f start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT end_OVERACCENT ‚Üí end_ARROW bo"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#p5", "title": "Subsequent popular practice models the mapping f f italic_f with other empirically designed artificial deep neural networks and learns the parameters ùúΩ \\bm{\\theta} bold_italic_Œ∏ from random initializa", "snippet": "Subsequent popular practice models the mapping f f italic_f with other empirically designed artificial deep neural networks and learns the parameters ùúΩ \\bm{\\theta} bold_italic_Œ∏ from random initialization via BP. Starting with the AlexNet [ KSH12 ] , the architectures of modern deep networks continue to be empirically revised and improved. Network architectures such as VGG [ SZ14 ] , ResNet [ HZR+16a ] , DenseNet [ HLV+17 ] , CNN, RNN or LSTM [ HS97 ] , Transformer [ VSP+17 ] , and a mixture of experts (MoE) [ SMM+17 , FZS22 ] , etc. have continued to push the performance envelope. As part of "}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#p6", "title": "Despite the wide application of deep neural networks, it is not clear what the underlying design principles of such a constructed network are. In particular, it is not clear what mathematical function", "snippet": "Despite the wide application of deep neural networks, it is not clear what the underlying design principles of such a constructed network are. In particular, it is not clear what mathematical function each layer of the network performs. In this chapter, based on the results from previous chapters, we develop a principled framework that will provide a fully rigorous mathematical interpretation of the role of a deep network, including its individual layers and the network as a whole."}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#p7", "title": "To understand deep networks and how they should be better designed, we must start with the objective of representation learning. In previous chapters, we have argued that the objective is to identify ", "snippet": "To understand deep networks and how they should be better designed, we must start with the objective of representation learning. In previous chapters, we have argued that the objective is to identify the intrinsically low-dimensional data distribution and then transform it to a compact and structured (say piecewise linear) representation. As we have seen in the previous chapter, the general approach to identifying a low-dimensional data distribution is through a compression process that progressively minimizes the entropy or coding rate of the distribution. However, up to this point, we have b"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#p8", "title": "As we have argued in the previous chapter, Section 3.4 in particular, one can measure the goodness of the resulting representation by the information of the representation gained from a ‚Äúlazy‚Äù represe", "snippet": "As we have argued in the previous chapter, Section 3.4 in particular, one can measure the goodness of the resulting representation by the information of the representation gained from a ‚Äúlazy‚Äù representation which models all data as one big Gaussian. 1 1 1 that we have seen in the previous chapter as one particular choice of interpretation of the sampled dataset. In particular, if we use a mixture of Gaussians (subspaces) 2 2 2 which we have studied thoroughly in the previous chapter. as prototypical distributions to approximate the non-linear distribution of interest , then we can efficiently"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#p9", "title": "As we will see in this chapter, once the objective of representation learning is clear, the role of a deep neural network is precisely to help optimize the objective iteratively. Each layer of a deep ", "snippet": "As we will see in this chapter, once the objective of representation learning is clear, the role of a deep neural network is precisely to help optimize the objective iteratively. Each layer of a deep neural network can be naturally derived as an iterative optimization step to incrementally maximize the information gain, including the popular architectures of ResNet, CNN, and Transformer, and other more advanced variants. In particular, this chapter aims to answer the following questions about deep networks: ‚Ä¢ Section 4.1 ‚Äî given a measure of goodness for a learned representation, how to constr"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S0.I1.i1.p1", "title": "Section 4.1 ‚Äî given a measure of goodness for a learned representation, how to construct the nonlinear mapping from the data to the optimal representation via unrolled optimization for the objective?", "snippet": "Section 4.1 ‚Äî given a measure of goodness for a learned representation, how to construct the nonlinear mapping from the data to the optimal representation via unrolled optimization for the objective?"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S0.I1.i2.p1", "title": "Section 4.2 ‚Äî how would the above unrolling approach provide a principled interpretation of the popular transformer architectures; if so, what are the associated objective and optimization mechanisms?", "snippet": "Section 4.2 ‚Äî how would the above unrolling approach provide a principled interpretation of the popular transformer architectures; if so, what are the associated objective and optimization mechanisms?"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S0.I1.i3.p1", "title": "Section 4.3 ‚Äî how would this framework guide us to design more efficient or more parsimonious deep architectures?", "snippet": "Section 4.3 ‚Äî how would this framework guide us to design more efficient or more parsimonious deep architectures?"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.p1", "title": "Now, if we agree that maximizing the rate reduction or information gain leads to the desired representation as discussed in Section 3.4 , the remaining question is how to construct and learn a (nonlin", "snippet": "Now, if we agree that maximizing the rate reduction or information gain leads to the desired representation as discussed in Section 3.4 , the remaining question is how to construct and learn a (nonlinear) mapping from the data ùëø \\bm{X} bold_italic_X to the optimal representation ùíÅ ‚àó \\bm{Z}^{*} bold_italic_Z start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT . This involves designing a network architecture and learning algorithm that can effectively capture the underlying structures in the data and faithfully realize the optimal representation."}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS1.p1", "title": "In the previous chapter, we presented the rate reduction objective ( 3.4.12 ) as a principled objective for learning linear discriminative representations of the data. We have, however, not specified ", "snippet": "In the previous chapter, we presented the rate reduction objective ( 3.4.12 ) as a principled objective for learning linear discriminative representations of the data. We have, however, not specified the architecture of the feature mapping ùíõ = f ‚Äã ( ùíô , ùúΩ ) \\bm{z}=f(\\bm{x},\\bm{\\theta}) bold_italic_z = italic_f ( bold_italic_x , bold_italic_Œ∏ ) for extracting such representations from input data ùíô \\bm{x} bold_italic_x . A straightforward choice is to use a conventional deep network, such as ResNet, for implementing f ‚Äã ( ùíô , ùúΩ ) f(\\bm{x},\\bm{\\theta}) italic_f ( bold_italic_x , bold_italic_Œ∏ ) ."}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS1.p2", "title": "In this chapter, we show that using gradient ascent to maximize the rate reduction Œî ‚Äã R œµ ‚Äã ( ùíÅ ‚à£ ùö∑ ) \\Delta R_{\\epsilon}(\\bm{Z}\\mid\\bm{\\Pi}) roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUB", "snippet": "In this chapter, we show that using gradient ascent to maximize the rate reduction Œî ‚Äã R œµ ‚Äã ( ùíÅ ‚à£ ùö∑ ) \\Delta R_{\\epsilon}(\\bm{Z}\\mid\\bm{\\Pi}) roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ‚à£ bold_Œ† ) as defined in ( 3.4.12 ) naturally leads to a ‚Äúwhite-box‚Äù deep network that realizes the desired mapping. All network layers, linear/nonlinear operators, and parameters are explicitly constructed in a purely forward propagation fashion . Moreover, such network architectures resemble existing empirically-designed deep networks, providing principled justifications f"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS1.SSS0.Px1.p1", "title": "From the previous chapter, we see that to seek a linear discriminative representation (LDR), mathematically, we are essentially seeking a continuous mapping f ‚Äã ( ‚ãÖ ) : ùíô ‚Ü¶ ùíõ f(\\cdot):\\bm{x}\\mapsto\\bm", "snippet": "From the previous chapter, we see that to seek a linear discriminative representation (LDR), mathematically, we are essentially seeking a continuous mapping f ‚Äã ( ‚ãÖ ) : ùíô ‚Ü¶ ùíõ f(\\cdot):\\bm{x}\\mapsto\\bm{z} italic_f ( ‚ãÖ ) : bold_italic_x ‚Ü¶ bold_italic_z from the data ùëø = [ ùíô 1 , ‚Ä¶ , ùíô N ] ‚àà ‚Ñù D √ó N \\bm{X}=[\\bm{x}_{1},\\ldots,\\bm{x}_{N}]\\in\\mathbb{R}^{D\\times N} bold_italic_X = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ] ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D √ó italic_N end_POSTSUPERSCRIPT (or initial features e"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS1.SSS0.Px1.p2", "title": "The question really boils down to whether there is a constructive way of finding such a continuous mapping f ‚Äã ( ‚ãÖ , ùúΩ ) f(\\cdot,\\bm{\\theta}) italic_f ( ‚ãÖ , bold_italic_Œ∏ ) from ùíô \\bm{x} bold_italic_x", "snippet": "The question really boils down to whether there is a constructive way of finding such a continuous mapping f ‚Äã ( ‚ãÖ , ùúΩ ) f(\\cdot,\\bm{\\theta}) italic_f ( ‚ãÖ , bold_italic_Œ∏ ) from ùíô \\bm{x} bold_italic_x to ùíõ \\bm{z} bold_italic_z ? To this end, let us consider incrementally maximizing the objective Œî ‚Äã R œµ \\Delta R_{\\epsilon} roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT as a function of ùíÅ ‚äÜ ùïä d ‚àí 1 \\bm{Z}\\subseteq\\mathbb{S}^{d-1} bold_italic_Z ‚äÜ blackboard_S start_POSTSUPERSCRIPT italic_d - 1 end_POSTSUPERSCRIPT . Although there might be many optimization schemes to choose from"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS1.SSS0.Px1.p3", "title": "Simple calculation shows that the gradient ‚àÇ Œî ‚Äã R œµ / ‚àÇ ùíÅ {\\partial\\Delta R_{\\epsilon}}/{\\partial\\bm{Z}} ‚àÇ roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT / ‚àÇ bold_italic_Z entails ev", "snippet": "Simple calculation shows that the gradient ‚àÇ Œî ‚Äã R œµ / ‚àÇ ùíÅ {\\partial\\Delta R_{\\epsilon}}/{\\partial\\bm{Z}} ‚àÇ roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT / ‚àÇ bold_italic_Z entails evaluating the following derivatives of the two terms in Œî ‚Äã R œµ \\Delta R_{\\epsilon} roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT : 1 2 ‚Äã ‚àÇ log ‚Äã det ( ùë∞ + Œ± ‚Äã ùíÅ ‚Äã ùíÅ ‚ä§ ) ‚àÇ ùíÅ ‚Äã ( ùíÅ ‚Ñì ) = Œ± ‚Äã ( ùë∞ + Œ± ‚Äã ùíÅ ‚Ñì ‚Äã ( ùíÅ ‚Ñì ) ‚ä§ ) ‚àí 1 ‚èü ùë¨ ‚Ñì ‚àà ‚Ñù d √ó d ‚Äã ùíÅ ‚Ñì , \\frac{1}{2}\\frac{\\partial\\log\\det(\\bm{I}\\!+\\!\\alpha\\bm{Z}\\bm{Z}^{\\top})}{\\partial\\bm{Z}}(\\bm{Z}^{\\ell})=\\underbrace{\\alpha(\\bm{I}\\!+\\!\\al"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS1.SSS0.Px1.p4", "title": "1 2 ‚Äã ‚àÇ ( Œ≥ k ‚Äã log ‚Äã det ( ùë∞ + Œ± k ‚Äã ùíÅ ‚Äã ùö∑ k ‚Äã ùíÅ ‚ä§ ) ) ‚àÇ ùíÅ ‚Äã ( ùíÅ ‚Ñì ) = Œ≥ k ‚Äã Œ± k ‚Äã ( ùë∞ + Œ± k ‚Äã ùíÅ ‚Ñì ‚Äã ùö∑ k ‚Äã ( ùíÅ ‚Ñì ) ‚ä§ ) ‚àí 1 ‚èü ùë™ k ‚Ñì ‚àà ‚Ñù d √ó d ‚Äã ùíÅ ‚Ñì ‚Äã ùö∑ k . \\frac{1}{2}\\frac{\\partial\\left(\\gamma_{k}\\lo", "snippet": "1 2 ‚Äã ‚àÇ ( Œ≥ k ‚Äã log ‚Äã det ( ùë∞ + Œ± k ‚Äã ùíÅ ‚Äã ùö∑ k ‚Äã ùíÅ ‚ä§ ) ) ‚àÇ ùíÅ ‚Äã ( ùíÅ ‚Ñì ) = Œ≥ k ‚Äã Œ± k ‚Äã ( ùë∞ + Œ± k ‚Äã ùíÅ ‚Ñì ‚Äã ùö∑ k ‚Äã ( ùíÅ ‚Ñì ) ‚ä§ ) ‚àí 1 ‚èü ùë™ k ‚Ñì ‚àà ‚Ñù d √ó d ‚Äã ùíÅ ‚Ñì ‚Äã ùö∑ k . \\frac{1}{2}\\frac{\\partial\\left(\\gamma_{k}\\log\\det(\\bm{I}+\\alpha_{k}\\bm{Z}\\bm{\\Pi}_{k}\\bm{Z}^{\\top})\\right)}{\\partial\\bm{Z}}(\\bm{Z}^{\\ell})=\\gamma_{k}\\underbrace{\\alpha_{k}(\\bm{I}+\\alpha_{k}\\bm{Z}^{\\ell}\\bm{\\Pi}_{k}(\\bm{Z}^{\\ell})^{\\top})^{-1}}_{\\bm{C}^{\\ell}_{k}\\;\\in\\mathbb{R}^{d\\times d}}\\bm{Z}^{\\ell}\\bm{\\Pi}_{k}. divide start_ARG 1 end_ARG start_ARG 2 end_ARG divide start_ARG ‚àÇ ( italic_Œ≥ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT rom"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmremark1.p1", "title": "For any ùíõ ‚Ñì ‚àà ‚Ñù d \\bm{z}^{\\ell}\\in\\mathbb{R}^{d} bold_italic_z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , ùë¨ ‚Ñì ‚Äã ùíõ ‚Ñì = Œ± ‚Äã ( ùíõ", "snippet": "For any ùíõ ‚Ñì ‚àà ‚Ñù d \\bm{z}^{\\ell}\\in\\mathbb{R}^{d} bold_italic_z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , ùë¨ ‚Ñì ‚Äã ùíõ ‚Ñì = Œ± ‚Äã ( ùíõ ‚Ñì ‚àí ùíÅ ‚Ñì ‚Äã ùíí ‚ãÜ ‚Ñì ) , where ùíí ‚ãÜ ‚Ñì ‚âê arg ‚Äã min ùíí ‚Ñì ‚Å° { Œ± ‚Äã ‚Äñ ùíõ ‚Ñì ‚àí ùíÅ ‚Ñì ‚Äã ùíí ‚Ñì ‚Äñ 2 2 + ‚Äñ ùíí ‚Ñì ‚Äñ 2 2 } . \\displaystyle\\bm{E}^{\\ell}\\bm{z}^{\\ell}=\\alpha(\\bm{z}^{\\ell}-\\bm{Z}^{\\ell}\\bm{q}^{\\ell}_{\\star}),\\qquad\\mbox{where}\\qquad\\bm{q}^{\\ell}_{\\star}\\doteq\\operatorname*{arg\\ min}_{\\bm{q}^{\\ell}}\\big{\\{}\\alpha\\|\\bm{z}^{\\ell}-\\bm{Z}^{\\ell}\\bm{q}^{\\ell}\\|_{2}^{2}+\\|\\bm{q}^{\\ell}\\|_{2}^{2}\\big{\\}}. bold_italic"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmremark1.p2", "title": "Essentially, the linear operations ùë¨ ‚Ñì \\bm{E}^{\\ell} bold_italic_E start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT and ùë™ k ‚Ñì \\bm{C}_{k}^{\\ell} bold_italic_C start_POSTSUBSCRIPT italic_k end_POSTSUBS", "snippet": "Essentially, the linear operations ùë¨ ‚Ñì \\bm{E}^{\\ell} bold_italic_E start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT and ùë™ k ‚Ñì \\bm{C}_{k}^{\\ell} bold_italic_C start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT in gradient ascend for rate reduction are determined by training data conducting ‚Äúauto-regressions‚Äù. The recent renewed understanding about ridge regression in an over-parameterized setting [ YYY+20 , WX20 ] indicates that using seemingly redundantly sampled data (from each subspace) as regressors does not lead to overfitting."}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS1.SSS0.Px2.p1", "title": "Notice that in the above, the gradient ascent considers all the features ùíÅ ‚Ñì = [ ùíõ 1 ‚Ñì , ‚Ä¶ , ùíõ N ‚Ñì ] \\bm{Z}^{\\ell}=[\\bm{z}^{\\ell}_{1},\\dots,\\bm{z}^{\\ell}_{N}] bold_italic_Z start_POSTSUPERSCRIPT roman", "snippet": "Notice that in the above, the gradient ascent considers all the features ùíÅ ‚Ñì = [ ùíõ 1 ‚Ñì , ‚Ä¶ , ùíõ N ‚Ñì ] \\bm{Z}^{\\ell}=[\\bm{z}^{\\ell}_{1},\\dots,\\bm{z}^{\\ell}_{N}] bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT = [ bold_italic_z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ] as free variables. The increment ùíÅ ‚Ñì + 1 ‚àí ùíÅ ‚Ñì = Œ∑ ‚Äã ‚àÇ Œî ‚Äã R œµ ‚àÇ ùíÅ ‚Äã ( ùíÅ ‚Ñì ) \\bm{Z}^{\\ell+1}-\\bm{Z}^{\\ell}=\\eta\\frac{\\partial\\Delta R_{\\epsilon}}{\\par"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS1.SSS0.Px2.p2", "title": "By inspecting the structure of the gradient ( 4.1.5 ), it suggests that a natural candidate for the increment transform g ‚Äã ( ùíõ ‚Ñì , ùúΩ ‚Ñì ) g(\\bm{z}^{\\ell},\\bm{\\theta}^{\\ell}) italic_g ( bold_italic_z s", "snippet": "By inspecting the structure of the gradient ( 4.1.5 ), it suggests that a natural candidate for the increment transform g ‚Äã ( ùíõ ‚Ñì , ùúΩ ‚Ñì ) g(\\bm{z}^{\\ell},\\bm{\\theta}^{\\ell}) italic_g ( bold_italic_z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT , bold_italic_Œ∏ start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ) is of the form: g ‚Äã ( ùíõ ‚Ñì , ùúΩ ‚Ñì ) ‚âê ùë¨ ‚Ñì ‚Äã ùíõ ‚Ñì ‚àí ‚àë k = 1 K Œ≥ k ‚Äã œÄ k ‚Äã ( ùíõ ‚Ñì ) ‚Äã ùë™ k ‚Ñì ‚Äã ùíõ ‚Ñì ‚àà ‚Ñù d , g(\\bm{z}^{\\ell},\\bm{\\theta}^{\\ell})\\;\\doteq\\;\\bm{E}^{\\ell}\\bm{z}^{\\ell}-\\sum_{k=1}^{K}\\gamma_{k}\\pi_{k}(\\bm{z}^{\\ell})\\bm{C}_{k}^{\\ell}\\bm{z}^{\\ell}\\in\\mathbb{R}^{d}, italic_g ( bold_"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS1.SSS0.Px2.p3", "title": "Since we only have the membership for the training samples, the function g ‚Äã ( ‚ãÖ ) g(\\cdot) italic_g ( ‚ãÖ ) defined in ( 4.1.10 ) can only be evaluated on the training. To extrapolate g ‚Äã ( ‚ãÖ ) g(\\cdot", "snippet": "Since we only have the membership for the training samples, the function g ‚Äã ( ‚ãÖ ) g(\\cdot) italic_g ( ‚ãÖ ) defined in ( 4.1.10 ) can only be evaluated on the training. To extrapolate g ‚Äã ( ‚ãÖ ) g(\\cdot) italic_g ( ‚ãÖ ) to the entire feature space, we need to estimate œÄ k ‚Äã ( ùíõ ‚Ñì ) \\pi_{k}(\\bm{z}^{\\ell}) italic_œÄ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ) in its second term. In conventional deep learning, this map is typically modeled as a deep network and learned from the training data, say via back propagation . Nevertheles"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS1.SSS0.Px2.p4", "title": "From the geometric interpretation of the linear maps ùë¨ ‚Ñì \\bm{E}^{\\ell} bold_italic_E start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT and ùë™ k ‚Ñì \\bm{C}_{k}^{\\ell} bold_italic_C start_POSTSUBSCRIPT ita", "snippet": "From the geometric interpretation of the linear maps ùë¨ ‚Ñì \\bm{E}^{\\ell} bold_italic_E start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT and ùë™ k ‚Ñì \\bm{C}_{k}^{\\ell} bold_italic_C start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT given by Remark 4.1 , the term ùíë k ‚Ñì ‚âê ùë™ k ‚Ñì ‚Äã ùíõ ‚Ñì \\bm{p}_{k}^{\\ell}\\doteq\\bm{C}^{\\ell}_{k}\\bm{z}^{\\ell} bold_italic_p start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ‚âê bold_italic_C start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRI"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS1.SSS0.Px2.p5", "title": "Overall, combining ( 4.1.8 ), ( 4.1.10 ), and ( 4.1.12 ), the increment feature transform from ùíõ ‚Ñì \\bm{z}^{\\ell} bold_italic_z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT to ùíõ ‚Ñì + 1 \\bm{z}^{\\ell", "snippet": "Overall, combining ( 4.1.8 ), ( 4.1.10 ), and ( 4.1.12 ), the increment feature transform from ùíõ ‚Ñì \\bm{z}^{\\ell} bold_italic_z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT to ùíõ ‚Ñì + 1 \\bm{z}^{\\ell+1} bold_italic_z start_POSTSUPERSCRIPT roman_‚Ñì + 1 end_POSTSUPERSCRIPT now becomes ùíõ ‚Ñì + 1 \\displaystyle\\bm{z}^{\\ell+1} bold_italic_z start_POSTSUPERSCRIPT roman_‚Ñì + 1 end_POSTSUPERSCRIPT ‚àù ùíõ ‚Ñì + Œ∑ ‚ãÖ ùë¨ ‚Ñì ‚Äã ùíõ ‚Ñì ‚àí Œ∑ ‚ãÖ ùùà ‚Äã ( [ ùë™ 1 ‚Ñì ‚Äã ùíõ ‚Ñì , ‚Ä¶ , ùë™ K ‚Ñì ‚Äã ùíõ ‚Ñì ] ) \\displaystyle\\propto\\;\\bm{z}^{\\ell}+\\eta\\cdot\\bm{E}^{\\ell}\\bm{z}^{\\ell}-\\eta\\cdot\\bm{\\sigma}\\big{(}[\\bm{C}^{\\ell}_{1}\\bm{z}^{\\ell},\\dots,\\bm{"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS1.SSS0.Px3.p1", "title": "Notice that the increment is constructed to emulate the gradient ascent for the rate reduction Œî ‚Äã R œµ \\Delta R_{\\epsilon} roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT . Hence by tr", "snippet": "Notice that the increment is constructed to emulate the gradient ascent for the rate reduction Œî ‚Äã R œµ \\Delta R_{\\epsilon} roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT . Hence by transforming the features iteratively via the above process, we expect the rate reduction to increase, as we will see in the experimental section. This iterative process, once converged say after L L italic_L iterations, gives the desired feature map f ‚Äã ( ùíô , ùúΩ ) f(\\bm{x},\\bm{\\theta}) italic_f ( bold_italic_x , bold_italic_Œ∏ ) on the input ùíô = ùíõ ‚Äã ‚Ä¶ ‚Äã ‚Ä¶ ‚Äã 0 \\bm{x}=\\bm{z}‚Ä¶‚Ä¶0 bold_italic_x = bold_ita"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS1.SSS0.Px3.p2", "title": "We summarize the training and evaluation of ReduNet in Algorithm 4.1 and Algorithm 4.2 , respectively. Notice that all parameters of the network are explicitly constructed layer by layer in a forward ", "snippet": "We summarize the training and evaluation of ReduNet in Algorithm 4.1 and Algorithm 4.2 , respectively. Notice that all parameters of the network are explicitly constructed layer by layer in a forward propagation fashion. The construction does not need any back propagation! The so-learned features can be directly used for classification, say via a nearest subspace classifier."}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmexample1.p1", "title": "To provide some intuition on how ReduNet transforms the features, we provide a simple example with mixed 3D Gaussians and visualize how the features are transformed in Figure 4.5 . Consider a mixture ", "snippet": "To provide some intuition on how ReduNet transforms the features, we provide a simple example with mixed 3D Gaussians and visualize how the features are transformed in Figure 4.5 . Consider a mixture of three Gaussian distributions in ‚Ñù 3 \\mathbb{R}^{3} blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT that is projected onto ùïä 2 \\mathbb{S}^{2} blackboard_S start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . We first generate data points for 3 classes: for k = 1 , 2 , 3 k=1,2,3 italic_k = 1 , 2 , 3 , ùëø k = [ ùíô k , 1 , ‚Ä¶ , ùíô k , m ] ‚àà ‚Ñù 3 √ó m \\bm{X}_{k}=[\\bm{x}_{k,1},\\ldots,\\bm{x}_{k,m}]\\in\\mat"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmexample1.p2", "title": "As shown in Figure 4.5 , we can observe that after the mapping f ‚Äã ( ‚ãÖ , ùúΩ ) f(\\cdot,\\bm{\\theta}) italic_f ( ‚ãÖ , bold_italic_Œ∏ ) , samples from the same class are highly compressed and converge to a s", "snippet": "As shown in Figure 4.5 , we can observe that after the mapping f ‚Äã ( ‚ãÖ , ùúΩ ) f(\\cdot,\\bm{\\theta}) italic_f ( ‚ãÖ , bold_italic_Œ∏ ) , samples from the same class are highly compressed and converge to a single cluster and the angle between two different clusters is approximately œÄ / 2 \\pi/2 italic_œÄ / 2 , which is well aligned with the optimal solution ùíÅ ‚ãÜ \\bm{Z}^{\\star} bold_italic_Z start_POSTSUPERSCRIPT ‚ãÜ end_POSTSUPERSCRIPT of the MCR 2 loss in ùïä 2 \\mathbb{S}^{2} blackboard_S start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . MCR 2 loss of features on different layers can be found in Figure 4.5 ( c"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmexample1.p3", "title": "‚ñ† \\blacksquare ‚ñ†", "snippet": "‚ñ† \\blacksquare ‚ñ†"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS2.p1", "title": "In the previous section, we derived the layer-wise architecture of a deep network, the ReduNet, using unrolled optimization for the rate reduction objective. Specifically, the compression term R œµ c ‚Äã", "snippet": "In the previous section, we derived the layer-wise architecture of a deep network, the ReduNet, using unrolled optimization for the rate reduction objective. Specifically, the compression term R œµ c ‚Äã ( ùíÅ ‚à£ ùö∑ ) R^{c}_{\\epsilon}(\\bm{Z}\\mid\\bm{\\Pi}) italic_R start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ‚à£ bold_Œ† ) in ( 4.1.1 ) is designed to compress representations from the same class. However, this formulation does not account for possible domain transformation or deformation of the input data. For instance, shifting an object"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS2.p2", "title": "For many clustering or classification tasks (such as object detection in images), we consider two samples as equivalent if they differ by certain classes of domain deformations or augmentations ùíØ = { ", "snippet": "For many clustering or classification tasks (such as object detection in images), we consider two samples as equivalent if they differ by certain classes of domain deformations or augmentations ùíØ = { œÑ } \\mathcal{T}=\\{\\tau\\} caligraphic_T = { italic_œÑ } . Hence, we are only interested in low-dimensional structures that are invariant to such deformations (i.e., ùíô ‚àà ‚Ñ≥ \\bm{x}\\in\\mathcal{M} bold_italic_x ‚àà caligraphic_M iff œÑ ‚Äã ( ùíô ) ‚àà ‚Ñ≥ \\tau(\\bm{x})\\in\\mathcal{M} italic_œÑ ( bold_italic_x ) ‚àà caligraphic_M for all œÑ ‚àà ùíØ \\tau\\in\\mathcal{T} italic_œÑ ‚àà caligraphic_T ), which are known to have sophist"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS2.p3", "title": "In many applications, such as serial data or imagery data, the semantic meaning (labels) of the data are invariant to certain transformations ùî§ ‚àà ùîæ \\mathfrak{g}\\in\\mathbb{G} fraktur_g ‚àà blackboard_G (", "snippet": "In many applications, such as serial data or imagery data, the semantic meaning (labels) of the data are invariant to certain transformations ùî§ ‚àà ùîæ \\mathfrak{g}\\in\\mathbb{G} fraktur_g ‚àà blackboard_G (for some group ùîæ \\mathbb{G} blackboard_G ) [ CW16b , ZKR+17 ] . For example, the meaning of an audio signal is invariant to shift in time; and the identity of an object in an image is invariant to translation in the image plane. Hence, we prefer the feature mapping f ‚Äã ( ùíô , ùúΩ ) f(\\bm{x},\\bm{\\theta}) italic_f ( bold_italic_x , bold_italic_Œ∏ ) is rigorously invariant to such transformations: Group "}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS2.p4", "title": "Now, we show that the MCR 2 principle is compatible with invariance in a natural and precise way: we only need to assign all transformed versions { ùíô ‚àò ùî§ ‚à£ ùî§ ‚àà ùîæ } \\{\\bm{x}\\circ\\mathfrak{g}\\mid\\mathfr", "snippet": "Now, we show that the MCR 2 principle is compatible with invariance in a natural and precise way: we only need to assign all transformed versions { ùíô ‚àò ùî§ ‚à£ ùî§ ‚àà ùîæ } \\{\\bm{x}\\circ\\mathfrak{g}\\mid\\mathfrak{g}\\in\\mathbb{G}\\} { bold_italic_x ‚àò fraktur_g ‚à£ fraktur_g ‚àà blackboard_G } into the same class as the data ùíô \\bm{x} bold_italic_x and map their features ùíõ \\bm{z} bold_italic_z all to the same subspace ùíÆ \\mathcal{S} caligraphic_S . Hence, all group equivariant information is encoded only inside the subspace, and any classifier defined on the resulting set of subspaces will be automatically invar"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS2.SSS0.Px1.p1", "title": "To classify one-dimensional data ùíô = [ x ‚Äã ( 0 ) , x ‚Äã ( 1 ) , ‚Ä¶ , x ‚Äã ( D ‚àí 1 ) ] ‚àà ‚Ñù D \\bm{x}=[x(0),x(1),\\ldots,x(D-1)]\\in\\mathbb{R}^{D} bold_italic_x = [ italic_x ( 0 ) , italic_x ( 1 ) , ‚Ä¶ , itali", "snippet": "To classify one-dimensional data ùíô = [ x ‚Äã ( 0 ) , x ‚Äã ( 1 ) , ‚Ä¶ , x ‚Äã ( D ‚àí 1 ) ] ‚àà ‚Ñù D \\bm{x}=[x(0),x(1),\\ldots,x(D-1)]\\in\\mathbb{R}^{D} bold_italic_x = [ italic_x ( 0 ) , italic_x ( 1 ) , ‚Ä¶ , italic_x ( italic_D - 1 ) ] ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT invariant under shifting, we take ùîæ \\mathbb{G} blackboard_G to be the group of all circular shifts. Each observation ùíô i \\bm{x}_{i} bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT generates a family { ùíô i ‚àò ùî§ | ùî§ ‚àà ùîæ } \\{\\bm{x}_{i}\\circ\\mathfrak{g}\\,|\\,\\mathfrak{g}\\in\\mathbb{G}\\} { bold_italic_x s"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS2.SSS0.Px1.p2", "title": "Notice that now the data covariance matrix: ùñºùóÇùóãùñº ‚Äã ( ùíÅ 1 ) ‚Äã ùñºùóÇùóãùñº ‚Äã ( ùíÅ 1 ) ‚ä§ \\displaystyle\\mathsf{circ}(\\bm{Z}^{1})\\mathsf{circ}(\\bm{Z}^{1})^{\\top} sansserif_circ ( bold_italic_Z start_POSTSUPERSCRIP", "snippet": "Notice that now the data covariance matrix: ùñºùóÇùóãùñº ‚Äã ( ùíÅ 1 ) ‚Äã ùñºùóÇùóãùñº ‚Äã ( ùíÅ 1 ) ‚ä§ \\displaystyle\\mathsf{circ}(\\bm{Z}^{1})\\mathsf{circ}(\\bm{Z}^{1})^{\\top} sansserif_circ ( bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) sansserif_circ ( bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT = \\displaystyle= = [ ùñºùóÇùóãùñº ‚Äã ( ùíõ 1 1 ) , ‚Ä¶ , ùñºùóÇùóãùñº ‚Äã ( ùíõ N 1 ) ] ‚Äã [ ùñºùóÇùóãùñº ‚Äã ( ùíõ 1 1 ) , ‚Ä¶ , ùñºùóÇùóãùñº ‚Äã ( ùíõ N 1 ) ] ‚ä§ \\displaystyle\\left[\\mathsf{circ}(\\bm{z}_{1}^{1}),\\dots,\\mathsf{circ}(\\bm{z}_{N}^{1})\\right]\\left[\\mathsf{circ}(\\bm{z}_{1}^{1}),\\dots,\\mathsf{c"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmproposition1.p1", "title": "The matrix ùë¨ 1 = Œ± ‚Äã ( ùë∞ + Œ± ‚Äã ùñºùóÇùóãùñº ‚Äã ( ùíÅ 1 ) ‚Äã ùñºùóÇùóãùñº ‚Äã ( ùíÅ 1 ) ‚ä§ ) ‚àí 1 \\bm{E}^{1}=\\alpha\\big{(}\\bm{I}+\\alpha\\mathsf{circ}(\\bm{Z}^{1})\\mathsf{circ}(\\bm{Z}^{1})^{\\top}\\big{)}^{-1} bold_italic_E start_PO", "snippet": "The matrix ùë¨ 1 = Œ± ‚Äã ( ùë∞ + Œ± ‚Äã ùñºùóÇùóãùñº ‚Äã ( ùíÅ 1 ) ‚Äã ùñºùóÇùóãùñº ‚Äã ( ùíÅ 1 ) ‚ä§ ) ‚àí 1 \\bm{E}^{1}=\\alpha\\big{(}\\bm{I}+\\alpha\\mathsf{circ}(\\bm{Z}^{1})\\mathsf{circ}(\\bm{Z}^{1})^{\\top}\\big{)}^{-1} bold_italic_E start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT = italic_Œ± ( bold_italic_I + italic_Œ± sansserif_circ ( bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) sansserif_circ ( bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT (4.1.18) is a circulant matrix and represents a circular convolution: ùë¨ 1 ‚Äã ùíõ = "}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS2.SSS0.Px1.p3", "title": "Not only do the first-layer parameters ùë¨ 1 \\bm{E}^{1} bold_italic_E start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT and ùë™ k 1 \\bm{C}^{1}_{k} bold_italic_C start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start", "snippet": "Not only do the first-layer parameters ùë¨ 1 \\bm{E}^{1} bold_italic_E start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT and ùë™ k 1 \\bm{C}^{1}_{k} bold_italic_C start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT of the ReduNet become circulant convolutions but also the next-layer features remain circulant matrices. That is, the incremental feature transform in ( 4.1.13 ) applied to all shifted versions of a ùíõ 1 ‚àà ‚Ñù d \\bm{z}^{1}\\in\\mathbb{R}^{d} bold_italic_z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRI"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS2.SSS0.Px2.p1", "title": "There is one problem though: In general, the set of all circular permutations of a vector ùíõ \\bm{z} bold_italic_z gives a full-rank matrix. That is, the d d italic_d ‚Äúaugmented‚Äù features associated wit", "snippet": "There is one problem though: In general, the set of all circular permutations of a vector ùíõ \\bm{z} bold_italic_z gives a full-rank matrix. That is, the d d italic_d ‚Äúaugmented‚Äù features associated with each sample (hence each class) typically already span the entire space ‚Ñù d \\mathbb{R}^{d} blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT . For instance, all shifted versions of a delta function Œ¥ ‚Äã ( d ) \\delta(d) italic_Œ¥ ( italic_d ) can generate any other signal as their (dense) weighted superposition. The MCR 2 objective ( 3.4.12 ) will not be able to distinguish classes as "}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS2.SSS0.Px2.p2", "title": "One natural remedy is to improve the separability of the data by ‚Äúlifting‚Äù the original signal to a higher dimensional space, e.g., by taking their responses to multiple, filters ùíå 1 , ‚Ä¶ , ùíå C ‚àà ‚Ñù d \\", "snippet": "One natural remedy is to improve the separability of the data by ‚Äúlifting‚Äù the original signal to a higher dimensional space, e.g., by taking their responses to multiple, filters ùíå 1 , ‚Ä¶ , ùíå C ‚àà ‚Ñù d \\bm{k}_{1},\\ldots,\\bm{k}_{C}\\in\\mathbb{R}^{d} bold_italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_k start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT : ùíõ ‚Äã [ c ] = ùíå c ‚äõ ùíô = ùñºùóÇùóãùñº ‚Äã ( ùíå c ) ‚Äã ùíô ‚àà ‚Ñù d , c = 1 , ‚Ä¶ , C . \\bm{z}[c]=\\bm{k}_{c}\\circledast\\bm{x}=\\mathsf{circ}(\\bm{k}_{c})\\bm{x}\\in\\mathbb{R}^{d},\\quad c=1,\\ldo"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS2.SSS0.Px2.p3", "title": "One way of resolving this conflict is to leverage additional structure within each class, in the form of sparsity : signals within each class are not generated as an arbitrary linear superposition of ", "snippet": "One way of resolving this conflict is to leverage additional structure within each class, in the form of sparsity : signals within each class are not generated as an arbitrary linear superposition of some base atoms (or motifs), but only sparse combinations of them and their shifted versions, as shown in Figure 4.7 . More precisely, let ùë´ k = [ ùíÖ k , 1 , ‚Ä¶ , ùíÖ k , c ] \\bm{D}_{k}=[\\bm{d}_{k,1},\\ldots,\\bm{d}_{k,c}] bold_italic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = [ bold_italic_d start_POSTSUBSCRIPT italic_k , 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_d start_POSTSUBSCRIPT italic_k , it"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS2.SSS0.Px2.p4", "title": "Nevertheless, for tasks such as classification, we are not necessarily interested in the precise optimal dictionary nor the precise sparse code for each individual signal. We are mainly interested if ", "snippet": "Nevertheless, for tasks such as classification, we are not necessarily interested in the precise optimal dictionary nor the precise sparse code for each individual signal. We are mainly interested if collectively the set of sparse codes for each class are adequately separable from those of other classes. Under the assumption of the sparse generative model, if the convolution kernels { ùíå c } c = 1 C \\{\\bm{k}_{c}\\}_{c=1}^{C} { bold_italic_k start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_c = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_C end_POSTSUPERSCRIPT match "}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS2.SSS0.Px2.p5", "title": "Therefore, in our framework, to a large extent the number of channels (or the width of the network) truly plays the role as the statistical resource whereas the number of layers (the depth of the netw", "snippet": "Therefore, in our framework, to a large extent the number of channels (or the width of the network) truly plays the role as the statistical resource whereas the number of layers (the depth of the network) plays the role as the computational resource . The theory of compressive sensing precisely characterizes how many measurements are needed in order to preserve the intrinsic low-dimensional structures (including separability) of the data [ WM21 ] ."}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS2.SSS0.Px2.p6", "title": "The multi-channel responses ùíõ ¬Ø \\bar{\\bm{z}} over¬Ø start_ARG bold_italic_z end_ARG should be sparse. So to approximate the sparse code ùíõ ¬Ø \\bar{\\bm{z}} over¬Ø start_ARG bold_italic_z end_ARG , we may t", "snippet": "The multi-channel responses ùíõ ¬Ø \\bar{\\bm{z}} over¬Ø start_ARG bold_italic_z end_ARG should be sparse. So to approximate the sparse code ùíõ ¬Ø \\bar{\\bm{z}} over¬Ø start_ARG bold_italic_z end_ARG , we may take an entry-wise sparsity-promoting nonlinear thresholding , say ùùâ ‚Äã ( ‚ãÖ ) \\bm{\\tau}(\\cdot) bold_italic_œÑ ( ‚ãÖ ) , on the above filter outputs by setting low (say absolute value below œµ \\epsilon italic_œµ ) or negative responses to be zero: ùíõ ¬Ø ‚âê ùùâ ‚Äã ( [ ùñºùóÇùóãùñº ‚Äã ( ùíå 1 ) ‚Äã ùíô , ‚Ä¶ , ùñºùóÇùóãùñº ‚Äã ( ùíå C ) ‚Äã ùíô ] ‚ä§ ) ‚àà ‚Ñù C √ó d . \\bar{\\bm{z}}\\doteq\\bm{\\tau}\\left(\\big{[}\\mathsf{circ}(\\bm{k}_{1})\\bm{x},\\ldots,\\math"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS2.SSS0.Px2.p7", "title": "The ReduNet constructed from circulant version of these multi-channel features ùíÅ ¬Ø ‚âê [ ùíõ ¬Ø 1 , ‚Ä¶ , ùíõ ¬Ø N ] ‚àà ‚Ñù C √ó d √ó N \\bar{\\bm{Z}}\\doteq[\\bar{\\bm{z}}_{1},\\ldots,\\bar{\\bm{z}}_{N}]\\in\\mathbb{R}^{C\\ti", "snippet": "The ReduNet constructed from circulant version of these multi-channel features ùíÅ ¬Ø ‚âê [ ùíõ ¬Ø 1 , ‚Ä¶ , ùíõ ¬Ø N ] ‚àà ‚Ñù C √ó d √ó N \\bar{\\bm{Z}}\\doteq[\\bar{\\bm{z}}_{1},\\ldots,\\bar{\\bm{z}}_{N}]\\in\\mathbb{R}^{C\\times d\\times N} over¬Ø start_ARG bold_italic_Z end_ARG ‚âê [ over¬Ø start_ARG bold_italic_z end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , over¬Ø start_ARG bold_italic_z end_ARG start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ] ‚àà blackboard_R start_POSTSUPERSCRIPT italic_C √ó italic_d √ó italic_N end_POSTSUPERSCRIPT , i.e., ùñºùóÇùóãùñº ‚Äã ( ùíÅ ¬Ø ) ‚âê [ ùñºùóÇùóãùñº ‚Äã ( ùíõ ¬Ø 1 ) , ‚Ä¶ , ùñºùóÇùóãùñº ‚Äã ( ùíõ ¬Ø N ) ] ‚àà ‚Ñù d ‚Äã C √ó d ‚Äã"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmproposition2.p1", "title": "The matrix ùë¨ ¬Ø ‚âê Œ± ‚Äã ( ùë∞ + Œ± ‚Äã ùñºùóÇùóãùñº ‚Äã ( ùíÅ ¬Ø ) ‚Äã ùñºùóÇùóãùñº ‚Äã ( ùíÅ ¬Ø ) ‚ä§ ) ‚àí 1 \\bar{\\bm{E}}\\doteq\\alpha\\left(\\bm{I}+\\alpha\\,\\mathsf{circ}(\\bar{\\bm{Z}})\\mathsf{circ}(\\bar{\\bm{Z}})^{\\top}\\right)^{-1} over¬Ø star", "snippet": "The matrix ùë¨ ¬Ø ‚âê Œ± ‚Äã ( ùë∞ + Œ± ‚Äã ùñºùóÇùóãùñº ‚Äã ( ùíÅ ¬Ø ) ‚Äã ùñºùóÇùóãùñº ‚Äã ( ùíÅ ¬Ø ) ‚ä§ ) ‚àí 1 \\bar{\\bm{E}}\\doteq\\alpha\\left(\\bm{I}+\\alpha\\,\\mathsf{circ}(\\bar{\\bm{Z}})\\mathsf{circ}(\\bar{\\bm{Z}})^{\\top}\\right)^{-1} over¬Ø start_ARG bold_italic_E end_ARG ‚âê italic_Œ± ( bold_italic_I + italic_Œ± sansserif_circ ( over¬Ø start_ARG bold_italic_Z end_ARG ) sansserif_circ ( over¬Ø start_ARG bold_italic_Z end_ARG ) start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT (4.1.27) is block circulant, i.e., ùë¨ ¬Ø = [ ùë¨ ¬Ø 1 , 1 ‚ãØ ùë¨ ¬Ø 1 , C ‚ãÆ ‚ã± ‚ãÆ ùë¨ ¬Ø C , 1 ‚ãØ ùë¨ ¬Ø C , C ] ‚àà ‚Ñù d ‚Äã C √ó d ‚Äã C , \\bar{\\bm{E}}="}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS2.SSS0.Px2.p8", "title": "From Proposition 4.2 , shift invariant ReduNet is a deep convolutional network for multi-channel 1D signals by construction. Notice that even if the initial lifting kernels are separated ( 4.1.26 ), t", "snippet": "From Proposition 4.2 , shift invariant ReduNet is a deep convolutional network for multi-channel 1D signals by construction. Notice that even if the initial lifting kernels are separated ( 4.1.26 ), the matrix inverse in ( 4.1.27 ) for computing ùë¨ ¬Ø \\bar{\\bm{E}} over¬Ø start_ARG bold_italic_E end_ARG (similarly for ùë™ k ¬Ø \\bar{\\bm{C}_{k}} over¬Ø start_ARG bold_italic_C start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG ) introduces ‚Äúcross talk‚Äù among all C C italic_C channels. Hence, these multi-channel convolutions in general are not depth-wise separable, unlike the Xception nets [ Cho17 ] t"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmremark2.p1", "title": "The calculation of ùë¨ ¬Ø \\bar{\\bm{E}} over¬Ø start_ARG bold_italic_E end_ARG in ( 4.1.27 ) requires inverting a matrix of size d ‚Äã C √ó d ‚Äã C dC\\times dC italic_d italic_C √ó italic_d italic_C , which in g", "snippet": "The calculation of ùë¨ ¬Ø \\bar{\\bm{E}} over¬Ø start_ARG bold_italic_E end_ARG in ( 4.1.27 ) requires inverting a matrix of size d ‚Äã C √ó d ‚Äã C dC\\times dC italic_d italic_C √ó italic_d italic_C , which in general has complexity O ‚Äã ( d 3 ‚Äã C 3 ) O(d^{3}C^{3}) italic_O ( italic_d start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT italic_C start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ) . Nevertheless, by using the fact that a circulant matrix can be diagonalized by the Discrete Fourier Transform (DFT) matrix, the complexity can be significantly reduced. As shown in [ CYY+22 ] , to compute ùë¨ ¬Ø \\bar{\\bm{E}} ove"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.SS2.SSS0.Px3.p1", "title": "Following the above derivation, we see that in order to find a linear discriminative representation (LDR) for multiple classes of signals/images that is invariant to translation, sparse coding, a mult", "snippet": "Following the above derivation, we see that in order to find a linear discriminative representation (LDR) for multiple classes of signals/images that is invariant to translation, sparse coding, a multi-layer architecture with multi-channel convolutions, different nonlinear activation, and spectrum computing all become necessary components for achieving the objective effectively and efficiently. Figure 4.9 illustrates the overall process of learning such a representation via invariant rate reduction on the input sparse codes."}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmexample2.p1", "title": "We next provide an empirical performance of the ReduNet on learning rotation invariant features on the real 10-class MNIST dataset. We impose a polar grid on the image ùíô ‚àà ‚Ñù H √ó W \\bm{x}\\in\\mathbb{R}^", "snippet": "We next provide an empirical performance of the ReduNet on learning rotation invariant features on the real 10-class MNIST dataset. We impose a polar grid on the image ùíô ‚àà ‚Ñù H √ó W \\bm{x}\\in\\mathbb{R}^{H\\times W} bold_italic_x ‚àà blackboard_R start_POSTSUPERSCRIPT italic_H √ó italic_W end_POSTSUPERSCRIPT , with its geometric center being the center of the 2D polar grid (as illustrated in Figure 4.10 ). For each radius r i r_{i} italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , i ‚àà [ C ] i\\in[C] italic_i ‚àà [ italic_C ] , we can sample Œì \\Gamma roman_Œì pixels with respect to each angle Œ≥ l "}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmexample2.p2", "title": "To evaluate the learned representation, each training sample is augmented by 20 of its rotated version, each shifted with stride=10. We compute the cosine similarities among the m √ó 20 m\\times 20 ital", "snippet": "To evaluate the learned representation, each training sample is augmented by 20 of its rotated version, each shifted with stride=10. We compute the cosine similarities among the m √ó 20 m\\times 20 italic_m √ó 20 augmented training inputs ùëø rotation \\bm{X}_{\\text{rotation}} bold_italic_X start_POSTSUBSCRIPT rotation end_POSTSUBSCRIPT and the results are shown in Figure 4.11 ( a ). We compare the cosine similarities among the learned features of all the augmented versions, i.e., ùíÅ ¬Ø rotation \\bar{\\bm{Z}}_{\\text{rotation}} over¬Ø start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT rotation end_POSTS"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmexample2.p3", "title": "‚ñ† \\blacksquare ‚ñ†", "snippet": "‚ñ† \\blacksquare ‚ñ†"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.p1", "title": "As we have seen in the previous section, we use the problem of classification to provide a rigorous interpretation for main architectural characteristics of popular deep networks such as the ResNet an", "snippet": "As we have seen in the previous section, we use the problem of classification to provide a rigorous interpretation for main architectural characteristics of popular deep networks such as the ResNet and the CNN: each layer of such networks can be viewed as to imitate a gradient step which increases the rate reduction (or information gain) objective. This perspective also leads to a somewhat surprising fact: the the parameters and operators of the layers of such a deep network, the ReduNet, can be computed in a purely forward fashion."}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.p2", "title": "Despite the theoretical and conceptual importance of the ReduNet, several factors limit it from being very practical. First, as we have discussed in the above, the computational cost of computing the ", "snippet": "Despite the theoretical and conceptual importance of the ReduNet, several factors limit it from being very practical. First, as we have discussed in the above, the computational cost of computing the matrix operators in each layer in a forward fashion can be very high. Second, the so-computed operators may not be so effective in optimizing the objective and it might take thousands of iterations (hence layers). As we have seen in Section 2.3.3 for LISTA, these two issues can be addressed by allowing to optimize those operators and make them learnable via back-propagation. 12 12 12 Or, perhaps, "}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.p3", "title": "The supervised classification setting in which the ReduNet was derived is also somewhat limiting. In practice, an image might not belong to a single class as it may contain multiple objects. Hence it ", "snippet": "The supervised classification setting in which the ReduNet was derived is also somewhat limiting. In practice, an image might not belong to a single class as it may contain multiple objects. Hence it would be more general to assume that different regions of the image belong to different low-dimensional models (say a Gaussian or a subspace). As we will see, such a generalization would lead to a both simple and general architecture which unifies the rate reduction and the denoising operations that we have seen in the previous chapter. Moreover, the so-obtained architecture resembles the popular "}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.SS1.p1", "title": "We consider a general learning setup associated with real-world signals. Let ùëø = [ ùíô 1 , ‚Ä¶ , ùíô N ] ‚àà ‚Ñù D √ó N \\bm{X}=\\begin{bmatrix}\\bm{x}_{1},\\dots,\\bm{x}_{N}\\end{bmatrix}\\in\\mathbb{R}^{D\\times N} bol", "snippet": "We consider a general learning setup associated with real-world signals. Let ùëø = [ ùíô 1 , ‚Ä¶ , ùíô N ] ‚àà ‚Ñù D √ó N \\bm{X}=\\begin{bmatrix}\\bm{x}_{1},\\dots,\\bm{x}_{N}\\end{bmatrix}\\in\\mathbb{R}^{D\\times N} bold_italic_X = [ start_ARG start_ROW start_CELL bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D √ó italic_N end_POSTSUPERSCRIPT denote random variables representing our data source. In vision tasks, each ùíô i ‚àà ‚Ñù D \\bm{x}_{i}\\in\\mathbb{R}^{D} bold_italic_x s"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmremark3.p1", "title": "In transformers, each input sample is typically converted into a sequence of tokens . A token is a basic unit of information derived from the raw input: in natural language processing, tokens are typi", "snippet": "In transformers, each input sample is typically converted into a sequence of tokens . A token is a basic unit of information derived from the raw input: in natural language processing, tokens are typically words or subwords; in computer vision, they correspond to image patches; and in other modalities, they may represent time steps, spatial locations, or other domain-specific units. A token embedding is a continuous vector representation of a token that serves as the input to a transformer. It maps each token to a point in a high-dimensional space, enabling the model to process symbolic inputs"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.SS1.SSS0.Px1.p1", "title": "Following the framework of rate reduction Section 4.1 , we contend that the goal of representation learning is to find a feature mapping f : ùëø ‚àà ‚Ñù D √ó N ‚Üí ùíÅ ‚àà ‚Ñù d √ó N f\\colon\\bm{X}\\in\\mathbb{R}^{D\\tim", "snippet": "Following the framework of rate reduction Section 4.1 , we contend that the goal of representation learning is to find a feature mapping f : ùëø ‚àà ‚Ñù D √ó N ‚Üí ùíÅ ‚àà ‚Ñù d √ó N f\\colon\\bm{X}\\in\\mathbb{R}^{D\\times N}\\to\\bm{Z}\\in\\mathbb{R}^{d\\times N} italic_f : bold_italic_X ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D √ó italic_N end_POSTSUPERSCRIPT ‚Üí bold_italic_Z ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d √ó italic_N end_POSTSUPERSCRIPT which transforms input tokens { ùíô i } i = 1 N ‚äÇ ‚Ñù D \\{\\bm{x}_{i}\\}_{i=1}^{N}\\subset\\mathbb{R}^{D} { bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } st"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmremark4.p1", "title": "The expression ( 4.2.3 ) for the coding rate can be viewed as a generalization of the coding rate R œµ c R_{\\epsilon}^{c} italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT i", "snippet": "The expression ( 4.2.3 ) for the coding rate can be viewed as a generalization of the coding rate R œµ c R_{\\epsilon}^{c} italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT used in the original rate reduction objective ( 3.4.13 ). In particular, the original objective is defined with respect to a set of known membership labels { ùö∑ k } \\{\\bm{\\Pi}_{k}\\} { bold_Œ† start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } specific to the particular data realization ùëø \\bm{X} bold_italic_X . In contrast, the current objective is defined with respect to su"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.SS1.SSS0.Px2.p1", "title": "Note that the rate reduction objective ( 4.2.1 ) is invariant to arbitrary joint rotations of the representations and subspaces. In particular, optimizing the rate reduction objective may not naturall", "snippet": "Note that the rate reduction objective ( 4.2.1 ) is invariant to arbitrary joint rotations of the representations and subspaces. In particular, optimizing the rate reduction objective may not naturally lead to axis-aligned (i.e., sparse ) representations. For instance, consider the three sets of learned representations in Figure 4.12 . The coding rate reduction increases from (a) to (b), but because it is invariant under rotations, remains the same from (b) to (c). Therefore, we would like to transform the representations (and their supporting subspaces) so that the representations ùíÅ \\bm{Z} bo"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.SS1.SSS0.Px2.p2", "title": "In practice, the ‚Ñì 0 \\ell_{0} roman_‚Ñì start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT norm is often relaxed to the ‚Ñì 1 \\ell_{1} roman_‚Ñì start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT norm to improve computational tr", "snippet": "In practice, the ‚Ñì 0 \\ell_{0} roman_‚Ñì start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT norm is often relaxed to the ‚Ñì 1 \\ell_{1} roman_‚Ñì start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT norm to improve computational traceability and enable convex optimization techniques [ WM22 ] . Motivated by this, we relax Problem ( 4.2.4 ) accordingly, leading to a formulation that remains faithful to the original sparsity objective while being more amenable to efficient algorithms as follow: max f ‚àà ‚Ñ± ‚Å° [ Œî ‚Äã R œµ ‚Äã ( ùíÅ ‚à£ ùëº [ K ] ) ‚àí Œª ‚Äã ‚Äñ ùíÅ ‚Äñ 1 ] s.t. ‚Äã ùíÅ = f ‚Äã ( ùëø ) , \\displaystyle\\max_{f\\in\\mathcal{F}}\\ [\\Delta R_{\\epsilo"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.SS1.SSS0.Px3.p1", "title": "Although easy to state, each term in the above objective is computationally challenging to optimize [ WM22 ] . Hence it is natural to adopt an approximation approach that realizes the global transform", "snippet": "Although easy to state, each term in the above objective is computationally challenging to optimize [ WM22 ] . Hence it is natural to adopt an approximation approach that realizes the global transformation f f italic_f to optimize ( 4.2.4 ) through a concatenation of multiple, say L L italic_L , simple incremental and local operations f ‚Ñì f^{\\ell} italic_f start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT that push the representation distribution towards the desired parsimonious model distribution: f : ùëø = ùíÅ 0 ‚Üí f 0 ùíÅ 1 ‚Üí ‚ãØ ‚Üí ùíÅ ‚Ñì ‚Üí f ‚Ñì ùíÅ ‚Ñì + 1 ‚Üí ‚ãØ ‚Üí f L ‚àí 1 ùíÅ L = ùíÅ , f\\colon\\bm{X}=\\bm{Z}^{0}\\x"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmremark5.p1", "title": "In contrast to other unrolled optimization approaches such as the ReduNet (see Section 4.1 ), we explicitly model the distribution of ùíÅ ‚Ñì \\bm{Z}^{\\ell} bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì end_", "snippet": "In contrast to other unrolled optimization approaches such as the ReduNet (see Section 4.1 ), we explicitly model the distribution of ùíÅ ‚Ñì \\bm{Z}^{\\ell} bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT at each layer, say as a mixture of linear subspaces or sparsely generated from a dictionary. The model parameters are learned from data (say via backward propagation with end-to-end training). This separation between forward ‚Äúoptimization‚Äù and backward ‚Äúlearning‚Äù clarifies the mathematical role of each layer as an operator that transforms the distribution of its input, whereas the "}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.SS1.SSS0.Px3.p2", "title": "Now, we show how to derive these incremental and local operations through an unrolled optimization perspective to solve Problem ( 4.2.5 ). Once we decide on using an incremental approach to optimizing", "snippet": "Now, we show how to derive these incremental and local operations through an unrolled optimization perspective to solve Problem ( 4.2.5 ). Once we decide on using an incremental approach to optimizing Problem ( 4.2.5 ), there are a variety of possible choices to achieve the optimization. Given a model for ùíÅ ‚Ñì \\bm{Z}^{\\ell} bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT , say a mixture of subspaces ùëº [ K ] \\bm{U}_{[K]} bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT , we opt for a two-step alternating minimization method with a strong conceptual basis. First, w"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.SS1.SSS0.Px4.p1", "title": "For the first step ( 4.2.7 ), the gradient of the coding rate ‚àá ùíÅ R œµ c \\nabla_{\\bm{Z}}R^{c}_{\\epsilon} ‚àá start_POSTSUBSCRIPT bold_italic_Z end_POSTSUBSCRIPT italic_R start_POSTSUPERSCRIPT italic_c en", "snippet": "For the first step ( 4.2.7 ), the gradient of the coding rate ‚àá ùíÅ R œµ c \\nabla_{\\bm{Z}}R^{c}_{\\epsilon} ‚àá start_POSTSUBSCRIPT bold_italic_Z end_POSTSUBSCRIPT italic_R start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT is costly to compute, as it involves K K italic_K separate matrix inverses, one for each of the K K italic_K subspaces with basis ùëº k ‚Ñì \\bm{U}_{k}^{\\ell} bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT : ‚àá ùíÅ R œµ c ‚Äã ( ùíÅ ‚à£ ùëº [ K ] ) = p N ‚Äã œµ 2 ‚Äã ‚àë k = 1 K ùëº k ‚Äã ùëº k ‚ä§ ‚Äã ùíÅ "}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmremark6.p1", "title": "The SSA operator in ( 4.2.12 ) resembles the attention operator in a typical transformer [ VSP+17 ] , except that here the linear operators of value, key, and query are all set to be the same as the s", "snippet": "The SSA operator in ( 4.2.12 ) resembles the attention operator in a typical transformer [ VSP+17 ] , except that here the linear operators of value, key, and query are all set to be the same as the subspace basis, i.e., ùëΩ k = ùë≤ k = ùë∏ k = ùëº k ‚àó \\bm{V}_{k}=\\bm{K}_{k}=\\bm{Q}_{k}=\\bm{U}_{k}^{*} bold_italic_V start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = bold_italic_K start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = bold_italic_Q start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT . Hence, we n"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.SS1.SSS0.Px5.p1", "title": "For the second step of alternating minimization, we need to minimize Œª ‚Äã ‚Äñ ùíÅ ‚Äñ 1 ‚àí R œµ ‚Äã ( ùíÅ ) \\lambda\\|\\bm{Z}\\|_{1}-R_{\\epsilon}(\\bm{Z}) italic_Œª ‚à• bold_italic_Z ‚à• start_POSTSUBSCRIPT 1 end_POSTSUBSC", "snippet": "For the second step of alternating minimization, we need to minimize Œª ‚Äã ‚Äñ ùíÅ ‚Äñ 1 ‚àí R œµ ‚Äã ( ùíÅ ) \\lambda\\|\\bm{Z}\\|_{1}-R_{\\epsilon}(\\bm{Z}) italic_Œª ‚à• bold_italic_Z ‚à• start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT - italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ) . Note that the gradient ‚àá R œµ ‚Äã ( ùíÅ ) \\nabla R_{\\epsilon}(\\bm{Z}) ‚àá italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ) involves a matrix inverse, and thus naive proximal gradient (see Section A.1.3 ) to optimize this problem becomes intractable on large-scale problems. We therefore take a diffe"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.SS2.p1", "title": "We now design a white-box transformer architecture, named the Coding RATE Transformer ( crate ), by unrolling the above updates. By combining the above two steps ( 4.2.14 ) and ( 4.2.18 ): 1. Local co", "snippet": "We now design a white-box transformer architecture, named the Coding RATE Transformer ( crate ), by unrolling the above updates. By combining the above two steps ( 4.2.14 ) and ( 4.2.18 ): 1. Local compression of tokens within a sample towards a mixture-of-subspace structure, leading to the multi-head subspace self-attention block ‚Äì MSSA ; 2. Global sparsification of token sets across all samples through sparse coding, leading to the sparsification block ‚Äì ISTA ; we can get the following rate-reduction-based transformer layer, illustrated in Figure 4.13 , ùíÅ ‚Ñì + 1 / 2 ‚âê ùíÅ ‚Ñì + MSSA ‚Äã ( ùíÅ ‚Ñì ‚à£ ùëº ["}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.I1.i1.p1", "title": "Local compression of tokens within a sample towards a mixture-of-subspace structure, leading to the multi-head subspace self-attention block ‚Äì MSSA ;", "snippet": "Local compression of tokens within a sample towards a mixture-of-subspace structure, leading to the multi-head subspace self-attention block ‚Äì MSSA ;"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.I1.i2.p1", "title": "Global sparsification of token sets across all samples through sparse coding, leading to the sparsification block ‚Äì ISTA ;", "snippet": "Global sparsification of token sets across all samples through sparse coding, leading to the sparsification block ‚Äì ISTA ;"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmremark7.p1", "title": "In contrast to other unrolled optimization approaches such as the ReduNet [ CYY+22 ] , we explicitly model the distribution of each ùíÅ ‚Ñì \\bm{Z}^{\\ell} bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì end_PO", "snippet": "In contrast to other unrolled optimization approaches such as the ReduNet [ CYY+22 ] , we explicitly model the distribution of each ùíÅ ‚Ñì \\bm{Z}^{\\ell} bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT and ùíÅ ‚Ñì + 1 / 2 \\bm{Z}^{\\ell+1/2} bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì + 1 / 2 end_POSTSUPERSCRIPT at each layer, either by a mixture of linear subspaces or sparsely generated from a dictionary. We introduced the interpretation that at each layer ‚Ñì \\ell roman_‚Ñì , the learned bases for the subspaces ùëº [ K ] ‚Ñì \\bm{U}_{[K]}^{\\ell} bold_italic_U start_POSTSUBSCRIPT [ italic_K ] en"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmremark7.p2", "title": "Hence, our methodology features a clear conceptual separation between forward ‚Äúoptimization‚Äù and backward ‚Äúlearning‚Äù for the so-derived white-box deep neural network. Namely, in its forward pass, we i", "snippet": "Hence, our methodology features a clear conceptual separation between forward ‚Äúoptimization‚Äù and backward ‚Äúlearning‚Äù for the so-derived white-box deep neural network. Namely, in its forward pass, we interpret each layer as an operator which, conditioned on a learned model (i.e., a codebook) for the distribution of its input, transforms this distribution towards a more parsimonious representation. In its backward propagation, the codebook of this model, for the distribution of the input to each layer, is updated to better fit a certain (supervised) input-output relationship, as illustrated in F"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.SS2.p2", "title": "We now present the empirical performance of the proposed networks crate by measuring their top-1 classification accuracy on ImageNet-1K as well as transfer learning performance on several widely used ", "snippet": "We now present the empirical performance of the proposed networks crate by measuring their top-1 classification accuracy on ImageNet-1K as well as transfer learning performance on several widely used downstream datasets. We summarize the results in Table 4.1 . The transfer learning methodology is to fine-tune using cross-entropy loss initializing from the pre-trained networks. As the designed white-box transformer architecture leverages parameter sharing in both the attention block ( MSSA ) and the nonlinearity block ( ISTA ), the crate -Base model (22.80 million) has a similar number of param"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.p1", "title": "So far, we wish that we have provided compelling evidence that the role of (popular) deep networks is to realize certain optimization algorithms for minimizing the coding rate (or maximizing the infor", "snippet": "So far, we wish that we have provided compelling evidence that the role of (popular) deep networks is to realize certain optimization algorithms for minimizing the coding rate (or maximizing the information gain) of the learned representations. However, readers who are familiar with optimization methods might have noticed that the above architectures (the ReduNet or the CRATE) correspond to rather basic optimization techniques. They may have plenty of room for improvement in efficiency or effectiveness. Moreover, if we believe the proposed theoretical framework for interpreting deep networks i"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.SS1.p1", "title": "In this subsection, we propose a minimalistic transformer architecture consisting of interpretable layers based on the MSSA operator. To derive a fully interpretable transformer architecture with only", "snippet": "In this subsection, we propose a minimalistic transformer architecture consisting of interpretable layers based on the MSSA operator. To derive a fully interpretable transformer architecture with only necessary components, we contend that the goal of representation learning is to compress a set of noisy initial token representations towards a mixture of low-dimensional subspaces. Here, we assume that the initial token representations ùíÅ ( 1 ) \\bm{Z}^{(1)} bold_italic_Z start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT are sampled from a mixture of low-rank Gaussians perturbed by noise as follows:"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmdefinition1.p1", "title": "Let C 1 , ‚Ä¶ , C K C_{1},\\dots,C_{K} italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , italic_C start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT be a partition of the index set [ N ] [N] [ italic_N ", "snippet": "Let C 1 , ‚Ä¶ , C K C_{1},\\dots,C_{K} italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , italic_C start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT be a partition of the index set [ N ] [N] [ italic_N ] and ùëº k ‚àà ùí™ d √ó p k \\bm{U}_{k}\\in\\mathcal{O}^{d\\times p_{k}} bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ‚àà caligraphic_O start_POSTSUPERSCRIPT italic_d √ó italic_p start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT denote the orthonormal basis of the k k italic_k -th subspace for each k ‚àà [ K ] k\\in[K] italic_k ‚àà [ italic_K ] . We say that the token representation"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.SS1.p2", "title": "This model serves as an idealized framework for approximating token representations in real-world pretrained LLMs. It assumes that the token representations are sampled from a mixture of multiple low-", "snippet": "This model serves as an idealized framework for approximating token representations in real-world pretrained LLMs. It assumes that the token representations are sampled from a mixture of multiple low-rank Gaussian distributions with noise. Under this model, the goal of representation learning is to compress a set of noisy initial token presentations into the corresponding subspace. In addition, this model aligns well with two well-established hypotheses about the structure of token representations in pretrained large language models: the ‚Äúlinear representation hypothesis‚Äù [ JRR+24 , PCV24 ] an"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmremark8.p1", "title": "The linear representation hypothesis posits that token representations in LLMs lie in low-dimensional linear subspaces that encode semantic features. Similarly, the superposition hypothesis suggests t", "snippet": "The linear representation hypothesis posits that token representations in LLMs lie in low-dimensional linear subspaces that encode semantic features. Similarly, the superposition hypothesis suggests that these representations can be approximately expressed as a sparse linear combination of these feature vectors. In Definition 4.1 , each basis ùëº k \\bm{U}_{k} bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT of the subspaces can be interpreted as a set of semantic features, where each feature corresponds to a specific aspect of the token‚Äôs meaning. Token representations are then appro"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.SS1.SSS0.Px1.p1", "title": "Now, we show that the MSSA operator (see ( 4.2.13 )) can incrementally denoise token representations generated from the above model. Spefically, we consider for each ‚Ñì = 1 , ‚Ä¶ , L \\ell=1,\\dots,L roman", "snippet": "Now, we show that the MSSA operator (see ( 4.2.13 )) can incrementally denoise token representations generated from the above model. Spefically, we consider for each ‚Ñì = 1 , ‚Ä¶ , L \\ell=1,\\dots,L roman_‚Ñì = 1 , ‚Ä¶ , italic_L , ùíÅ ( ‚Ñì + 1 ) = ùíÅ ( ‚Ñì ) + Œ∑ ‚Äã ‚àë k = 1 K ùëº k ‚Äã ùëº k T ‚Äã ùíÅ ( ‚Ñì ) ‚Äã œÜ ‚Äã ( ùíÅ ( ‚Ñì ) T ‚Äã ùëº k ‚Äã ùëº k T ‚Äã ùíÅ ( ‚Ñì ) ) , \\displaystyle\\bm{Z}^{(\\ell+1)}=\\bm{Z}^{(\\ell)}+\\eta\\sum_{k=1}^{K}\\bm{U}_{k}\\bm{U}_{k}^{T}\\bm{Z}^{(\\ell)}\\varphi\\left(\\bm{Z}^{(\\ell)^{T}}\\bm{U}_{k}\\bm{U}_{k}^{T}\\bm{Z}^{(\\ell)}\\right), bold_italic_Z start_POSTSUPERSCRIPT ( roman_‚Ñì + 1 ) end_POSTSUPERSCRIPT = bold_italic_"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.SS1.SSS0.Px1.p2", "title": "Now, let the columns of ùíÅ k ( ‚Ñì ) \\bm{Z}_{k}^{(\\ell)} bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( roman_‚Ñì ) end_POSTSUPERSCRIPT denotes the token representatio", "snippet": "Now, let the columns of ùíÅ k ( ‚Ñì ) \\bm{Z}_{k}^{(\\ell)} bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( roman_‚Ñì ) end_POSTSUPERSCRIPT denotes the token representations from the k k italic_k -th subspace at the ‚Ñì \\ell roman_‚Ñì -th layer. To quantify the denoising capability, we define the signal-to-noise ratio (SNR) for each block of the token representations at the ‚Ñì \\ell roman_‚Ñì -th layer as follows: SNR ‚Äã ( ùíÅ k ( ‚Ñì ) ) ‚âê ‚Äñ ùëº k ‚Äã ùëº k T ‚Äã ùíÅ k ( ‚Ñì ) ‚Äñ F ‚Äñ ( ùë∞ ‚àí ùëº k ‚Äã ùëº k T ) ‚Äã ùíÅ k ( ‚Ñì ) ‚Äñ F , ‚àÄ k ‚àà [ K ] . \\displaystyle\\mathrm{SNR}(\\bm{Z}_{k}^{(\\ell)})\\doteq\\fr"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.SS1.SSS0.Px1.p3", "title": "With the above setup, we now characterize the denoising performance of the MSSA operator.", "snippet": "With the above setup, we now characterize the denoising performance of the MSSA operator."}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmtheorem1.p1", "title": "Let ùêô ( 1 ) \\bm{Z}^{(1)} bold_italic_Z start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT be defined in Definition 4.1 and œÜ ‚Äã ( ‚ãÖ ) \\varphi(\\cdot) italic_œÜ ( ‚ãÖ ) in Eq. ( 4.3.2 ) be œÜ ‚Äã ( ùê± ) = h ‚Äã ( œÉ ", "snippet": "Let ùêô ( 1 ) \\bm{Z}^{(1)} bold_italic_Z start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT be defined in Definition 4.1 and œÜ ‚Äã ( ‚ãÖ ) \\varphi(\\cdot) italic_œÜ ( ‚ãÖ ) in Eq. ( 4.3.2 ) be œÜ ‚Äã ( ùê± ) = h ‚Äã ( œÉ ‚Äã ( ùê± ) ) \\varphi(\\bm{x})=h\\left(\\sigma(\\bm{x})\\right) italic_œÜ ( bold_italic_x ) = italic_h ( italic_œÉ ( bold_italic_x ) ) , where œÉ : ‚Ñù N ‚Üí ‚Ñù N \\sigma:\\mathbb{R}^{N}\\to\\mathbb{R}^{N} italic_œÉ : blackboard_R start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ‚Üí blackboard_R start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT is the soft-max function and h : ‚Ñù N ‚Üí ‚Ñù N h:\\mathbb{R}^{N}\\to\\mathbb{R}^{"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.SS1.SSS0.Px1.p4", "title": "This theorem demonstrates that when the initial token representations are sampled from a mixture of low-rank Gaussian distributions with a noise level O ‚Äã ( log ‚Å° N / p ) O(\\sqrt{\\log N}/\\sqrt{p}) ita", "snippet": "This theorem demonstrates that when the initial token representations are sampled from a mixture of low-rank Gaussian distributions with a noise level O ‚Äã ( log ‚Å° N / p ) O(\\sqrt{\\log N}/\\sqrt{p}) italic_O ( square-root start_ARG roman_log italic_N end_ARG / square-root start_ARG italic_p end_ARG ) , we show that each layer of the proposed transformer denoises token representations at a linear rate. This indicates the MSSA operator‚Äôs efficiency in reducing noise across layers. Notably, our theoretical results are well-supported by experimental observations in Figure 4.16 . This theorem provide"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmremark9.p1", "title": "Under this model, the goal of representation learning is to compress a set of noisy initial token presentations into the corresponding subspace. However, we should point out that in real-world applica", "snippet": "Under this model, the goal of representation learning is to compress a set of noisy initial token presentations into the corresponding subspace. However, we should point out that in real-world applications, where token representations exhibit more complicated structures, the goal of representation learning is to find a compact and structured representation by compressing token sets."}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.SS1.SSS0.Px2.p1", "title": "Now, we formally propose an attention-only transformer architecture. Specifically, by unrolling the iterative optimization steps ( 4.3.2 ) as layers of a deep network, we construct a transformer archi", "snippet": "Now, we formally propose an attention-only transformer architecture. Specifically, by unrolling the iterative optimization steps ( 4.3.2 ) as layers of a deep network, we construct a transformer architecture in Figure 4.17 . Each layer of the proposed architecture only consists of the MSSA operator and a skip connection. For language tasks, we additionally incorporate LayerNorm before the MSSA operator to improve performance. The complete architecture is built by stacking such layers, along with essential task-specific pre-processing and post-processing steps, such as positional encoding, toke"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.SS1.SSS0.Px2.p2", "title": "Generally speaking, the standard decoder-only transformer architecture is composed of the following key components [ VSP+17 ] : (1) positional encoding, (2) multi-head QKV self-attention mechanisms, (", "snippet": "Generally speaking, the standard decoder-only transformer architecture is composed of the following key components [ VSP+17 ] : (1) positional encoding, (2) multi-head QKV self-attention mechanisms, (3) feed-forward MLP networks, (4) layer normalization, and (5) residual connections. In contrast, our proposed transformer architecture adopts a streamlined design by incorporating several key simplications. Specifically, it employs shared-QKV subspace self-attention mechanisms, excludes MLP layers, and reduces the frequency of LayerNorm."}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.SS2.p1", "title": "In this subsection, we propose a new transformer attention operator whose computational complexity scales linearly with the number of tokens based on the coding rate reduction objective. Specifically,", "snippet": "In this subsection, we propose a new transformer attention operator whose computational complexity scales linearly with the number of tokens based on the coding rate reduction objective. Specifically, we derive a novel variational form of the MCR 2 objective and show that the architecture that results from unrolled gradient descent of this variational objective leads to a new attention module called Token Statistics Self-Attention ( TSSA ). TSSA has linear computational and memory complexity and radically departs from the typical attention architecture that computes pairwise similarities betwe"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.SS2.SSS0.Px1.p1", "title": "To begin, we consider a general form of MCR 2 -like objectives based on concave functions of the spectrum of a matrix. Namely, for a given PSD matrix ùë¥ ‚àà ùñØùñ≤ùñ£ ‚Äã ( d ) \\bm{M}\\in\\mathsf{PSD}(d) bold_ital", "snippet": "To begin, we consider a general form of MCR 2 -like objectives based on concave functions of the spectrum of a matrix. Namely, for a given PSD matrix ùë¥ ‚àà ùñØùñ≤ùñ£ ‚Äã ( d ) \\bm{M}\\in\\mathsf{PSD}(d) bold_italic_M ‚àà sansserif_PSD ( italic_d ) and any scalar c ‚â• 0 c\\geq 0 italic_c ‚â• 0 we have that log ‚Äã det ( ùë∞ + c ‚Äã ùë¥ ) = ‚àë i = 1 d log ‚Å° ( 1 + c ‚Äã Œª i ‚Äã ( ùë¥ ) ) \\log\\det(\\bm{I}+c\\bm{M})=\\sum_{i=1}^{d}\\log(1+c\\lambda_{i}(\\bm{M})) roman_log roman_det ( bold_italic_I + italic_c bold_italic_M ) = ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT roman_lo"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.SS2.SSS0.Px1.p2", "title": "For the above objective, we now note the following result:", "snippet": "For the above objective, we now note the following result:"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmtheorem2.p1", "title": "Let f : [ 0 , ‚àû ) ‚Üí ‚Ñù f\\colon[0,\\infty)\\to\\mathbb{R} italic_f : [ 0 , ‚àû ) ‚Üí blackboard_R be non-decreasing, concave, and obey f ‚Äã ( 0 ) = 0 f(0)=0 italic_f ( 0 ) = 0 , and let F : ùñØùñ≤ùñ£ ‚Äã ( d ) ‚Üí ‚Ñù F\\co", "snippet": "Let f : [ 0 , ‚àû ) ‚Üí ‚Ñù f\\colon[0,\\infty)\\to\\mathbb{R} italic_f : [ 0 , ‚àû ) ‚Üí blackboard_R be non-decreasing, concave, and obey f ‚Äã ( 0 ) = 0 f(0)=0 italic_f ( 0 ) = 0 , and let F : ùñØùñ≤ùñ£ ‚Äã ( d ) ‚Üí ‚Ñù F\\colon\\mathsf{PSD}(d)\\to\\mathbb{R} italic_F : sansserif_PSD ( italic_d ) ‚Üí blackboard_R have the form F ‚Äã ( ùêå ) = ‚àë i = 1 d f ‚Äã ( Œª i ‚Äã ( ùêå ) ) F(\\bm{M})=\\sum_{i=1}^{d}f(\\lambda_{i}(\\bm{M})) italic_F ( bold_italic_M ) = ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT italic_f ( italic_Œª start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_itali"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.SS2.SSS0.Px1.p3", "title": "Using the above result, we can replace ( 4.3.6 ) with an equivalent variational objective with form R c , f var ‚Äã ( ùíÅ , ùö∑ ‚à£ ùëº [ K ] ) ‚âê 1 2 ‚Äã ‚àë k = 1 K N k N ‚Äã ‚àë i = 1 d f ‚Äã ( 1 N k ‚Äã ( ùëº k ‚ä§ ‚Äã ùíÅ ‚Äã Di", "snippet": "Using the above result, we can replace ( 4.3.6 ) with an equivalent variational objective with form R c , f var ‚Äã ( ùíÅ , ùö∑ ‚à£ ùëº [ K ] ) ‚âê 1 2 ‚Äã ‚àë k = 1 K N k N ‚Äã ‚àë i = 1 d f ‚Äã ( 1 N k ‚Äã ( ùëº k ‚ä§ ‚Äã ùíÅ ‚Äã Diag ‚Äã ( ùùÖ k ) ‚Äã ùíÅ ‚ä§ ‚Äã ùëº k ) i ‚Äã i ) , R^{\\rm var}_{c,f}(\\bm{Z},\\bm{\\Pi}\\mid\\bm{U}_{[K]})\\doteq\\frac{1}{2}\\sum_{k=1}^{K}\\frac{N_{k}}{N}\\sum_{i=1}^{d}f\\left(\\frac{1}{N_{k}}(\\bm{U}_{k}^{\\top}\\bm{Z}\\mathrm{Diag}(\\bm{\\pi}_{k})\\bm{Z}^{\\top}\\bm{U}_{k})_{ii}\\right), italic_R start_POSTSUPERSCRIPT roman_var end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_c , italic_f end_POSTSUBSCRIPT ( bold_italic_Z , bold_"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.SS2.SSS0.Px1.p4", "title": "Following this approach, we compute a gradient descent step on R c , f var R_{c,f}^{\\rm var} italic_R start_POSTSUBSCRIPT italic_c , italic_f end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_var end_POST", "snippet": "Following this approach, we compute a gradient descent step on R c , f var R_{c,f}^{\\rm var} italic_R start_POSTSUBSCRIPT italic_c , italic_f end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_var end_POSTSUPERSCRIPT w.r.t. ùíÅ \\bm{Z} bold_italic_Z . To begin this computation, first let ùùÖ ‚àà ‚Ñù N \\bm{\\pi}\\in\\mathbb{R}^{N} bold_italic_œÄ ‚àà blackboard_R start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT be any element-wise non-negative vector. Then we have ‚àá ùíÅ 1 2 ‚Äã ‚àë i = 1 d f ‚Äã ( ( ùíÅ ‚Äã Diag ‚Äã ( ùùÖ ) ‚Äã ùíÅ ‚ä§ ) i ‚Äã i ) = Diag ‚Äã ( ‚àá f ‚Äã [ ùíÅ ‚äô 2 ‚Äã ùùÖ ] ) ‚Äã ùíÅ ‚Äã Diag ‚Äã ( ùùÖ ) , \\nabla_{\\bm{Z}}\\ \\frac{1}{2}\\sum_{i="}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.SS2.SSS0.Px2.p1", "title": "Given the proposed attention operator in ( 4.3.11 ), first recall that the rows of ùö∑ \\bm{\\Pi} bold_Œ† are non-negative and sum to 1 , so our operator takes a weighted average of K K italic_K ‚Äúattention", "snippet": "Given the proposed attention operator in ( 4.3.11 ), first recall that the rows of ùö∑ \\bm{\\Pi} bold_Œ† are non-negative and sum to 1 , so our operator takes a weighted average of K K italic_K ‚Äúattention head‚Äù-esque operators and then adds a residual connection. Using that ‚àë k = 1 K Œ† j ‚Äã k = 1 \\sum_{k=1}^{K}\\Pi_{jk}=1 ‚àë start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_Œ† start_POSTSUBSCRIPT italic_j italic_k end_POSTSUBSCRIPT = 1 , we can rewrite ( 4.3.11 ) as: ùíõ j + = ‚àë k = 1 K Œ† j ‚Äã k ‚Äã [ ùíõ j ‚Äã ‚àí œÑ n ‚Äã ùëº k ‚Äã ùë´ ‚Äã ( ùíÅ , ùùÖ k ‚à£ ùëº k ) ‚Äã ùëº k "}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.SS2.SSS0.Px2.p2", "title": "From this, we arrive at the core interpretation of our attention head + residual operators [ ùë∞ ‚àí ( œÑ / n ) ‚Äã ùëº k ‚Äã ùë´ k ‚Äã ùëº k ‚ä§ ] [\\bm{I}-(\\tau/n)\\bm{U}_{k}\\bm{D}_{k}\\bm{U}_{k}^{\\top}] [ bold_italic_I ", "snippet": "From this, we arrive at the core interpretation of our attention head + residual operators [ ùë∞ ‚àí ( œÑ / n ) ‚Äã ùëº k ‚Äã ùë´ k ‚Äã ùëº k ‚ä§ ] [\\bm{I}-(\\tau/n)\\bm{U}_{k}\\bm{D}_{k}\\bm{U}_{k}^{\\top}] [ bold_italic_I - ( italic_œÑ / italic_n ) bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ] . Namely, this operator does an approximate low-rank data-dependent projection, where directions which have a large amount of ‚Äúpower‚Äù after the p"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.SS2.SSS0.Px3.p1", "title": "Having introduced our proposed attention operator, we now discuss further practical considerations. First, until this point in the presentation, we have avoided discussion of how tokens are ‚Äúgrouped‚Äù ", "snippet": "Having introduced our proposed attention operator, we now discuss further practical considerations. First, until this point in the presentation, we have avoided discussion of how tokens are ‚Äúgrouped‚Äù into various attention heads via the ùö∑ \\bm{\\Pi} bold_Œ† matrix, but clearly a means of constructing ùö∑ \\bm{\\Pi} bold_Œ† is needed to implement our method. Additionally, our variational form in Theorem 4.2 requires the ùëº \\bm{U} bold_italic_U matrices to be square and orthogonal, but one would ideally like to use smaller matrices (i.e., reduce the number of columns in ùëº \\bm{U} bold_italic_U ) for effic"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.SS2.SSS0.Px3.p2", "title": "In practice, we do not enforce the orthogonality constraints. To reduce the number of columns in the ùëº \\bm{U} bold_italic_U matrices, we note that similar to CRATE [ YBP+23 ] , if we assume the featur", "snippet": "In practice, we do not enforce the orthogonality constraints. To reduce the number of columns in the ùëº \\bm{U} bold_italic_U matrices, we note that similar to CRATE [ YBP+23 ] , if we assume the features ùíÅ \\bm{Z} bold_italic_Z within group k k italic_k are (approximately) clustered around a low-dimensional subspace ‚Äî say of dimension p p italic_p ‚Äî then the within-group- k k italic_k covariance ùíÅ ‚Äã Diag ‚Äã ( ùùÖ k ) ‚Äã ùíÅ ‚ä§ \\bm{Z}\\mathrm{Diag}(\\bm{\\pi}_{k})\\bm{Z}^{\\top} bold_italic_Z roman_Diag ( bold_italic_œÄ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) bold_italic_Z start_POSTSUPERSCRIPT ‚ä§ end"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmcorollary1.p1", "title": "Let f : [ 0 , ‚àû ) ‚Üí ‚Ñù f\\colon[0,\\infty)\\to\\mathbb{R} italic_f : [ 0 , ‚àû ) ‚Üí blackboard_R be non-decreasing, concave, and obey f ‚Äã ( 0 ) = 0 f(0)=0 italic_f ( 0 ) = 0 , and let F : ùñØùñ≤ùñ£ ‚Äã ( p ) ‚Üí ‚Ñù F\\co", "snippet": "Let f : [ 0 , ‚àû ) ‚Üí ‚Ñù f\\colon[0,\\infty)\\to\\mathbb{R} italic_f : [ 0 , ‚àû ) ‚Üí blackboard_R be non-decreasing, concave, and obey f ‚Äã ( 0 ) = 0 f(0)=0 italic_f ( 0 ) = 0 , and let F : ùñØùñ≤ùñ£ ‚Äã ( p ) ‚Üí ‚Ñù F\\colon\\mathsf{PSD}(p)\\to\\mathbb{R} italic_F : sansserif_PSD ( italic_p ) ‚Üí blackboard_R have the form F ‚Äã ( ùêå ) = ‚àë i = 1 p f ‚Äã ( Œª i ‚Äã ( ùêå ) ) F(\\bm{M})=\\sum_{i=1}^{p}f(\\lambda_{i}(\\bm{M})) italic_F ( bold_italic_M ) = ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT italic_f ( italic_Œª start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( bold_itali"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.SS2.SSS0.Px3.p3", "title": "The final step to define our attention operator is to estimate the group membership ùö∑ \\bm{\\Pi} bold_Œ† . For this we posit a simple model of how each feature ùíõ j \\bm{z}_{j} bold_italic_z start_POSTSUBS", "snippet": "The final step to define our attention operator is to estimate the group membership ùö∑ \\bm{\\Pi} bold_Œ† . For this we posit a simple model of how each feature ùíõ j \\bm{z}_{j} bold_italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT deviates from its supporting subspace and then find the optimal subspace assignment. [ YBP+23 ] show that if we independently model each ùíõ j \\bm{z}_{j} bold_italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT as belonging to a low-dimensional Gaussian mixture model, where each Gaussian has a covariance matrix with identical spectrum and is supported on a subspa"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S4.p1", "title": "The materials presented in this chapter are based on a series of recent works on this topic, including [ CYY+22 , WLP+24 , WLY+25 , WDL+25 , YBP+23 ] . These contributions encompass both theoretical a", "snippet": "The materials presented in this chapter are based on a series of recent works on this topic, including [ CYY+22 , WLP+24 , WLY+25 , WDL+25 , YBP+23 ] . These contributions encompass both theoretical advances and practical methodologies for constructing interpretable deep networks through unrolled optimization. Many of the key results and proofs discussed in this chapter are derived directly from, or inspired by, these foundational works."}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S4.p2", "title": "The idea of unrolling an optimization algorithm to construct a neural network traces back to the seminal work [ GL10 ] . In this work, the authors demonstrated that sparse coding algorithms‚Äîsuch as th", "snippet": "The idea of unrolling an optimization algorithm to construct a neural network traces back to the seminal work [ GL10 ] . In this work, the authors demonstrated that sparse coding algorithms‚Äîsuch as the Iterative Shrinkage-Thresholding Algorithm (ISTA)‚Äîcan be unrolled to form multilayer perceptrons (MLPs), effectively bridging iterative optimization and neural network design. Notably, [ MLE19 ] demonstrated that such unrolled networks are more interpretable, parameter-efficient, and effective compared to generic networks. In this chapter, we build on this perspective to develop principled, whit"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S4.p3", "title": "Conventional DNNs ReduNets Objectives input/output fitting information gain Deep architectures trial & error iterative optimization Layer operators empirical projected gradient Shift invariance CNNs+a", "snippet": "Conventional DNNs ReduNets Objectives input/output fitting information gain Deep architectures trial & error iterative optimization Layer operators empirical projected gradient Shift invariance CNNs+augmentation invariant ReduNets Initializations random/pre-design forward unrolled Training/fine-tuning back prop forward/back prop Interpretability black box white box Representations hidden/latent incoherent subspaces"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmexercise1.p1", "title": "Let ùíÅ = [ ùíÅ 1 , ‚Ä¶ , ùíÅ K ] ‚àà ‚Ñù d √ó m \\bm{Z}=[\\bm{Z}_{1},\\dots,\\bm{Z}_{K}]\\in\\mathbb{R}^{d\\times m} bold_italic_Z = [ bold_italic_Z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_Z start_POST", "snippet": "Let ùíÅ = [ ùíÅ 1 , ‚Ä¶ , ùíÅ K ] ‚àà ‚Ñù d √ó m \\bm{Z}=[\\bm{Z}_{1},\\dots,\\bm{Z}_{K}]\\in\\mathbb{R}^{d\\times m} bold_italic_Z = [ bold_italic_Z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_Z start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT ] ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d √ó italic_m end_POSTSUPERSCRIPT with ùíÅ k ‚àà ‚Ñù d √ó m k \\bm{Z}_{k}\\in\\mathbb{R}^{d\\times m_{k}} bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d √ó italic_m start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT for each k ‚àà [ K ] k\\in[K] italic_k ‚àà "}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmexercise1.p2", "title": "2. Please show that R ‚Äã ( ùíÅ ) ‚â§ ‚àë k = 1 K log ‚Äã det ( ùë∞ + Œ± ‚Äã ùíÅ k ‚Äã ùíÅ k T ) , \\displaystyle R(\\bm{Z})\\leq\\sum_{k=1}^{K}\\log\\det\\left(\\bm{I}+\\alpha\\bm{Z}_{k}\\bm{Z}_{k}^{T}\\right), italic_R ( bold_itali", "snippet": "2. Please show that R ‚Äã ( ùíÅ ) ‚â§ ‚àë k = 1 K log ‚Äã det ( ùë∞ + Œ± ‚Äã ùíÅ k ‚Äã ùíÅ k T ) , \\displaystyle R(\\bm{Z})\\leq\\sum_{k=1}^{K}\\log\\det\\left(\\bm{I}+\\alpha\\bm{Z}_{k}\\bm{Z}_{k}^{T}\\right), italic_R ( bold_italic_Z ) ‚â§ ‚àë start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_log roman_det ( bold_italic_I + italic_Œ± bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ) , where the equality holds if and only if ùíÅ k T ‚Äã ùíÅ l = ùüé \\bm{Z}_{"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmexercise1.p3", "title": "3. Given some Œ± > 0 \\alpha>0 italic_Œ± > 0 , let Œ± k = m ‚Äã Œ± / m k \\alpha_{k}=m\\alpha/m_{k} italic_Œ± start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = italic_m italic_Œ± / italic_m start_POSTSUBSCRIPT it", "snippet": "3. Given some Œ± > 0 \\alpha>0 italic_Œ± > 0 , let Œ± k = m ‚Äã Œ± / m k \\alpha_{k}=m\\alpha/m_{k} italic_Œ± start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = italic_m italic_Œ± / italic_m start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT for each k ‚àà [ K ] k\\in[K] italic_k ‚àà [ italic_K ] . Please derive the closed-form for the first-order critical point of the following function: f ‚Äã ( ùíÅ k ) = 1 2 ‚Äã log ‚Äã det ( ùë∞ + Œ± ‚Äã ùíÅ k ‚Äã ùíÅ k T ) ‚àí m k 2 ‚Äã m ‚Äã log ‚Äã det ( ùë∞ + Œ± k ‚Äã ùíÅ k ‚Äã ùíÅ k T ) ‚àí Œª 2 ‚Äã ‚Äñ ùíÅ k ‚Äñ F 2 . \\displaystyle f(\\bm{Z}_{k})=\\frac{1}{2}\\log\\det\\left(\\bm{I}+\\alpha\\bm{Z}_{k}\\bm{Z}_{k}^{T}\\right)-\\frac{"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmexercise2.p1", "title": "Let ùë® ‚àà ‚Ñù n √ó n \\bm{A}\\in\\mathbb{R}^{n\\times n} bold_italic_A ‚àà blackboard_R start_POSTSUPERSCRIPT italic_n √ó italic_n end_POSTSUPERSCRIPT . If ‚Äñ ùë® ‚Äñ < 1 \\|\\bm{A}\\|<1 ‚à• bold_italic_A ‚à• < 1 , please sh", "snippet": "Let ùë® ‚àà ‚Ñù n √ó n \\bm{A}\\in\\mathbb{R}^{n\\times n} bold_italic_A ‚àà blackboard_R start_POSTSUPERSCRIPT italic_n √ó italic_n end_POSTSUPERSCRIPT . If ‚Äñ ùë® ‚Äñ < 1 \\|\\bm{A}\\|<1 ‚à• bold_italic_A ‚à• < 1 , please show ( ùë∞ ‚àí ùë® ) ‚àí 1 = ‚àë k = 1 ‚àû ùë® k . \\displaystyle\\left(\\bm{I}-\\bm{A}\\right)^{-1}=\\sum_{k=1}^{\\infty}\\bm{A}^{k}. ( bold_italic_I - bold_italic_A ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT = ‚àë start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚àû end_POSTSUPERSCRIPT bold_italic_A start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT . (4.5.1) Hint: The proof consists of two ste"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmexercise3.p1", "title": "Please compute the gradients in ( 4.3.9 ) and ( 4.3.10 ).", "snippet": "Please compute the gradients in ( 4.3.9 ) and ( 4.3.10 )."}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmexercise4.p1", "title": "Please show Corollary 4.1 when K ‚Äã p ‚â§ d Kp\\leq d italic_K italic_p ‚â§ italic_d .", "snippet": "Please show Corollary 4.1 when K ‚Äã p ‚â§ d Kp\\leq d italic_K italic_p ‚â§ italic_d ."}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmremark1", "title": "Remark 4.1 (Interpretation of ùë¨ ‚Ñì \\bm{E}^{\\ell} bold_italic_E start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT and ùë™ j ‚Ñì \\bm{C}_{j}^{\\ell} bold_italic_C start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT", "snippet": "Remark 4.1 (Interpretation of ùë¨ ‚Ñì \\bm{E}^{\\ell} bold_italic_E start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT and ùë™ j ‚Ñì \\bm{C}_{j}^{\\ell} bold_italic_C start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT as linear operators) . For any ùíõ ‚Ñì ‚àà ‚Ñù d \\bm{z}^{\\ell}\\in\\mathbb{R}^{d} bold_italic_z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , ùë¨ ‚Ñì ‚Äã ùíõ ‚Ñì = Œ± ‚Äã ( ùíõ ‚Ñì ‚àí ùíÅ ‚Ñì ‚Äã ùíí ‚ãÜ ‚Ñì ) , where ùíí ‚ãÜ ‚Ñì ‚âê arg ‚Äã min ùíí ‚Ñì ‚Å° { Œ± ‚Äã ‚Äñ ùíõ ‚Ñì ‚àí ùíÅ ‚Ñì ‚Äã ùíí ‚Ñì ‚Äñ 2 2 + ‚Äñ ùíí ‚Ñì ‚Äñ 2 2 } . \\displaystyle\\bm{E}^{\\ell}\\"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmexample1", "title": "Example 4.1 .", "snippet": "Example 4.1 . To provide some intuition on how ReduNet transforms the features, we provide a simple example with mixed 3D Gaussians and visualize how the features are transformed in Figure 4.5 . Consider a mixture of three Gaussian distributions in ‚Ñù 3 \\mathbb{R}^{3} blackboard_R start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT that is projected onto ùïä 2 \\mathbb{S}^{2} blackboard_S start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . We first generate data points for 3 classes: for k = 1 , 2 , 3 k=1,2,3 italic_k = 1 , 2 , 3 , ùëø k = [ ùíô k , 1 , ‚Ä¶ , ùíô k , m ] ‚àà ‚Ñù 3 √ó m \\bm{X}_{k}=[\\bm{x}_{k,1},\\ldots,\\bm{x}"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmproposition1", "title": "Proposition 4.1 (Convolution structures of ùë¨ 1 \\bm{E}^{1} bold_italic_E start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT and ùë™ k 1 \\bm{C}^{1}_{k} bold_italic_C start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT s", "snippet": "Proposition 4.1 (Convolution structures of ùë¨ 1 \\bm{E}^{1} bold_italic_E start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT and ùë™ k 1 \\bm{C}^{1}_{k} bold_italic_C start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) . The matrix ùë¨ 1 = Œ± ‚Äã ( ùë∞ + Œ± ‚Äã ùñºùóÇùóãùñº ‚Äã ( ùíÅ 1 ) ‚Äã ùñºùóÇùóãùñº ‚Äã ( ùíÅ 1 ) ‚ä§ ) ‚àí 1 \\bm{E}^{1}=\\alpha\\big{(}\\bm{I}+\\alpha\\mathsf{circ}(\\bm{Z}^{1})\\mathsf{circ}(\\bm{Z}^{1})^{\\top}\\big{)}^{-1} bold_italic_E start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT = italic_Œ± ( bold_italic_I + italic_Œ± sansserif_circ ( bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT )"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmproposition2", "title": "Proposition 4.2 (Multi-channel convolution structures of ùë¨ ¬Ø \\bar{\\bm{E}} over¬Ø start_ARG bold_italic_E end_ARG and ùë™ ¬Ø k \\bar{\\bm{C}}_{k} over¬Ø start_ARG bold_italic_C end_ARG start_POSTSUBSCRIPT ita", "snippet": "Proposition 4.2 (Multi-channel convolution structures of ùë¨ ¬Ø \\bar{\\bm{E}} over¬Ø start_ARG bold_italic_E end_ARG and ùë™ ¬Ø k \\bar{\\bm{C}}_{k} over¬Ø start_ARG bold_italic_C end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) . The matrix ùë¨ ¬Ø ‚âê Œ± ‚Äã ( ùë∞ + Œ± ‚Äã ùñºùóÇùóãùñº ‚Äã ( ùíÅ ¬Ø ) ‚Äã ùñºùóÇùóãùñº ‚Äã ( ùíÅ ¬Ø ) ‚ä§ ) ‚àí 1 \\bar{\\bm{E}}\\doteq\\alpha\\left(\\bm{I}+\\alpha\\,\\mathsf{circ}(\\bar{\\bm{Z}})\\mathsf{circ}(\\bar{\\bm{Z}})^{\\top}\\right)^{-1} over¬Ø start_ARG bold_italic_E end_ARG ‚âê italic_Œ± ( bold_italic_I + italic_Œ± sansserif_circ ( over¬Ø start_ARG bold_italic_Z end_ARG ) sansserif_circ ( over¬Ø start_ARG bold_italic_Z en"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmremark2", "title": "Remark 4.2 (Reducing Computational Complexity in the Frequency Domain) .", "snippet": "Remark 4.2 (Reducing Computational Complexity in the Frequency Domain) . The calculation of ùë¨ ¬Ø \\bar{\\bm{E}} over¬Ø start_ARG bold_italic_E end_ARG in ( 4.1.27 ) requires inverting a matrix of size d ‚Äã C √ó d ‚Äã C dC\\times dC italic_d italic_C √ó italic_d italic_C , which in general has complexity O ‚Äã ( d 3 ‚Äã C 3 ) O(d^{3}C^{3}) italic_O ( italic_d start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT italic_C start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ) . Nevertheless, by using the fact that a circulant matrix can be diagonalized by the Discrete Fourier Transform (DFT) matrix, the complexity can be signif"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmexample2", "title": "Example 4.2 (Invariant Classification of Digits) .", "snippet": "Example 4.2 (Invariant Classification of Digits) . We next provide an empirical performance of the ReduNet on learning rotation invariant features on the real 10-class MNIST dataset. We impose a polar grid on the image ùíô ‚àà ‚Ñù H √ó W \\bm{x}\\in\\mathbb{R}^{H\\times W} bold_italic_x ‚àà blackboard_R start_POSTSUPERSCRIPT italic_H √ó italic_W end_POSTSUPERSCRIPT , with its geometric center being the center of the 2D polar grid (as illustrated in Figure 4.10 ). For each radius r i r_{i} italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , i ‚àà [ C ] i\\in[C] italic_i ‚àà [ italic_C ] , we can sample Œì \\G"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmremark3", "title": "Remark 4.3 .", "snippet": "Remark 4.3 . In transformers, each input sample is typically converted into a sequence of tokens . A token is a basic unit of information derived from the raw input: in natural language processing, tokens are typically words or subwords; in computer vision, they correspond to image patches; and in other modalities, they may represent time steps, spatial locations, or other domain-specific units. A token embedding is a continuous vector representation of a token that serves as the input to a transformer. It maps each token to a point in a high-dimensional space, enabling the model to process sy"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmremark4", "title": "Remark 4.4 .", "snippet": "Remark 4.4 . The expression ( 4.2.3 ) for the coding rate can be viewed as a generalization of the coding rate R œµ c R_{\\epsilon}^{c} italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT used in the original rate reduction objective ( 3.4.13 ). In particular, the original objective is defined with respect to a set of known membership labels { ùö∑ k } \\{\\bm{\\Pi}_{k}\\} { bold_Œ† start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } specific to the particular data realization ùëø \\bm{X} bold_italic_X . In contrast, the current objective is defined with "}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmremark5", "title": "Remark 4.5 .", "snippet": "Remark 4.5 . In contrast to other unrolled optimization approaches such as the ReduNet (see Section 4.1 ), we explicitly model the distribution of ùíÅ ‚Ñì \\bm{Z}^{\\ell} bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT at each layer, say as a mixture of linear subspaces or sparsely generated from a dictionary. The model parameters are learned from data (say via backward propagation with end-to-end training). This separation between forward ‚Äúoptimization‚Äù and backward ‚Äúlearning‚Äù clarifies the mathematical role of each layer as an operator that transforms the distribution of its input,"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmremark6", "title": "Remark 4.6 .", "snippet": "Remark 4.6 . The SSA operator in ( 4.2.12 ) resembles the attention operator in a typical transformer [ VSP+17 ] , except that here the linear operators of value, key, and query are all set to be the same as the subspace basis, i.e., ùëΩ k = ùë≤ k = ùë∏ k = ùëº k ‚àó \\bm{V}_{k}=\\bm{K}_{k}=\\bm{Q}_{k}=\\bm{U}_{k}^{*} bold_italic_V start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = bold_italic_K start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = bold_italic_Q start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT "}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmremark7", "title": "Remark 4.7 ( The roles of the forward pass and backward propagation ) .", "snippet": "Remark 4.7 ( The roles of the forward pass and backward propagation ) . In contrast to other unrolled optimization approaches such as the ReduNet [ CYY+22 ] , we explicitly model the distribution of each ùíÅ ‚Ñì \\bm{Z}^{\\ell} bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT and ùíÅ ‚Ñì + 1 / 2 \\bm{Z}^{\\ell+1/2} bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì + 1 / 2 end_POSTSUPERSCRIPT at each layer, either by a mixture of linear subspaces or sparsely generated from a dictionary. We introduced the interpretation that at each layer ‚Ñì \\ell roman_‚Ñì , the learned bases for the subspaces ùëº [ K ]"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmdefinition1", "title": "Definition 4.1 .", "snippet": "Definition 4.1 . Let C 1 , ‚Ä¶ , C K C_{1},\\dots,C_{K} italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , italic_C start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT be a partition of the index set [ N ] [N] [ italic_N ] and ùëº k ‚àà ùí™ d √ó p k \\bm{U}_{k}\\in\\mathcal{O}^{d\\times p_{k}} bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ‚àà caligraphic_O start_POSTSUPERSCRIPT italic_d √ó italic_p start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT denote the orthonormal basis of the k k italic_k -th subspace for each k ‚àà [ K ] k\\in[K] italic_k ‚àà [ italic_K ] . We say that the tok"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmremark8", "title": "Remark 4.8 .", "snippet": "Remark 4.8 . The linear representation hypothesis posits that token representations in LLMs lie in low-dimensional linear subspaces that encode semantic features. Similarly, the superposition hypothesis suggests that these representations can be approximately expressed as a sparse linear combination of these feature vectors. In Definition 4.1 , each basis ùëº k \\bm{U}_{k} bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT of the subspaces can be interpreted as a set of semantic features, where each feature corresponds to a specific aspect of the token‚Äôs meaning. Token representations a"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmtheorem1", "title": "Theorem 4.1 .", "snippet": "Theorem 4.1 . Let ùêô ( 1 ) \\bm{Z}^{(1)} bold_italic_Z start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT be defined in Definition 4.1 and œÜ ‚Äã ( ‚ãÖ ) \\varphi(\\cdot) italic_œÜ ( ‚ãÖ ) in Eq. ( 4.3.2 ) be œÜ ‚Äã ( ùê± ) = h ‚Äã ( œÉ ‚Äã ( ùê± ) ) \\varphi(\\bm{x})=h\\left(\\sigma(\\bm{x})\\right) italic_œÜ ( bold_italic_x ) = italic_h ( italic_œÉ ( bold_italic_x ) ) , where œÉ : ‚Ñù N ‚Üí ‚Ñù N \\sigma:\\mathbb{R}^{N}\\to\\mathbb{R}^{N} italic_œÉ : blackboard_R start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ‚Üí blackboard_R start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT is the soft-max function and h : ‚Ñù N ‚Üí ‚Ñù N h:\\mathbb{R}^{N}\\"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmremark9", "title": "Remark 4.9 .", "snippet": "Remark 4.9 . Under this model, the goal of representation learning is to compress a set of noisy initial token presentations into the corresponding subspace. However, we should point out that in real-world applications, where token representations exhibit more complicated structures, the goal of representation learning is to find a compact and structured representation by compressing token sets."}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmtheorem2", "title": "Theorem 4.2 .", "snippet": "Theorem 4.2 . Let f : [ 0 , ‚àû ) ‚Üí ‚Ñù f\\colon[0,\\infty)\\to\\mathbb{R} italic_f : [ 0 , ‚àû ) ‚Üí blackboard_R be non-decreasing, concave, and obey f ‚Äã ( 0 ) = 0 f(0)=0 italic_f ( 0 ) = 0 , and let F : ùñØùñ≤ùñ£ ‚Äã ( d ) ‚Üí ‚Ñù F\\colon\\mathsf{PSD}(d)\\to\\mathbb{R} italic_F : sansserif_PSD ( italic_d ) ‚Üí blackboard_R have the form F ‚Äã ( ùêå ) = ‚àë i = 1 d f ‚Äã ( Œª i ‚Äã ( ùêå ) ) F(\\bm{M})=\\sum_{i=1}^{d}f(\\lambda_{i}(\\bm{M})) italic_F ( bold_italic_M ) = ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT italic_f ( italic_Œª start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIP"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmcorollary1", "title": "Corollary 4.1 .", "snippet": "Corollary 4.1 . Let f : [ 0 , ‚àû ) ‚Üí ‚Ñù f\\colon[0,\\infty)\\to\\mathbb{R} italic_f : [ 0 , ‚àû ) ‚Üí blackboard_R be non-decreasing, concave, and obey f ‚Äã ( 0 ) = 0 f(0)=0 italic_f ( 0 ) = 0 , and let F : ùñØùñ≤ùñ£ ‚Äã ( p ) ‚Üí ‚Ñù F\\colon\\mathsf{PSD}(p)\\to\\mathbb{R} italic_F : sansserif_PSD ( italic_p ) ‚Üí blackboard_R have the form F ‚Äã ( ùêå ) = ‚àë i = 1 p f ‚Äã ( Œª i ‚Äã ( ùêå ) ) F(\\bm{M})=\\sum_{i=1}^{p}f(\\lambda_{i}(\\bm{M})) italic_F ( bold_italic_M ) = ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT italic_f ( italic_Œª start_POSTSUBSCRIPT italic_i end_POSTSUBSCR"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmexercise1", "title": "Exercise 4.1 .", "snippet": "Exercise 4.1 . Let ùíÅ = [ ùíÅ 1 , ‚Ä¶ , ùíÅ K ] ‚àà ‚Ñù d √ó m \\bm{Z}=[\\bm{Z}_{1},\\dots,\\bm{Z}_{K}]\\in\\mathbb{R}^{d\\times m} bold_italic_Z = [ bold_italic_Z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_Z start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT ] ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d √ó italic_m end_POSTSUPERSCRIPT with ùíÅ k ‚àà ‚Ñù d √ó m k \\bm{Z}_{k}\\in\\mathbb{R}^{d\\times m_{k}} bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d √ó italic_m start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT for each k ‚àà [ K ] k\\in"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmexercise2", "title": "Exercise 4.2 (Neumann series for matrix inverse) .", "snippet": "Exercise 4.2 (Neumann series for matrix inverse) . Let ùë® ‚àà ‚Ñù n √ó n \\bm{A}\\in\\mathbb{R}^{n\\times n} bold_italic_A ‚àà blackboard_R start_POSTSUPERSCRIPT italic_n √ó italic_n end_POSTSUPERSCRIPT . If ‚Äñ ùë® ‚Äñ < 1 \\|\\bm{A}\\|<1 ‚à• bold_italic_A ‚à• < 1 , please show ( ùë∞ ‚àí ùë® ) ‚àí 1 = ‚àë k = 1 ‚àû ùë® k . \\displaystyle\\left(\\bm{I}-\\bm{A}\\right)^{-1}=\\sum_{k=1}^{\\infty}\\bm{A}^{k}. ( bold_italic_I - bold_italic_A ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT = ‚àë start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚àû end_POSTSUPERSCRIPT bold_italic_A start_POSTSUPERSCRIPT italic_k end_POSTSUPERS"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmexercise3", "title": "Exercise 4.3 .", "snippet": "Exercise 4.3 . Please compute the gradients in ( 4.3.9 ) and ( 4.3.10 )."}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#Thmexercise4", "title": "Exercise 4.4 .", "snippet": "Exercise 4.4 . Please show Corollary 4.1 when K ‚Äã p ‚â§ d Kp\\leq d italic_K italic_p ‚â§ italic_d ."}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S0.E1", "title": "f ‚Äã ( ‚ãÖ , ùúΩ ) : ùíô ‚Üí f 0 ùíõ 0 ‚Üí ‚ãØ ‚Üí ùíõ ‚Ñì ‚Üí f ‚Ñì ùíõ ‚Ñì + 1 ‚Üí ‚ãØ ‚Üí ùíõ L = ùíõ . f(\\cdot,\\bm{\\theta})\\colon\\bm{x}\\xrightarrow{\\hskip 2.84526ptf^{0}\\hskip 2.84526pt}\\bm{z}^{0}\\rightarrow\\cdots\\rightarrow\\bm{z}^{\\el", "snippet": "f ‚Äã ( ‚ãÖ , ùúΩ ) : ùíô ‚Üí f 0 ùíõ 0 ‚Üí ‚ãØ ‚Üí ùíõ ‚Ñì ‚Üí f ‚Ñì ùíõ ‚Ñì + 1 ‚Üí ‚ãØ ‚Üí ùíõ L = ùíõ . f(\\cdot,\\bm{\\theta})\\colon\\bm{x}\\xrightarrow{\\hskip 2.84526ptf^{0}\\hskip 2.84526pt}\\bm{z}^{0}\\rightarrow\\cdots\\rightarrow\\bm{z}^{\\ell}\\xrightarrow{\\hskip 2.84526ptf^{\\ell}\\hskip 2.84526pt}\\bm{z}^{\\ell+1}\\rightarrow\\cdots\\to\\bm{z}^{L}=\\bm{z}. italic_f ( ‚ãÖ , bold_italic_Œ∏ ) : bold_italic_x start_ARROW start_OVERACCENT italic_f start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT end_OVERACCENT ‚Üí end_ARROW bold_italic_z start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT ‚Üí ‚ãØ ‚Üí bold_italic_z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT start_"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E1", "title": "Œî ‚Äã R œµ ‚Äã ( ùíÅ ‚à£ ùö∑ ) ‚âê 1 2 ‚Äã log ‚Äã det ( ùë∞ + Œ± ‚Äã ùíÅ ‚Äã ùíÅ ‚ä§ ) ‚èü R œµ ‚Äã ( ùíÅ ) ‚àí ‚àë k = 1 K Œ≥ k 2 ‚Äã log ‚Äã det ( ùë∞ + Œ± k ‚Äã ùíÅ ‚Äã ùö∑ k ‚Äã ùíÅ ‚ä§ ) ‚èü R œµ c ‚Äã ( ùíÅ ‚à£ ùö∑ ) , \\begin{split}\\Delta R_{\\epsilon}(\\bm{Z}\\mid\\bm{\\", "snippet": "Œî ‚Äã R œµ ‚Äã ( ùíÅ ‚à£ ùö∑ ) ‚âê 1 2 ‚Äã log ‚Äã det ( ùë∞ + Œ± ‚Äã ùíÅ ‚Äã ùíÅ ‚ä§ ) ‚èü R œµ ‚Äã ( ùíÅ ) ‚àí ‚àë k = 1 K Œ≥ k 2 ‚Äã log ‚Äã det ( ùë∞ + Œ± k ‚Äã ùíÅ ‚Äã ùö∑ k ‚Äã ùíÅ ‚ä§ ) ‚èü R œµ c ‚Äã ( ùíÅ ‚à£ ùö∑ ) , \\begin{split}\\Delta R_{\\epsilon}(\\bm{Z}\\mid\\bm{\\Pi})\\doteq\\underbrace{\\frac{1}{2}\\log\\det\\Big{(}\\bm{I}+{\\alpha}\\bm{Z}\\bm{Z}^{\\top}\\Big{)}}_{R_{\\epsilon}(\\bm{Z})}\\;-\\;\\underbrace{\\sum_{k=1}^{K}\\frac{\\gamma_{k}}{2}\\log\\det\\Big{(}\\bm{I}+{\\alpha_{k}}\\bm{Z}\\bm{\\Pi}_{k}\\bm{Z}^{\\top}\\Big{)}}_{R_{\\epsilon}^{c}(\\bm{Z}\\mid\\bm{\\Pi})},\\end{split} start_ROW start_CELL roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ‚à£ bold_Œ† )"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E2", "title": "ùíÅ ‚Ñì + 1 ‚àù ùíÅ ‚Ñì + Œ∑ ‚ãÖ ‚àÇ Œî ‚Äã R œµ ‚àÇ ùíÅ ‚Äã ( ùíÅ ‚Ñì ) s.t. ùíÅ ‚Ñì + 1 ‚äÜ ùïä d ‚àí 1 , ‚Ñì = 1 , 2 , ‚Ä¶ , \\bm{Z}^{\\ell+1}\\;\\propto\\;\\bm{Z}^{\\ell}+\\eta\\cdot\\frac{\\partial\\Delta R_{\\epsilon}}{\\partial\\bm{Z}}(\\bm{Z}^{\\ell})\\", "snippet": "ùíÅ ‚Ñì + 1 ‚àù ùíÅ ‚Ñì + Œ∑ ‚ãÖ ‚àÇ Œî ‚Äã R œµ ‚àÇ ùíÅ ‚Äã ( ùíÅ ‚Ñì ) s.t. ùíÅ ‚Ñì + 1 ‚äÜ ùïä d ‚àí 1 , ‚Ñì = 1 , 2 , ‚Ä¶ , \\bm{Z}^{\\ell+1}\\;\\propto\\;\\bm{Z}^{\\ell}+\\eta\\cdot\\frac{\\partial\\Delta R_{\\epsilon}}{\\partial\\bm{Z}}(\\bm{Z}^{\\ell})\\quad\\mbox{s.t.}\\quad\\bm{Z}^{\\ell+1}\\subseteq\\mathbb{S}^{d-1},\\quad\\ell=1,2,\\ldots, bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì + 1 end_POSTSUPERSCRIPT ‚àù bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT + italic_Œ∑ ‚ãÖ divide start_ARG ‚àÇ roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT end_ARG start_ARG ‚àÇ bold_italic_Z end_ARG ( bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E3", "title": "1 2 ‚Äã ‚àÇ log ‚Äã det ( ùë∞ + Œ± ‚Äã ùíÅ ‚Äã ùíÅ ‚ä§ ) ‚àÇ ùíÅ ‚Äã ( ùíÅ ‚Ñì ) = Œ± ‚Äã ( ùë∞ + Œ± ‚Äã ùíÅ ‚Ñì ‚Äã ( ùíÅ ‚Ñì ) ‚ä§ ) ‚àí 1 ‚èü ùë¨ ‚Ñì ‚àà ‚Ñù d √ó d ‚Äã ùíÅ ‚Ñì , \\frac{1}{2}\\frac{\\partial\\log\\det(\\bm{I}\\!+\\!\\alpha\\bm{Z}\\bm{Z}^{\\top})}{\\partial\\bm{Z", "snippet": "1 2 ‚Äã ‚àÇ log ‚Äã det ( ùë∞ + Œ± ‚Äã ùíÅ ‚Äã ùíÅ ‚ä§ ) ‚àÇ ùíÅ ‚Äã ( ùíÅ ‚Ñì ) = Œ± ‚Äã ( ùë∞ + Œ± ‚Äã ùíÅ ‚Ñì ‚Äã ( ùíÅ ‚Ñì ) ‚ä§ ) ‚àí 1 ‚èü ùë¨ ‚Ñì ‚àà ‚Ñù d √ó d ‚Äã ùíÅ ‚Ñì , \\frac{1}{2}\\frac{\\partial\\log\\det(\\bm{I}\\!+\\!\\alpha\\bm{Z}\\bm{Z}^{\\top})}{\\partial\\bm{Z}}(\\bm{Z}^{\\ell})=\\underbrace{\\alpha(\\bm{I}\\!+\\!\\alpha\\bm{Z}^{\\ell}(\\bm{Z}^{\\ell})^{\\top})^{-1}}_{\\bm{E}^{\\ell}\\;\\in\\mathbb{R}^{d\\times d}}\\bm{Z}^{\\ell}, divide start_ARG 1 end_ARG start_ARG 2 end_ARG divide start_ARG ‚àÇ roman_log roman_det ( bold_italic_I + italic_Œ± bold_italic_Z bold_italic_Z start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ) end_ARG start_ARG ‚àÇ bold_italic_Z end_ARG ( bold_italic_Z s"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E4", "title": "1 2 ‚Äã ‚àÇ ( Œ≥ k ‚Äã log ‚Äã det ( ùë∞ + Œ± k ‚Äã ùíÅ ‚Äã ùö∑ k ‚Äã ùíÅ ‚ä§ ) ) ‚àÇ ùíÅ ‚Äã ( ùíÅ ‚Ñì ) = Œ≥ k ‚Äã Œ± k ‚Äã ( ùë∞ + Œ± k ‚Äã ùíÅ ‚Ñì ‚Äã ùö∑ k ‚Äã ( ùíÅ ‚Ñì ) ‚ä§ ) ‚àí 1 ‚èü ùë™ k ‚Ñì ‚àà ‚Ñù d √ó d ‚Äã ùíÅ ‚Ñì ‚Äã ùö∑ k . \\frac{1}{2}\\frac{\\partial\\left(\\gamma_{k}\\lo", "snippet": "1 2 ‚Äã ‚àÇ ( Œ≥ k ‚Äã log ‚Äã det ( ùë∞ + Œ± k ‚Äã ùíÅ ‚Äã ùö∑ k ‚Äã ùíÅ ‚ä§ ) ) ‚àÇ ùíÅ ‚Äã ( ùíÅ ‚Ñì ) = Œ≥ k ‚Äã Œ± k ‚Äã ( ùë∞ + Œ± k ‚Äã ùíÅ ‚Ñì ‚Äã ùö∑ k ‚Äã ( ùíÅ ‚Ñì ) ‚ä§ ) ‚àí 1 ‚èü ùë™ k ‚Ñì ‚àà ‚Ñù d √ó d ‚Äã ùíÅ ‚Ñì ‚Äã ùö∑ k . \\frac{1}{2}\\frac{\\partial\\left(\\gamma_{k}\\log\\det(\\bm{I}+\\alpha_{k}\\bm{Z}\\bm{\\Pi}_{k}\\bm{Z}^{\\top})\\right)}{\\partial\\bm{Z}}(\\bm{Z}^{\\ell})=\\gamma_{k}\\underbrace{\\alpha_{k}(\\bm{I}+\\alpha_{k}\\bm{Z}^{\\ell}\\bm{\\Pi}_{k}(\\bm{Z}^{\\ell})^{\\top})^{-1}}_{\\bm{C}^{\\ell}_{k}\\;\\in\\mathbb{R}^{d\\times d}}\\bm{Z}^{\\ell}\\bm{\\Pi}_{k}. divide start_ARG 1 end_ARG start_ARG 2 end_ARG divide start_ARG ‚àÇ ( italic_Œ≥ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT rom"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E5", "title": "‚àÇ Œî ‚Äã R œµ ‚àÇ ùíÅ ‚Äã ( ùíÅ ‚Ñì ) = ùë¨ ‚Ñì ‚èü Expansion ‚Äã ùíÅ ‚Ñì ‚àí ‚àë k = 1 K Œ≥ k ‚Äã ùë™ k ‚Ñì ‚èü Compression ‚Äã ùíÅ ‚Ñì ‚Äã ùö∑ k . \\frac{\\partial\\Delta R_{\\epsilon}}{\\partial\\bm{Z}}(\\bm{Z}^{\\ell})=\\underbrace{\\bm{E}^{\\ell}}_{\\text{", "snippet": "‚àÇ Œî ‚Äã R œµ ‚àÇ ùíÅ ‚Äã ( ùíÅ ‚Ñì ) = ùë¨ ‚Ñì ‚èü Expansion ‚Äã ùíÅ ‚Ñì ‚àí ‚àë k = 1 K Œ≥ k ‚Äã ùë™ k ‚Ñì ‚èü Compression ‚Äã ùíÅ ‚Ñì ‚Äã ùö∑ k . \\frac{\\partial\\Delta R_{\\epsilon}}{\\partial\\bm{Z}}(\\bm{Z}^{\\ell})=\\underbrace{\\bm{E}^{\\ell}}_{\\text{Expansion}}\\bm{Z}^{\\ell}\\;-\\;\\sum_{k=1}^{K}\\gamma_{k}\\underbrace{\\bm{C}_{k}^{\\ell}}_{\\text{Compression}}\\bm{Z}^{\\ell}\\bm{\\Pi}_{k}. divide start_ARG ‚àÇ roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT end_ARG start_ARG ‚àÇ bold_italic_Z end_ARG ( bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ) = under‚èü start_ARG bold_italic_E start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPER"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E7", "title": "ùë¨ ‚Ñì = Œ± ‚Äã ùëº ‚Ñì ‚Äã diag ‚Å° ( 1 1 + Œ± ‚Äã Œª 1 ‚Ñì , ‚Ä¶ , 1 1 + Œ± ‚Äã Œª d ‚Ñì ) ‚Äã ( ùëº ‚Ñì ) ‚ä§ . \\bm{E}^{\\ell}=\\alpha\\bm{U}^{\\ell}\\,\\operatorname{diag}\\left(\\frac{1}{1+\\alpha\\lambda^{\\ell}_{1}},\\ldots,\\frac{1}{1+\\alpha", "snippet": "ùë¨ ‚Ñì = Œ± ‚Äã ùëº ‚Ñì ‚Äã diag ‚Å° ( 1 1 + Œ± ‚Äã Œª 1 ‚Ñì , ‚Ä¶ , 1 1 + Œ± ‚Äã Œª d ‚Ñì ) ‚Äã ( ùëº ‚Ñì ) ‚ä§ . \\bm{E}^{\\ell}=\\alpha\\bm{U}^{\\ell}\\,\\operatorname{diag}\\left(\\frac{1}{1+\\alpha\\lambda^{\\ell}_{1}},\\ldots,\\frac{1}{1+\\alpha\\lambda^{\\ell}_{d}}\\right)\\left(\\bm{U}^{\\ell}\\right)^{\\top}. bold_italic_E start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT = italic_Œ± bold_italic_U start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT roman_diag ( divide start_ARG 1 end_ARG start_ARG 1 + italic_Œ± italic_Œª start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_ARG , ‚Ä¶ , divide start_ARG 1 end_A"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E8", "title": "ùíõ ‚Ñì + 1 ‚àù ùíõ ‚Ñì + Œ∑ ‚ãÖ g ‚Äã ( ùíõ ‚Ñì , ùúΩ ‚Ñì ) subject to ùíõ ‚Ñì + 1 ‚àà ùïä d ‚àí 1 \\bm{z}^{\\ell+1}\\;\\propto\\;\\bm{z}^{\\ell}+\\eta\\cdot g(\\bm{z}^{\\ell},\\bm{\\theta}^{\\ell})\\quad\\mbox{subject to}\\quad\\bm{z}^{\\ell+1}\\in\\ma", "snippet": "ùíõ ‚Ñì + 1 ‚àù ùíõ ‚Ñì + Œ∑ ‚ãÖ g ‚Äã ( ùíõ ‚Ñì , ùúΩ ‚Ñì ) subject to ùíõ ‚Ñì + 1 ‚àà ùïä d ‚àí 1 \\bm{z}^{\\ell+1}\\;\\propto\\;\\bm{z}^{\\ell}+\\eta\\cdot g(\\bm{z}^{\\ell},\\bm{\\theta}^{\\ell})\\quad\\mbox{subject to}\\quad\\bm{z}^{\\ell+1}\\in\\mathbb{S}^{d-1} bold_italic_z start_POSTSUPERSCRIPT roman_‚Ñì + 1 end_POSTSUPERSCRIPT ‚àù bold_italic_z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT + italic_Œ∑ ‚ãÖ italic_g ( bold_italic_z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT , bold_italic_Œ∏ start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ) subject to bold_italic_z start_POSTSUPERSCRIPT roman_‚Ñì + 1 end_POSTSUPERSCRIPT ‚àà blackboard_S st"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E9", "title": "ùíõ Àô = g ‚Äã ( ùíõ , Œ∏ ) . \\dot{\\bm{z}}=g(\\bm{z},\\theta). overÀô start_ARG bold_italic_z end_ARG = italic_g ( bold_italic_z , italic_Œ∏ ) . (4.1.9)", "snippet": "ùíõ Àô = g ‚Äã ( ùíõ , Œ∏ ) . \\dot{\\bm{z}}=g(\\bm{z},\\theta). overÀô start_ARG bold_italic_z end_ARG = italic_g ( bold_italic_z , italic_Œ∏ ) . (4.1.9)"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.Ex2", "title": "ùíÅ Àô = ‚àÇ Œî ‚Äã R œµ ‚àÇ ùíÅ , \\dot{\\bm{Z}}=\\frac{\\partial\\Delta R_{\\epsilon}}{\\partial\\bm{Z}}, overÀô start_ARG bold_italic_Z end_ARG = divide start_ARG ‚àÇ roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POST", "snippet": "ùíÅ Àô = ‚àÇ Œî ‚Äã R œµ ‚àÇ ùíÅ , \\dot{\\bm{Z}}=\\frac{\\partial\\Delta R_{\\epsilon}}{\\partial\\bm{Z}}, overÀô start_ARG bold_italic_Z end_ARG = divide start_ARG ‚àÇ roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT end_ARG start_ARG ‚àÇ bold_italic_Z end_ARG ,"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E10", "title": "g ‚Äã ( ùíõ ‚Ñì , ùúΩ ‚Ñì ) ‚âê ùë¨ ‚Ñì ‚Äã ùíõ ‚Ñì ‚àí ‚àë k = 1 K Œ≥ k ‚Äã œÄ k ‚Äã ( ùíõ ‚Ñì ) ‚Äã ùë™ k ‚Ñì ‚Äã ùíõ ‚Ñì ‚àà ‚Ñù d , g(\\bm{z}^{\\ell},\\bm{\\theta}^{\\ell})\\;\\doteq\\;\\bm{E}^{\\ell}\\bm{z}^{\\ell}-\\sum_{k=1}^{K}\\gamma_{k}\\pi_{k}(\\bm{z}^{\\ell", "snippet": "g ‚Äã ( ùíõ ‚Ñì , ùúΩ ‚Ñì ) ‚âê ùë¨ ‚Ñì ‚Äã ùíõ ‚Ñì ‚àí ‚àë k = 1 K Œ≥ k ‚Äã œÄ k ‚Äã ( ùíõ ‚Ñì ) ‚Äã ùë™ k ‚Ñì ‚Äã ùíõ ‚Ñì ‚àà ‚Ñù d , g(\\bm{z}^{\\ell},\\bm{\\theta}^{\\ell})\\;\\doteq\\;\\bm{E}^{\\ell}\\bm{z}^{\\ell}-\\sum_{k=1}^{K}\\gamma_{k}\\pi_{k}(\\bm{z}^{\\ell})\\bm{C}_{k}^{\\ell}\\bm{z}^{\\ell}\\in\\mathbb{R}^{d}, italic_g ( bold_italic_z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT , bold_italic_Œ∏ start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ) ‚âê bold_italic_E start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT bold_italic_z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT - ‚àë start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E11", "title": "ùùÖ ^ ‚Äã ( ùíõ ‚Ñì ) ‚âê softmax ‚Å° ( ‚àí Œª ‚Äã [ ‚Äñ ùë™ 1 ‚Ñì ‚Äã ùíõ ‚Ñì ‚Äñ 2 ‚ãÆ ‚Äñ ùë™ K ‚Ñì ‚Äã ùíõ ‚Ñì ‚Äñ 2 ] ) = 1 ‚àë k = 1 K exp ‚Å° ( ‚àí Œª ‚Äã ‚Äñ ùë™ k ‚Ñì ‚Äã ùíõ ‚Ñì ‚Äñ 2 ) ‚Äã [ exp ‚Å° ( ‚àí Œª ‚Äã ‚Äñ ùë™ 1 ‚Ñì ‚Äã ùíõ ‚Ñì ‚Äñ 2 ) ‚ãÆ exp ‚Å° ( ‚àí Œª ‚Äã ‚Äñ ùë™ K ‚Ñì ‚Äã ùíõ ‚Ñì ‚Äñ 2 ) ", "snippet": "ùùÖ ^ ‚Äã ( ùíõ ‚Ñì ) ‚âê softmax ‚Å° ( ‚àí Œª ‚Äã [ ‚Äñ ùë™ 1 ‚Ñì ‚Äã ùíõ ‚Ñì ‚Äñ 2 ‚ãÆ ‚Äñ ùë™ K ‚Ñì ‚Äã ùíõ ‚Ñì ‚Äñ 2 ] ) = 1 ‚àë k = 1 K exp ‚Å° ( ‚àí Œª ‚Äã ‚Äñ ùë™ k ‚Ñì ‚Äã ùíõ ‚Ñì ‚Äñ 2 ) ‚Äã [ exp ‚Å° ( ‚àí Œª ‚Äã ‚Äñ ùë™ 1 ‚Ñì ‚Äã ùíõ ‚Ñì ‚Äñ 2 ) ‚ãÆ exp ‚Å° ( ‚àí Œª ‚Äã ‚Äñ ùë™ K ‚Ñì ‚Äã ùíõ ‚Ñì ‚Äñ 2 ) ] ‚àà [ 0 , 1 ] K . \\widehat{\\bm{\\pi}}(\\bm{z}^{\\ell})\\doteq\\operatorname{\\mathrm{softmax}}\\left(-\\lambda\\begin{bmatrix}\\|\\bm{C}^{\\ell}_{1}\\bm{z}^{\\ell}\\|_{2}\\\\ \\vdots\\\\ \\|\\bm{C}^{\\ell}_{K}\\bm{z}^{\\ell}\\|_{2}\\end{bmatrix}\\right)=\\frac{1}{\\sum_{k=1}^{K}\\exp(-\\lambda\\|\\bm{C}^{\\ell}_{k}\\bm{z}^{\\ell}\\|_{2})}\\begin{bmatrix}\\exp(-\\lambda\\|\\bm{C}^{\\ell}_{1}\\bm{z}^{\\ell}\\|_{2})\\\\ \\vdots\\\\ \\exp(-\\lambda\\|\\bm{"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E13X", "title": "ùíõ ‚Ñì + 1 \\displaystyle\\bm{z}^{\\ell+1} bold_italic_z start_POSTSUPERSCRIPT roman_‚Ñì + 1 end_POSTSUPERSCRIPT ‚àù ùíõ ‚Ñì + Œ∑ ‚ãÖ ùë¨ ‚Ñì ‚Äã ùíõ ‚Ñì ‚àí Œ∑ ‚ãÖ ùùà ‚Äã ( [ ùë™ 1 ‚Ñì ‚Äã ùíõ ‚Ñì , ‚Ä¶ , ùë™ K ‚Ñì ‚Äã ùíõ ‚Ñì ] ) \\displaystyle\\propto\\;\\bm", "snippet": "ùíõ ‚Ñì + 1 \\displaystyle\\bm{z}^{\\ell+1} bold_italic_z start_POSTSUPERSCRIPT roman_‚Ñì + 1 end_POSTSUPERSCRIPT ‚àù ùíõ ‚Ñì + Œ∑ ‚ãÖ ùë¨ ‚Ñì ‚Äã ùíõ ‚Ñì ‚àí Œ∑ ‚ãÖ ùùà ‚Äã ( [ ùë™ 1 ‚Ñì ‚Äã ùíõ ‚Ñì , ‚Ä¶ , ùë™ K ‚Ñì ‚Äã ùíõ ‚Ñì ] ) \\displaystyle\\propto\\;\\bm{z}^{\\ell}+\\eta\\cdot\\bm{E}^{\\ell}\\bm{z}^{\\ell}-\\eta\\cdot\\bm{\\sigma}\\big{(}[\\bm{C}^{\\ell}_{1}\\bm{z}^{\\ell},\\dots,\\bm{C}^{\\ell}_{K}\\bm{z}^{\\ell}]\\big{)} ‚àù bold_italic_z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT + italic_Œ∑ ‚ãÖ bold_italic_E start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT bold_italic_z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT - italic_Œ∑ ‚ãÖ bold_italic_œÉ ( [ bold_italic"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E13Xa", "title": "= ùíõ ‚Ñì + Œ∑ ‚ãÖ g ‚Äã ( ùíõ ‚Ñì , ùúΩ ‚Ñì ) s.t. ùíõ ‚Ñì + 1 ‚àà ùïä d ‚àí 1 , \\displaystyle=\\;\\bm{z}^{\\ell}+\\eta\\cdot g(\\bm{z}^{\\ell},\\bm{\\theta}^{\\ell})\\qquad\\mbox{s.t.}\\quad\\bm{z}^{\\ell+1}\\in\\mathbb{S}^{d-1}, = bold_itali", "snippet": "= ùíõ ‚Ñì + Œ∑ ‚ãÖ g ‚Äã ( ùíõ ‚Ñì , ùúΩ ‚Ñì ) s.t. ùíõ ‚Ñì + 1 ‚àà ùïä d ‚àí 1 , \\displaystyle=\\;\\bm{z}^{\\ell}+\\eta\\cdot g(\\bm{z}^{\\ell},\\bm{\\theta}^{\\ell})\\qquad\\mbox{s.t.}\\quad\\bm{z}^{\\ell+1}\\in\\mathbb{S}^{d-1}, = bold_italic_z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT + italic_Œ∑ ‚ãÖ italic_g ( bold_italic_z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT , bold_italic_Œ∏ start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ) s.t. bold_italic_z start_POSTSUPERSCRIPT roman_‚Ñì + 1 end_POSTSUPERSCRIPT ‚àà blackboard_S start_POSTSUPERSCRIPT italic_d - 1 end_POSTSUPERSCRIPT ,"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E14X", "title": "f ‚Äã ( ùíô , ùúΩ ) = \\displaystyle f(\\bm{x},\\bm{\\theta})\\;= italic_f ( bold_italic_x , bold_italic_Œ∏ ) = f L ‚àò f L ‚àí 1 ‚àò ‚ãØ ‚àò f 1 ‚àò f 0 ‚Äã ( ùíõ 0 ) , \\displaystyle\\;\\;f^{L}\\circ f^{L-1}\\circ\\cdots\\circ f^{1}\\", "snippet": "f ‚Äã ( ùíô , ùúΩ ) = \\displaystyle f(\\bm{x},\\bm{\\theta})\\;= italic_f ( bold_italic_x , bold_italic_Œ∏ ) = f L ‚àò f L ‚àí 1 ‚àò ‚ãØ ‚àò f 1 ‚àò f 0 ‚Äã ( ùíõ 0 ) , \\displaystyle\\;\\;f^{L}\\circ f^{L-1}\\circ\\cdots\\circ f^{1}\\circ f^{0}(\\bm{z}^{0}), italic_f start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT ‚àò italic_f start_POSTSUPERSCRIPT italic_L - 1 end_POSTSUPERSCRIPT ‚àò ‚ãØ ‚àò italic_f start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ‚àò italic_f start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT ( bold_italic_z start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT ) , (4.1.14)"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E14Xa", "title": "f ‚Ñì ‚Äã ( ùíõ ‚Ñì , ùúΩ ‚Ñì ) ‚âê \\displaystyle f^{\\ell}(\\bm{z}^{\\ell},\\bm{\\theta}^{\\ell})\\;\\doteq italic_f start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ( bold_italic_z start_POSTSUPERSCRIPT roman_‚Ñì end_POST", "snippet": "f ‚Ñì ‚Äã ( ùíõ ‚Ñì , ùúΩ ‚Ñì ) ‚âê \\displaystyle f^{\\ell}(\\bm{z}^{\\ell},\\bm{\\theta}^{\\ell})\\;\\doteq italic_f start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ( bold_italic_z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT , bold_italic_Œ∏ start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ) ‚âê ùíõ ‚Ñì + 1 = ùí´ ùïä n ‚àí 1 ‚Äã [ ùíõ ‚Ñì + Œ∑ ‚ãÖ g ‚Äã ( ùíõ ‚Ñì , ùúΩ ‚Ñì ) ] , \\displaystyle\\;\\;\\bm{z}^{\\ell+1}=\\mathcal{P}_{\\mathbb{S}^{n-1}}[\\bm{z}^{\\ell}+\\eta\\cdot g(\\bm{z}^{\\ell},\\bm{\\theta}^{\\ell})], bold_italic_z start_POSTSUPERSCRIPT roman_‚Ñì + 1 end_POSTSUPERSCRIPT = caligraphic_P start_POSTSUBSCRIPT blackboard_S start_POSTSUPERSCRI"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E14Xb", "title": "g ‚Äã ( ùíõ ‚Ñì , ùúΩ ‚Ñì ) = \\displaystyle g(\\bm{z}^{\\ell},\\bm{\\theta}^{\\ell})\\;= italic_g ( bold_italic_z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT , bold_italic_Œ∏ start_POSTSUPERSCRIPT roman_‚Ñì end_PO", "snippet": "g ‚Äã ( ùíõ ‚Ñì , ùúΩ ‚Ñì ) = \\displaystyle g(\\bm{z}^{\\ell},\\bm{\\theta}^{\\ell})\\;= italic_g ( bold_italic_z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT , bold_italic_Œ∏ start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ) = ùë¨ ‚Ñì ‚Äã ùíõ ‚Ñì ‚àí ùùà ‚Äã ( [ ùë™ 1 ‚Ñì ‚Äã ùíõ ‚Ñì , ‚Ä¶ , ùë™ K ‚Ñì ‚Äã ùíõ ‚Ñì ] ) . \\displaystyle\\;\\;\\bm{E}^{\\ell}\\bm{z}^{\\ell}-\\bm{\\sigma}\\big{(}[\\bm{C}^{\\ell}_{1}\\bm{z}^{\\ell},\\dots,\\bm{C}^{\\ell}_{K}\\bm{z}^{\\ell}]\\big{)}. bold_italic_E start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT bold_italic_z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT - bold_italic_œÉ ( [ bold_italic_C start_POSTSUPERSCRIPT ro"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E15", "title": "Group Invariance: ‚Äã f ‚Äã ( ùíô ‚àò ùî§ , ùúΩ ) ‚àº f ‚Äã ( ùíô , ùúΩ ) , ‚àÄ ùî§ ‚àà ùîæ , \\mbox{\\em Group Invariance:}\\;f(\\bm{x}\\circ\\mathfrak{g},\\bm{\\theta})\\sim f(\\bm{x},\\bm{\\theta}),\\ \\forall\\mathfrak{g}\\in\\mathbb{G}, Gro", "snippet": "Group Invariance: ‚Äã f ‚Äã ( ùíô ‚àò ùî§ , ùúΩ ) ‚àº f ‚Äã ( ùíô , ùúΩ ) , ‚àÄ ùî§ ‚àà ùîæ , \\mbox{\\em Group Invariance:}\\;f(\\bm{x}\\circ\\mathfrak{g},\\bm{\\theta})\\sim f(\\bm{x},\\bm{\\theta}),\\ \\forall\\mathfrak{g}\\in\\mathbb{G}, Group Invariance: italic_f ( bold_italic_x ‚àò fraktur_g , bold_italic_Œ∏ ) ‚àº italic_f ( bold_italic_x , bold_italic_Œ∏ ) , ‚àÄ fraktur_g ‚àà blackboard_G , (4.1.15)"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E16", "title": "ùñºùóÇùóãùñº ‚Äã ( ùíô ) ‚âê [ x ‚Äã ( 0 ) x ‚Äã ( D ‚àí 1 ) ‚Ä¶ x ‚Äã ( 2 ) x ‚Äã ( 1 ) x ‚Äã ( 1 ) x ‚Äã ( 0 ) x ‚Äã ( D ‚àí 1 ) ‚ãØ x ‚Äã ( 2 ) ‚ãÆ x ‚Äã ( 1 ) x ‚Äã ( 0 ) ‚ã± ‚ãÆ x ‚Äã ( D ‚àí 2 ) ‚ãÆ ‚ã± ‚ã± x ‚Äã ( D ‚àí 1 ) x ‚Äã ( D ‚àí 1 ) x ‚Äã ( D ‚àí 2 ) ‚Ä¶ x", "snippet": "ùñºùóÇùóãùñº ‚Äã ( ùíô ) ‚âê [ x ‚Äã ( 0 ) x ‚Äã ( D ‚àí 1 ) ‚Ä¶ x ‚Äã ( 2 ) x ‚Äã ( 1 ) x ‚Äã ( 1 ) x ‚Äã ( 0 ) x ‚Äã ( D ‚àí 1 ) ‚ãØ x ‚Äã ( 2 ) ‚ãÆ x ‚Äã ( 1 ) x ‚Äã ( 0 ) ‚ã± ‚ãÆ x ‚Äã ( D ‚àí 2 ) ‚ãÆ ‚ã± ‚ã± x ‚Äã ( D ‚àí 1 ) x ‚Äã ( D ‚àí 1 ) x ‚Äã ( D ‚àí 2 ) ‚Ä¶ x ‚Äã ( 1 ) x ‚Äã ( 0 ) ] ‚àà ‚Ñù D √ó D . \\mathsf{circ}(\\bm{x})\\,\\doteq\\,\\left[\\begin{array}[]{ccccc}x(0)&x(D-1)&\\dots&x(2)&x(1)\\\\ x(1)&x(0)&x(D-1)&\\cdots&x(2)\\\\ \\vdots&x(1)&x(0)&\\ddots&\\vdots\\\\ x(D-2)&\\vdots&\\ddots&\\ddots&x(D-1)\\\\ x(D-1)&x(D-2)&\\dots&x(1)&x(0)\\end{array}\\right]\\in\\mathbb{R}^{D\\times D}. sansserif_circ ( bold_italic_x ) ‚âê [ start_ARRAY start_ROW start_CELL italic_x ( 0 ) end_CELL start_CEL"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E18", "title": "ùë¨ 1 = Œ± ‚Äã ( ùë∞ + Œ± ‚Äã ùñºùóÇùóãùñº ‚Äã ( ùíÅ 1 ) ‚Äã ùñºùóÇùóãùñº ‚Äã ( ùíÅ 1 ) ‚ä§ ) ‚àí 1 \\bm{E}^{1}=\\alpha\\big{(}\\bm{I}+\\alpha\\mathsf{circ}(\\bm{Z}^{1})\\mathsf{circ}(\\bm{Z}^{1})^{\\top}\\big{)}^{-1} bold_italic_E start_POSTSUPERSCRI", "snippet": "ùë¨ 1 = Œ± ‚Äã ( ùë∞ + Œ± ‚Äã ùñºùóÇùóãùñº ‚Äã ( ùíÅ 1 ) ‚Äã ùñºùóÇùóãùñº ‚Äã ( ùíÅ 1 ) ‚ä§ ) ‚àí 1 \\bm{E}^{1}=\\alpha\\big{(}\\bm{I}+\\alpha\\mathsf{circ}(\\bm{Z}^{1})\\mathsf{circ}(\\bm{Z}^{1})^{\\top}\\big{)}^{-1} bold_italic_E start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT = italic_Œ± ( bold_italic_I + italic_Œ± sansserif_circ ( bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) sansserif_circ ( bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT (4.1.18)"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.Ex4", "title": "ùë¨ 1 ‚Äã ùíõ = ùíÜ 1 ‚äõ ùíõ , \\bm{E}^{1}\\bm{z}=\\bm{e}_{1}\\circledast\\bm{z}, bold_italic_E start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT bold_italic_z = bold_italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ‚äõ bold", "snippet": "ùë¨ 1 ‚Äã ùíõ = ùíÜ 1 ‚äõ ùíõ , \\bm{E}^{1}\\bm{z}=\\bm{e}_{1}\\circledast\\bm{z}, bold_italic_E start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT bold_italic_z = bold_italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ‚äõ bold_italic_z ,"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.Ex5", "title": "( ùíÜ 1 ‚äõ ùíõ ) i ‚âê ‚àë j = 0 d ‚àí 1 e 1 ‚Äã ( j ) ‚Äã x ‚Äã ( i + d ‚àí j ‚Äã mod ‚Äã d ) . (\\bm{e}_{1}\\circledast\\bm{z})_{i}\\doteq\\sum_{j=0}^{d-1}e_{1}(j)x(i+d-j\\,\\,\\textsf{mod}\\,\\,d). ( bold_italic_e start_POSTSUBSCR", "snippet": "( ùíÜ 1 ‚äõ ùíõ ) i ‚âê ‚àë j = 0 d ‚àí 1 e 1 ‚Äã ( j ) ‚Äã x ‚Äã ( i + d ‚àí j ‚Äã mod ‚Äã d ) . (\\bm{e}_{1}\\circledast\\bm{z})_{i}\\doteq\\sum_{j=0}^{d-1}e_{1}(j)x(i+d-j\\,\\,\\textsf{mod}\\,\\,d). ( bold_italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ‚äõ bold_italic_z ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ‚âê ‚àë start_POSTSUBSCRIPT italic_j = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d - 1 end_POSTSUPERSCRIPT italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_j ) italic_x ( italic_i + italic_d - italic_j mod italic_d ) ."}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E19", "title": "ùñºùóÇùóãùñº ‚Äã ( ùíõ 1 ) + Œ∑ ‚ãÖ ùë¨ 1 ‚Äã ùñºùóÇùóãùñº ‚Äã ( ùíõ 1 ) ‚àí Œ∑ ‚ãÖ ùùà ‚Äã ( [ ùë™ 1 1 ‚Äã ùñºùóÇùóãùñº ‚Äã ( ùíõ 1 ) , ‚Ä¶ , ùë™ K 1 ‚Äã ùñºùóÇùóãùñº ‚Äã ( ùíõ 1 ) ] ) , \\mathsf{circ}(\\bm{z}^{1})+\\eta\\cdot\\bm{E}^{1}\\mathsf{circ}(\\bm{z}^{1})-\\eta\\cdot\\bm{\\s", "snippet": "ùñºùóÇùóãùñº ‚Äã ( ùíõ 1 ) + Œ∑ ‚ãÖ ùë¨ 1 ‚Äã ùñºùóÇùóãùñº ‚Äã ( ùíõ 1 ) ‚àí Œ∑ ‚ãÖ ùùà ‚Äã ( [ ùë™ 1 1 ‚Äã ùñºùóÇùóãùñº ‚Äã ( ùíõ 1 ) , ‚Ä¶ , ùë™ K 1 ‚Äã ùñºùóÇùóãùñº ‚Äã ( ùíõ 1 ) ] ) , \\mathsf{circ}(\\bm{z}^{1})+\\eta\\cdot\\bm{E}^{1}\\mathsf{circ}(\\bm{z}^{1})-\\eta\\cdot\\bm{\\sigma}\\Big{(}[\\bm{C}_{1}^{1}\\mathsf{circ}(\\bm{z}^{1}),\\ldots,\\bm{C}^{1}_{K}\\mathsf{circ}(\\bm{z}^{1})]\\Big{)}, sansserif_circ ( bold_italic_z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) + italic_Œ∑ ‚ãÖ bold_italic_E start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT sansserif_circ ( bold_italic_z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) - italic_Œ∑ ‚ãÖ bold_italic_œÉ ( [ bold_italic_C start_POSTSUBSCRIPT 1"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E20", "title": "ùíõ 2 ‚àù ùíõ 1 + Œ∑ ‚ãÖ g ‚Äã ( ùíõ 1 , ùúΩ 1 ) = ùíõ 1 + Œ∑ ‚ãÖ ùíÜ 1 ‚äõ ùíõ 1 ‚àí Œ∑ ‚ãÖ ùùà ‚Äã ( [ ùíÑ 1 1 ‚äõ ùíõ 1 , ‚Ä¶ , ùíÑ K 1 ‚äõ ùíõ 1 ] ) , \\bm{z}^{2}\\propto\\bm{z}^{1}+\\eta\\cdot g(\\bm{z}^{1},\\bm{\\theta}^{1})=\\bm{z}^{1}+\\eta\\cdot\\bm{e}", "snippet": "ùíõ 2 ‚àù ùíõ 1 + Œ∑ ‚ãÖ g ‚Äã ( ùíõ 1 , ùúΩ 1 ) = ùíõ 1 + Œ∑ ‚ãÖ ùíÜ 1 ‚äõ ùíõ 1 ‚àí Œ∑ ‚ãÖ ùùà ‚Äã ( [ ùíÑ 1 1 ‚äõ ùíõ 1 , ‚Ä¶ , ùíÑ K 1 ‚äõ ùíõ 1 ] ) , \\bm{z}^{2}\\propto\\bm{z}^{1}+\\eta\\cdot g(\\bm{z}^{1},\\bm{\\theta}^{1})=\\bm{z}^{1}+\\eta\\cdot\\bm{e}_{1}\\circledast\\bm{z}^{1}-\\eta\\cdot\\bm{\\sigma}\\Big{(}[\\bm{c}_{1}^{1}\\circledast\\bm{z}^{1},\\dots,\\bm{c}^{1}_{K}\\circledast\\bm{z}^{1}]\\Big{)}, bold_italic_z start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ‚àù bold_italic_z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT + italic_Œ∑ ‚ãÖ italic_g ( bold_italic_z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , bold_italic_Œ∏ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.Ex6", "title": "ùñºùóÇùóãùñº ‚Äã ( ùíÅ 2 ) = [ ùñºùóÇùóãùñº ‚Äã ( ùíõ 1 1 + Œ∑ ‚Äã g ‚Äã ( ùíõ 1 1 , ùúΩ 1 ) ) , ‚Ä¶ , ùñºùóÇùóãùñº ‚Äã ( ùíõ N 1 + Œ∑ ‚Äã g ‚Äã ( ùíõ N 1 , ùúΩ 1 ) ) ] . \\mathsf{circ}(\\bm{Z}^{2})=\\big{[}\\mathsf{circ}(\\bm{z}_{1}^{1}+\\eta g(\\bm{z}_{1}^{1},\\", "snippet": "ùñºùóÇùóãùñº ‚Äã ( ùíÅ 2 ) = [ ùñºùóÇùóãùñº ‚Äã ( ùíõ 1 1 + Œ∑ ‚Äã g ‚Äã ( ùíõ 1 1 , ùúΩ 1 ) ) , ‚Ä¶ , ùñºùóÇùóãùñº ‚Äã ( ùíõ N 1 + Œ∑ ‚Äã g ‚Äã ( ùíõ N 1 , ùúΩ 1 ) ) ] . \\mathsf{circ}(\\bm{Z}^{2})=\\big{[}\\mathsf{circ}(\\bm{z}_{1}^{1}+\\eta g(\\bm{z}_{1}^{1},\\bm{\\theta}^{1})),\\dots,\\mathsf{circ}(\\bm{z}_{N}^{1}+\\eta g(\\bm{z}_{N}^{1},\\bm{\\theta}^{1}))\\big{]}. sansserif_circ ( bold_italic_Z start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) = [ sansserif_circ ( bold_italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT + italic_Œ∑ italic_g ( bold_italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E21", "title": "ùíõ ‚Äã [ c ] = ùíå c ‚äõ ùíô = ùñºùóÇùóãùñº ‚Äã ( ùíå c ) ‚Äã ùíô ‚àà ‚Ñù d , c = 1 , ‚Ä¶ , C . \\bm{z}[c]=\\bm{k}_{c}\\circledast\\bm{x}=\\mathsf{circ}(\\bm{k}_{c})\\bm{x}\\in\\mathbb{R}^{d},\\quad c=1,\\ldots,C. bold_italic_z [ italic_c ] =", "snippet": "ùíõ ‚Äã [ c ] = ùíå c ‚äõ ùíô = ùñºùóÇùóãùñº ‚Äã ( ùíå c ) ‚Äã ùíô ‚àà ‚Ñù d , c = 1 , ‚Ä¶ , C . \\bm{z}[c]=\\bm{k}_{c}\\circledast\\bm{x}=\\mathsf{circ}(\\bm{k}_{c})\\bm{x}\\in\\mathbb{R}^{d},\\quad c=1,\\ldots,C. bold_italic_z [ italic_c ] = bold_italic_k start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ‚äõ bold_italic_x = sansserif_circ ( bold_italic_k start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ) bold_italic_x ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , italic_c = 1 , ‚Ä¶ , italic_C . (4.1.21)"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E22X", "title": "ùñºùóÇùóãùñº ‚Äã ( ùíõ ¬Ø ) ‚âê [ ùñºùóÇùóãùñº ‚Äã ( ùíõ ‚Äã [ 1 ] ) ‚ãÆ ùñºùóÇùóãùñº ‚Äã ( ùíõ ‚Äã [ C ] ) ] ‚àà ‚Ñù d ‚Äã C √ó d , ùö∫ ¬Ø ‚Äã ( ùíõ ¬Ø ) ‚âê [ ùñºùóÇùóãùñº ‚Äã ( ùíõ ‚Äã [ 1 ] ) ‚ãÆ ùñºùóÇùóãùñº ‚Äã ( ùíõ ‚Äã [ C ] ) ] ‚Äã [ ùñºùóÇùóãùñº ‚Äã ( ùíõ ‚Äã [ 1 ] ) ‚ä§ , ‚Ä¶ , ùñºùóÇùóãùñº ‚Äã ( ùíõ ‚Äã [ C ] ) ‚ä§", "snippet": "ùñºùóÇùóãùñº ‚Äã ( ùíõ ¬Ø ) ‚âê [ ùñºùóÇùóãùñº ‚Äã ( ùíõ ‚Äã [ 1 ] ) ‚ãÆ ùñºùóÇùóãùñº ‚Äã ( ùíõ ‚Äã [ C ] ) ] ‚àà ‚Ñù d ‚Äã C √ó d , ùö∫ ¬Ø ‚Äã ( ùíõ ¬Ø ) ‚âê [ ùñºùóÇùóãùñº ‚Äã ( ùíõ ‚Äã [ 1 ] ) ‚ãÆ ùñºùóÇùóãùñº ‚Äã ( ùíõ ‚Äã [ C ] ) ] ‚Äã [ ùñºùóÇùóãùñº ‚Äã ( ùíõ ‚Äã [ 1 ] ) ‚ä§ , ‚Ä¶ , ùñºùóÇùóãùñº ‚Äã ( ùíõ ‚Äã [ C ] ) ‚ä§ ] ‚àà ‚Ñù d ‚Äã C √ó d ‚Äã C , \\displaystyle\\mathsf{circ}(\\bar{\\bm{z}})\\doteq\\left[\\begin{matrix}\\mathsf{circ}(\\bm{z}[1])\\\\ \\vdots\\\\ \\mathsf{circ}(\\bm{z}[C])\\end{matrix}\\right]\\in\\mathbb{R}^{dC\\times d},\\quad\\bar{\\bm{\\Sigma}}(\\bar{\\bm{z}})\\doteq\\left[\\begin{matrix}\\mathsf{circ}(\\bm{z}[1])\\\\ \\vdots\\\\ \\mathsf{circ}(\\bm{z}[C])\\end{matrix}\\right]\\left[\\begin{matrix}\\mathsf{circ}(\\bm{z}[1])^{\\top},\\ldots,\\math"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E23", "title": "ùíô = ùíÖ k , 1 ‚äõ z 1 + ‚Ä¶ + ùíÖ k , c ‚äõ z c = ùñºùóÇùóãùñº ‚Äã ( ùë´ k ) ‚Äã ùíõ , \\bm{x}=\\bm{d}_{k,1}\\circledast z_{1}+\\ldots+\\bm{d}_{k,c}\\circledast z_{c}=\\mathsf{circ}(\\bm{D}_{k})\\bm{z}, bold_italic_x = bold_italic_d st", "snippet": "ùíô = ùíÖ k , 1 ‚äõ z 1 + ‚Ä¶ + ùíÖ k , c ‚äõ z c = ùñºùóÇùóãùñº ‚Äã ( ùë´ k ) ‚Äã ùíõ , \\bm{x}=\\bm{d}_{k,1}\\circledast z_{1}+\\ldots+\\bm{d}_{k,c}\\circledast z_{c}=\\mathsf{circ}(\\bm{D}_{k})\\bm{z}, bold_italic_x = bold_italic_d start_POSTSUBSCRIPT italic_k , 1 end_POSTSUBSCRIPT ‚äõ italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + ‚Ä¶ + bold_italic_d start_POSTSUBSCRIPT italic_k , italic_c end_POSTSUBSCRIPT ‚äõ italic_z start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT = sansserif_circ ( bold_italic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) bold_italic_z , (4.1.23)"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E24", "title": "ùíô = [ ùñºùóÇùóãùñº ‚Äã ( ùë´ 1 ) , ùñºùóÇùóãùñº ‚Äã ( ùë´ 2 ) , ‚Ä¶ , ùñºùóÇùóãùñº ‚Äã ( ùë´ K ) ] ‚Äã ùíõ ¬Ø , \\bm{x}=\\big{[}\\mathsf{circ}(\\bm{D}_{1}),\\mathsf{circ}(\\bm{D}_{2}),\\ldots,\\mathsf{circ}(\\bm{D}_{K})\\big{]}\\bar{\\bm{z}}, bold_italic_", "snippet": "ùíô = [ ùñºùóÇùóãùñº ‚Äã ( ùë´ 1 ) , ùñºùóÇùóãùñº ‚Äã ( ùë´ 2 ) , ‚Ä¶ , ùñºùóÇùóãùñº ‚Äã ( ùë´ K ) ] ‚Äã ùíõ ¬Ø , \\bm{x}=\\big{[}\\mathsf{circ}(\\bm{D}_{1}),\\mathsf{circ}(\\bm{D}_{2}),\\ldots,\\mathsf{circ}(\\bm{D}_{K})\\big{]}\\bar{\\bm{z}}, bold_italic_x = [ sansserif_circ ( bold_italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , sansserif_circ ( bold_italic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) , ‚Ä¶ , sansserif_circ ( bold_italic_D start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT ) ] over¬Ø start_ARG bold_italic_z end_ARG , (4.1.24)"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E25", "title": "[ ùíå 1 ‚äõ ùíô , ùíå 2 ‚äõ ùíô , ‚Ä¶ , ùíå C ‚äõ ùíô ] ‚ä§ = [ ùñºùóÇùóãùñº ‚Äã ( ùíå 1 ) ‚Äã ùíô , ‚Ä¶ , ùñºùóÇùóãùñº ‚Äã ( ùíå C ) ‚Äã ùíô ] ‚ä§ ‚àà ‚Ñù C √ó d \\big{[}\\bm{k}_{1}\\circledast\\bm{x},\\bm{k}_{2}\\circledast\\bm{x},\\ldots,\\bm{k}_{C}\\circledast\\bm{x}\\bi", "snippet": "[ ùíå 1 ‚äõ ùíô , ùíå 2 ‚äõ ùíô , ‚Ä¶ , ùíå C ‚äõ ùíô ] ‚ä§ = [ ùñºùóÇùóãùñº ‚Äã ( ùíå 1 ) ‚Äã ùíô , ‚Ä¶ , ùñºùóÇùóãùñº ‚Äã ( ùíå C ) ‚Äã ùíô ] ‚ä§ ‚àà ‚Ñù C √ó d \\big{[}\\bm{k}_{1}\\circledast\\bm{x},\\bm{k}_{2}\\circledast\\bm{x},\\ldots,\\bm{k}_{C}\\circledast\\bm{x}\\big{]}^{\\top}=\\big{[}\\mathsf{circ}(\\bm{k}_{1})\\bm{x},\\ldots,\\mathsf{circ}(\\bm{k}_{C})\\bm{x}\\big{]}^{\\top}\\in\\mathbb{R}^{C\\times d} [ bold_italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ‚äõ bold_italic_x , bold_italic_k start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ‚äõ bold_italic_x , ‚Ä¶ , bold_italic_k start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT ‚äõ bold_italic_x ] start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIP"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E26", "title": "ùíõ ¬Ø ‚âê ùùâ ‚Äã ( [ ùñºùóÇùóãùñº ‚Äã ( ùíå 1 ) ‚Äã ùíô , ‚Ä¶ , ùñºùóÇùóãùñº ‚Äã ( ùíå C ) ‚Äã ùíô ] ‚ä§ ) ‚àà ‚Ñù C √ó d . \\bar{\\bm{z}}\\doteq\\bm{\\tau}\\left(\\big{[}\\mathsf{circ}(\\bm{k}_{1})\\bm{x},\\ldots,\\mathsf{circ}(\\bm{k}_{C})\\bm{x}\\big{]}^{\\top}", "snippet": "ùíõ ¬Ø ‚âê ùùâ ‚Äã ( [ ùñºùóÇùóãùñº ‚Äã ( ùíå 1 ) ‚Äã ùíô , ‚Ä¶ , ùñºùóÇùóãùñº ‚Äã ( ùíå C ) ‚Äã ùíô ] ‚ä§ ) ‚àà ‚Ñù C √ó d . \\bar{\\bm{z}}\\doteq\\bm{\\tau}\\left(\\big{[}\\mathsf{circ}(\\bm{k}_{1})\\bm{x},\\ldots,\\mathsf{circ}(\\bm{k}_{C})\\bm{x}\\big{]}^{\\top}\\right)\\in\\mathbb{R}^{C\\times d}. over¬Ø start_ARG bold_italic_z end_ARG ‚âê bold_italic_œÑ ( [ sansserif_circ ( bold_italic_k start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) bold_italic_x , ‚Ä¶ , sansserif_circ ( bold_italic_k start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT ) bold_italic_x ] start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ) ‚àà blackboard_R start_POSTSUPERSCRIPT italic_C √ó italic_d end_POSTSUPERSCR"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E27", "title": "ùë¨ ¬Ø ‚âê Œ± ‚Äã ( ùë∞ + Œ± ‚Äã ùñºùóÇùóãùñº ‚Äã ( ùíÅ ¬Ø ) ‚Äã ùñºùóÇùóãùñº ‚Äã ( ùíÅ ¬Ø ) ‚ä§ ) ‚àí 1 \\bar{\\bm{E}}\\doteq\\alpha\\left(\\bm{I}+\\alpha\\,\\mathsf{circ}(\\bar{\\bm{Z}})\\mathsf{circ}(\\bar{\\bm{Z}})^{\\top}\\right)^{-1} over¬Ø start_ARG bold_", "snippet": "ùë¨ ¬Ø ‚âê Œ± ‚Äã ( ùë∞ + Œ± ‚Äã ùñºùóÇùóãùñº ‚Äã ( ùíÅ ¬Ø ) ‚Äã ùñºùóÇùóãùñº ‚Äã ( ùíÅ ¬Ø ) ‚ä§ ) ‚àí 1 \\bar{\\bm{E}}\\doteq\\alpha\\left(\\bm{I}+\\alpha\\,\\mathsf{circ}(\\bar{\\bm{Z}})\\mathsf{circ}(\\bar{\\bm{Z}})^{\\top}\\right)^{-1} over¬Ø start_ARG bold_italic_E end_ARG ‚âê italic_Œ± ( bold_italic_I + italic_Œ± sansserif_circ ( over¬Ø start_ARG bold_italic_Z end_ARG ) sansserif_circ ( over¬Ø start_ARG bold_italic_Z end_ARG ) start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT (4.1.27)"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.Ex7", "title": "ùë¨ ¬Ø = [ ùë¨ ¬Ø 1 , 1 ‚ãØ ùë¨ ¬Ø 1 , C ‚ãÆ ‚ã± ‚ãÆ ùë¨ ¬Ø C , 1 ‚ãØ ùë¨ ¬Ø C , C ] ‚àà ‚Ñù d ‚Äã C √ó d ‚Äã C , \\bar{\\bm{E}}=\\left[\\begin{matrix}\\bar{\\bm{E}}_{1,1}&\\cdots&\\bar{\\bm{E}}_{1,C}\\\\ \\vdots&\\ddots&\\vdots\\\\ \\bar{\\bm{E}}_{C,1", "snippet": "ùë¨ ¬Ø = [ ùë¨ ¬Ø 1 , 1 ‚ãØ ùë¨ ¬Ø 1 , C ‚ãÆ ‚ã± ‚ãÆ ùë¨ ¬Ø C , 1 ‚ãØ ùë¨ ¬Ø C , C ] ‚àà ‚Ñù d ‚Äã C √ó d ‚Äã C , \\bar{\\bm{E}}=\\left[\\begin{matrix}\\bar{\\bm{E}}_{1,1}&\\cdots&\\bar{\\bm{E}}_{1,C}\\\\ \\vdots&\\ddots&\\vdots\\\\ \\bar{\\bm{E}}_{C,1}&\\cdots&\\bar{\\bm{E}}_{C,C}\\\\ \\end{matrix}\\right]\\in\\mathbb{R}^{dC\\times dC}, over¬Ø start_ARG bold_italic_E end_ARG = [ start_ARG start_ROW start_CELL over¬Ø start_ARG bold_italic_E end_ARG start_POSTSUBSCRIPT 1 , 1 end_POSTSUBSCRIPT end_CELL start_CELL ‚ãØ end_CELL start_CELL over¬Ø start_ARG bold_italic_E end_ARG start_POSTSUBSCRIPT 1 , italic_C end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CEL"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.Ex8", "title": "ùë¨ ¬Ø ‚ãÖ vec ‚Äã ( ùíõ ¬Ø ) = vec ‚Äã ( ùíÜ ¬Ø ‚äõ ùíõ ¬Ø ) . \\bar{\\bm{E}}\\cdot\\textsf{vec}(\\bar{\\bm{z}})=\\textsf{vec}(\\bar{\\bm{e}}\\circledast\\bar{\\bm{z}}). over¬Ø start_ARG bold_italic_E end_ARG ‚ãÖ vec ( over¬Ø start_ARG", "snippet": "ùë¨ ¬Ø ‚ãÖ vec ‚Äã ( ùíõ ¬Ø ) = vec ‚Äã ( ùíÜ ¬Ø ‚äõ ùíõ ¬Ø ) . \\bar{\\bm{E}}\\cdot\\textsf{vec}(\\bar{\\bm{z}})=\\textsf{vec}(\\bar{\\bm{e}}\\circledast\\bar{\\bm{z}}). over¬Ø start_ARG bold_italic_E end_ARG ‚ãÖ vec ( over¬Ø start_ARG bold_italic_z end_ARG ) = vec ( over¬Ø start_ARG bold_italic_e end_ARG ‚äõ over¬Ø start_ARG bold_italic_z end_ARG ) ."}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.Ex9", "title": "( ùíÜ ¬Ø ‚äõ ùíõ ¬Ø ) ‚Äã [ c ] ‚âê ‚àë c ‚Ä≤ = 1 C ùíÜ ¬Ø ‚Äã [ c , c ‚Ä≤ ] ‚äõ ùíõ ¬Ø ‚Äã [ c ‚Ä≤ ] , ‚àÄ c = 1 , ‚Ä¶ , C . (\\bar{\\bm{e}}\\circledast\\bar{\\bm{z}})[c]\\doteq\\sum_{c^{\\prime}=1}^{C}\\bar{\\bm{e}}[c,c^{\\prime}]\\circledast\\bar", "snippet": "( ùíÜ ¬Ø ‚äõ ùíõ ¬Ø ) ‚Äã [ c ] ‚âê ‚àë c ‚Ä≤ = 1 C ùíÜ ¬Ø ‚Äã [ c , c ‚Ä≤ ] ‚äõ ùíõ ¬Ø ‚Äã [ c ‚Ä≤ ] , ‚àÄ c = 1 , ‚Ä¶ , C . (\\bar{\\bm{e}}\\circledast\\bar{\\bm{z}})[c]\\doteq\\sum_{c^{\\prime}=1}^{C}\\bar{\\bm{e}}[c,c^{\\prime}]\\circledast\\bar{\\bm{z}}[c^{\\prime}],\\quad\\forall c=1,\\ldots,C. ( over¬Ø start_ARG bold_italic_e end_ARG ‚äõ over¬Ø start_ARG bold_italic_z end_ARG ) [ italic_c ] ‚âê ‚àë start_POSTSUBSCRIPT italic_c start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_C end_POSTSUPERSCRIPT over¬Ø start_ARG bold_italic_e end_ARG [ italic_c , italic_c start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ] ‚äõ"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.E2", "title": "R œµ ‚Äã ( ùíÅ ) ‚âê 1 2 ‚Äã logdet ‚Äã ( ùë∞ + d N ‚Äã œµ 2 ‚Äã ùíÅ ‚ä§ ‚Äã ùíÅ ) = 1 2 ‚Äã logdet ‚Äã ( ùë∞ + d N ‚Äã œµ 2 ‚Äã ùíÅ ‚Äã ùíÅ ‚ä§ ) . R_{\\epsilon}(\\bm{Z})\\doteq\\frac{1}{2}\\textrm{logdet}\\left(\\bm{I}+\\frac{d}{N\\epsilon^{2}}\\bm{Z}^{", "snippet": "R œµ ‚Äã ( ùíÅ ) ‚âê 1 2 ‚Äã logdet ‚Äã ( ùë∞ + d N ‚Äã œµ 2 ‚Äã ùíÅ ‚ä§ ‚Äã ùíÅ ) = 1 2 ‚Äã logdet ‚Äã ( ùë∞ + d N ‚Äã œµ 2 ‚Äã ùíÅ ‚Äã ùíÅ ‚ä§ ) . R_{\\epsilon}(\\bm{Z})\\doteq\\frac{1}{2}\\textrm{logdet}\\left(\\bm{I}+\\frac{d}{N\\epsilon^{2}}\\bm{Z}^{\\top}\\bm{Z}\\right)=\\frac{1}{2}\\textrm{logdet}\\left(\\bm{I}+\\frac{d}{N\\epsilon^{2}}\\bm{Z}\\bm{Z}^{\\top}\\right). italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ) ‚âê divide start_ARG 1 end_ARG start_ARG 2 end_ARG logdet ( bold_italic_I + divide start_ARG italic_d end_ARG start_ARG italic_N italic_œµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG bold_italic_Z start_POSTSUPER"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.E3X", "title": "R œµ c ‚Äã ( ùíÅ ‚à£ ùëº [ K ] ) \\displaystyle R_{\\epsilon}^{c}(\\bm{Z}\\mid\\bm{U}_{[K]}) italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_", "snippet": "R œµ c ‚Äã ( ùíÅ ‚à£ ùëº [ K ] ) \\displaystyle R_{\\epsilon}^{c}(\\bm{Z}\\mid\\bm{U}_{[K]}) italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_Z ‚à£ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT ) ‚âê ‚àë k = 1 K R œµ ‚Äã ( ùëº k ‚ä§ ‚Äã ùíÅ ) = 1 2 ‚Äã ‚àë k = 1 K log ‚Äã det ( ùë∞ + p N ‚Äã œµ 2 ‚Äã ( ùëº k ‚ä§ ‚Äã ùíÅ ) ‚ä§ ‚Äã ( ùëº k ‚ä§ ‚Äã ùíÅ ) ) . \\displaystyle\\doteq\\sum_{k=1}^{K}R_{\\epsilon}(\\bm{U}_{k}^{\\top}\\bm{Z})=\\frac{1}{2}\\sum_{k=1}^{K}\\log\\det\\left(\\bm{I}+\\frac{p}{N\\epsilon^{2}}(\\bm{U}_{k}^{\\top}\\bm{Z})^{\\top}(\\bm{U}_{k}^{\\top}\\bm{Z})\\right). ‚âê ‚àë start"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.E4", "title": "max f ‚àà ‚Ñ± ‚Å° [ Œî ‚Äã R œµ ‚Äã ( ùíÅ ‚à£ ùëº [ K ] ) ‚àí Œª ‚Äã ‚Äñ ùíÅ ‚Äñ 0 ] s.t. ‚Äã ùíÅ = f ‚Äã ( ùëø ) , \\max_{f\\in\\mathcal{F}}\\ [\\Delta R_{\\epsilon}(\\bm{Z}\\mid\\bm{U}_{[K]})-\\lambda\\|\\bm{Z}\\|_{0}]\\qquad\\text{s.t.}\\ \\bm{Z}=f(\\b", "snippet": "max f ‚àà ‚Ñ± ‚Å° [ Œî ‚Äã R œµ ‚Äã ( ùíÅ ‚à£ ùëº [ K ] ) ‚àí Œª ‚Äã ‚Äñ ùíÅ ‚Äñ 0 ] s.t. ‚Äã ùíÅ = f ‚Äã ( ùëø ) , \\max_{f\\in\\mathcal{F}}\\ [\\Delta R_{\\epsilon}(\\bm{Z}\\mid\\bm{U}_{[K]})-\\lambda\\|\\bm{Z}\\|_{0}]\\qquad\\text{s.t.}\\ \\bm{Z}=f(\\bm{X}), roman_max start_POSTSUBSCRIPT italic_f ‚àà caligraphic_F end_POSTSUBSCRIPT [ roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ‚à£ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT ) - italic_Œª ‚à• bold_italic_Z ‚à• start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ] s.t. bold_italic_Z = italic_f ( bold_italic_X ) , (4.2.4)"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.E5X", "title": "max f ‚àà ‚Ñ± ‚Å° [ Œî ‚Äã R œµ ‚Äã ( ùíÅ ‚à£ ùëº [ K ] ) ‚àí Œª ‚Äã ‚Äñ ùíÅ ‚Äñ 1 ] s.t. ‚Äã ùíÅ = f ‚Äã ( ùëø ) , \\displaystyle\\max_{f\\in\\mathcal{F}}\\ [\\Delta R_{\\epsilon}(\\bm{Z}\\mid\\bm{U}_{[K]})-\\lambda\\|\\bm{Z}\\|_{1}]\\qquad\\text{s.t.}", "snippet": "max f ‚àà ‚Ñ± ‚Å° [ Œî ‚Äã R œµ ‚Äã ( ùíÅ ‚à£ ùëº [ K ] ) ‚àí Œª ‚Äã ‚Äñ ùíÅ ‚Äñ 1 ] s.t. ‚Äã ùíÅ = f ‚Äã ( ùëø ) , \\displaystyle\\max_{f\\in\\mathcal{F}}\\ [\\Delta R_{\\epsilon}(\\bm{Z}\\mid\\bm{U}_{[K]})-\\lambda\\|\\bm{Z}\\|_{1}]\\qquad\\text{s.t.}\\ \\bm{Z}=f(\\bm{X}), roman_max start_POSTSUBSCRIPT italic_f ‚àà caligraphic_F end_POSTSUBSCRIPT [ roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ‚à£ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT ) - italic_Œª ‚à• bold_italic_Z ‚à• start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ] s.t. bold_italic_Z = italic_f ( bold_italic_X ) , (4.2.5)"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.E6", "title": "f : ùëø = ùíÅ 0 ‚Üí f 0 ùíÅ 1 ‚Üí ‚ãØ ‚Üí ùíÅ ‚Ñì ‚Üí f ‚Ñì ùíÅ ‚Ñì + 1 ‚Üí ‚ãØ ‚Üí f L ‚àí 1 ùíÅ L = ùíÅ , f\\colon\\bm{X}=\\bm{Z}^{0}\\xrightarrow{\\hskip 2.84526ptf^{0}\\hskip 2.84526pt}\\bm{Z}^{1}\\rightarrow\\cdots\\rightarrow\\bm{Z}^{\\ell}\\xri", "snippet": "f : ùëø = ùíÅ 0 ‚Üí f 0 ùíÅ 1 ‚Üí ‚ãØ ‚Üí ùíÅ ‚Ñì ‚Üí f ‚Ñì ùíÅ ‚Ñì + 1 ‚Üí ‚ãØ ‚Üí f L ‚àí 1 ùíÅ L = ùíÅ , f\\colon\\bm{X}=\\bm{Z}^{0}\\xrightarrow{\\hskip 2.84526ptf^{0}\\hskip 2.84526pt}\\bm{Z}^{1}\\rightarrow\\cdots\\rightarrow\\bm{Z}^{\\ell}\\xrightarrow{\\hskip 2.84526ptf^{\\ell}\\hskip 2.84526pt}\\bm{Z}^{\\ell+1}\\rightarrow\\cdots\\xrightarrow{\\hskip 2.84526ptf^{L-1}}\\bm{Z}^{L}=\\bm{Z}, italic_f : bold_italic_X = bold_italic_Z start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT start_ARROW start_OVERACCENT italic_f start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT end_OVERACCENT ‚Üí end_ARROW bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ‚Üí ‚ãØ ‚Üí bold"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.E8", "title": "ùíÅ ‚Ñì + 1 = arg ‚Äã min ùíÅ ‚Å° { Œª ‚Äã ‚Äñ ùíÅ ‚Äñ 1 + 1 2 ‚Äã ‚Äñ ùíÅ ‚Ñì + 1 / 2 ‚àí ùë´ ‚Ñì ‚Äã ùíÅ ‚Äñ F 2 } . \\bm{Z}^{\\ell+1}=\\operatorname*{arg\\ min}_{{\\bm{Z}}}\\bigg{\\{}\\lambda\\|\\bm{Z}\\|_{1}+\\frac{1}{2}\\|\\bm{Z}^{\\ell+1/2}-\\bm{D}^", "snippet": "ùíÅ ‚Ñì + 1 = arg ‚Äã min ùíÅ ‚Å° { Œª ‚Äã ‚Äñ ùíÅ ‚Äñ 1 + 1 2 ‚Äã ‚Äñ ùíÅ ‚Ñì + 1 / 2 ‚àí ùë´ ‚Ñì ‚Äã ùíÅ ‚Äñ F 2 } . \\bm{Z}^{\\ell+1}=\\operatorname*{arg\\ min}_{{\\bm{Z}}}\\bigg{\\{}\\lambda\\|\\bm{Z}\\|_{1}+\\frac{1}{2}\\|\\bm{Z}^{\\ell+1/2}-\\bm{D}^{\\ell}{\\bm{Z}}\\|_{F}^{2}\\bigg{\\}}. bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì + 1 end_POSTSUPERSCRIPT = start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT bold_italic_Z end_POSTSUBSCRIPT { italic_Œª ‚à• bold_italic_Z ‚à• start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + divide start_ARG 1 end_ARG start_ARG 2 end_ARG ‚à• bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì + 1 / 2 end_POSTSUPERSCRIPT - b"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.E9", "title": "‚àá ùíÅ R œµ c ‚Äã ( ùíÅ ‚à£ ùëº [ K ] ) = p N ‚Äã œµ 2 ‚Äã ‚àë k = 1 K ùëº k ‚Äã ùëº k ‚ä§ ‚Äã ùíÅ ‚Äã ( ùë∞ + p N ‚Äã œµ 2 ‚Äã ( ùëº k ‚ä§ ‚Äã ùíÅ ) ‚ä§ ‚Äã ( ùëº k ‚ä§ ‚Äã ùíÅ ) ) ‚àí 1 . \\nabla_{\\bm{Z}}R_{\\epsilon}^{c}(\\bm{Z}\\mid\\bm{U}_{[K]})=\\frac{p}{N\\epsil", "snippet": "‚àá ùíÅ R œµ c ‚Äã ( ùíÅ ‚à£ ùëº [ K ] ) = p N ‚Äã œµ 2 ‚Äã ‚àë k = 1 K ùëº k ‚Äã ùëº k ‚ä§ ‚Äã ùíÅ ‚Äã ( ùë∞ + p N ‚Äã œµ 2 ‚Äã ( ùëº k ‚ä§ ‚Äã ùíÅ ) ‚ä§ ‚Äã ( ùëº k ‚ä§ ‚Äã ùíÅ ) ) ‚àí 1 . \\nabla_{\\bm{Z}}R_{\\epsilon}^{c}(\\bm{Z}\\mid\\bm{U}_{[K]})=\\frac{p}{N\\epsilon^{2}}\\sum_{k=1}^{K}\\bm{U}_{k}\\bm{U}_{k}^{\\top}\\bm{Z}\\Big{(}\\bm{I}+\\frac{p}{N\\epsilon^{2}}(\\bm{U}_{k}^{\\top}\\bm{Z})^{\\top}(\\bm{U}_{k}^{\\top}\\bm{Z})\\Big{)}^{-1}. ‚àá start_POSTSUBSCRIPT bold_italic_Z end_POSTSUBSCRIPT italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_Z ‚à£ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBS"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.E14", "title": "ùíÅ ‚Ñì + 1 / 2 = ( 1 ‚àí Œ∫ ‚Äã p N ‚Äã œµ 2 ) ùíÅ ‚Ñì + Œ∫ ‚Äã p N ‚Äã œµ 2 MSSA ( ùíÅ ‚Ñì | ùëº [ K ] ‚Ñì ) . \\bm{Z}^{\\ell+1/2}=\\left(1-\\frac{\\kappa p}{N\\epsilon^{2}}\\right)\\bm{Z}^{\\ell}+\\frac{\\kappa p}{N\\epsilon^{2}}\\mathrm{MS", "snippet": "ùíÅ ‚Ñì + 1 / 2 = ( 1 ‚àí Œ∫ ‚Äã p N ‚Äã œµ 2 ) ùíÅ ‚Ñì + Œ∫ ‚Äã p N ‚Äã œµ 2 MSSA ( ùíÅ ‚Ñì | ùëº [ K ] ‚Ñì ) . \\bm{Z}^{\\ell+1/2}=\\left(1-\\frac{\\kappa p}{N\\epsilon^{2}}\\right)\\bm{Z}^{\\ell}+\\frac{\\kappa p}{N\\epsilon^{2}}\\mathrm{MSSA}\\left(\\bm{Z}^{\\ell}\\ \\middle|\\ \\bm{U}_{[K]}^{\\ell}\\right). bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì + 1 / 2 end_POSTSUPERSCRIPT = ( 1 - divide start_ARG italic_Œ∫ italic_p end_ARG start_ARG italic_N italic_œµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ) bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT + divide start_ARG italic_Œ∫ italic_p end_ARG start_ARG italic_N itali"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.E15", "title": "R œµ ‚Äã ( ùíÅ ‚Ñì + 1 / 2 ) ‚âà R œµ ‚Äã ( ùë´ ‚Ñì ‚Äã ùíÅ ‚Ñì + 1 ) ‚âà R œµ ‚Äã ( ùíÅ ‚Ñì + 1 ) . R_{\\epsilon}(\\bm{Z}^{\\ell+1/2})\\approx R_{\\epsilon}(\\bm{D}^{\\ell}\\bm{Z}^{\\ell+1})\\approx R_{\\epsilon}(\\bm{Z}^{\\ell+1}). italic_R s", "snippet": "R œµ ‚Äã ( ùíÅ ‚Ñì + 1 / 2 ) ‚âà R œµ ‚Äã ( ùë´ ‚Ñì ‚Äã ùíÅ ‚Ñì + 1 ) ‚âà R œµ ‚Äã ( ùíÅ ‚Ñì + 1 ) . R_{\\epsilon}(\\bm{Z}^{\\ell+1/2})\\approx R_{\\epsilon}(\\bm{D}^{\\ell}\\bm{Z}^{\\ell+1})\\approx R_{\\epsilon}(\\bm{Z}^{\\ell+1}). italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì + 1 / 2 end_POSTSUPERSCRIPT ) ‚âà italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_D start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì + 1 end_POSTSUPERSCRIPT ) ‚âà italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z start_POST"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.E16", "title": "ùíÅ ‚Ñì + 1 ‚âà arg ‚Äã min ùíÅ ‚Å° [ Œª ‚Äã ‚Äñ ùíÅ ‚Äñ 1 + 1 2 ‚Äã ‚Äñ ùíÅ ‚Ñì + 1 / 2 ‚àí ùë´ ‚Ñì ‚Äã ùíÅ ‚Äñ F 2 ] . \\bm{Z}^{\\ell+1}\\approx\\operatorname*{arg\\ min}_{\\bm{Z}}\\left[\\lambda\\|\\bm{Z}\\|_{1}+\\frac{1}{2}\\|\\bm{Z}^{\\ell+1/2}-\\bm{D}", "snippet": "ùíÅ ‚Ñì + 1 ‚âà arg ‚Äã min ùíÅ ‚Å° [ Œª ‚Äã ‚Äñ ùíÅ ‚Äñ 1 + 1 2 ‚Äã ‚Äñ ùíÅ ‚Ñì + 1 / 2 ‚àí ùë´ ‚Ñì ‚Äã ùíÅ ‚Äñ F 2 ] . \\bm{Z}^{\\ell+1}\\approx\\operatorname*{arg\\ min}_{\\bm{Z}}\\left[\\lambda\\|\\bm{Z}\\|_{1}+\\frac{1}{2}\\|\\bm{Z}^{\\ell+1/2}-\\bm{D}^{\\ell}\\bm{Z}\\|_{F}^{2}\\right]. bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì + 1 end_POSTSUPERSCRIPT ‚âà start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT bold_italic_Z end_POSTSUBSCRIPT [ italic_Œª ‚à• bold_italic_Z ‚à• start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + divide start_ARG 1 end_ARG start_ARG 2 end_ARG ‚à• bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì + 1 / 2 end_POSTSUPERSCRIPT - bold"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.E17", "title": "ùíÅ ‚Ñì + 1 ‚âà arg ‚Äã min ùíÅ ‚â• ùüé ‚Å° [ Œª ‚Äã ‚Äñ ùíÅ ‚Äñ 1 + 1 2 ‚Äã ‚Äñ ùíÅ ‚Ñì + 1 / 2 ‚àí ùë´ ‚Ñì ‚Äã ùíÅ ‚Äñ F 2 ] . \\bm{Z}^{\\ell+1}\\approx\\operatorname*{arg\\ min}_{\\bm{Z}\\geq\\bm{0}}\\left[\\lambda\\|\\bm{Z}\\|_{1}+\\frac{1}{2}\\|\\bm{Z}^{\\e", "snippet": "ùíÅ ‚Ñì + 1 ‚âà arg ‚Äã min ùíÅ ‚â• ùüé ‚Å° [ Œª ‚Äã ‚Äñ ùíÅ ‚Äñ 1 + 1 2 ‚Äã ‚Äñ ùíÅ ‚Ñì + 1 / 2 ‚àí ùë´ ‚Ñì ‚Äã ùíÅ ‚Äñ F 2 ] . \\bm{Z}^{\\ell+1}\\approx\\operatorname*{arg\\ min}_{\\bm{Z}\\geq\\bm{0}}\\left[\\lambda\\|\\bm{Z}\\|_{1}+\\frac{1}{2}\\|\\bm{Z}^{\\ell+1/2}-\\bm{D}^{\\ell}\\bm{Z}\\|_{F}^{2}\\right]. bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì + 1 end_POSTSUPERSCRIPT ‚âà start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT bold_italic_Z ‚â• bold_0 end_POSTSUBSCRIPT [ italic_Œª ‚à• bold_italic_Z ‚à• start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + divide start_ARG 1 end_ARG start_ARG 2 end_ARG ‚à• bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì + 1 / 2 end"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.E20", "title": "ùíÅ ‚Ñì + 1 / 2 ‚âê ùíÅ ‚Ñì + MSSA ‚Äã ( ùíÅ ‚Ñì ‚à£ ùëº [ K ] ‚Ñì ) , ùíÅ ‚Ñì + 1 ‚âê ISTA ‚Äã ( ùíÅ ‚Ñì + 1 / 2 ‚à£ ùë´ ‚Ñì ) . \\bm{Z}^{\\ell+1/2}\\doteq\\bm{Z}^{\\ell}+\\texttt{MSSA}(\\bm{Z}^{\\ell}\\mid\\bm{U}_{[K]}^{\\ell}),\\qquad\\bm{Z}^{\\ell+1}", "snippet": "ùíÅ ‚Ñì + 1 / 2 ‚âê ùíÅ ‚Ñì + MSSA ‚Äã ( ùíÅ ‚Ñì ‚à£ ùëº [ K ] ‚Ñì ) , ùíÅ ‚Ñì + 1 ‚âê ISTA ‚Äã ( ùíÅ ‚Ñì + 1 / 2 ‚à£ ùë´ ‚Ñì ) . \\bm{Z}^{\\ell+1/2}\\doteq\\bm{Z}^{\\ell}+\\texttt{MSSA}(\\bm{Z}^{\\ell}\\mid\\bm{U}_{[K]}^{\\ell}),\\qquad\\bm{Z}^{\\ell+1}\\doteq\\texttt{ISTA}(\\bm{Z}^{\\ell+1/2}\\mid\\bm{D}^{\\ell}). bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì + 1 / 2 end_POSTSUPERSCRIPT ‚âê bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT + MSSA ( bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ‚à£ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ) , bold_italic"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.E6", "title": "R c , f ‚Äã ( ùíÅ , ùö∑ ) ‚âê 1 2 ‚Äã ‚àë k = 1 K N k N ‚Äã F ‚Äã ( 1 N k ‚Äã ùíÅ ‚Äã Diag ‚Äã ( ùùÖ k ) ‚Äã ùíÅ ‚ä§ ) . R_{c,f}(\\bm{Z},\\bm{\\Pi})\\doteq\\frac{1}{2}\\sum_{k=1}^{K}\\frac{N_{k}}{N}F\\left(\\frac{1}{N_{k}}\\bm{Z}\\mathrm{Diag}", "snippet": "R c , f ‚Äã ( ùíÅ , ùö∑ ) ‚âê 1 2 ‚Äã ‚àë k = 1 K N k N ‚Äã F ‚Äã ( 1 N k ‚Äã ùíÅ ‚Äã Diag ‚Äã ( ùùÖ k ) ‚Äã ùíÅ ‚ä§ ) . R_{c,f}(\\bm{Z},\\bm{\\Pi})\\doteq\\frac{1}{2}\\sum_{k=1}^{K}\\frac{N_{k}}{N}F\\left(\\frac{1}{N_{k}}\\bm{Z}\\mathrm{Diag}(\\bm{\\pi}_{k})\\bm{Z}^{\\top}\\right). italic_R start_POSTSUBSCRIPT italic_c , italic_f end_POSTSUBSCRIPT ( bold_italic_Z , bold_Œ† ) ‚âê divide start_ARG 1 end_ARG start_ARG 2 end_ARG ‚àë start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT divide start_ARG italic_N start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG start_ARG italic_N end_ARG italic_F "}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.E7", "title": "F ‚Äã ( ùë¥ ) ‚â§ ‚àë i = 1 d f ‚Äã ( ( ùë∏ ‚ä§ ‚Äã ùë¥ ‚Äã ùë∏ ) i ‚Äã i ) . F(\\bm{M})\\leq\\sum_{i=1}^{d}f\\left((\\bm{Q}^{\\top}\\bm{M}\\bm{Q})_{ii}\\right). italic_F ( bold_italic_M ) ‚â§ ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POS", "snippet": "F ‚Äã ( ùë¥ ) ‚â§ ‚àë i = 1 d f ‚Äã ( ( ùë∏ ‚ä§ ‚Äã ùë¥ ‚Äã ùë∏ ) i ‚Äã i ) . F(\\bm{M})\\leq\\sum_{i=1}^{d}f\\left((\\bm{Q}^{\\top}\\bm{M}\\bm{Q})_{ii}\\right). italic_F ( bold_italic_M ) ‚â§ ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT italic_f ( ( bold_italic_Q start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_M bold_italic_Q ) start_POSTSUBSCRIPT italic_i italic_i end_POSTSUBSCRIPT ) . (4.3.7)"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.E8", "title": "R c , f var ‚Äã ( ùíÅ , ùö∑ ‚à£ ùëº [ K ] ) ‚âê 1 2 ‚Äã ‚àë k = 1 K N k N ‚Äã ‚àë i = 1 d f ‚Äã ( 1 N k ‚Äã ( ùëº k ‚ä§ ‚Äã ùíÅ ‚Äã Diag ‚Äã ( ùùÖ k ) ‚Äã ùíÅ ‚ä§ ‚Äã ùëº k ) i ‚Äã i ) , R^{\\rm var}_{c,f}(\\bm{Z},\\bm{\\Pi}\\mid\\bm{U}_{[K]})\\doteq\\frac{1", "snippet": "R c , f var ‚Äã ( ùíÅ , ùö∑ ‚à£ ùëº [ K ] ) ‚âê 1 2 ‚Äã ‚àë k = 1 K N k N ‚Äã ‚àë i = 1 d f ‚Äã ( 1 N k ‚Äã ( ùëº k ‚ä§ ‚Äã ùíÅ ‚Äã Diag ‚Äã ( ùùÖ k ) ‚Äã ùíÅ ‚ä§ ‚Äã ùëº k ) i ‚Äã i ) , R^{\\rm var}_{c,f}(\\bm{Z},\\bm{\\Pi}\\mid\\bm{U}_{[K]})\\doteq\\frac{1}{2}\\sum_{k=1}^{K}\\frac{N_{k}}{N}\\sum_{i=1}^{d}f\\left(\\frac{1}{N_{k}}(\\bm{U}_{k}^{\\top}\\bm{Z}\\mathrm{Diag}(\\bm{\\pi}_{k})\\bm{Z}^{\\top}\\bm{U}_{k})_{ii}\\right), italic_R start_POSTSUPERSCRIPT roman_var end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_c , italic_f end_POSTSUBSCRIPT ( bold_italic_Z , bold_Œ† ‚à£ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT ) ‚âê divide start_ARG 1 end_ARG "}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.E9", "title": "‚àá ùíÅ 1 2 ‚Äã ‚àë i = 1 d f ‚Äã ( ( ùíÅ ‚Äã Diag ‚Äã ( ùùÖ ) ‚Äã ùíÅ ‚ä§ ) i ‚Äã i ) = Diag ‚Äã ( ‚àá f ‚Äã [ ùíÅ ‚äô 2 ‚Äã ùùÖ ] ) ‚Äã ùíÅ ‚Äã Diag ‚Äã ( ùùÖ ) , \\nabla_{\\bm{Z}}\\ \\frac{1}{2}\\sum_{i=1}^{d}f((\\bm{Z}\\mathrm{Diag}(\\bm{\\pi})\\bm{Z}^{\\to", "snippet": "‚àá ùíÅ 1 2 ‚Äã ‚àë i = 1 d f ‚Äã ( ( ùíÅ ‚Äã Diag ‚Äã ( ùùÖ ) ‚Äã ùíÅ ‚ä§ ) i ‚Äã i ) = Diag ‚Äã ( ‚àá f ‚Äã [ ùíÅ ‚äô 2 ‚Äã ùùÖ ] ) ‚Äã ùíÅ ‚Äã Diag ‚Äã ( ùùÖ ) , \\nabla_{\\bm{Z}}\\ \\frac{1}{2}\\sum_{i=1}^{d}f((\\bm{Z}\\mathrm{Diag}(\\bm{\\pi})\\bm{Z}^{\\top})_{ii})=\\;\\mathrm{Diag}(\\nabla f[\\bm{Z}^{\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\scalebox{0.8}{$\\scriptstyle\\odot$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\odot$}}$}}{\\raisebox{1.3pt}{$\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\scalebox{0.8}{$\\scriptstyle\\odot$"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.E11", "title": "ùíõ j + = ùíõ j ‚àí œÑ ‚Äã ‚àá ùíõ j R c , f var ‚Äã ( ùíÅ , ùö∑ ‚à£ ùëº [ K ] ) = ùíõ j ‚àí œÑ N ‚Äã ‚àë k = 1 K Œ† j ‚Äã k ‚Äã ùëº k ‚Äã ùë´ ‚Äã ( ùíÅ , ùùÖ k ‚à£ ùëº k ) ‚Äã ùëº k ‚ä§ ‚Äã ùíõ j \\bm{z}_{j}^{+}=\\bm{z}_{j}-\\tau\\nabla_{\\bm{z}_{j}}R_{c,f}^{\\rm var}", "snippet": "ùíõ j + = ùíõ j ‚àí œÑ ‚Äã ‚àá ùíõ j R c , f var ‚Äã ( ùíÅ , ùö∑ ‚à£ ùëº [ K ] ) = ùíõ j ‚àí œÑ N ‚Äã ‚àë k = 1 K Œ† j ‚Äã k ‚Äã ùëº k ‚Äã ùë´ ‚Äã ( ùíÅ , ùùÖ k ‚à£ ùëº k ) ‚Äã ùëº k ‚ä§ ‚Äã ùíõ j \\bm{z}_{j}^{+}=\\bm{z}_{j}-\\tau\\nabla_{\\bm{z}_{j}}R_{c,f}^{\\rm var}(\\bm{Z},\\bm{\\Pi}\\mid\\bm{U}_{[K]})=\\bm{z}_{j}-\\frac{\\tau}{N}\\sum_{k=1}^{K}\\Pi_{jk}\\bm{U}_{k}\\bm{D}(\\bm{Z},\\bm{\\pi}_{k}\\mid\\bm{U}_{k})\\bm{U}_{k}^{\\top}\\bm{z}_{j} bold_italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT = bold_italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT - italic_œÑ ‚àá start_POSTSUBSCRIPT bold_italic_z start_POSTSUBSCRIPT itali"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.E12", "title": "ùíõ j + = ‚àë k = 1 K Œ† j ‚Äã k ‚Äã [ ùíõ j ‚Äã ‚àí œÑ n ‚Äã ùëº k ‚Äã ùë´ ‚Äã ( ùíÅ , ùùÖ k ‚à£ ùëº k ) ‚Äã ùëº k ‚ä§ ‚èü action of one attention head ‚Äã ùíõ j ] . \\bm{z}_{j}^{+}=\\sum_{k=1}^{K}\\Pi_{jk}\\Big{[}\\bm{z}_{j}\\underbrace{-\\frac{\\tau}{", "snippet": "ùíõ j + = ‚àë k = 1 K Œ† j ‚Äã k ‚Äã [ ùíõ j ‚Äã ‚àí œÑ n ‚Äã ùëº k ‚Äã ùë´ ‚Äã ( ùíÅ , ùùÖ k ‚à£ ùëº k ) ‚Äã ùëº k ‚ä§ ‚èü action of one attention head ‚Äã ùíõ j ] . \\bm{z}_{j}^{+}=\\sum_{k=1}^{K}\\Pi_{jk}\\Big{[}\\bm{z}_{j}\\underbrace{-\\frac{\\tau}{n}\\bm{U}_{k}\\bm{D}(\\bm{Z},\\bm{\\pi}_{k}\\mid\\bm{U}_{k})\\bm{U}_{k}^{\\top}}_{\\text{action of one attention head}}\\bm{z}_{j}\\Big{]}. bold_italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT = ‚àë start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_Œ† start_POSTSUBSCRIPT italic_j italic_k end_POSTSUBSCRI"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.E13", "title": "R c , f ‚Äã ( ùíÅ , ùö∑ ) ‚â§ R c , f var ‚Äã ( ùíÅ , ùö∑ ‚à£ ùëº [ K ] ) , R_{c,f}(\\bm{Z},\\bm{\\Pi})\\leq R_{c,f}^{\\rm var}(\\bm{Z},\\bm{\\Pi}\\mid\\bm{U}_{[K]}), italic_R start_POSTSUBSCRIPT italic_c , italic_f end_POSTSUBS", "snippet": "R c , f ‚Äã ( ùíÅ , ùö∑ ) ‚â§ R c , f var ‚Äã ( ùíÅ , ùö∑ ‚à£ ùëº [ K ] ) , R_{c,f}(\\bm{Z},\\bm{\\Pi})\\leq R_{c,f}^{\\rm var}(\\bm{Z},\\bm{\\Pi}\\mid\\bm{U}_{[K]}), italic_R start_POSTSUBSCRIPT italic_c , italic_f end_POSTSUBSCRIPT ( bold_italic_Z , bold_Œ† ) ‚â§ italic_R start_POSTSUBSCRIPT italic_c , italic_f end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_var end_POSTSUPERSCRIPT ( bold_italic_Z , bold_Œ† ‚à£ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT ) , (4.3.13)"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S3.E15", "title": "TSSA ‚Äã ( ùíÅ ‚à£ ùëº [ K ] ) ‚âê ‚àí œÑ n ‚Äã ‚àë k = 1 K ùëº k ‚Äã ùë´ ‚Äã ( ùíÅ , ùùÖ k ‚à£ ùëº k ) ‚Äã ùëº k ‚ä§ ‚Äã ùíÅ ‚Äã diag ‚Å° ( ùùÖ k ) , \\texttt{TSSA}(\\bm{Z}\\mid\\bm{U}_{[K]})\\doteq-\\frac{\\tau}{n}\\sum_{k=1}^{K}\\bm{U}_{k}\\bm{D}(\\bm{Z},\\b", "snippet": "TSSA ‚Äã ( ùíÅ ‚à£ ùëº [ K ] ) ‚âê ‚àí œÑ n ‚Äã ‚àë k = 1 K ùëº k ‚Äã ùë´ ‚Äã ( ùíÅ , ùùÖ k ‚à£ ùëº k ) ‚Äã ùëº k ‚ä§ ‚Äã ùíÅ ‚Äã diag ‚Å° ( ùùÖ k ) , \\texttt{TSSA}(\\bm{Z}\\mid\\bm{U}_{[K]})\\doteq-\\frac{\\tau}{n}\\sum_{k=1}^{K}\\bm{U}_{k}\\bm{D}(\\bm{Z},\\bm{\\pi}_{k}\\mid\\bm{U}_{k})\\bm{U}_{k}^{\\top}\\bm{Z}\\operatorname{diag}(\\bm{\\pi}_{k}), TSSA ( bold_italic_Z ‚à£ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT ) ‚âê - divide start_ARG italic_œÑ end_ARG start_ARG italic_n end_ARG ‚àë start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRI"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx40", "title": "Œ± ‚âê d N ‚Äã œµ 2 , Œ± k ‚âê d tr ‚Äã ( ùö∑ k ) ‚Äã œµ 2 , Œ≥ k ‚âê tr ‚Äã ( ùö∑ k ) N , for ‚Äã k = 1 , ‚Ä¶ , K . \\displaystyle\\alpha\\doteq\\frac{d}{N\\epsilon^{2}},\\qquad\\alpha_{k}\\doteq\\frac{d}{\\mathrm{tr}(\\bm{\\Pi}_{k})\\epsi", "snippet": "Œ± ‚âê d N ‚Äã œµ 2 , Œ± k ‚âê d tr ‚Äã ( ùö∑ k ) ‚Äã œµ 2 , Œ≥ k ‚âê tr ‚Äã ( ùö∑ k ) N , for ‚Äã k = 1 , ‚Ä¶ , K . \\displaystyle\\alpha\\doteq\\frac{d}{N\\epsilon^{2}},\\qquad\\alpha_{k}\\doteq\\frac{d}{\\mathrm{tr}(\\bm{\\Pi}_{k})\\epsilon^{2}},\\qquad\\gamma_{k}\\doteq\\frac{\\mathrm{tr}(\\bm{\\Pi}_{k})}{N},\\qquad\\text{for}\\ k=1,\\ldots,K. italic_Œ± ‚âê divide start_ARG italic_d end_ARG start_ARG italic_N italic_œµ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG , italic_Œ± start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ‚âê divide start_ARG italic_d end_ARG start_ARG roman_tr ( bold_Œ† start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) itali"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx41", "title": "ùë¨ ‚Ñì ‚Äã ùíõ ‚Ñì = Œ± ‚Äã ( ùíõ ‚Ñì ‚àí ùíÅ ‚Ñì ‚Äã ùíí ‚ãÜ ‚Ñì ) , where ùíí ‚ãÜ ‚Ñì ‚âê arg ‚Äã min ùíí ‚Ñì ‚Å° { Œ± ‚Äã ‚Äñ ùíõ ‚Ñì ‚àí ùíÅ ‚Ñì ‚Äã ùíí ‚Ñì ‚Äñ 2 2 + ‚Äñ ùíí ‚Ñì ‚Äñ 2 2 } . \\displaystyle\\bm{E}^{\\ell}\\bm{z}^{\\ell}=\\alpha(\\bm{z}^{\\ell}-\\bm{Z}^{\\ell}\\bm{q}^{", "snippet": "ùë¨ ‚Ñì ‚Äã ùíõ ‚Ñì = Œ± ‚Äã ( ùíõ ‚Ñì ‚àí ùíÅ ‚Ñì ‚Äã ùíí ‚ãÜ ‚Ñì ) , where ùíí ‚ãÜ ‚Ñì ‚âê arg ‚Äã min ùíí ‚Ñì ‚Å° { Œ± ‚Äã ‚Äñ ùíõ ‚Ñì ‚àí ùíÅ ‚Ñì ‚Äã ùíí ‚Ñì ‚Äñ 2 2 + ‚Äñ ùíí ‚Ñì ‚Äñ 2 2 } . \\displaystyle\\bm{E}^{\\ell}\\bm{z}^{\\ell}=\\alpha(\\bm{z}^{\\ell}-\\bm{Z}^{\\ell}\\bm{q}^{\\ell}_{\\star}),\\qquad\\mbox{where}\\qquad\\bm{q}^{\\ell}_{\\star}\\doteq\\operatorname*{arg\\ min}_{\\bm{q}^{\\ell}}\\big{\\{}\\alpha\\|\\bm{z}^{\\ell}-\\bm{Z}^{\\ell}\\bm{q}^{\\ell}\\|_{2}^{2}+\\|\\bm{q}^{\\ell}\\|_{2}^{2}\\big{\\}}. bold_italic_E start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT bold_italic_z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT = italic_Œ± ( bold_italic_z start_POSTSUPERSCRIPT roman_‚Ñì end_POS"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx42", "title": "‚àë k = 1 K Œ≥ k ‚Äã œÄ k ‚Äã ( ùíõ ‚Ñì ) ‚Äã ùë™ k ‚Ñì ‚Äã ùíõ ‚Ñì ‚âà ‚àë k = 1 K Œ≥ k ‚Äã œÄ ^ k ‚Äã ( ùíõ ‚Ñì ) ‚Äã ùë™ k ‚Ñì ‚Äã ùíõ ‚Ñì ‚âê ùùà ‚Äã ( [ ùë™ 1 ‚Ñì ‚Äã ùíõ ‚Ñì , ‚Ä¶ , ùë™ K ‚Ñì ‚Äã ùíõ ‚Ñì ] ) , \\displaystyle\\sum_{k=1}^{K}\\gamma_{k}\\pi_{k}(\\bm{z}^{\\ell})\\bm", "snippet": "‚àë k = 1 K Œ≥ k ‚Äã œÄ k ‚Äã ( ùíõ ‚Ñì ) ‚Äã ùë™ k ‚Ñì ‚Äã ùíõ ‚Ñì ‚âà ‚àë k = 1 K Œ≥ k ‚Äã œÄ ^ k ‚Äã ( ùíõ ‚Ñì ) ‚Äã ùë™ k ‚Ñì ‚Äã ùíõ ‚Ñì ‚âê ùùà ‚Äã ( [ ùë™ 1 ‚Ñì ‚Äã ùíõ ‚Ñì , ‚Ä¶ , ùë™ K ‚Ñì ‚Äã ùíõ ‚Ñì ] ) , \\displaystyle\\sum_{k=1}^{K}\\gamma_{k}\\pi_{k}(\\bm{z}^{\\ell})\\bm{C}_{k}^{\\ell}\\bm{z}^{\\ell}\\;\\approx\\;\\sum_{k=1}^{K}\\gamma_{k}\\widehat{\\pi}_{k}(\\bm{z}^{\\ell})\\bm{C}^{\\ell}_{k}\\bm{z}^{\\ell}\\;\\doteq\\;\\bm{\\sigma}\\Big{(}[\\bm{C}^{\\ell}_{1}\\bm{z}^{\\ell},\\dots,\\bm{C}^{\\ell}_{K}\\bm{z}^{\\ell}]\\Big{)}, ‚àë start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT italic_Œ≥ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_œÄ s"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E13", "title": "ùíõ ‚Ñì + 1 \\displaystyle\\bm{z}^{\\ell+1} bold_italic_z start_POSTSUPERSCRIPT roman_‚Ñì + 1 end_POSTSUPERSCRIPT ‚àù ùíõ ‚Ñì + Œ∑ ‚ãÖ ùë¨ ‚Ñì ‚Äã ùíõ ‚Ñì ‚àí Œ∑ ‚ãÖ ùùà ‚Äã ( [ ùë™ 1 ‚Ñì ‚Äã ùíõ ‚Ñì , ‚Ä¶ , ùë™ K ‚Ñì ‚Äã ùíõ ‚Ñì ] ) \\displaystyle\\propto\\;\\bm", "snippet": "ùíõ ‚Ñì + 1 \\displaystyle\\bm{z}^{\\ell+1} bold_italic_z start_POSTSUPERSCRIPT roman_‚Ñì + 1 end_POSTSUPERSCRIPT ‚àù ùíõ ‚Ñì + Œ∑ ‚ãÖ ùë¨ ‚Ñì ‚Äã ùíõ ‚Ñì ‚àí Œ∑ ‚ãÖ ùùà ‚Äã ( [ ùë™ 1 ‚Ñì ‚Äã ùíõ ‚Ñì , ‚Ä¶ , ùë™ K ‚Ñì ‚Äã ùíõ ‚Ñì ] ) \\displaystyle\\propto\\;\\bm{z}^{\\ell}+\\eta\\cdot\\bm{E}^{\\ell}\\bm{z}^{\\ell}-\\eta\\cdot\\bm{\\sigma}\\big{(}[\\bm{C}^{\\ell}_{1}\\bm{z}^{\\ell},\\dots,\\bm{C}^{\\ell}_{K}\\bm{z}^{\\ell}]\\big{)} ‚àù bold_italic_z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT + italic_Œ∑ ‚ãÖ bold_italic_E start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT bold_italic_z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT - italic_Œ∑ ‚ãÖ bold_italic_œÉ ( [ bold_italic"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E14", "title": "f ‚Äã ( ùíô , ùúΩ ) = \\displaystyle f(\\bm{x},\\bm{\\theta})\\;= italic_f ( bold_italic_x , bold_italic_Œ∏ ) = f L ‚àò f L ‚àí 1 ‚àò ‚ãØ ‚àò f 1 ‚àò f 0 ‚Äã ( ùíõ 0 ) , \\displaystyle\\;\\;f^{L}\\circ f^{L-1}\\circ\\cdots\\circ f^{1}\\", "snippet": "f ‚Äã ( ùíô , ùúΩ ) = \\displaystyle f(\\bm{x},\\bm{\\theta})\\;= italic_f ( bold_italic_x , bold_italic_Œ∏ ) = f L ‚àò f L ‚àí 1 ‚àò ‚ãØ ‚àò f 1 ‚àò f 0 ‚Äã ( ùíõ 0 ) , \\displaystyle\\;\\;f^{L}\\circ f^{L-1}\\circ\\cdots\\circ f^{1}\\circ f^{0}(\\bm{z}^{0}), italic_f start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT ‚àò italic_f start_POSTSUPERSCRIPT italic_L - 1 end_POSTSUPERSCRIPT ‚àò ‚ãØ ‚àò italic_f start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ‚àò italic_f start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT ( bold_italic_z start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT ) , (4.1.14) f ‚Ñì ‚Äã ( ùíõ ‚Ñì , ùúΩ ‚Ñì ) ‚âê \\displaystyle f^{\\ell}(\\bm{z}^{\\ell},\\bm{"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx43", "title": "ùñºùóÇùóãùñº ‚Äã ( ùíÅ 1 ) ‚Äã ùñºùóÇùóãùñº ‚Äã ( ùíÅ 1 ) ‚ä§ \\displaystyle\\mathsf{circ}(\\bm{Z}^{1})\\mathsf{circ}(\\bm{Z}^{1})^{\\top} sansserif_circ ( bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) sansserif_circ ( b", "snippet": "ùñºùóÇùóãùñº ‚Äã ( ùíÅ 1 ) ‚Äã ùñºùóÇùóãùñº ‚Äã ( ùíÅ 1 ) ‚ä§ \\displaystyle\\mathsf{circ}(\\bm{Z}^{1})\\mathsf{circ}(\\bm{Z}^{1})^{\\top} sansserif_circ ( bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) sansserif_circ ( bold_italic_Z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT = \\displaystyle= = [ ùñºùóÇùóãùñº ‚Äã ( ùíõ 1 1 ) , ‚Ä¶ , ùñºùóÇùóãùñº ‚Äã ( ùíõ N 1 ) ] ‚Äã [ ùñºùóÇùóãùñº ‚Äã ( ùíõ 1 1 ) , ‚Ä¶ , ùñºùóÇùóãùñº ‚Äã ( ùíõ N 1 ) ] ‚ä§ \\displaystyle\\left[\\mathsf{circ}(\\bm{z}_{1}^{1}),\\dots,\\mathsf{circ}(\\bm{z}_{N}^{1})\\right]\\left[\\mathsf{circ}(\\bm{z}_{1}^{1}),\\dots,\\mathsf{circ}(\\bm{z}_{N}^{1})\\right]^{\\top} [ sansser"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S1.E22", "title": "ùñºùóÇùóãùñº ‚Äã ( ùíõ ¬Ø ) ‚âê [ ùñºùóÇùóãùñº ‚Äã ( ùíõ ‚Äã [ 1 ] ) ‚ãÆ ùñºùóÇùóãùñº ‚Äã ( ùíõ ‚Äã [ C ] ) ] ‚àà ‚Ñù d ‚Äã C √ó d , ùö∫ ¬Ø ‚Äã ( ùíõ ¬Ø ) ‚âê [ ùñºùóÇùóãùñº ‚Äã ( ùíõ ‚Äã [ 1 ] ) ‚ãÆ ùñºùóÇùóãùñº ‚Äã ( ùíõ ‚Äã [ C ] ) ] ‚Äã [ ùñºùóÇùóãùñº ‚Äã ( ùíõ ‚Äã [ 1 ] ) ‚ä§ , ‚Ä¶ , ùñºùóÇùóãùñº ‚Äã ( ùíõ ‚Äã [ C ] ) ‚ä§", "snippet": "ùñºùóÇùóãùñº ‚Äã ( ùíõ ¬Ø ) ‚âê [ ùñºùóÇùóãùñº ‚Äã ( ùíõ ‚Äã [ 1 ] ) ‚ãÆ ùñºùóÇùóãùñº ‚Äã ( ùíõ ‚Äã [ C ] ) ] ‚àà ‚Ñù d ‚Äã C √ó d , ùö∫ ¬Ø ‚Äã ( ùíõ ¬Ø ) ‚âê [ ùñºùóÇùóãùñº ‚Äã ( ùíõ ‚Äã [ 1 ] ) ‚ãÆ ùñºùóÇùóãùñº ‚Äã ( ùíõ ‚Äã [ C ] ) ] ‚Äã [ ùñºùóÇùóãùñº ‚Äã ( ùíõ ‚Äã [ 1 ] ) ‚ä§ , ‚Ä¶ , ùñºùóÇùóãùñº ‚Äã ( ùíõ ‚Äã [ C ] ) ‚ä§ ] ‚àà ‚Ñù d ‚Äã C √ó d ‚Äã C , \\displaystyle\\mathsf{circ}(\\bar{\\bm{z}})\\doteq\\left[\\begin{matrix}\\mathsf{circ}(\\bm{z}[1])\\\\ \\vdots\\\\ \\mathsf{circ}(\\bm{z}[C])\\end{matrix}\\right]\\in\\mathbb{R}^{dC\\times d},\\quad\\bar{\\bm{\\Sigma}}(\\bar{\\bm{z}})\\doteq\\left[\\begin{matrix}\\mathsf{circ}(\\bm{z}[1])\\\\ \\vdots\\\\ \\mathsf{circ}(\\bm{z}[C])\\end{matrix}\\right]\\left[\\begin{matrix}\\mathsf{circ}(\\bm{z}[1])^{\\top},\\ldots,\\math"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx44", "title": "max ùíÅ ‚àà ‚Ñù d √ó N ‚Äã Œî ‚Äã R œµ ‚Äã ( ùíÅ ‚à£ ùëº [ K ] ) ‚âê R œµ ‚Äã ( ùíÅ ) ‚àí R œµ c ‚Äã ( ùíÅ ‚à£ ùëº [ K ] ) . \\displaystyle\\mathrm{max}_{\\bm{Z}\\in\\mathbb{R}^{d\\times N}}\\ \\Delta R_{\\epsilon}(\\bm{Z}\\mid\\bm{U}_{[K]})\\doteq R_{", "snippet": "max ùíÅ ‚àà ‚Ñù d √ó N ‚Äã Œî ‚Äã R œµ ‚Äã ( ùíÅ ‚à£ ùëº [ K ] ) ‚âê R œµ ‚Äã ( ùíÅ ) ‚àí R œµ c ‚Äã ( ùíÅ ‚à£ ùëº [ K ] ) . \\displaystyle\\mathrm{max}_{\\bm{Z}\\in\\mathbb{R}^{d\\times N}}\\ \\Delta R_{\\epsilon}(\\bm{Z}\\mid\\bm{U}_{[K]})\\doteq R_{\\epsilon}(\\bm{Z})-R^{c}_{\\epsilon}(\\bm{Z}\\mid\\bm{U}_{[K]}). roman_max start_POSTSUBSCRIPT bold_italic_Z ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d √ó italic_N end_POSTSUPERSCRIPT end_POSTSUBSCRIPT roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ‚à£ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT ) ‚âê italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBS"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.E3", "title": "R œµ c ‚Äã ( ùíÅ ‚à£ ùëº [ K ] ) \\displaystyle R_{\\epsilon}^{c}(\\bm{Z}\\mid\\bm{U}_{[K]}) italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_", "snippet": "R œµ c ‚Äã ( ùíÅ ‚à£ ùëº [ K ] ) \\displaystyle R_{\\epsilon}^{c}(\\bm{Z}\\mid\\bm{U}_{[K]}) italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_Z ‚à£ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT ) ‚âê ‚àë k = 1 K R œµ ‚Äã ( ùëº k ‚ä§ ‚Äã ùíÅ ) = 1 2 ‚Äã ‚àë k = 1 K log ‚Äã det ( ùë∞ + p N ‚Äã œµ 2 ‚Äã ( ùëº k ‚ä§ ‚Äã ùíÅ ) ‚ä§ ‚Äã ( ùëº k ‚ä§ ‚Äã ùíÅ ) ) . \\displaystyle\\doteq\\sum_{k=1}^{K}R_{\\epsilon}(\\bm{U}_{k}^{\\top}\\bm{Z})=\\frac{1}{2}\\sum_{k=1}^{K}\\log\\det\\left(\\bm{I}+\\frac{p}{N\\epsilon^{2}}(\\bm{U}_{k}^{\\top}\\bm{Z})^{\\top}(\\bm{U}_{k}^{\\top}\\bm{Z})\\right). ‚âê ‚àë start"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#S2.E5", "title": "max f ‚àà ‚Ñ± ‚Å° [ Œî ‚Äã R œµ ‚Äã ( ùíÅ ‚à£ ùëº [ K ] ) ‚àí Œª ‚Äã ‚Äñ ùíÅ ‚Äñ 1 ] s.t. ‚Äã ùíÅ = f ‚Äã ( ùëø ) , \\displaystyle\\max_{f\\in\\mathcal{F}}\\ [\\Delta R_{\\epsilon}(\\bm{Z}\\mid\\bm{U}_{[K]})-\\lambda\\|\\bm{Z}\\|_{1}]\\qquad\\text{s.t.}", "snippet": "max f ‚àà ‚Ñ± ‚Å° [ Œî ‚Äã R œµ ‚Äã ( ùíÅ ‚à£ ùëº [ K ] ) ‚àí Œª ‚Äã ‚Äñ ùíÅ ‚Äñ 1 ] s.t. ‚Äã ùíÅ = f ‚Äã ( ùëø ) , \\displaystyle\\max_{f\\in\\mathcal{F}}\\ [\\Delta R_{\\epsilon}(\\bm{Z}\\mid\\bm{U}_{[K]})-\\lambda\\|\\bm{Z}\\|_{1}]\\qquad\\text{s.t.}\\ \\bm{Z}=f(\\bm{X}), roman_max start_POSTSUBSCRIPT italic_f ‚àà caligraphic_F end_POSTSUBSCRIPT [ roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ‚à£ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT ) - italic_Œª ‚à• bold_italic_Z ‚à• start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ] s.t. bold_italic_Z = italic_f ( bold_italic_X ) , (4.2.5)"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx45", "title": "ùíÅ ‚Ñì + 1 / 2 = ùíÅ ‚Ñì ‚àí Œ∫ ‚Äã ‚àá ùíÅ R œµ c ‚Äã ( ùíÅ ‚à£ ùëº [ K ] ‚Ñì ) . \\displaystyle\\bm{Z}^{\\ell+1/2}=\\bm{Z}^{\\ell}-\\kappa\\nabla_{\\bm{Z}}R^{c}_{\\epsilon}(\\bm{Z}\\mid\\bm{U}_{[K]}^{\\ell}). bold_italic_Z start_POSTSUPER", "snippet": "ùíÅ ‚Ñì + 1 / 2 = ùíÅ ‚Ñì ‚àí Œ∫ ‚Äã ‚àá ùíÅ R œµ c ‚Äã ( ùíÅ ‚à£ ùëº [ K ] ‚Ñì ) . \\displaystyle\\bm{Z}^{\\ell+1/2}=\\bm{Z}^{\\ell}-\\kappa\\nabla_{\\bm{Z}}R^{c}_{\\epsilon}(\\bm{Z}\\mid\\bm{U}_{[K]}^{\\ell}). bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì + 1 / 2 end_POSTSUPERSCRIPT = bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT - italic_Œ∫ ‚àá start_POSTSUBSCRIPT bold_italic_Z end_POSTSUBSCRIPT italic_R start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ‚à£ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì end_PO"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx46", "title": "‚àá ùíÅ R œµ c ‚Äã ( ùíÅ ‚à£ ùëº [ K ] ) \\displaystyle\\nabla_{\\bm{Z}}R_{\\epsilon}^{c}(\\bm{Z}\\mid\\bm{U}_{[K]}) ‚àá start_POSTSUBSCRIPT bold_italic_Z end_POSTSUBSCRIPT italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUB", "snippet": "‚àá ùíÅ R œµ c ‚Äã ( ùíÅ ‚à£ ùëº [ K ] ) \\displaystyle\\nabla_{\\bm{Z}}R_{\\epsilon}^{c}(\\bm{Z}\\mid\\bm{U}_{[K]}) ‚àá start_POSTSUBSCRIPT bold_italic_Z end_POSTSUBSCRIPT italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_Z ‚à£ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT ) ‚âà p N ‚Äã œµ 2 ‚Äã ‚àë k = 1 K ùëº k ‚Äã ùëº k ‚ä§ ‚Äã ùíÅ ‚Äã ( ùë∞ ‚àí p N ‚Äã œµ 2 ‚Äã ( ùëº k ‚ä§ ‚Äã ùíÅ ) ‚ä§ ‚Äã ( ùëº k ‚ä§ ‚Äã ùíÅ ) ) \\displaystyle\\approx\\frac{p}{N\\epsilon^{2}}\\sum_{k=1}^{K}\\bm{U}_{k}\\bm{U}_{k}^{\\top}\\bm{Z}\\left(\\bm{I}-\\frac{p}{N\\epsilon^{2}}(\\bm{U}_{k}^{\\top}\\bm{Z})^{\\top}(\\bm{"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx47", "title": "‚àá ùíÅ R œµ c ‚Äã ( ùíÅ ‚à£ ùëº [ K ] ) ‚âà p N ‚Äã œµ 2 ‚Äã ùíÅ ‚àí ( p N ‚Äã œµ 2 ) 2 ‚Äã MSSA ‚Å° ( ùíÅ ‚Ñì ‚à£ ùëº [ K ] ‚Ñì ) , \\displaystyle\\nabla_{\\bm{Z}}R_{\\epsilon}^{c}(\\bm{Z}\\mid\\bm{U}_{[K]})\\approx\\frac{p}{N\\epsilon^{2}}\\bm{Z}-\\l", "snippet": "‚àá ùíÅ R œµ c ‚Äã ( ùíÅ ‚à£ ùëº [ K ] ) ‚âà p N ‚Äã œµ 2 ‚Äã ùíÅ ‚àí ( p N ‚Äã œµ 2 ) 2 ‚Äã MSSA ‚Å° ( ùíÅ ‚Ñì ‚à£ ùëº [ K ] ‚Ñì ) , \\displaystyle\\nabla_{\\bm{Z}}R_{\\epsilon}^{c}(\\bm{Z}\\mid\\bm{U}_{[K]})\\approx\\frac{p}{N\\epsilon^{2}}\\bm{Z}-\\left(\\frac{p}{N\\epsilon^{2}}\\right)^{2}\\operatorname{MSSA}\\left(\\bm{Z}^{\\ell}\\mid\\bm{U}_{[K]}^{\\ell}\\right), ‚àá start_POSTSUBSCRIPT bold_italic_Z end_POSTSUBSCRIPT italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ( bold_italic_Z ‚à£ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT ) ‚âà divide start_ARG italic_p end_ARG start_ARG "}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx48", "title": "SSA ‚Äã ( ùíÅ ‚à£ ùëº k ) ‚âê ( ùëº k ‚ä§ ‚Äã ùíÅ ) ‚Äã softmax ‚Äã ( ( ùëº k ‚ä§ ‚Äã ùíÅ ) ‚ä§ ‚Äã ( ùëº k ‚ä§ ‚Äã ùíÅ ) ) , ‚àÄ k ‚àà [ K ] , \\displaystyle\\mathrm{SSA}\\left(\\bm{Z}\\mid\\bm{U}_{k}\\right)\\doteq(\\bm{U}_{k}^{\\top}\\bm{Z})\\mathrm{softm", "snippet": "SSA ‚Äã ( ùíÅ ‚à£ ùëº k ) ‚âê ( ùëº k ‚ä§ ‚Äã ùíÅ ) ‚Äã softmax ‚Äã ( ( ùëº k ‚ä§ ‚Äã ùíÅ ) ‚ä§ ‚Äã ( ùëº k ‚ä§ ‚Äã ùíÅ ) ) , ‚àÄ k ‚àà [ K ] , \\displaystyle\\mathrm{SSA}\\left(\\bm{Z}\\mid\\bm{U}_{k}\\right)\\doteq(\\bm{U}_{k}^{\\top}\\bm{Z})\\mathrm{softmax}\\left((\\bm{U}_{k}^{\\top}\\bm{Z})^{\\top}(\\bm{U}_{k}^{\\top}\\bm{Z})\\right),\\ \\forall k\\in[K], roman_SSA ( bold_italic_Z ‚à£ bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ‚âê ( bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_Z ) roman_softmax ( ( bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSU"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx49", "title": "ùíÅ ‚Ñì + 1 ‚âà arg ‚Äã min ùíÅ ‚Å° ‚Äñ ùíÅ ‚Äñ 1 subject to ùíÅ ‚Ñì + 1 / 2 = ùë´ ‚Ñì ‚Äã ùíÅ . \\displaystyle\\bm{Z}^{\\ell+1}\\approx\\operatorname*{arg\\ min}_{\\bm{Z}}\\|\\bm{Z}\\|_{1}\\quad\\mbox{subject to}\\quad\\bm{Z}^{\\ell+1/2}=\\bm{D}", "snippet": "ùíÅ ‚Ñì + 1 ‚âà arg ‚Äã min ùíÅ ‚Å° ‚Äñ ùíÅ ‚Äñ 1 subject to ùíÅ ‚Ñì + 1 / 2 = ùë´ ‚Ñì ‚Äã ùíÅ . \\displaystyle\\bm{Z}^{\\ell+1}\\approx\\operatorname*{arg\\ min}_{\\bm{Z}}\\|\\bm{Z}\\|_{1}\\quad\\mbox{subject to}\\quad\\bm{Z}^{\\ell+1/2}=\\bm{D}^{\\ell}\\bm{Z}. bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì + 1 end_POSTSUPERSCRIPT ‚âà start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT bold_italic_Z end_POSTSUBSCRIPT ‚à• bold_italic_Z ‚à• start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT subject to bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì + 1 / 2 end_POSTSUPERSCRIPT = bold_italic_D start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT bold_ita"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx50", "title": "ùíÅ ‚Ñì + 1 \\displaystyle\\bm{Z}^{\\ell+1} bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì + 1 end_POSTSUPERSCRIPT = ISTA ‚Äã ( ùíÅ ‚Ñì + 1 / 2 ‚à£ ùë´ ‚Ñì ) , \\displaystyle=\\mathrm{ISTA}({\\bm{Z}^{\\ell+1/2}\\mid\\bm{D}^{\\ell", "snippet": "ùíÅ ‚Ñì + 1 \\displaystyle\\bm{Z}^{\\ell+1} bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì + 1 end_POSTSUPERSCRIPT = ISTA ‚Äã ( ùíÅ ‚Ñì + 1 / 2 ‚à£ ùë´ ‚Ñì ) , \\displaystyle=\\mathrm{ISTA}({\\bm{Z}^{\\ell+1/2}\\mid\\bm{D}^{\\ell}}), = roman_ISTA ( bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì + 1 / 2 end_POSTSUPERSCRIPT ‚à£ bold_italic_D start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ) , (4.2.18) where ISTA ‚Äã ( ùíÅ ‚à£ ùë´ ) \\displaystyle\\text{where}\\quad\\mathrm{ISTA}({\\bm{Z}\\mid\\bm{D}}) where roman_ISTA ( bold_italic_Z ‚à£ bold_italic_D ) ‚âê ReLU ‚Å° ( ùíÅ ‚àí Œ∑ ‚Äã ùë´ ‚ä§ ‚Äã ( ùë´ ‚Äã ùíÅ ‚àí ùíÅ ) ‚àí Œ∑ ‚Äã Œª ‚Äã ùüè ) . \\displaystyle\\doteq\\operatorname{ReL"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx51", "title": "ùíõ i = ùëº k ‚Äã ùíÇ i ‚èü ùê¨ùê¢ùê†ùêßùêöùê• + ‚àë j ‚â† k K ùëº j ‚Äã ùíÜ i , j ‚èü ùêßùê®ùê¢ùê¨ùêû , ‚àÄ i ‚àà C k , \\displaystyle\\bm{z}_{i}=\\underbrace{\\bm{U}_{k}\\bm{a}_{i}}_{\\bf signal}+\\underbrace{\\sum_{j\\neq k}^{K}\\bm{U}_{j}\\bm{e}_{i,j}}_{\\", "snippet": "ùíõ i = ùëº k ‚Äã ùíÇ i ‚èü ùê¨ùê¢ùê†ùêßùêöùê• + ‚àë j ‚â† k K ùëº j ‚Äã ùíÜ i , j ‚èü ùêßùê®ùê¢ùê¨ùêû , ‚àÄ i ‚àà C k , \\displaystyle\\bm{z}_{i}=\\underbrace{\\bm{U}_{k}\\bm{a}_{i}}_{\\bf signal}+\\underbrace{\\sum_{j\\neq k}^{K}\\bm{U}_{j}\\bm{e}_{i,j}}_{\\bf noise},\\ \\forall i\\in C_{k}, bold_italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = under‚èü start_ARG bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG start_POSTSUBSCRIPT bold_signal end_POSTSUBSCRIPT + under‚èü start_ARG ‚àë start_POSTSUBSCRIPT italic_j ‚â† italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx52", "title": "ùíÅ ( ‚Ñì + 1 ) = ùíÅ ( ‚Ñì ) + Œ∑ ‚Äã ‚àë k = 1 K ùëº k ‚Äã ùëº k T ‚Äã ùíÅ ( ‚Ñì ) ‚Äã œÜ ‚Äã ( ùíÅ ( ‚Ñì ) T ‚Äã ùëº k ‚Äã ùëº k T ‚Äã ùíÅ ( ‚Ñì ) ) , \\displaystyle\\bm{Z}^{(\\ell+1)}=\\bm{Z}^{(\\ell)}+\\eta\\sum_{k=1}^{K}\\bm{U}_{k}\\bm{U}_{k}^{T}\\bm{Z", "snippet": "ùíÅ ( ‚Ñì + 1 ) = ùíÅ ( ‚Ñì ) + Œ∑ ‚Äã ‚àë k = 1 K ùëº k ‚Äã ùëº k T ‚Äã ùíÅ ( ‚Ñì ) ‚Äã œÜ ‚Äã ( ùíÅ ( ‚Ñì ) T ‚Äã ùëº k ‚Äã ùëº k T ‚Äã ùíÅ ( ‚Ñì ) ) , \\displaystyle\\bm{Z}^{(\\ell+1)}=\\bm{Z}^{(\\ell)}+\\eta\\sum_{k=1}^{K}\\bm{U}_{k}\\bm{U}_{k}^{T}\\bm{Z}^{(\\ell)}\\varphi\\left(\\bm{Z}^{(\\ell)^{T}}\\bm{U}_{k}\\bm{U}_{k}^{T}\\bm{Z}^{(\\ell)}\\right), bold_italic_Z start_POSTSUPERSCRIPT ( roman_‚Ñì + 1 ) end_POSTSUPERSCRIPT = bold_italic_Z start_POSTSUPERSCRIPT ( roman_‚Ñì ) end_POSTSUPERSCRIPT + italic_Œ∑ ‚àë start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSU"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx53", "title": "SNR ‚Äã ( ùíÅ k ( ‚Ñì ) ) ‚âê ‚Äñ ùëº k ‚Äã ùëº k T ‚Äã ùíÅ k ( ‚Ñì ) ‚Äñ F ‚Äñ ( ùë∞ ‚àí ùëº k ‚Äã ùëº k T ) ‚Äã ùíÅ k ( ‚Ñì ) ‚Äñ F , ‚àÄ k ‚àà [ K ] . \\displaystyle\\mathrm{SNR}(\\bm{Z}_{k}^{(\\ell)})\\doteq\\frac{\\|\\bm{U}_{k}\\bm{U}_{k}^{T}\\bm{Z}_{k}", "snippet": "SNR ‚Äã ( ùíÅ k ( ‚Ñì ) ) ‚âê ‚Äñ ùëº k ‚Äã ùëº k T ‚Äã ùíÅ k ( ‚Ñì ) ‚Äñ F ‚Äñ ( ùë∞ ‚àí ùëº k ‚Äã ùëº k T ) ‚Äã ùíÅ k ( ‚Ñì ) ‚Äñ F , ‚àÄ k ‚àà [ K ] . \\displaystyle\\mathrm{SNR}(\\bm{Z}_{k}^{(\\ell)})\\doteq\\frac{\\|\\bm{U}_{k}\\bm{U}_{k}^{T}\\bm{Z}_{k}^{(\\ell)}\\|_{F}}{\\|(\\bm{I}-\\bm{U}_{k}\\bm{U}_{k}^{T})\\bm{Z}_{k}^{(\\ell)}\\|_{F}},\\quad\\forall k\\in[K]. roman_SNR ( bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( roman_‚Ñì ) end_POSTSUPERSCRIPT ) ‚âê divide start_ARG ‚à• bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT itali"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx54", "title": "[ ùëº 1 ‚Ä¶ ùëº K ] ‚àà ùí™ d √ó K ‚Äã p . \\displaystyle\\begin{bmatrix}\\bm{U}_{1}&\\dots&\\bm{U}_{K}\\end{bmatrix}\\in\\mathcal{O}^{d\\times Kp}. [ start_ARG start_ROW start_CELL bold_italic_U start_POSTSUBSCRIPT 1 end_", "snippet": "[ ùëº 1 ‚Ä¶ ùëº K ] ‚àà ùí™ d √ó K ‚Äã p . \\displaystyle\\begin{bmatrix}\\bm{U}_{1}&\\dots&\\bm{U}_{K}\\end{bmatrix}\\in\\mathcal{O}^{d\\times Kp}. [ start_ARG start_ROW start_CELL bold_italic_U start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_CELL start_CELL ‚Ä¶ end_CELL start_CELL bold_italic_U start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] ‚àà caligraphic_O start_POSTSUPERSCRIPT italic_d √ó italic_K italic_p end_POSTSUPERSCRIPT . (4.3.4)"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx55", "title": "œÑ ‚àà ( 1 2 , 1 1 + N ‚Äã exp ‚Å° ( ‚àí 9 ‚Äã p / 32 ) ] . \\displaystyle\\tau\\in\\left(\\frac{1}{2},\\frac{1}{1+N\\exp(-9p/32)}\\right]. italic_œÑ ‚àà ( divide start_ARG 1 end_ARG start_ARG 2 end_ARG , divide start_ARG ", "snippet": "œÑ ‚àà ( 1 2 , 1 1 + N ‚Äã exp ‚Å° ( ‚àí 9 ‚Äã p / 32 ) ] . \\displaystyle\\tau\\in\\left(\\frac{1}{2},\\frac{1}{1+N\\exp(-9p/32)}\\right]. italic_œÑ ‚àà ( divide start_ARG 1 end_ARG start_ARG 2 end_ARG , divide start_ARG 1 end_ARG start_ARG 1 + italic_N roman_exp ( - 9 italic_p / 32 ) end_ARG ] ."}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx56", "title": "SNR ‚Äã ( ùíÅ k ( ‚Ñì + 1 ) ) = ( 1 + Œ∑ ‚Äã œÑ ) ‚Äã SNR ‚Äã ( ùíÅ k ( ‚Ñì ) ) , ‚àÄ k ‚àà [ K ] . \\displaystyle\\mathrm{SNR}(\\bm{Z}_{k}^{(\\ell+1)})=(1+\\eta\\tau)\\mathrm{SNR}(\\bm{Z}_{k}^{(\\ell)}),\\ \\forall k\\in[K]. roman_SN", "snippet": "SNR ‚Äã ( ùíÅ k ( ‚Ñì + 1 ) ) = ( 1 + Œ∑ ‚Äã œÑ ) ‚Äã SNR ‚Äã ( ùíÅ k ( ‚Ñì ) ) , ‚àÄ k ‚àà [ K ] . \\displaystyle\\mathrm{SNR}(\\bm{Z}_{k}^{(\\ell+1)})=(1+\\eta\\tau)\\mathrm{SNR}(\\bm{Z}_{k}^{(\\ell)}),\\ \\forall k\\in[K]. roman_SNR ( bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( roman_‚Ñì + 1 ) end_POSTSUPERSCRIPT ) = ( 1 + italic_Œ∑ italic_œÑ ) roman_SNR ( bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( roman_‚Ñì ) end_POSTSUPERSCRIPT ) , ‚àÄ italic_k ‚àà [ italic_K ] . (4.3.5)"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx57", "title": "‚àá ùíÅ R c , f var ‚Äã ( ùíÅ , ùö∑ ‚à£ ùëº [ K ] ) = 1 n ‚Äã ‚àë k = 1 K ùëº k ‚Äã Diag ‚Äã ( ‚àá f ‚Äã [ ( ùëº k ‚ä§ ‚Äã ùíÅ ) ‚äô 2 ‚Äã ùùÖ k ‚ü® ùùÖ k , ùüè ‚ü© ] ) ‚èü ‚âê ùë´ ‚Äã ( ùíÅ , ùùÖ k ‚à£ ùëº k ) ‚Äã ùëº k ‚ä§ ‚Äã ùíÅ ‚Äã Diag ‚Äã ( ùùÖ k ) . \\displaystyle\\nabla_{\\bm", "snippet": "‚àá ùíÅ R c , f var ‚Äã ( ùíÅ , ùö∑ ‚à£ ùëº [ K ] ) = 1 n ‚Äã ‚àë k = 1 K ùëº k ‚Äã Diag ‚Äã ( ‚àá f ‚Äã [ ( ùëº k ‚ä§ ‚Äã ùíÅ ) ‚äô 2 ‚Äã ùùÖ k ‚ü® ùùÖ k , ùüè ‚ü© ] ) ‚èü ‚âê ùë´ ‚Äã ( ùíÅ , ùùÖ k ‚à£ ùëº k ) ‚Äã ùëº k ‚ä§ ‚Äã ùíÅ ‚Äã Diag ‚Äã ( ùùÖ k ) . \\displaystyle\\nabla_{\\bm{Z}}R^{\\rm var}_{c,f}(\\bm{Z},\\bm{\\Pi}\\mid\\bm{U}_{[K]})=\\frac{1}{n}\\sum_{k=1}^{K}\\bm{U}_{k}\\underbrace{\\mathrm{Diag}\\left(\\nabla f\\left[(\\bm{U}_{k}^{\\top}\\bm{Z})^{\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\scalebox{0.8}{$\\scriptstyle\\odot$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\odot$}}$}}{\\raisebox{1.3pt}{$"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx58", "title": "ùö∑ = [ ùùÇ ‚Äã ( ùíõ 1 ‚à£ ùëº [ K ] ) ‚ä§ ‚ãÆ ùùÇ ‚Äã ( ùíõ n ‚à£ ùëº [ K ] ) ‚ä§ ] , where ùùÇ ‚Äã ( ùíõ j ‚à£ ùëº [ K ] ) ‚âê softmax ‚Å° ( 1 2 ‚Äã Œ∑ ‚Äã [ ‚Äñ ùëº 1 ‚ä§ ‚Äã ùíõ j ‚Äñ 2 2 ‚ãÆ ‚Äñ ùëº K ‚ä§ ‚Äã ùíõ j ‚Äñ 2 2 ] ) , ‚àÄ j ‚àà [ n ] , \\displaystyle\\bm{\\Pi}=\\b", "snippet": "ùö∑ = [ ùùÇ ‚Äã ( ùíõ 1 ‚à£ ùëº [ K ] ) ‚ä§ ‚ãÆ ùùÇ ‚Äã ( ùíõ n ‚à£ ùëº [ K ] ) ‚ä§ ] , where ùùÇ ‚Äã ( ùíõ j ‚à£ ùëº [ K ] ) ‚âê softmax ‚Å° ( 1 2 ‚Äã Œ∑ ‚Äã [ ‚Äñ ùëº 1 ‚ä§ ‚Äã ùíõ j ‚Äñ 2 2 ‚ãÆ ‚Äñ ùëº K ‚ä§ ‚Äã ùíõ j ‚Äñ 2 2 ] ) , ‚àÄ j ‚àà [ n ] , \\displaystyle\\bm{\\Pi}=\\begin{bmatrix}\\bm{\\nu}(\\bm{z}_{1}\\mid\\bm{U}_{[K]})^{\\top}\\\\ \\vdots\\\\ \\bm{\\nu}(\\bm{z}_{n}\\mid\\bm{U}_{[K]})^{\\top}\\end{bmatrix},\\quad\\text{where}\\quad\\bm{\\nu}(\\bm{z}_{j}\\mid\\bm{U}_{[K]})\\doteq\\operatorname{softmax}\\left(\\frac{1}{2\\eta}\\begin{bmatrix}\\|\\bm{U}_{1}^{\\top}\\bm{z}_{j}\\|_{2}^{2}\\\\ \\vdots\\\\ \\|\\bm{U}_{K}^{\\top}\\bm{z}_{j}\\|_{2}^{2}\\end{bmatrix}\\right),\\quad\\forall j\\in[n], bold_Œ† = [ start_ARG"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx59", "title": "R ‚Äã ( ùíÅ ) = log ‚Äã det ( ùë∞ + Œ± ‚Äã ùíÅ ‚Äã ùíÅ T ) . \\displaystyle R(\\bm{Z})=\\log\\det\\left(\\bm{I}+\\alpha\\bm{Z}\\bm{Z}^{T}\\right). italic_R ( bold_italic_Z ) = roman_log roman_det ( bold_italic_I + italic_Œ± bold", "snippet": "R ‚Äã ( ùíÅ ) = log ‚Äã det ( ùë∞ + Œ± ‚Äã ùíÅ ‚Äã ùíÅ T ) . \\displaystyle R(\\bm{Z})=\\log\\det\\left(\\bm{I}+\\alpha\\bm{Z}\\bm{Z}^{T}\\right). italic_R ( bold_italic_Z ) = roman_log roman_det ( bold_italic_I + italic_Œ± bold_italic_Z bold_italic_Z start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ) ."}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx60", "title": "‚àá 2 R ‚Äã ( ùíÅ ) ‚Äã [ ùë´ , ùë´ ] = Œ± ‚Äã Tr ‚Äã ( ùëø ‚àí 1 ‚Äã ùë´ ‚Äã ùë´ T ) ‚àí Œ± 2 2 ‚Äã Tr ‚Äã ( ùëø ‚àí 1 ‚Äã ( ùíÅ ‚Äã ùë´ T + ùë´ ‚Äã ùíÅ T ) ‚Äã ùëø ‚àí 1 ‚Äã ( ùíÅ ‚Äã ùë´ T + ùë´ ‚Äã ùíÅ T ) ) , \\displaystyle\\nabla^{2}R(\\bm{Z})[\\bm{D},\\bm{D}]=\\alpha\\mathr", "snippet": "‚àá 2 R ‚Äã ( ùíÅ ) ‚Äã [ ùë´ , ùë´ ] = Œ± ‚Äã Tr ‚Äã ( ùëø ‚àí 1 ‚Äã ùë´ ‚Äã ùë´ T ) ‚àí Œ± 2 2 ‚Äã Tr ‚Äã ( ùëø ‚àí 1 ‚Äã ( ùíÅ ‚Äã ùë´ T + ùë´ ‚Äã ùíÅ T ) ‚Äã ùëø ‚àí 1 ‚Äã ( ùíÅ ‚Äã ùë´ T + ùë´ ‚Äã ùíÅ T ) ) , \\displaystyle\\nabla^{2}R(\\bm{Z})[\\bm{D},\\bm{D}]=\\alpha\\mathrm{Tr}\\left(\\bm{X}^{-1}\\bm{D}\\bm{D}^{T}\\right)-\\frac{\\alpha^{2}}{2}\\mathrm{Tr}\\left(\\bm{X}^{-1}\\left(\\bm{Z}\\bm{D}^{T}+\\bm{D}\\bm{Z}^{T}\\right)\\bm{X}^{-1}\\left(\\bm{Z}\\bm{D}^{T}+\\bm{D}\\bm{Z}^{T}\\right)\\right), ‚àá start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_R ( bold_italic_Z ) [ bold_italic_D , bold_italic_D ] = italic_Œ± roman_Tr ( bold_italic_X start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx61", "title": "‚àá 2 R ‚Äã ( ùíÅ ) ‚Äã [ ùë´ , ùë´ ] ‚âê ‚ü® lim t ‚Üí 0 ‚àá R ‚Äã ( ùíÅ + t ‚Äã ùë´ ) ‚àí ‚àá R ‚Äã ( ùíÅ ) t , ùë´ ‚ü© . \\displaystyle\\nabla^{2}R(\\bm{Z})[\\bm{D},\\bm{D}]\\doteq\\left\\langle\\lim_{t\\to 0}\\frac{\\nabla R(\\bm{Z}+t\\bm{D})-\\nabla ", "snippet": "‚àá 2 R ‚Äã ( ùíÅ ) ‚Äã [ ùë´ , ùë´ ] ‚âê ‚ü® lim t ‚Üí 0 ‚àá R ‚Äã ( ùíÅ + t ‚Äã ùë´ ) ‚àí ‚àá R ‚Äã ( ùíÅ ) t , ùë´ ‚ü© . \\displaystyle\\nabla^{2}R(\\bm{Z})[\\bm{D},\\bm{D}]\\doteq\\left\\langle\\lim_{t\\to 0}\\frac{\\nabla R(\\bm{Z}+t\\bm{D})-\\nabla R(\\bm{Z})}{t},\\bm{D}\\right\\rangle. ‚àá start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_R ( bold_italic_Z ) [ bold_italic_D , bold_italic_D ] ‚âê ‚ü® roman_lim start_POSTSUBSCRIPT italic_t ‚Üí 0 end_POSTSUBSCRIPT divide start_ARG ‚àá italic_R ( bold_italic_Z + italic_t bold_italic_D ) - ‚àá italic_R ( bold_italic_Z ) end_ARG start_ARG italic_t end_ARG , bold_italic_D ‚ü© ."}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx62", "title": "R ‚Äã ( ùíÅ ) ‚â§ ‚àë k = 1 K log ‚Äã det ( ùë∞ + Œ± ‚Äã ùíÅ k ‚Äã ùíÅ k T ) , \\displaystyle R(\\bm{Z})\\leq\\sum_{k=1}^{K}\\log\\det\\left(\\bm{I}+\\alpha\\bm{Z}_{k}\\bm{Z}_{k}^{T}\\right), italic_R ( bold_italic_Z ) ‚â§ ‚àë start_POST", "snippet": "R ‚Äã ( ùíÅ ) ‚â§ ‚àë k = 1 K log ‚Äã det ( ùë∞ + Œ± ‚Äã ùíÅ k ‚Äã ùíÅ k T ) , \\displaystyle R(\\bm{Z})\\leq\\sum_{k=1}^{K}\\log\\det\\left(\\bm{I}+\\alpha\\bm{Z}_{k}\\bm{Z}_{k}^{T}\\right), italic_R ( bold_italic_Z ) ‚â§ ‚àë start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_log roman_det ( bold_italic_I + italic_Œ± bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ) ,"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx63", "title": "f ‚Äã ( ùíÅ k ) = 1 2 ‚Äã log ‚Äã det ( ùë∞ + Œ± ‚Äã ùíÅ k ‚Äã ùíÅ k T ) ‚àí m k 2 ‚Äã m ‚Äã log ‚Äã det ( ùë∞ + Œ± k ‚Äã ùíÅ k ‚Äã ùíÅ k T ) ‚àí Œª 2 ‚Äã ‚Äñ ùíÅ k ‚Äñ F 2 . \\displaystyle f(\\bm{Z}_{k})=\\frac{1}{2}\\log\\det\\left(\\bm{I}+\\alpha\\bm{Z}_{", "snippet": "f ‚Äã ( ùíÅ k ) = 1 2 ‚Äã log ‚Äã det ( ùë∞ + Œ± ‚Äã ùíÅ k ‚Äã ùíÅ k T ) ‚àí m k 2 ‚Äã m ‚Äã log ‚Äã det ( ùë∞ + Œ± k ‚Äã ùíÅ k ‚Äã ùíÅ k T ) ‚àí Œª 2 ‚Äã ‚Äñ ùíÅ k ‚Äñ F 2 . \\displaystyle f(\\bm{Z}_{k})=\\frac{1}{2}\\log\\det\\left(\\bm{I}+\\alpha\\bm{Z}_{k}\\bm{Z}_{k}^{T}\\right)-\\frac{m_{k}}{2m}\\log\\det\\left(\\bm{I}+\\alpha_{k}\\bm{Z}_{k}\\bm{Z}_{k}^{T}\\right)-\\frac{\\lambda}{2}\\|\\bm{Z}_{k}\\|_{F}^{2}. italic_f ( bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) = divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_log roman_det ( bold_italic_I + italic_Œ± bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_Z start_POSTS"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx64", "title": "ùíÅ k = ùë∑ k ‚Äã ùö∫ k ‚Äã ùë∏ k T = [ ùë∑ k , 1 ùë∑ k , 2 ] ‚Äã [ ùö∫ ~ k ùüé ùüé ùüé ] ‚Äã [ ùë∏ k , 1 T ùë∏ k , 2 T ] , \\displaystyle\\bm{Z}_{k}=\\bm{P}_{k}\\bm{\\Sigma}_{k}\\bm{Q}_{k}^{T}=\\begin{bmatrix}\\bm{P}_{k,1}&\\bm{P}_{k,2}\\end", "snippet": "ùíÅ k = ùë∑ k ‚Äã ùö∫ k ‚Äã ùë∏ k T = [ ùë∑ k , 1 ùë∑ k , 2 ] ‚Äã [ ùö∫ ~ k ùüé ùüé ùüé ] ‚Äã [ ùë∏ k , 1 T ùë∏ k , 2 T ] , \\displaystyle\\bm{Z}_{k}=\\bm{P}_{k}\\bm{\\Sigma}_{k}\\bm{Q}_{k}^{T}=\\begin{bmatrix}\\bm{P}_{k,1}&\\bm{P}_{k,2}\\end{bmatrix}\\begin{bmatrix}\\tilde{\\bm{\\Sigma}}_{k}&\\bm{0}\\\\ \\bm{0}&\\bm{0}\\end{bmatrix}\\begin{bmatrix}\\bm{Q}_{k,1}^{T}\\\\ \\bm{Q}_{k,2}^{T}\\end{bmatrix}, bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = bold_italic_P start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_Œ£ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_Q start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUP"}, {"page": "Chapter 4 Deep Representations from Unrolled Optimization", "href": "Ch4.html#A2.S3.EGx65", "title": "( ùë∞ ‚àí ùë® ) ‚àí 1 = ‚àë k = 1 ‚àû ùë® k . \\displaystyle\\left(\\bm{I}-\\bm{A}\\right)^{-1}=\\sum_{k=1}^{\\infty}\\bm{A}^{k}. ( bold_italic_I - bold_italic_A ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT = ‚àë start_PO", "snippet": "( ùë∞ ‚àí ùë® ) ‚àí 1 = ‚àë k = 1 ‚àû ùë® k . \\displaystyle\\left(\\bm{I}-\\bm{A}\\right)^{-1}=\\sum_{k=1}^{\\infty}\\bm{A}^{k}. ( bold_italic_I - bold_italic_A ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT = ‚àë start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚àû end_POSTSUPERSCRIPT bold_italic_A start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT . (4.5.1)"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#top", "title": "Chapter 5 Consistent and Self-Consistent Representations", "snippet": ""}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1", "title": "5.1 Learning Consistent Representations", "snippet": "5.1 Learning Consistent Representations Here we give a formal definition of consistent representations, which are closely related to the concept of autoencoding. Definition 5.1 (Consistent Representations) . Given data ùëø \\bm{X} bold_italic_X , an consistent representation is a pair of functions ( f : ùí≥ ‚Üí ùíµ , g : ùíµ ‚Üí ùí≥ ) (f\\colon\\mathcal{X}\\to\\mathcal{Z},g\\colon\\mathcal{Z}\\to\\mathcal{X}) ( italic_f : caligraphic_X ‚Üí caligraphic_Z , italic_g : caligraphic_Z ‚Üí caligraphic_X ) , such that the features ùíÅ = f ‚Äã ( ùëø ) \\bm{Z}=f(\\bm{X}) bold_italic_Z = italic_f ( bold_italic_X ) are compact and structu"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2", "title": "5.2 Learning Self-Consistent Representations", "snippet": "5.2 Learning Self-Consistent Representations In earlier chapters, we have studied methods that would allow us to learn a low-dimensional distribution via (lossy) compression. As we have mentioned in Chapter 1 and demonstrated in the previous chapters, the progresses made in machine intelligence largely rely on finding computationally feasible and efficient solutions to realize the desired compression, not only computable or tractable in theory, but also scalable in practice: computable ‚üπ tractable ‚üπ scalable . \\mbox{{computable}}\\;\\Longrightarrow\\;\\mbox{{tractable}}\\;\\Longrightarrow\\;\\mbox{{sc"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3", "title": "5.3 Continuous Learning Self-Consistent Representations", "snippet": "5.3 Continuous Learning Self-Consistent Representations 5.3.1 Class-wise Incremental Learning As we have seen, deep neural networks have demonstrated a great ability to learn representations for hundreds or even thousands of classes of objects, in both discriminative and generative contexts. However, networks typically must be trained offline, with uniformly sampled data from all classes simultaneously. It has been known that when an (open-loop) network is updated to learn new classes without data from the old ones, previously learned knowledge will fall victim to the problem of catastrophic f"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S4", "title": "5.4 Summary and Notes", "snippet": "5.4 Summary and Notes Historically, autoencoding has been one of the important drivers of research innovation in neural networks for learning, although the most practically impressive demonstrations of deep learning have probably been in other domains (such as discriminative classification, with AlexNet [ KSH12 ] , or generative modeling with GPT architectures [ BMR+20 ] ). Works we have featured throughout the chapter, especially the work of [ HS06 ] , served as catalysts of research interest in neural networks during times when they were otherwise not prominent in the machine learning resear"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S5", "title": "5.5 Exercises and Extensions", "snippet": "5.5 Exercises and Extensions Exercise 5.1 (Conceptual Understanding of Manifold Flattening) . Consider data lying on a curved manifold ‚Ñ≥ \\mathcal{M} caligraphic_M embedded in ‚Ñù D \\mathbb{R}^{D} blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT (like a curved surface in 3 3 3 -dimensional space), as discussed in the manifold flattening subsection of Section 5.1.2 . In this exercise, we will describe the basic ingredients of the manifold flattening algorithm from [ PPR+24 ] . A manifold is called flat if it is an open set in Euclidean space (or more generally, an open set in a subs"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.SS1", "title": "5.1.1 Linear Autoencoding via PCA", "snippet": "5.1.1 Linear Autoencoding via PCA According to [ Bal11 ] , the phrase ‚Äúautoencoder‚Äù was first introduced by Hinton and Rumelhart [ RHW86 ] so that a deep representation can be learned via back propagation (BP) in a self-supervision fashion‚Äîreconstructing the original data is the self-supervising task. However, the very same concept of seeking a compact and consistent representation has been rooted in many classic studies. As we have already seen in Chapter 2 , the classical PCA, ICA, and sparse dictionary learning all share a similar goal. The only difference is when the underlying data distri"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.SS2", "title": "5.1.2 Nonlinear PCA and Autoencoding", "snippet": "5.1.2 Nonlinear PCA and Autoencoding Of course, one should expect that things will no longer be so simple when we deal with more complex distributions whose underlying low-dimensional structure could be nonlinear. Data on a Nonlinear Submanifold. So, to move beyond the linear structure addressed by PCA, we may assume that the data distribution lies on a (smooth) submanifold ‚Ñ≥ \\mathcal{M} caligraphic_M . The intrinsic dimension of the submanifold, say d d italic_d , is typically much lower than the dimension of the ambient space ‚Ñù D \\mathbb{R}^{D} blackboard_R start_POSTSUPERSCRIPT italic_D end"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.SS3", "title": "5.1.3 Sparse Autoencoding", "snippet": "5.1.3 Sparse Autoencoding In the above autoencoding schemes, the dimension of the feature space d d italic_d is typically chosen to be much lower than that the original data space D D italic_D so as to explicitly enforce or promote the learned representation to be low-dimensional. However, in practice, we normally do not know the intrinsic dimension of the data distribution. Hence, the choice of the feature space dimension for autoencoding is often done empirically. In more general situations, the data distribution can be a mixture of a few low-dimensional subspaces or submanifolds. In these c"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.SS4", "title": "5.1.4 Variational Autoencoding", "snippet": "5.1.4 Variational Autoencoding In the classical conception of autoencoding, following Hinton and Rumelhart [ RHW86 ] , the data distribution plays a very minor role in the formulation, in spite of its centrality to the representation we ultimately learn. Indeed, in the naive framework, one hopes that by training a deep network to reconstruct samples from the data distribution with a suitably-configured bottleneck for the representation ùíõ \\bm{z} bold_italic_z , the learned encoders f f italic_f and g g italic_g will naturally end up corresponding to a compact and structured feature representati"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.SS1", "title": "5.2.1 Closed-Loop Transcription via Stackelberg Games", "snippet": "5.2.1 Closed-Loop Transcription via Stackelberg Games How do we try to ensure a learned representation is self-consistent? As usual, let us assume ùëø = ‚à™ k = 1 K ùëø k \\bm{X}=\\cup_{k=1}^{K}\\bm{X}_{k} bold_italic_X = ‚à™ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT with each subset of samples ùëø k \\bm{X}_{k} bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT belonging to a low-dimensional submanifold: ùëø k ‚äÇ ‚Ñ≥ k , k = 1 , ‚Ä¶ , K \\bm{X}_{k}\\subset\\mathcal{M}_{k},k=1,\\ldots,K bo"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.SS2", "title": "5.2.2 A Mixture of Low-Dimensional Gaussians", "snippet": "5.2.2 A Mixture of Low-Dimensional Gaussians In the above, we have argued that it is possible to formulate the problem of learning a data distribution as a closed-loop autoencoding problem. We also saw empirically that such a scheme seems to work. The remaining question is when and why such a scheme should works. It is difficult to answer this question for the most general cases with arbitrary data distributions. Nevertheless, as usual, let us see if we can arrive at a rigorous justification for the ideal case when the data distribution is a mixture of low-dimensional subspaces or low-rank Gau"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS1", "title": "5.3.1 Class-wise Incremental Learning", "snippet": "5.3.1 Class-wise Incremental Learning As we have seen, deep neural networks have demonstrated a great ability to learn representations for hundreds or even thousands of classes of objects, in both discriminative and generative contexts. However, networks typically must be trained offline, with uniformly sampled data from all classes simultaneously. It has been known that when an (open-loop) network is updated to learn new classes without data from the old ones, previously learned knowledge will fall victim to the problem of catastrophic forgetting [ MC89 ] . This is known in neuroscience as th"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS2", "title": "5.3.2 Sample-wise Continuous Unsupervised Learning", "snippet": "5.3.2 Sample-wise Continuous Unsupervised Learning As we know, the closed-loop CTRL formulation can already learn a decent autoencoding, even without class information, with the CTRL-Binary program: max ùúΩ ‚Å° min ùúº Œî ‚Äã R œµ ‚Äã ( ùíÅ , ùíÅ ^ ) \\displaystyle\\max_{\\bm{\\theta}}\\min_{\\bm{\\eta}}\\quad\\Delta R_{\\epsilon}(\\bm{Z},\\hat{\\bm{Z}}) roman_max start_POSTSUBSCRIPT bold_italic_Œ∏ end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT bold_italic_Œ∑ end_POSTSUBSCRIPT roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z , over^ start_ARG bold_italic_Z end_ARG ) (5.3.6) However, note that"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.SS1.SSS0.Px1", "title": "Online PCA.", "snippet": "Online PCA. Notice that in the above construction, the linear transform ùëº \\bm{U} bold_italic_U used for the encoding and decoding is computed ‚Äúoffline‚Äù from all the input data before hand. One question is whether this transform can be learned ‚Äúonline‚Äù as the input data come in order? This question was answered by the work of Oja in 1982 [ Oja82 ] . Example 5.1 (Normalized Hebbian learning scheme for PCA) . Consider a sequence of i.i.d. random vectors ùíô 1 , ‚Ä¶ , ùíô i , ‚Ä¶ ‚àà ‚Ñù n \\bm{x}_{1},\\ldots,\\bm{x}_{i},\\ldots\\in\\mathbb{R}^{n} bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_ita"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.SS2.SSS0.Px1", "title": "Data on a Nonlinear Submanifold.", "snippet": "Data on a Nonlinear Submanifold. So, to move beyond the linear structure addressed by PCA, we may assume that the data distribution lies on a (smooth) submanifold ‚Ñ≥ \\mathcal{M} caligraphic_M . The intrinsic dimension of the submanifold, say d d italic_d , is typically much lower than the dimension of the ambient space ‚Ñù D \\mathbb{R}^{D} blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT . From this geometric perspective, we typically want to find a nonlinear mapping f f italic_f such that the resulting manifold f ‚Äã ( ‚Ñ≥ ) f(\\mathcal{M}) italic_f ( caligraphic_M ) is flattened, as i"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.SS2.SSS0.Px2", "title": "A Classical Attempt via a Two-Layer Network.", "snippet": "A Classical Attempt via a Two-Layer Network. As we have seen above, in the case of PCA, a one-layer linear neural network is sufficient. That is no longer the case for NLPCA. In 1991, Kramer [ Kra91 ] proposed to solve NLPCA by using a two-layer neural network to represent the encoder mapping f f italic_f (or its inverse g g italic_g ) based on the universal representation property of two-layer networks with sigmoid activation: ùíõ = ùëæ 2 ‚Äã œÉ ‚Äã ( ùëæ 1 ‚Äã ùíô + ùíÉ ) , \\bm{z}=\\bm{W}_{2}\\sigma(\\bm{W}_{1}\\bm{x}+\\bm{b}), bold_italic_z = bold_italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_œÉ ( bold_"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.SS2.SSS0.Px3", "title": "Manifold Flattening via a Deeper Network.", "snippet": "Manifold Flattening via a Deeper Network. Based on the modern practice of deep networks, such classical shallow and wide network architectures are known to be rather difficult to train effectively and efficiently via back propagation (BP), partly due to the diminishing gradient of the sigmoid function. Hence, the modern practice normally suggests to further decompose the nonlinear transform f f italic_f (or g g italic_g ) into a composition of many more layers of simpler transforms, resulting a deeper network architecture [ HS06 ] , as illustrated in Figure 5.4 . In the modern context, further"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.SS1.SSS0.Px1", "title": "Measuring distance in the feature space.", "snippet": "Measuring distance in the feature space. However, as we have discussed above, if we do not have the option to compute the distance between ùëø \\bm{X} bold_italic_X and ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG , we are left with the option of comparing their corresponding features ùíÅ \\bm{Z} bold_italic_Z and ùíÅ ^ = f ‚Äã ( ùëø ^ , ùúΩ ) \\hat{\\bm{Z}}=f(\\hat{\\bm{X}},\\bm{\\theta}) over^ start_ARG bold_italic_Z end_ARG = italic_f ( over^ start_ARG bold_italic_X end_ARG , bold_italic_Œ∏ ) . Notice that under the MCR 2 objective, the distributions of the resulting ùíÅ \\bm{Z} bold_italic_Z or ùíÅ ^ \\hat"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.SS1.SSS0.Px2", "title": "Encoder and decoder as a two-player game.", "snippet": "Encoder and decoder as a two-player game. Obviously, to ensure the learned auto-encoding to be self-consistent, the main goal of the decoder g ‚Äã ( ‚ãÖ , ùúº ) g(\\cdot,\\bm{\\eta}) italic_g ( ‚ãÖ , bold_italic_Œ∑ ) is to minimize the distance between ùíÅ \\bm{Z} bold_italic_Z and ùíÅ ^ \\hat{\\bm{Z}} over^ start_ARG bold_italic_Z end_ARG . That is, to learn g g italic_g , we want to minimize the distance d ‚Äã ( ùíÅ , ùíÅ ^ ) d(\\bm{Z},\\hat{\\bm{Z}}) italic_d ( bold_italic_Z , over^ start_ARG bold_italic_Z end_ARG ) : min g ‚Å° d ‚Äã ( ùíÅ , ùíÅ ^ ) ‚âê min Œ∑ ‚Äã ‚àë k = 1 K Œî ‚Äã R ‚Äã ( ùíÅ k , ùíÅ ^ k ) = min ùúº ‚Äã ‚àë k = 1 K Œî ‚Äã R ‚Äã ( ùíÅ k"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.SS1.SSS0.Px3", "title": "Visualizing correlation of features ùíÅ \\bm{Z} bold_italic_Z and decoded features ùíÅ ^ \\hat{\\bm{Z}} over^ start_ARG bold_italic_Z end_ARG .", "snippet": "Visualizing correlation of features ùíÅ \\bm{Z} bold_italic_Z and decoded features ùíÅ ^ \\hat{\\bm{Z}} over^ start_ARG bold_italic_Z end_ARG . We visualize the cosine similarity between ùíÅ \\bm{Z} bold_italic_Z and ùíÅ ^ \\hat{\\bm{Z}} over^ start_ARG bold_italic_Z end_ARG learned from the multi-class objective ( 5.2.20 ) on MNIST, CIFAR-10 and ImageNet (10 classes), which indicates how close ùíõ ^ = f ‚àò g ‚Äã ( ùíõ ) \\hat{\\bm{z}}=f\\circ g(\\bm{z}) over^ start_ARG bold_italic_z end_ARG = italic_f ‚àò italic_g ( bold_italic_z ) is from ùíõ \\bm{z} bold_italic_z . Results in Figure 5.8 show that ùíÅ \\bm{Z} bold_italic_Z "}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.SS1.SSS0.Px4", "title": "Visualizing auto-encoding of the data ùëø \\bm{X} bold_italic_X and the decoded ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG .", "snippet": "Visualizing auto-encoding of the data ùëø \\bm{X} bold_italic_X and the decoded ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG . We compare some representative ùëø \\bm{X} bold_italic_X and ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG on MNIST, CIFAR-10 and ImageNet (10 classes) to verify how close ùíô ^ = g ‚àò f ‚Äã ( ùíô ) \\hat{\\bm{x}}=g\\circ f(\\bm{x}) over^ start_ARG bold_italic_x end_ARG = italic_g ‚àò italic_f ( bold_italic_x ) is to ùíô \\bm{x} bold_italic_x . The results are shown in Figure 5.9 , and visualizations are created from training samples. Visually, the auto-encoded ùíô ^ \\hat{\\"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS1.SSS0.Px1", "title": "LDR memory sampling and replay.", "snippet": "LDR memory sampling and replay. The simple linear structures of LDR make it uniquely suited for incremental learning: the distribution of features ùíÅ j \\bm{Z}_{j} bold_italic_Z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT of each previously learned class can be explicitly and concisely represented by a principal subspace ùíÆ j \\mathcal{S}_{j} caligraphic_S start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT in the feature space. To preserve the memory of an old class j j italic_j , we only need to preserve the subspace while learning new classes. To this end, we simply sample m m italic_m representa"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS1.SSS0.Px2", "title": "Incremental learning LDR with an old-memory constraint.", "snippet": "Incremental learning LDR with an old-memory constraint. Notice that, with the learned auto-encoding ( 5.2.9 ), one can replay and use the images, say ùëø ^ o ‚Äã l ‚Äã d = g ‚Äã ( ùíÅ o ‚Äã l ‚Äã d , ùúº ) \\hat{\\bm{X}}_{old}=g(\\bm{Z}_{old},\\bm{\\eta}) over^ start_ARG bold_italic_X end_ARG start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT = italic_g ( bold_italic_Z start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT , bold_italic_Œ∑ ) , associated with the memory features to avoid forgetting while learning new classes. This is typically how generative models have been used for prior i"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS1.SSS0.Px3", "title": "Jointly optimal memory via incremental reviewing.", "snippet": "Jointly optimal memory via incremental reviewing. As we will see, the above constrained minimax program can already achieve state of the art performance for incremental learning. Nevertheless, developing an optimal memory for all classes cannot rely on graceful forgetting alone. Even for humans, if an object class is learned only once, we should expect the learned memory to fade as we continue to learn new others, unless the memory can be consolidated by reviewing old object classes. To emulate this phase of memory forming, after incrementally learning a whole dataset, we may go back to review"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS1.SSS0.Px4", "title": "Experimental verification.", "snippet": "Experimental verification. We show some experimental results on the following datasets: MNIST [ LBB+98a ] and CIFAR-10 [ KNH14 ] . All experiments are conducted for the more challenging class-IL setting. For both MNIST and CIFAR-10, the 10 classes are split into 5 tasks with 2 classes each or 10 tasks with 1 class each. For the encoder f f italic_f and decoder g g italic_g , we adopt a very simple network architecture modified from DCGAN [ RMC16 ] , which is merely a four-layer convolutional network. Here we only show some qualitative visual results and more experiments and analytical analysis"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS1.SSS0.Px5", "title": "Visualizing auto-encoding properties.", "snippet": "Visualizing auto-encoding properties. We begin by qualitatively visualizing some representative images ùëø \\bm{X} bold_italic_X and the corresponding replayed ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG on MNIST and CIFAR-10. The model is learned incrementally with the datasets split into 5 tasks. Results are shown in Figure 5.11 , where we observe that the reconstructed ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG preserves the main visual characteristics of ùëø \\bm{X} bold_italic_X including shapes and textures. For a simpler dataset like MNIST, the replayed ùëø ^ \\hat{\\bm{X}}"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS1.SSS0.Px6", "title": "Principal subspaces of the learned features.", "snippet": "Principal subspaces of the learned features. Most generative memory-based methods utilize autoencoders, VAEs, or GANs for replay purposes. The structure or distribution of the learned features ùíÅ j \\bm{Z}_{j} bold_italic_Z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT for each class is unclear in the feature space. The features ùíÅ j \\bm{Z}_{j} bold_italic_Z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT of the LDR memory, on the other hand, have a clear linear structure. Figure 5.12 visualizes correlations among all learned features | ùíÅ ‚ä§ ‚Äã ùíÅ | |\\bm{Z}^{\\top}\\bm{Z}| | bold_italic_Z start_POSTSU"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS1.SSS0.Px7", "title": "Replay images of samples from principal components.", "snippet": "Replay images of samples from principal components. Since features of each class can be modeled as a principal subspace, we further visualize the individual principal components within each of those subspaces. Figure 5.13 shows the images replayed from sampled features along the top-4 principal components for different classes, on MNIST and CIFAR-10 respectively. Each row represents samples along one principal component and they clearly show similar visual characteristics but distinctively different from those in other rows. We see that the model remembers different poses of ‚Äò4‚Äô after having l"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS1.SSS0.Px8", "title": "Effectiveness of incremental reviewing.", "snippet": "Effectiveness of incremental reviewing. We verify how the incrementally learned LDR memory can be further consolidated with an unsupervised incremental reviewing phase described before. Experiments are conducted on CIFAR-10, with 10 steps. Figure 5.14 left shows replayed images of the first class ‚Äòairplane‚Äô at the end of incremental learning of all ten classes, sampled along the top-3 principal components ‚Äì every two rows (16 images) are along one principal direction. Their visual quality remains very decent ‚Äì observed almost no forgetting. The right figure shows replayed images after reviewin"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS2.SSS0.Px1", "title": "Sample-wise constraints for unsupervised transcription.", "snippet": "Sample-wise constraints for unsupervised transcription. To improve discriminative and generative properties of representations learned in the unsupervised setting, we propose two additional mechanisms for the above CTRL-Binary maximin game ( 5.3.6 ). For simplicity and uniformity, here these will be formulated as equality constraints over rate reduction measures, but in practice they can be enforced softly during optimization."}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS2.SSS0.Px2", "title": "Sample-wise self-consistency via closed-loop transcription.", "snippet": "Sample-wise self-consistency via closed-loop transcription. First, to address the issue that CTRL-Binary does not learn a sample-wise consistent autoencoding, we need to promote ùíô ^ \\hat{\\bm{x}} over^ start_ARG bold_italic_x end_ARG to be close to ùíô \\bm{x} bold_italic_x for each sample. In the CTRL framework, this can be achieved by enforcing their corresponding features ùíõ = f ‚Äã ( ùíô ) \\bm{z}=f(\\bm{x}) bold_italic_z = italic_f ( bold_italic_x ) and ùíõ ^ = f ‚Äã ( ùíô ^ ) \\hat{\\bm{z}}=f(\\hat{\\bm{x}}) over^ start_ARG bold_italic_z end_ARG = italic_f ( over^ start_ARG bold_italic_x end_ARG ) to be clos"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS2.SSS0.Px3", "title": "Self-supervision via compressing augmented samples.", "snippet": "Self-supervision via compressing augmented samples. Since we do not know any class label information between samples in the unsupervised settings, the best we can do is to view every sample and its augmentations (say via translation, rotation, occlusion etc) as one ‚Äúclass‚Äù‚Äîa basic idea behind almost all self-supervised learning methods. In the rate reduction framework, it is natural to compress the features of each sample and its augmentations. In this work, we adopt the standard transformations in SimCLR [ CKN+20 ] and denote such a transformation as œÑ \\tau italic_œÑ . We denote each augmented"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS2.SSS0.Px4", "title": "Unsupervised representation learning via closed-loop transcription.", "snippet": "Unsupervised representation learning via closed-loop transcription. So far, we know the CTRL-Binary objective Œî ‚Äã R œµ ‚Äã ( ùíÅ , ùíÅ ^ ) \\Delta R_{\\epsilon}(\\bm{Z},\\hat{\\bm{Z}}) roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z , over^ start_ARG bold_italic_Z end_ARG ) in ( 5.3.6 ) helps align the distributions while sample-wise self-consistency ( 5.3.7 ) and sample-wise augmentation ( 5.3.8 ) help align and compress features associated with each sample. Besides consistency, we also want learned representations are maximally discriminative for different samples (here v"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS2.SSS0.Px5", "title": "Unsupervised CTRL.", "snippet": "Unsupervised CTRL. Putting these elements together, we propose to learn a representation via the following constrained maximin program, which we refer to as unsupervised CTRL (u-CTRL): max Œ∏ ‚Å° min Œ∑ \\displaystyle\\max_{\\theta}\\min_{\\eta}\\quad roman_max start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT italic_Œ∑ end_POSTSUBSCRIPT R œµ ‚Äã ( ùíÅ ) + Œî ‚Äã R œµ ‚Äã ( ùíÅ , ùíÅ ^ ) \\displaystyle R_{\\epsilon}(\\bm{Z})+\\Delta R_{\\epsilon}(\\bm{Z},\\hat{\\bm{Z}}) italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ) + roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSU"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS2.SSS0.Px6", "title": "Unsupervised conditional image generation via rate reduction.", "snippet": "Unsupervised conditional image generation via rate reduction. The highly-structured feature distribution also suggests that the learned representation can be very useful for generative purposes. For example, we can organize the sample features into meaningful clusters, and model them with low-dimensional (Gaussian) distributions or subspaces. By sampling from these compact models, we can conditionally regenerate meaningful samples from computed clusters. This is known as unsupervised conditional image generation [ HKJ+21 ] . To cluster features, we exploit the fact that the rate reduction fram"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S4.SS0.SSS0.Px1", "title": "Shallow vs. deep neural networks, for autoencoding and more.", "snippet": "Shallow vs. deep neural networks, for autoencoding and more. In Section 5.1.2 , we discussed Cybenko‚Äôs universal approximation theorem and how it states that in principle, a neural network with a single hidden layer (and suitable elementwise nonlinearities) is sufficient to approximate any suitably regular target function. Of course, in practice, the major architectural reason for the dominance of neural networks in practice has been the refinement of techniques for training deeper neural networks. Why is depth necessary? From a fundamental point of view, the issue of depth separations, which "}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#p1", "title": "‚Äú Everything should be made as simple as possible, but not any simpler .‚Äù ‚Äì Albert Einstein", "snippet": "‚Äú Everything should be made as simple as possible, but not any simpler .‚Äù ‚Äì Albert Einstein"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#p2", "title": "In the past chapters, we have established a basic fact that the fundamental goal of learning is to learn a data distribution with low-dimensional supports and transform it to a compact and structured ", "snippet": "In the past chapters, we have established a basic fact that the fundamental goal of learning is to learn a data distribution with low-dimensional supports and transform it to a compact and structured representation. Such a representation reveals intrinsic low-dimensional structures of the data distribution and facilitates subsequent tasks such as classification and generation."}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#p3", "title": "A fundamental approach to learning a good representation of such a distribution is through compression . To make the goal of compression measurable and computable, it can be done explicitly by learnin", "snippet": "A fundamental approach to learning a good representation of such a distribution is through compression . To make the goal of compression measurable and computable, it can be done explicitly by learning a coding scheme that minimizes the coding rate (entropy) or maximizes the information gain (coding rate reduction). In this context, the fundamental role of a deep neural network is to realize a certain iterative optimization algorithm that incrementally optimizes the learned representations in terms of those measures: f : ùëø ‚Üí f 0 ùíÅ 0 ‚Üí ‚ãØ ‚Üí ùíÅ ‚Ñì ‚Üí f ‚Ñì ùíÅ ‚Ñì + 1 ‚Üí ‚ãØ ‚Üí ùíÅ L = ùíÅ . f\\colon\\bm{X}\\xrighta"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#p4", "title": "However, when we try to achieve a certain objective through optimization, there is no guarantee that the solution ùíÅ \\bm{Z} bold_italic_Z found in the end by incremental optimization is the correct sol", "snippet": "However, when we try to achieve a certain objective through optimization, there is no guarantee that the solution ùíÅ \\bm{Z} bold_italic_Z found in the end by incremental optimization is the correct solution. In fact, even if the optimization process manages to find the globally optimal solution ùíÅ ‚àó \\bm{Z}^{*} bold_italic_Z start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT , there is no guarantee that the solution corresponds to a complete representation of the data distribution. 1 1 1 This could be due to many reasons: for example, the data available for learning the distribution might not be suffici"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#p5", "title": "Of course, the only way to verify this is to see whether there exists a decoding map, say g g italic_g , that can decode the learned representation to reproduce the original data (distribution) well e", "snippet": "Of course, the only way to verify this is to see whether there exists a decoding map, say g g italic_g , that can decode the learned representation to reproduce the original data (distribution) well enough: ùëø ‚Üí ‚Ñ∞ = f ùíÅ ‚Üí ùíü = g ùëø ^ \\bm{X}\\xrightarrow{\\hskip 2.84526pt\\mathcal{E}=f\\hskip 2.84526pt}\\bm{Z}\\xrightarrow{\\hskip 2.84526pt\\mathcal{D}=g\\hskip 2.84526pt}\\hat{\\bm{X}} bold_italic_X start_ARROW start_OVERACCENT caligraphic_E = italic_f end_OVERACCENT ‚Üí end_ARROW bold_italic_Z start_ARROW start_OVERACCENT caligraphic_D = italic_g end_OVERACCENT ‚Üí end_ARROW over^ start_ARG bold_italic_X end_AR"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#p6", "title": "In many practical and natural learning scenarios, it can be difficult or even impossible to compare distributions of the data ùëø \\bm{X} bold_italic_X and ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X ", "snippet": "In many practical and natural learning scenarios, it can be difficult or even impossible to compare distributions of the data ùëø \\bm{X} bold_italic_X and ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG . We are left with the only option to compare in the learned feature ùíÅ \\bm{Z} bold_italic_Z with its image ùíÅ ^ \\hat{\\bm{Z}} over^ start_ARG bold_italic_Z end_ARG under the encoder f f italic_f : ùëø ‚Üí ‚Ñ∞ = f ùíÅ ‚Üí ùíü = g ùëø ^ ‚Üí ‚Ñ∞ = f ùíÅ ^ ‚Äã ? \\bm{X}\\xrightarrow{\\hskip 2.84526pt\\mathcal{E}=f\\hskip 2.84526pt}\\bm{Z}\\xrightarrow{\\hskip 2.84526pt\\mathcal{D}=g\\hskip 2.84526pt}\\hat{\\bm{X}}\\xrightarrow{\\h"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#p7", "title": "Furthermore, in many practical and natural learning scenarios, we normally do not have sufficient samples of the data distribution all at once. For example, animals and humans develop their visual mem", "snippet": "Furthermore, in many practical and natural learning scenarios, we normally do not have sufficient samples of the data distribution all at once. For example, animals and humans develop their visual memory through continuously taking in increments of observations all their life. In Section 5.3 , we will study how to extend the closed-loop transcription framework to learn a self-consistent representation in a continuous learning setting."}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#p8", "title": "Of course, a fundamental motivation why we ever want to identify the low-dimensional structures in a data distribution and find a good representation is to make it easy to use the data for various tas", "snippet": "Of course, a fundamental motivation why we ever want to identify the low-dimensional structures in a data distribution and find a good representation is to make it easy to use the data for various tasks of intelligence, such as classification, completion, and prediction. Therefore, the resulting joint representation ( ùíô , ùíõ ) (\\bm{x},\\bm{z}) ( bold_italic_x , bold_italic_z ) must be structured in such a way that is best suited for these tasks. In next chapter, we will see how the learned representation can be structured to facilitate conditioned completion or generation tasks."}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.p1", "title": "Here we give a formal definition of consistent representations, which are closely related to the concept of autoencoding.", "snippet": "Here we give a formal definition of consistent representations, which are closely related to the concept of autoencoding."}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#Thmdefinition1.p1", "title": "Given data ùëø \\bm{X} bold_italic_X , an consistent representation is a pair of functions ( f : ùí≥ ‚Üí ùíµ , g : ùíµ ‚Üí ùí≥ ) (f\\colon\\mathcal{X}\\to\\mathcal{Z},g\\colon\\mathcal{Z}\\to\\mathcal{X}) ( italic_f : calig", "snippet": "Given data ùëø \\bm{X} bold_italic_X , an consistent representation is a pair of functions ( f : ùí≥ ‚Üí ùíµ , g : ùíµ ‚Üí ùí≥ ) (f\\colon\\mathcal{X}\\to\\mathcal{Z},g\\colon\\mathcal{Z}\\to\\mathcal{X}) ( italic_f : caligraphic_X ‚Üí caligraphic_Z , italic_g : caligraphic_Z ‚Üí caligraphic_X ) , such that the features ùíÅ = f ‚Äã ( ùëø ) \\bm{Z}=f(\\bm{X}) bold_italic_Z = italic_f ( bold_italic_X ) are compact and structured, and the autoencoding ùëø ^ ‚âê g ‚Äã ( ùíÅ ) = g ‚Äã ( f ‚Äã ( ùëø ) ) \\hat{\\bm{X}}\\doteq g(\\bm{Z})=g(f(\\bm{X})) over^ start_ARG bold_italic_X end_ARG ‚âê italic_g ( bold_italic_Z ) = italic_g ( italic_f ( bold_italic_X"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.I1.i1.p1", "title": "We say that it is sample-wise consistent if ùëø ‚âà ùëø ^ \\bm{X}\\approx\\hat{\\bm{X}} bold_italic_X ‚âà over^ start_ARG bold_italic_X end_ARG in certain norm with high probability.", "snippet": "We say that it is sample-wise consistent if ùëø ‚âà ùëø ^ \\bm{X}\\approx\\hat{\\bm{X}} bold_italic_X ‚âà over^ start_ARG bold_italic_X end_ARG in certain norm with high probability."}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.I1.i2.p1", "title": "We say that the representation is distributionally consistent if Law ‚Å° ( ùëø ) ‚âà Law ‚Å° ( ùëø ^ ) \\operatorname{Law}(\\bm{X})\\approx\\operatorname{Law}(\\hat{\\bm{X}}) roman_Law ( bold_italic_X ) ‚âà roman_Law (", "snippet": "We say that the representation is distributionally consistent if Law ‚Å° ( ùëø ) ‚âà Law ‚Å° ( ùëø ^ ) \\operatorname{Law}(\\bm{X})\\approx\\operatorname{Law}(\\hat{\\bm{X}}) roman_Law ( bold_italic_X ) ‚âà roman_Law ( over^ start_ARG bold_italic_X end_ARG ) ."}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.p2", "title": "Acute readers may have noticed that if we do not impose certain requirements on the representation ùíÅ \\bm{Z} bold_italic_Z sought, the above problem has a trivial solution: One may simply choose the fu", "snippet": "Acute readers may have noticed that if we do not impose certain requirements on the representation ùíÅ \\bm{Z} bold_italic_Z sought, the above problem has a trivial solution: One may simply choose the functions f f italic_f and g g italic_g to be the identity map! Hence, the true purpose of seeking for an autoencoding is to try to ensure that so obtained ùíÅ \\bm{Z} bold_italic_Z is both more compact and more structured than ùëø \\bm{X} bold_italic_X . Firstly, for compactness, ùíÅ \\bm{Z} bold_italic_Z should better reveal the intrinsic low-dimensionality of ùëø \\bm{X} bold_italic_X . Therefore, the repres"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.p3", "title": "From the definition of consistent representation, it requires that the representation ùíÅ \\bm{Z} bold_italic_Z is sufficient to recover the original data distribution ùëø \\bm{X} bold_italic_X to some degr", "snippet": "From the definition of consistent representation, it requires that the representation ùíÅ \\bm{Z} bold_italic_Z is sufficient to recover the original data distribution ùëø \\bm{X} bold_italic_X to some degree of accuracy. For sample-wise consistency, a typical choice is to minimize the expected reconstruction error: d ‚Äã ( ùëø , ùëø ^ ) = ùîº ‚Äã [ ‚Äñ ùëø ‚àí ùëø ^ ‚Äñ 2 2 ] . d(\\bm{X},\\hat{\\bm{X}})=\\mathbb{E}[\\|\\bm{X}-\\hat{\\bm{X}}\\|_{2}^{2}]. italic_d ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG ) = blackboard_E [ ‚à• bold_italic_X - over^ start_ARG bold_italic_X end_ARG ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBS"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.p4", "title": "Hence, computation aside, when we seek a good autoencoding for a data distribution ùëø \\bm{X} bold_italic_X , conceptually we try to find an encoder f f italic_f and decoder g g italic_g such that min f", "snippet": "Hence, computation aside, when we seek a good autoencoding for a data distribution ùëø \\bm{X} bold_italic_X , conceptually we try to find an encoder f f italic_f and decoder g g italic_g such that min f , g ‚Å° [ ‚àí Œî ‚Äã R œµ ‚Äã ( ùíÅ ) + d ‚Äã ( ùëø , ùëø ^ ) ] . \\min_{f,g}[-\\Delta R_{\\epsilon}(\\bm{Z})+d(\\bm{X},\\hat{\\bm{X}})]. roman_min start_POSTSUBSCRIPT italic_f , italic_g end_POSTSUBSCRIPT [ - roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ) + italic_d ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG ) ] . (5.1.5) For the rest of this chapter, we will study how to "}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.SS1.p1", "title": "According to [ Bal11 ] , the phrase ‚Äúautoencoder‚Äù was first introduced by Hinton and Rumelhart [ RHW86 ] so that a deep representation can be learned via back propagation (BP) in a self-supervision fa", "snippet": "According to [ Bal11 ] , the phrase ‚Äúautoencoder‚Äù was first introduced by Hinton and Rumelhart [ RHW86 ] so that a deep representation can be learned via back propagation (BP) in a self-supervision fashion‚Äîreconstructing the original data is the self-supervising task. However, the very same concept of seeking a compact and consistent representation has been rooted in many classic studies. As we have already seen in Chapter 2 , the classical PCA, ICA, and sparse dictionary learning all share a similar goal. The only difference is when the underlying data distribution is simple (linear and indep"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.SS1.p2", "title": "It is instructive to see how the notion of consistency we have defined plays out in the simple case of PCA: here, the consistent encoding and decoding mappings are given by a single-layer linear trans", "snippet": "It is instructive to see how the notion of consistency we have defined plays out in the simple case of PCA: here, the consistent encoding and decoding mappings are given by a single-layer linear transform: ùëø ‚Üí ‚Ñ∞ = ùëº ‚ä§ ùíÅ ‚Üí ùíü = ùëº ùëø ^ , \\bm{X}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}=\\bm{U}^{\\top}\\hskip 5.69054pt}\\bm{Z}\\xrightarrow{\\hskip 5.69054pt\\mathcal{D}=\\bm{U}\\hskip 5.69054pt}\\hat{\\bm{X}}, bold_italic_X start_ARROW start_OVERACCENT caligraphic_E = bold_italic_U start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT end_OVERACCENT ‚Üí end_ARROW bold_italic_Z start_ARROW start_OVERACCENT caligraphic_D = bo"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.SS1.p3", "title": "As we saw in Chapter 2 , when the distribution of ùíô \\bm{x} bold_italic_x is indeed supported on a low-dimensional subspace ùëº o \\bm{U}_{o} bold_italic_U start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT ,", "snippet": "As we saw in Chapter 2 , when the distribution of ùíô \\bm{x} bold_italic_x is indeed supported on a low-dimensional subspace ùëº o \\bm{U}_{o} bold_italic_U start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT , the compactness of the representation ùíõ \\bm{z} bold_italic_z produced by ‚Ñ∞ \\mathcal{E} caligraphic_E is a direct consequence of correctly estimating (and enforcing) the dimension of this subspace. Finally, recall that gradient descent on the reconstruction criterion exactly yields these sample-wise consistent mappings: indeed, the optimal solution to the problem min ùëº ‚ä§ ‚Äã ùëº = ùë∞ ‚Å° ùîº ùíô ‚Äã [ ‚Äñ ùíô ‚àí ùëº "}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.SS1.SSS0.Px1.p1", "title": "Notice that in the above construction, the linear transform ùëº \\bm{U} bold_italic_U used for the encoding and decoding is computed ‚Äúoffline‚Äù from all the input data before hand. One question is whether", "snippet": "Notice that in the above construction, the linear transform ùëº \\bm{U} bold_italic_U used for the encoding and decoding is computed ‚Äúoffline‚Äù from all the input data before hand. One question is whether this transform can be learned ‚Äúonline‚Äù as the input data come in order? This question was answered by the work of Oja in 1982 [ Oja82 ] ."}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#Thmexample1.p1", "title": "Consider a sequence of i.i.d. random vectors ùíô 1 , ‚Ä¶ , ùíô i , ‚Ä¶ ‚àà ‚Ñù n \\bm{x}_{1},\\ldots,\\bm{x}_{i},\\ldots\\in\\mathbb{R}^{n} bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_x star", "snippet": "Consider a sequence of i.i.d. random vectors ùíô 1 , ‚Ä¶ , ùíô i , ‚Ä¶ ‚àà ‚Ñù n \\bm{x}_{1},\\ldots,\\bm{x}_{i},\\ldots\\in\\mathbb{R}^{n} bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , ‚Ä¶ ‚àà blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT with covariance ùö∫ ‚àà ‚Ñù n √ó n \\bm{\\Sigma}\\in\\mathbb{R}^{n\\times n} bold_Œ£ ‚àà blackboard_R start_POSTSUPERSCRIPT italic_n √ó italic_n end_POSTSUPERSCRIPT . Let ùíñ 0 ‚àà ‚Ñù n \\bm{u}_{0}\\in\\mathbb{R}^{n} bold_italic_u start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT ita"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.SS1.SSS0.Px1.p2", "title": "The normalized Hebbian scheme ( 5.1.9 ) can be interpreted as a first-order approximation to a stochastic projected gradient descent scheme on the objective of the problem ( 5.1.7 ) (with batch size 1", "snippet": "The normalized Hebbian scheme ( 5.1.9 ) can be interpreted as a first-order approximation to a stochastic projected gradient descent scheme on the objective of the problem ( 5.1.7 ) (with batch size 1 1 1 , and with the number of columns of ùëº \\bm{U} bold_italic_U equal to 1 1 1 ) as long as ‚Äñ ùíñ ‚Äñ 2 = 1 \\|\\bm{u}\\|_{2}=1 ‚à• bold_italic_u ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 1 , which is maintained by the projection operation in ( 5.1.9 ). It is worth keeping its existence in the back of one‚Äôs mind, both as a proof of correctness for the use of stochastic gradient methods for optimizing rec"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.SS2.p1", "title": "Of course, one should expect that things will no longer be so simple when we deal with more complex distributions whose underlying low-dimensional structure could be nonlinear.", "snippet": "Of course, one should expect that things will no longer be so simple when we deal with more complex distributions whose underlying low-dimensional structure could be nonlinear."}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.SS2.SSS0.Px1.p1", "title": "So, to move beyond the linear structure addressed by PCA, we may assume that the data distribution lies on a (smooth) submanifold ‚Ñ≥ \\mathcal{M} caligraphic_M . The intrinsic dimension of the submanifo", "snippet": "So, to move beyond the linear structure addressed by PCA, we may assume that the data distribution lies on a (smooth) submanifold ‚Ñ≥ \\mathcal{M} caligraphic_M . The intrinsic dimension of the submanifold, say d d italic_d , is typically much lower than the dimension of the ambient space ‚Ñù D \\mathbb{R}^{D} blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT . From this geometric perspective, we typically want to find a nonlinear mapping f f italic_f such that the resulting manifold f ‚Äã ( ‚Ñ≥ ) f(\\mathcal{M}) italic_f ( caligraphic_M ) is flattened, as illustrated by the example shown i"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.SS2.SSS0.Px2.p1", "title": "As we have seen above, in the case of PCA, a one-layer linear neural network is sufficient. That is no longer the case for NLPCA. In 1991, Kramer [ Kra91 ] proposed to solve NLPCA by using a two-layer", "snippet": "As we have seen above, in the case of PCA, a one-layer linear neural network is sufficient. That is no longer the case for NLPCA. In 1991, Kramer [ Kra91 ] proposed to solve NLPCA by using a two-layer neural network to represent the encoder mapping f f italic_f (or its inverse g g italic_g ) based on the universal representation property of two-layer networks with sigmoid activation: ùíõ = ùëæ 2 ‚Äã œÉ ‚Äã ( ùëæ 1 ‚Äã ùíô + ùíÉ ) , \\bm{z}=\\bm{W}_{2}\\sigma(\\bm{W}_{1}\\bm{x}+\\bm{b}), bold_italic_z = bold_italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_œÉ ( bold_italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCR"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.SS2.SSS0.Px2.p2", "title": "Unfortunately, unlike the above case of PCA, there is in general no closed-form learning scheme for the parameters ùúΩ = ( ùëæ , ùíÉ ) \\bm{\\theta}=(\\bm{W},\\bm{b}) bold_italic_Œ∏ = ( bold_italic_W , bold_ital", "snippet": "Unfortunately, unlike the above case of PCA, there is in general no closed-form learning scheme for the parameters ùúΩ = ( ùëæ , ùíÉ ) \\bm{\\theta}=(\\bm{W},\\bm{b}) bold_italic_Œ∏ = ( bold_italic_W , bold_italic_b ) of these networks. Hence it was proposed to train the network via back propagation with the supervision of reconstruction error: min ùúΩ ‚Å° ùîº ‚Äã [ ‚Äñ ùíô ‚àí ùíô ^ ‚Äã ( ùúΩ ) ‚Äñ 2 2 ] . \\min_{\\bm{\\theta}}\\mathbb{E}[\\|\\bm{x}-\\hat{\\bm{x}}(\\bm{\\theta})\\|_{2}^{2}]. roman_min start_POSTSUBSCRIPT bold_italic_Œ∏ end_POSTSUBSCRIPT blackboard_E [ ‚à• bold_italic_x - over^ start_ARG bold_italic_x end_ARG ( bold_italic"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.SS2.SSS0.Px3.p1", "title": "Based on the modern practice of deep networks, such classical shallow and wide network architectures are known to be rather difficult to train effectively and efficiently via back propagation (BP), pa", "snippet": "Based on the modern practice of deep networks, such classical shallow and wide network architectures are known to be rather difficult to train effectively and efficiently via back propagation (BP), partly due to the diminishing gradient of the sigmoid function. Hence, the modern practice normally suggests to further decompose the nonlinear transform f f italic_f (or g g italic_g ) into a composition of many more layers of simpler transforms, resulting a deeper network architecture [ HS06 ] , as illustrated in Figure 5.4 . In the modern context, further elaborations over the basic reconstructio"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.SS2.SSS0.Px3.p2", "title": "In light of universal approximation theorems such as Cybenko‚Äôs, one may initially wonder why, conceptually, deeper autoencoders should be preferred to shallow ones. From purely an expressivity perspec", "snippet": "In light of universal approximation theorems such as Cybenko‚Äôs, one may initially wonder why, conceptually, deeper autoencoders should be preferred to shallow ones. From purely an expressivity perspective, we can understand this phenomenon through a geometric angle related to the task of flattening the nonlinear manifold on which our hypothesized data distribution is supported. A purely constructive approach to flattening the manifold proceeds incrementally, in parallel to what we have seen in Chapters 3 and 4 with the interaction between diffusion, denoising, and compression. In the geometric"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.SS3.p1", "title": "In the above autoencoding schemes, the dimension of the feature space d d italic_d is typically chosen to be much lower than that the original data space D D italic_D so as to explicitly enforce or pr", "snippet": "In the above autoencoding schemes, the dimension of the feature space d d italic_d is typically chosen to be much lower than that the original data space D D italic_D so as to explicitly enforce or promote the learned representation to be low-dimensional. However, in practice, we normally do not know the intrinsic dimension of the data distribution. Hence, the choice of the feature space dimension for autoencoding is often done empirically. In more general situations, the data distribution can be a mixture of a few low-dimensional subspaces or submanifolds. In these cases, it is no longer feas"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.SS3.p2", "title": "The sparse autoencoder is meant to resolve some of these limitations. In particular, the dimension of the feature space can be equal to or even higher than that of the data space, as illustrated in Fi", "snippet": "The sparse autoencoder is meant to resolve some of these limitations. In particular, the dimension of the feature space can be equal to or even higher than that of the data space, as illustrated in Figure 5.5 . However, the features are required to be highly sparse in the feature space. So if we impose sparsity as the measure of parsimony in addition to the rate reduction in the objective ( 5.1.5 ), we obtain a new objective for the sparse autoencoding: min f , g ‚Å° [ ‚Äñ ùíÅ ‚Äñ 0 ‚àí Œî ‚Äã R œµ ‚Äã ( ùíÅ ) + d ‚Äã ( ùëø , ùëø ^ ) ] , \\min_{f,g}[\\|\\bm{Z}\\|_{0}-\\Delta R_{\\epsilon}(\\bm{Z})+d(\\bm{X},\\hat{\\bm{X}})], r"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.SS3.p3", "title": "As a method for learning autoencoding pairs in an end-to-end fashion, sparse autoencoding has been practiced in the past [ RPC+06 , LRM+12 ] , but nearly all modern autoencoding frameworks are instead", "snippet": "As a method for learning autoencoding pairs in an end-to-end fashion, sparse autoencoding has been practiced in the past [ RPC+06 , LRM+12 ] , but nearly all modern autoencoding frameworks are instead based on a different, probabilistic autoencoding framework, which we will study now."}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.SS4.p1", "title": "In the classical conception of autoencoding, following Hinton and Rumelhart [ RHW86 ] , the data distribution plays a very minor role in the formulation, in spite of its centrality to the representati", "snippet": "In the classical conception of autoencoding, following Hinton and Rumelhart [ RHW86 ] , the data distribution plays a very minor role in the formulation, in spite of its centrality to the representation we ultimately learn. Indeed, in the naive framework, one hopes that by training a deep network to reconstruct samples from the data distribution with a suitably-configured bottleneck for the representation ùíõ \\bm{z} bold_italic_z , the learned encoders f f italic_f and g g italic_g will naturally end up corresponding to a compact and structured feature representation for the data. This is rarely"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.SS4.SSSx1.p1", "title": "In the manifold model for the data distribution, the key mathematical objects are the support of the data distribution, namely the manifold ‚Ñ≥ \\mathcal{M} caligraphic_M , and the density of the data on", "snippet": "In the manifold model for the data distribution, the key mathematical objects are the support of the data distribution, namely the manifold ‚Ñ≥ \\mathcal{M} caligraphic_M , and the density of the data on the support, say p p italic_p . When we formulate autoencoding from the probabilistic perspective, we often think of the high-dimensional input ùíô \\bm{x} bold_italic_x as having a density p p italic_p with support on ‚Ñù D \\mathbb{R}^{D} blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT ; one can think of adding a very small amount of noise to the (degenerate) distribution supported on"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.SS4.SSSx1.p2", "title": "In the variational autoencoding framework, we navigate this tradeoff through three key insights: 1. We posit simple distributions for ùíõ \\bm{z} bold_italic_z and ùíô \\bm{x} bold_italic_x conditional on ùíõ", "snippet": "In the variational autoencoding framework, we navigate this tradeoff through three key insights: 1. We posit simple distributions for ùíõ \\bm{z} bold_italic_z and ùíô \\bm{x} bold_italic_x conditional on ùíõ \\bm{z} bold_italic_z , but make their parameters depend in a highly flexible way on the input data ùíô \\bm{x} bold_italic_x (where relevant) using deep networks. 2. We replace the posterior p ‚Äã ( ùíõ ‚à£ ùíô ; ùúΩ ) p(\\bm{z}\\mid\\bm{x};\\,\\bm{\\theta}) italic_p ( bold_italic_z ‚à£ bold_italic_x ; bold_italic_Œ∏ ) , used for encoding and whose form is implied (by Bayes rule) by our modeling choices for ùíõ \\bm{z} b"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.I2.i1.p1", "title": "We posit simple distributions for ùíõ \\bm{z} bold_italic_z and ùíô \\bm{x} bold_italic_x conditional on ùíõ \\bm{z} bold_italic_z , but make their parameters depend in a highly flexible way on the input data ", "snippet": "We posit simple distributions for ùíõ \\bm{z} bold_italic_z and ùíô \\bm{x} bold_italic_x conditional on ùíõ \\bm{z} bold_italic_z , but make their parameters depend in a highly flexible way on the input data ùíô \\bm{x} bold_italic_x (where relevant) using deep networks."}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.I2.i2.p1", "title": "We replace the posterior p ‚Äã ( ùíõ ‚à£ ùíô ; ùúΩ ) p(\\bm{z}\\mid\\bm{x};\\,\\bm{\\theta}) italic_p ( bold_italic_z ‚à£ bold_italic_x ; bold_italic_Œ∏ ) , used for encoding and whose form is implied (by Bayes rule) by", "snippet": "We replace the posterior p ‚Äã ( ùíõ ‚à£ ùíô ; ùúΩ ) p(\\bm{z}\\mid\\bm{x};\\,\\bm{\\theta}) italic_p ( bold_italic_z ‚à£ bold_italic_x ; bold_italic_Œ∏ ) , used for encoding and whose form is implied (by Bayes rule) by our modeling choices for ùíõ \\bm{z} bold_italic_z , with a tractable approximation q ‚Äã ( ùíõ ‚à£ ùíô ; ùúº ) q(\\bm{z}\\mid\\bm{x};\\,\\bm{\\eta}) italic_q ( bold_italic_z ‚à£ bold_italic_x ; bold_italic_Œ∑ ) , which has its own parameters ùúº \\bm{\\eta} bold_italic_Œ∑ ."}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.I2.i3.p1", "title": "We jointly learn ùúΩ \\bm{\\theta} bold_italic_Œ∏ and ùúº \\bm{\\eta} bold_italic_Œ∑ via maximizing a tractable lower bound for the likelihood p ‚Äã ( ùíô ; ùúΩ ) p(\\bm{x};\\,\\bm{\\theta}) italic_p ( bold_italic_x ; bo", "snippet": "We jointly learn ùúΩ \\bm{\\theta} bold_italic_Œ∏ and ùúº \\bm{\\eta} bold_italic_Œ∑ via maximizing a tractable lower bound for the likelihood p ‚Äã ( ùíô ; ùúΩ ) p(\\bm{x};\\,\\bm{\\theta}) italic_p ( bold_italic_x ; bold_italic_Œ∏ ) , known as the evidence lower bound (ELBO)."}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.SS4.SSSx2.p1", "title": "There is a general methodology for maximizing the ELBO objective in Equation 5.1.14 using stochastic gradient descent and various tractable Monte Carlo estimators for the associated gradients. However", "snippet": "There is a general methodology for maximizing the ELBO objective in Equation 5.1.14 using stochastic gradient descent and various tractable Monte Carlo estimators for the associated gradients. However, the task is simpler under the Gaussian assumptions we have made above. In this case, the ELBO reads max ùúΩ , ùúº ‚Å° ùîº ùíô ‚Äã ùîº ùíõ ‚àº q ( ‚ãÖ ‚à£ ùíô ; ùúº ) ‚Äã [ log ‚Å° p ‚Äã ( ùíô , ùíõ ; ùúΩ ) q ‚Äã ( ùíõ ‚à£ ùíô ; ùúº ) ] \\displaystyle\\max_{\\bm{\\theta},\\bm{\\eta}}\\,\\mathbb{E}_{\\bm{x}}\\mathbb{E}_{\\bm{z}\\sim q(\\,\\cdot\\,\\mid\\bm{x};\\,\\bm{\\eta})}\\left[\\log\\frac{p(\\bm{x},\\bm{z};\\,\\bm{\\theta})}{q(\\bm{z}\\mid\\bm{x};\\,\\bm{\\eta})}\\right] ro"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.SS4.SSSx3.p1", "title": "VAEs are typically trained by alternating stochastic gradient ascent on the ELBO objective ( Equation 5.1.15 ), given individual samples ùíô \\bm{x} bold_italic_x from the true data distribution and from", "snippet": "VAEs are typically trained by alternating stochastic gradient ascent on the ELBO objective ( Equation 5.1.15 ), given individual samples ùíô \\bm{x} bold_italic_x from the true data distribution and from ùíõ ‚àº q ( ‚ãÖ ‚à£ ùíô ; ùúº ) \\bm{z}\\sim q(\\,\\cdot\\,\\mid\\bm{x};\\,\\bm{\\eta}) bold_italic_z ‚àº italic_q ( ‚ãÖ ‚à£ bold_italic_x ; bold_italic_Œ∑ ) . In particular, it is standard to collect and train on many independently-generated samples ùíõ i \\bm{z}^{i} bold_italic_z start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT for each sample ùíô \\bm{x} bold_italic_x . To take gradients of Equation 5.1.15 with respect to the"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.p1", "title": "In earlier chapters, we have studied methods that would allow us to learn a low-dimensional distribution via (lossy) compression. As we have mentioned in Chapter 1 and demonstrated in the previous cha", "snippet": "In earlier chapters, we have studied methods that would allow us to learn a low-dimensional distribution via (lossy) compression. As we have mentioned in Chapter 1 and demonstrated in the previous chapters, the progresses made in machine intelligence largely rely on finding computationally feasible and efficient solutions to realize the desired compression, not only computable or tractable in theory, but also scalable in practice: computable ‚üπ tractable ‚üπ scalable . \\mbox{{computable}}\\;\\Longrightarrow\\;\\mbox{{tractable}}\\;\\Longrightarrow\\;\\mbox{{scalable}}. computable ‚üπ tractable ‚üπ scalable ."}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.p2", "title": "While we celebrate the engineering marvels of such large man-made machine learning systems, we also must admit that, compared to intelligence in nature, this approach to improve machine intelligence i", "snippet": "While we celebrate the engineering marvels of such large man-made machine learning systems, we also must admit that, compared to intelligence in nature, this approach to improve machine intelligence is unnecessarily resource demanding. Natural intelligent beings, including animals and humans, simply cannot afford such a brute force solution to learn because they must operate with a very limited budget in energy, space and time, subject to many strict physical constraints."}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.p3", "title": "Firstly, there is strong scientific evidence that our brain does not conduct global end-to-end back propagation to improve or correct its predictions. Instead, it was long known in neuroscience that o", "snippet": "Firstly, there is strong scientific evidence that our brain does not conduct global end-to-end back propagation to improve or correct its predictions. Instead, it was long known in neuroscience that our brain corrects errors with local closed-loop feedback, such as predictive coding. This was the scientific basis that had inspired Norbert Wiener to develop the theory of feedback control and the Cybernetics program back in the 1940s."}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.p4", "title": "Secondly, we saw in the previous sections that in order to learn a consistent representation, one needs to learn a bi-directional autoencoding: ùëø ‚Üí ‚Ñ∞ = f ùíÅ ‚Üí ùíü = g ùëø ^ . \\bm{X}\\xrightarrow{\\hskip 2.84", "snippet": "Secondly, we saw in the previous sections that in order to learn a consistent representation, one needs to learn a bi-directional autoencoding: ùëø ‚Üí ‚Ñ∞ = f ùíÅ ‚Üí ùíü = g ùëø ^ . \\bm{X}\\xrightarrow{\\hskip 2.84526pt\\mathcal{E}=f\\hskip 2.84526pt}\\bm{Z}\\xrightarrow{\\hskip 2.84526pt\\mathcal{D}=g\\hskip 2.84526pt}\\hat{\\bm{X}}. bold_italic_X start_ARROW start_OVERACCENT caligraphic_E = italic_f end_OVERACCENT ‚Üí end_ARROW bold_italic_Z start_ARROW start_OVERACCENT caligraphic_D = italic_g end_OVERACCENT ‚Üí end_ARROW over^ start_ARG bold_italic_X end_ARG . (5.2.2) It requires to enforce the observed input data ùëø"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.p5", "title": "As we know from the previous chapter, in order to ensure a representation is consistent, we need to compare the generated ùëø ^ ‚àº p ‚Äã ( ùíô ^ ) \\hat{\\bm{X}}\\sim p(\\hat{\\bm{x}}) over^ start_ARG bold_italic", "snippet": "As we know from the previous chapter, in order to ensure a representation is consistent, we need to compare the generated ùëø ^ ‚àº p ‚Äã ( ùíô ^ ) \\hat{\\bm{X}}\\sim p(\\hat{\\bm{x}}) over^ start_ARG bold_italic_X end_ARG ‚àº italic_p ( over^ start_ARG bold_italic_x end_ARG ) and the original ùëø ‚àº p ‚Äã ( ùíô ) \\bm{X}\\sim p(\\bm{x}) bold_italic_X ‚àº italic_p ( bold_italic_x ) , at least in distribution. Even when we do have access to ùëø \\bm{X} bold_italic_X and ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG , technically, computing and minimizing distance of two distributions can be problematic, especially"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.p6", "title": "As an early attempt to alleviate the above difficulty in computing and minimizing the distance between two (low-dimensional) distributions, people had suggested to learn the generator/decoder g g ital", "snippet": "As an early attempt to alleviate the above difficulty in computing and minimizing the distance between two (low-dimensional) distributions, people had suggested to learn the generator/decoder g g italic_g via discriminative approaches [ Tu07 ] . This line of thought has led to the idea of Generative Adversarial Nets (GAN) [ GPM+14a ] . It introduces a discriminator d d italic_d , usually modeled by a deep network, to discern differences between the generated samples ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG and the real ones ùëø \\bm{X} bold_italic_X : ùíÅ ‚Üí g ‚Äã ( ùíõ , Œ∑ ) ùëø ^ , ùëø ‚Üí d ‚Äã"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.p7", "title": "One may show that finding an equilibrium for the above Stackelberg game is equivalent to minimizing the Jensen-Shannon divergence : ùíü J ‚Äã S ‚Äã ( p ‚Äã ( ùíô ) , p g ‚Äã ( ùíô ^ ) ) = ùíü K ‚Äã L ‚Äã ( p ‚à• ( p + p g ", "snippet": "One may show that finding an equilibrium for the above Stackelberg game is equivalent to minimizing the Jensen-Shannon divergence : ùíü J ‚Äã S ‚Äã ( p ‚Äã ( ùíô ) , p g ‚Äã ( ùíô ^ ) ) = ùíü K ‚Äã L ‚Äã ( p ‚à• ( p + p g ) / 2 ) + ùíü K ‚Äã L ‚Äã ( p g ‚à• ( p + p g ) / 2 ) . \\mathcal{D}_{JS}(p(\\bm{x}),p_{g}(\\hat{\\bm{x}}))=\\mathcal{D}_{KL}\\big{(}p\\|(p+p_{g})/{2}\\big{)}+\\mathcal{D}_{KL}\\big{(}p_{g}\\|(p+p_{g})/{2}\\big{)}. caligraphic_D start_POSTSUBSCRIPT italic_J italic_S end_POSTSUBSCRIPT ( italic_p ( bold_italic_x ) , italic_p start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ( over^ start_ARG bold_italic_x end_ARG ) ) = ca"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.p8", "title": "So, instead, it has also been suggested to replace JS-divergence with the earth mover (EM) distance or the Wasserstein distance. 7 7 7 Roughly speaking, for distributions with potentially non-overlapp", "snippet": "So, instead, it has also been suggested to replace JS-divergence with the earth mover (EM) distance or the Wasserstein distance. 7 7 7 Roughly speaking, for distributions with potentially non-overlapping low-dimensional supports, the JS-divergence behaves like the ‚Ñì 0 \\ell^{0} roman_‚Ñì start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT -norm, and the EM-distance behaves like the ‚Ñì 1 \\ell^{1} roman_‚Ñì start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT -norm. However, both the JS-divergence and Wasserstein distance can only be approximately computed between two general distributions. 8 8 8 For instance, the Was"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.p9", "title": "If it is difficult to compare distributions of the data ùëø \\bm{X} bold_italic_X and ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG , would it possible to compare in the learned feature ùíÅ \\bm{Z}", "snippet": "If it is difficult to compare distributions of the data ùëø \\bm{X} bold_italic_X and ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG , would it possible to compare in the learned feature ùíÅ \\bm{Z} bold_italic_Z with its image ùíÅ ^ \\hat{\\bm{Z}} over^ start_ARG bold_italic_Z end_ARG under the encoder f f italic_f : ùëø ‚Üí ‚Ñ∞ = f ùíÅ ‚Üí ùíü = g ùëø ^ ‚Üí ‚Ñ∞ = f ùíÅ ^ ‚Äã ? \\bm{X}\\xrightarrow{\\hskip 2.84526pt\\mathcal{E}=f\\hskip 2.84526pt}\\bm{Z}\\xrightarrow{\\hskip 2.84526pt\\mathcal{D}=g\\hskip 2.84526pt}\\hat{\\bm{X}}\\xrightarrow{\\hskip 2.84526pt\\mathcal{E}=f\\hskip 2.84526pt}\\hat{\\bm{Z}}? bold_italic_X start_ARROW s"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#Thmdefinition2.p1", "title": "Given data ùëø \\bm{X} bold_italic_X , we call an self-consistent representation to be a pair of functions ( f : ùí≥ ‚Üí ùíµ , g : ùíµ ‚Üí ùí≥ ) (f\\colon\\mathcal{X}\\to\\mathcal{Z},g\\colon\\mathcal{Z}\\to\\mathcal{X}) ( ", "snippet": "Given data ùëø \\bm{X} bold_italic_X , we call an self-consistent representation to be a pair of functions ( f : ùí≥ ‚Üí ùíµ , g : ùíµ ‚Üí ùí≥ ) (f\\colon\\mathcal{X}\\to\\mathcal{Z},g\\colon\\mathcal{Z}\\to\\mathcal{X}) ( italic_f : caligraphic_X ‚Üí caligraphic_Z , italic_g : caligraphic_Z ‚Üí caligraphic_X ) , such that the features ùíÅ = f ‚Äã ( ùëø ) \\bm{Z}=f(\\bm{X}) bold_italic_Z = italic_f ( bold_italic_X ) are compact and structured, and the autoencoding features ùíÅ ^ ‚âê f ‚àò g ‚Äã ( ùíÅ ) \\hat{\\bm{Z}}\\doteq f\\circ g(\\bm{Z}) over^ start_ARG bold_italic_Z end_ARG ‚âê italic_f ‚àò italic_g ( bold_italic_Z ) is close to ùíÅ \\bm{Z} bo"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.I1.i1.p1", "title": "We say that it is sample-wise self-consistent if ùíÅ ‚âà ùíÅ ^ \\bm{Z}\\approx\\hat{\\bm{Z}} bold_italic_Z ‚âà over^ start_ARG bold_italic_Z end_ARG in a certain norm with high probability.", "snippet": "We say that it is sample-wise self-consistent if ùíÅ ‚âà ùíÅ ^ \\bm{Z}\\approx\\hat{\\bm{Z}} bold_italic_Z ‚âà over^ start_ARG bold_italic_Z end_ARG in a certain norm with high probability."}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.I1.i2.p1", "title": "We say that the representation is distributionally self-consistent if Law ‚Å° ( ùíÅ ) ‚âà Law ‚Å° ( ùíÅ ^ ) \\operatorname{Law}(\\bm{Z})\\approx\\operatorname{Law}(\\hat{\\bm{Z}}) roman_Law ( bold_italic_Z ) ‚âà roman_", "snippet": "We say that the representation is distributionally self-consistent if Law ‚Å° ( ùíÅ ) ‚âà Law ‚Å° ( ùíÅ ^ ) \\operatorname{Law}(\\bm{Z})\\approx\\operatorname{Law}(\\hat{\\bm{Z}}) roman_Law ( bold_italic_Z ) ‚âà roman_Law ( over^ start_ARG bold_italic_Z end_ARG ) ."}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.SS1.p1", "title": "How do we try to ensure a learned representation is self-consistent? As usual, let us assume ùëø = ‚à™ k = 1 K ùëø k \\bm{X}=\\cup_{k=1}^{K}\\bm{X}_{k} bold_italic_X = ‚à™ start_POSTSUBSCRIPT italic_k = 1 end_PO", "snippet": "How do we try to ensure a learned representation is self-consistent? As usual, let us assume ùëø = ‚à™ k = 1 K ùëø k \\bm{X}=\\cup_{k=1}^{K}\\bm{X}_{k} bold_italic_X = ‚à™ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT with each subset of samples ùëø k \\bm{X}_{k} bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT belonging to a low-dimensional submanifold: ùëø k ‚äÇ ‚Ñ≥ k , k = 1 , ‚Ä¶ , K \\bm{X}_{k}\\subset\\mathcal{M}_{k},k=1,\\ldots,K bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSC"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.SS1.p2", "title": "One issue with learning such a one-sided mapping ( 5.2.7 ) via maximizing the above objective ( 5.2.8 ) is that it tends to expand the dimension of the learned subspace for features in each class 10 1", "snippet": "One issue with learning such a one-sided mapping ( 5.2.7 ) via maximizing the above objective ( 5.2.8 ) is that it tends to expand the dimension of the learned subspace for features in each class 10 10 10 if the dimension of the feature space d d italic_d is too high, maximizing the rate reduction may over-estimate the dimension of each class. Hence, to learn a good representation, one needs to pre-select a proper dimension for the feature space, as achieved in the experiments in [ YCY+20 ] . In fact the same ‚Äúmodel selection‚Äù problem persists even in the simplest single-subspace case, which i"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.SS1.SSS0.Px1.p1", "title": "However, as we have discussed above, if we do not have the option to compute the distance between ùëø \\bm{X} bold_italic_X and ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG , we are left with t", "snippet": "However, as we have discussed above, if we do not have the option to compute the distance between ùëø \\bm{X} bold_italic_X and ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG , we are left with the option of comparing their corresponding features ùíÅ \\bm{Z} bold_italic_Z and ùíÅ ^ = f ‚Äã ( ùëø ^ , ùúΩ ) \\hat{\\bm{Z}}=f(\\hat{\\bm{X}},\\bm{\\theta}) over^ start_ARG bold_italic_Z end_ARG = italic_f ( over^ start_ARG bold_italic_X end_ARG , bold_italic_Œ∏ ) . Notice that under the MCR 2 objective, the distributions of the resulting ùíÅ \\bm{Z} bold_italic_Z or ùíÅ ^ \\hat{\\bm{Z}} over^ start_ARG bold_italic_Z en"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.SS1.SSS0.Px1.p2", "title": "If we are interested in discerning any differences in the distributions of the original data ùëø \\bm{X} bold_italic_X and their decoded ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG , we may vi", "snippet": "If we are interested in discerning any differences in the distributions of the original data ùëø \\bm{X} bold_italic_X and their decoded ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG , we may view the feature encoder f ‚Äã ( ‚ãÖ , ùúΩ ) f(\\cdot,\\bm{\\theta}) italic_f ( ‚ãÖ , bold_italic_Œ∏ ) as a ‚Äúdiscriminator‚Äù to magnify any difference between all pairs of ùëø k \\bm{X}_{k} bold_italic_X start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT and ùëø ^ k \\hat{\\bm{X}}_{k} over^ start_ARG bold_italic_X end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , by simply maximizing the distance d ‚Äã ( ùëø , ùëø ^ ) d(\\"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.SS1.SSS0.Px1.p3", "title": "Nevertheless, here the discriminative encoder f f italic_f naturally generalizes to cases when the data distributions are multi-class and multi-modal, and the discriminativeness is measured with a mor", "snippet": "Nevertheless, here the discriminative encoder f f italic_f naturally generalizes to cases when the data distributions are multi-class and multi-modal, and the discriminativeness is measured with a more refined measure‚Äîthe rate reduction‚Äîinstead of the typical two-class loss (e.g., cross entropy) used in GANs. That is, we may view the encoder f f italic_f as a generalized discriminator that replaces the binary classifier d d italic_d in ( 5.2.3 ): ùíÅ ‚Üí g ‚Äã ( ùíõ , ùúº ) ùëø ^ , ùëø ‚Üí f ‚Äã ( ùíô , ùúΩ ) { ùíÅ ^ , ùíÅ } . \\bm{Z}\\xrightarrow{\\hskip 5.69054ptg(\\bm{z},\\bm{\\eta})\\hskip 5.69054pt}\\hat{\\bm{X}},\\,\\bm{X}\\"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.SS1.SSS0.Px2.p1", "title": "Obviously, to ensure the learned auto-encoding to be self-consistent, the main goal of the decoder g ‚Äã ( ‚ãÖ , ùúº ) g(\\cdot,\\bm{\\eta}) italic_g ( ‚ãÖ , bold_italic_Œ∑ ) is to minimize the distance between ùíÅ", "snippet": "Obviously, to ensure the learned auto-encoding to be self-consistent, the main goal of the decoder g ‚Äã ( ‚ãÖ , ùúº ) g(\\cdot,\\bm{\\eta}) italic_g ( ‚ãÖ , bold_italic_Œ∑ ) is to minimize the distance between ùíÅ \\bm{Z} bold_italic_Z and ùíÅ ^ \\hat{\\bm{Z}} over^ start_ARG bold_italic_Z end_ARG . That is, to learn g g italic_g , we want to minimize the distance d ‚Äã ( ùíÅ , ùíÅ ^ ) d(\\bm{Z},\\hat{\\bm{Z}}) italic_d ( bold_italic_Z , over^ start_ARG bold_italic_Z end_ARG ) : min g ‚Å° d ‚Äã ( ùíÅ , ùíÅ ^ ) ‚âê min Œ∑ ‚Äã ‚àë k = 1 K Œî ‚Äã R ‚Äã ( ùíÅ k , ùíÅ ^ k ) = min ùúº ‚Äã ‚àë k = 1 K Œî ‚Äã R ‚Äã ( ùíÅ k , f ‚Äã ( g ‚Äã ( ùíÅ k , ùúº ) , ùúΩ ) ) , \\min_{g"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#Thmexample2.p1", "title": "One may wonder why we need the mapping f ‚Äã ( ‚ãÖ , ùúΩ ) f(\\cdot,\\bm{\\theta}) italic_f ( ‚ãÖ , bold_italic_Œ∏ ) to function as a discriminator between ùëø \\bm{X} bold_italic_X and ùëø ^ \\hat{\\bm{X}} over^ start_", "snippet": "One may wonder why we need the mapping f ‚Äã ( ‚ãÖ , ùúΩ ) f(\\cdot,\\bm{\\theta}) italic_f ( ‚ãÖ , bold_italic_Œ∏ ) to function as a discriminator between ùëø \\bm{X} bold_italic_X and ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG by maximizing max ùúΩ ‚Å° Œî ‚Äã R œµ ‚Äã ( f ‚Äã ( ùëø , ùúΩ ) , f ‚Äã ( ùëø ^ , ùúΩ ) ) \\max_{\\bm{\\theta}}\\Delta R_{\\epsilon}\\big{(}f(\\bm{X},\\bm{\\theta}),f(\\hat{\\bm{X}},\\bm{\\theta})\\big{)} roman_max start_POSTSUBSCRIPT bold_italic_Œ∏ end_POSTSUBSCRIPT roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( italic_f ( bold_italic_X , bold_italic_Œ∏ ) , italic_f ( over^ start_ARG bold_"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.SS1.SSS0.Px2.p2", "title": "Comparing the contractive and contrastive nature of ( 5.2.15 ) and ( 5.2.12 ) on the same distance measure, we see the roles of the encoder f ‚Äã ( ‚ãÖ , ùúΩ ) f(\\cdot,\\bm{\\theta}) italic_f ( ‚ãÖ , bold_itali", "snippet": "Comparing the contractive and contrastive nature of ( 5.2.15 ) and ( 5.2.12 ) on the same distance measure, we see the roles of the encoder f ‚Äã ( ‚ãÖ , ùúΩ ) f(\\cdot,\\bm{\\theta}) italic_f ( ‚ãÖ , bold_italic_Œ∏ ) and the decoder g ‚Äã ( ‚ãÖ , ùúº ) g(\\cdot,\\bm{\\eta}) italic_g ( ‚ãÖ , bold_italic_Œ∑ ) naturally as ‚Äú a two-player game ‚Äù: while the encoder f f italic_f tries to magnify the difference between the original data and their transcribed data, the decoder g g italic_g aims to minimize the difference. Now for convenience, let us define the ‚Äúclosed-loop encoding‚Äù function: h ‚Äã ( ùíô , ùúΩ , ùúº ) ‚âê f ‚Äã ( g ‚Äã ("}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.SS1.SSS0.Px2.p3", "title": "Ideally, we want this function to be very close to f ‚Äã ( ùíô , ùúΩ ) f(\\bm{x},\\bm{\\theta}) italic_f ( bold_italic_x , bold_italic_Œ∏ ) or at least the distributions of their images should be close. With th", "snippet": "Ideally, we want this function to be very close to f ‚Äã ( ùíô , ùúΩ ) f(\\bm{x},\\bm{\\theta}) italic_f ( bold_italic_x , bold_italic_Œ∏ ) or at least the distributions of their images should be close. With this notation, combining ( 5.2.15 ) and ( 5.2.12 ), a closed-loop notion of ‚Äúdistance‚Äù between ùëø \\bm{X} bold_italic_X and ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG can be computed as an equilibrium point to the following Stackelberg game (cf Section A.3 ) for the same utility in terms of rate reduction: ùíü ‚Äã ( ùëø , ùëø ^ ) ‚âê max ùúΩ ‚Å° min ùúº ‚Äã ‚àë k = 1 K Œî ‚Äã R œµ ‚Äã ( f ‚Äã ( ùëø k , ùúΩ ) , h ‚Äã ( ùëø k "}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.SS1.SSS0.Px2.p4", "title": "Notice that this only measures the difference between (features of) the original data and its transcribed version. It does not measure how good the representation ùíÅ \\bm{Z} bold_italic_Z (or ùíÅ ^ \\hat{\\", "snippet": "Notice that this only measures the difference between (features of) the original data and its transcribed version. It does not measure how good the representation ùíÅ \\bm{Z} bold_italic_Z (or ùíÅ ^ \\hat{\\bm{Z}} over^ start_ARG bold_italic_Z end_ARG ) is for the multiple classes within ùëø \\bm{X} bold_italic_X (or ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG ). To this end, we may combine the above distance with the original MCR 2 -type objectives ( 5.2.8 ): namely, the rate reduction Œî ‚Äã R œµ ‚Äã ( ùíÅ ) \\Delta R_{\\epsilon}(\\bm{Z}) roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.SS1.SSS0.Px2.p5", "title": "Hence, the overall ‚Äúmulti-class‚Äù Stackelberg game for learning the closed-loop transcription, named CTRL-Multi, is max ùúΩ ‚Å° min ùúº ‚Å° ùíØ ùëø ‚Äã ( ùúΩ , ùúº ) \\displaystyle\\max_{\\bm{\\theta}}\\min_{\\bm{\\eta}}\\mathc", "snippet": "Hence, the overall ‚Äúmulti-class‚Äù Stackelberg game for learning the closed-loop transcription, named CTRL-Multi, is max ùúΩ ‚Å° min ùúº ‚Å° ùíØ ùëø ‚Äã ( ùúΩ , ùúº ) \\displaystyle\\max_{\\bm{\\theta}}\\min_{\\bm{\\eta}}\\mathcal{T}_{\\bm{X}}(\\bm{\\theta},\\bm{\\eta}) roman_max start_POSTSUBSCRIPT bold_italic_Œ∏ end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT bold_italic_Œ∑ end_POSTSUBSCRIPT caligraphic_T start_POSTSUBSCRIPT bold_italic_X end_POSTSUBSCRIPT ( bold_italic_Œ∏ , bold_italic_Œ∑ ) (5.2.18) ‚âê \\displaystyle\\doteq ‚âê Œî ‚Äã R œµ ‚Äã ( f ‚Äã ( ùëø , ùúΩ ) ) ‚èü Expansive encode + Œî ‚Äã R œµ ‚Äã ( h ‚Äã ( ùëø , ùúΩ , ùúº ) ) ‚èü Compressive decode + ‚àë"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.SS1.SSS0.Px2.p6", "title": "Notice that, without the terms associated with the generative part h h italic_h or with all such terms fixed as constant, the above objective is precisely the original MCR 2 objective introduced in Ch", "snippet": "Notice that, without the terms associated with the generative part h h italic_h or with all such terms fixed as constant, the above objective is precisely the original MCR 2 objective introduced in Chapter 3 . In an unsupervised setting, if we view each sample (and its augmentations) as its own class, the above formulation remains exactly the same. The number of classes k k italic_k is simply the number of independent samples. In addition, notice that the above game‚Äôs objective function depends only on (features of) the data ùëø \\bm{X} bold_italic_X , hence one can learn the encoder and decoder "}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.SS1.SSS0.Px2.p7", "title": "As a special case, if ùëø \\bm{X} bold_italic_X only has one class, the Stackelberg game reduces to a special ‚Äútwo-class‚Äù or ‚Äúbinary‚Äù form, 11 11 11 as the first two rate reduction terms automatically be", "snippet": "As a special case, if ùëø \\bm{X} bold_italic_X only has one class, the Stackelberg game reduces to a special ‚Äútwo-class‚Äù or ‚Äúbinary‚Äù form, 11 11 11 as the first two rate reduction terms automatically become zero named CTRL-Binary, max ùúΩ ‚Å° min ùúº ‚Å° ùíØ ùëø b ‚Äã ( ùúΩ , ùúº ) ‚âê Œî ‚Äã R œµ ‚Äã ( f ‚Äã ( ùëø , ùúΩ ) , h ‚Äã ( ùëø , ùúΩ , ùúº ) ) = Œî ‚Äã R œµ ‚Äã ( ùíÅ ‚Äã ( ùúΩ ) , ùíÅ ^ ‚Äã ( ùúΩ , ùúº ) ) , \\max_{\\bm{\\theta}}\\min_{\\bm{\\eta}}\\mathcal{T}^{b}_{\\bm{X}}(\\bm{\\theta},\\bm{\\eta})\\doteq\\Delta R_{\\epsilon}\\big{(}f(\\bm{X},\\bm{\\theta}),h(\\bm{X},\\bm{\\theta},\\bm{\\eta})\\big{)}=\\Delta R_{\\epsilon}\\big{(}\\bm{Z}(\\bm{\\theta}),\\hat{\\bm{Z}}(\\bm{\\the"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.SS1.SSS0.Px2.p8", "title": "Sometimes, even when ùëø \\bm{X} bold_italic_X contains multiple classes/modes, one could still view all classes together as one class. Then, the above binary objective is to align the union distribution", "snippet": "Sometimes, even when ùëø \\bm{X} bold_italic_X contains multiple classes/modes, one could still view all classes together as one class. Then, the above binary objective is to align the union distribution of all classes with their decoded ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG . This is typically a simpler task to achieve than the multi-class one ( 5.2.20 ), since it does not require learning of a more refined multi-class CTRL for the data, as we will later see in experiments. Notice that one good characteristic of the above formulation is that all quantities in the objectives are "}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.SS1.SSS0.Px2.p9", "title": "One may notice that the above learning framework draws inspiration from closed-loop error correction widely practiced in feedback control systems. The closed-loop mechanism is used to form an overall ", "snippet": "One may notice that the above learning framework draws inspiration from closed-loop error correction widely practiced in feedback control systems. The closed-loop mechanism is used to form an overall feedback system between the two encoding and decoding networks for correcting any ‚Äúerror‚Äù in the distributions between the data ùíô \\bm{x} bold_italic_x and the decoded ùíô ^ \\hat{\\bm{x}} over^ start_ARG bold_italic_x end_ARG . Using terminology from control theory, one may view the encoding network f f italic_f as a ‚Äúsensor‚Äù for error feedback while the decoding network g g italic_g as a ‚Äúcontroller‚Äù"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.SS1.SSS0.Px2.p10", "title": "The remaining question is whether the above framework can indeed learn a good (autoencoding) presentation of a given dataset? Before we give some formal theoretical justification (in the next subsecti", "snippet": "The remaining question is whether the above framework can indeed learn a good (autoencoding) presentation of a given dataset? Before we give some formal theoretical justification (in the next subsection), we present some empirical results."}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.SS1.SSS0.Px3.p1", "title": "We visualize the cosine similarity between ùíÅ \\bm{Z} bold_italic_Z and ùíÅ ^ \\hat{\\bm{Z}} over^ start_ARG bold_italic_Z end_ARG learned from the multi-class objective ( 5.2.20 ) on MNIST, CIFAR-10 and Im", "snippet": "We visualize the cosine similarity between ùíÅ \\bm{Z} bold_italic_Z and ùíÅ ^ \\hat{\\bm{Z}} over^ start_ARG bold_italic_Z end_ARG learned from the multi-class objective ( 5.2.20 ) on MNIST, CIFAR-10 and ImageNet (10 classes), which indicates how close ùíõ ^ = f ‚àò g ‚Äã ( ùíõ ) \\hat{\\bm{z}}=f\\circ g(\\bm{z}) over^ start_ARG bold_italic_z end_ARG = italic_f ‚àò italic_g ( bold_italic_z ) is from ùíõ \\bm{z} bold_italic_z . Results in Figure 5.8 show that ùíÅ \\bm{Z} bold_italic_Z and ùíÅ ^ \\hat{\\bm{Z}} over^ start_ARG bold_italic_Z end_ARG are aligned very well within each class. The block-diagonal patterns for MNIST"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.SS1.SSS0.Px4.p1", "title": "We compare some representative ùëø \\bm{X} bold_italic_X and ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG on MNIST, CIFAR-10 and ImageNet (10 classes) to verify how close ùíô ^ = g ‚àò f ‚Äã ( ùíô ) \\h", "snippet": "We compare some representative ùëø \\bm{X} bold_italic_X and ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG on MNIST, CIFAR-10 and ImageNet (10 classes) to verify how close ùíô ^ = g ‚àò f ‚Äã ( ùíô ) \\hat{\\bm{x}}=g\\circ f(\\bm{x}) over^ start_ARG bold_italic_x end_ARG = italic_g ‚àò italic_f ( bold_italic_x ) is to ùíô \\bm{x} bold_italic_x . The results are shown in Figure 5.9 , and visualizations are created from training samples. Visually, the auto-encoded ùíô ^ \\hat{\\bm{x}} over^ start_ARG bold_italic_x end_ARG faithfully captures major visual features from its respective training sample ùíô \\bm{x} bo"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.SS2.p1", "title": "In the above, we have argued that it is possible to formulate the problem of learning a data distribution as a closed-loop autoencoding problem. We also saw empirically that such a scheme seems to wor", "snippet": "In the above, we have argued that it is possible to formulate the problem of learning a data distribution as a closed-loop autoencoding problem. We also saw empirically that such a scheme seems to work. The remaining question is when and why such a scheme should works. It is difficult to answer this question for the most general cases with arbitrary data distributions. Nevertheless, as usual, let us see if we can arrive at a rigorous justification for the ideal case when the data distribution is a mixture of low-dimensional subspaces or low-rank Gaussians. A clear characterization and understa"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.SS2.p2", "title": "To this end, let us first suppose that ùëø \\bm{X} bold_italic_X is distributed according to a mixture of low-dimensional Gaussians, and the label (i.e., subspace assignment) for ùëø \\bm{X} bold_italic_X i", "snippet": "To this end, let us first suppose that ùëø \\bm{X} bold_italic_X is distributed according to a mixture of low-dimensional Gaussians, and the label (i.e., subspace assignment) for ùëø \\bm{X} bold_italic_X is given by ùíö \\bm{y} bold_italic_y . Then, let us set up a minimax optimization problem to learn the data distribution, say through learning an encoding of ùëø \\bm{X} bold_italic_X into representations ùíÅ \\bm{Z} bold_italic_Z which are supported on a mixture of orthogonal subspaces, and a decoding of ùíÅ \\bm{Z} bold_italic_Z back into ùëø \\bm{X} bold_italic_X . Then, the representation we want to achieve "}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.SS2.p3", "title": "Under mild conditions, in order to realize the desired encoder and decoder which realize ùíÅ \\bm{Z} bold_italic_Z from a data source ùëø \\bm{X} bold_italic_X that is already distributed according to a mix", "snippet": "Under mild conditions, in order to realize the desired encoder and decoder which realize ùíÅ \\bm{Z} bold_italic_Z from a data source ùëø \\bm{X} bold_italic_X that is already distributed according to a mixture of correlated low-dimensional Gaussians, we only require a linear encoder and decoder to disentangle and whiten the Gaussians. We then study this setting in the case where ùúΩ \\bm{\\theta} bold_italic_Œ∏ and ùúº \\bm{\\eta} bold_italic_Œ∑ parameterize matrices whose operator norm is constrained."}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.SS2.p4", "title": "We want to understand what kinds of optima are learned in this setting. One suitable solution concept for this kind of game, where the decoder‚Äôs optimal solution is defined solely with respect to the ", "snippet": "We want to understand what kinds of optima are learned in this setting. One suitable solution concept for this kind of game, where the decoder‚Äôs optimal solution is defined solely with respect to the encoder (and not, in particular, with respect to some other intrinsic property of the decoder), is a Stackelberg equilibrium where the decoder follows the encoder. Namely, at such an equilibrium, the decoder should optimize its objective; meanwhile the encoder should optimize its objective, given that whatever it picks, the decoder will pick an optimal response (which may affect the encoder object"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#Thmtheorem1.p1", "title": "Suppose that ùêó \\bm{X} bold_italic_X is distributed on a mixture of subspaces. Under certain realistic yet technical conditions, it holds that all sequential equilibria of ( 5.2.22 ) obey: ‚Ä¢ The ùíÅ k \\b", "snippet": "Suppose that ùêó \\bm{X} bold_italic_X is distributed on a mixture of subspaces. Under certain realistic yet technical conditions, it holds that all sequential equilibria of ( 5.2.22 ) obey: ‚Ä¢ The ùíÅ k \\bm{Z}_{k} bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT lie on orthogonal subspaces and are isotropic on those subspaces, i.e., maximizing the information gain. ‚Ä¢ The autoencoding is self-consistent, i.e., the subspaces spanned by ùíÅ k \\bm{Z}_{k} bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT and ùíÅ ^ k \\hat{\\bm{Z}}_{k} over^ start_ARG bold_italic_Z end_ARG start_POSTSUBS"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.I2.i1.p1", "title": "The ùíÅ k \\bm{Z}_{k} bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT lie on orthogonal subspaces and are isotropic on those subspaces, i.e., maximizing the information gain.", "snippet": "The ùíÅ k \\bm{Z}_{k} bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT lie on orthogonal subspaces and are isotropic on those subspaces, i.e., maximizing the information gain."}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.I2.i2.p1", "title": "The autoencoding is self-consistent, i.e., the subspaces spanned by ùíÅ k \\bm{Z}_{k} bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT and ùíÅ ^ k \\hat{\\bm{Z}}_{k} over^ start_ARG bold_italic_Z", "snippet": "The autoencoding is self-consistent, i.e., the subspaces spanned by ùíÅ k \\bm{Z}_{k} bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT and ùíÅ ^ k \\hat{\\bm{Z}}_{k} over^ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT are the same for all k k italic_k ."}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.SS2.p5", "title": "This notion of self-consistency is the most one can expect if there are only geometric assumptions on the data, i.e., there are no statistical assumptions. If we assume that the columns of ùëø \\bm{X} bo", "snippet": "This notion of self-consistency is the most one can expect if there are only geometric assumptions on the data, i.e., there are no statistical assumptions. If we assume that the columns of ùëø \\bm{X} bold_italic_X is drawn from a low-rank Gaussian mixture model, then analogous versions of this theorem certify that ùíÅ k \\bm{Z}_{k} bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT are also low-rank Gaussians whose covariance is isotropic, for instance. Essentially, this result validates, via the simple case of Gaussian mixtures on subspaces, that minimax games to optimize the information"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS1.p1", "title": "As we have seen, deep neural networks have demonstrated a great ability to learn representations for hundreds or even thousands of classes of objects, in both discriminative and generative contexts. H", "snippet": "As we have seen, deep neural networks have demonstrated a great ability to learn representations for hundreds or even thousands of classes of objects, in both discriminative and generative contexts. However, networks typically must be trained offline, with uniformly sampled data from all classes simultaneously. It has been known that when an (open-loop) network is updated to learn new classes without data from the old ones, previously learned knowledge will fall victim to the problem of catastrophic forgetting [ MC89 ] . This is known in neuroscience as the stability-plasticity dilemma: the ch"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS1.p2", "title": "In contrast, natural neural systems (e.g. animal brains) do not seem to suffer from such catastrophic forgetting at all. They are capable of developing new memory of new objects while retaining memory", "snippet": "In contrast, natural neural systems (e.g. animal brains) do not seem to suffer from such catastrophic forgetting at all. They are capable of developing new memory of new objects while retaining memory of previously learned objects. This ability, for either natural or artificial neural systems, is often referred to as incremental learning, continual learning, sequential learning , or life-long learning [ AR20 ] ."}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS1.p3", "title": "While many recent works have highlighted how artificial neural systems can be trained in more flexible ways, the strongest existing efforts toward answering the stability-plasticity dilemma for artifi", "snippet": "While many recent works have highlighted how artificial neural systems can be trained in more flexible ways, the strongest existing efforts toward answering the stability-plasticity dilemma for artificial neural networks typically require either storing raw exemplars [ RKS+17 , CRE+19 ] or providing external mechanisms [ KPR+17 ] . Raw exemplars, particularly in the case of high-dimensional inputs like images, are costly and difficult to scale, while external mechanisms‚Äîwhich typically include secondary networks and representation spaces for generative replay, incremental allocation of network"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS1.p4", "title": "Here we are interested in an incremental learning setting that is similar to nature. It counters these existing practices with two key qualities. 1. The first is that it should be memory-based. When l", "snippet": "Here we are interested in an incremental learning setting that is similar to nature. It counters these existing practices with two key qualities. 1. The first is that it should be memory-based. When learning new classes, no raw exemplars of old classes are available to train the network together with new data. This implies that one has to rely on a compact and thus structured ‚Äúmemory‚Äù learned for old classes, such as incrementally learned generative representations of the old classes, as well as the associated encoding and decoding mappings [ KK18 ] . 2. The second is that it should be self-co"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.I1.i1.p1", "title": "The first is that it should be memory-based. When learning new classes, no raw exemplars of old classes are available to train the network together with new data. This implies that one has to rely on ", "snippet": "The first is that it should be memory-based. When learning new classes, no raw exemplars of old classes are available to train the network together with new data. This implies that one has to rely on a compact and thus structured ‚Äúmemory‚Äù learned for old classes, such as incrementally learned generative representations of the old classes, as well as the associated encoding and decoding mappings [ KK18 ] ."}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.I1.i2.p1", "title": "The second is that it should be self-contained. Incremental learning takes place in a single neural system with a fixed capacity, and in a common representation space. The ability to minimize forgetti", "snippet": "The second is that it should be self-contained. Incremental learning takes place in a single neural system with a fixed capacity, and in a common representation space. The ability to minimize forgetting is implied by optimizing an overall learning objective, without external networks, architectural modifications, or resource allocation mechanisms."}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS1.p5", "title": "The incoherent linear structures for features of different classes closely resemble how objects are encoded in different areas of the inferotemporal cortex of animal brains [ CT17 , BSM+20 ] . The clo", "snippet": "The incoherent linear structures for features of different classes closely resemble how objects are encoded in different areas of the inferotemporal cortex of animal brains [ CT17 , BSM+20 ] . The closed-loop transcription ùëø ‚Üí ùíÅ ‚Üí ùëø ^ ‚Üí ùíÅ ^ \\bm{X}\\rightarrow\\bm{Z}\\rightarrow\\hat{\\bm{X}}\\rightarrow\\hat{\\bm{Z}} bold_italic_X ‚Üí bold_italic_Z ‚Üí over^ start_ARG bold_italic_X end_ARG ‚Üí over^ start_ARG bold_italic_Z end_ARG also resembles popularly hypothesized mechanisms for memory formation [ VST+20 , JT20 ] . This leads to a question: since memory in the brains is formed in an incremental fashion,"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS1.SSS0.Px1.p1", "title": "The simple linear structures of LDR make it uniquely suited for incremental learning: the distribution of features ùíÅ j \\bm{Z}_{j} bold_italic_Z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT of each p", "snippet": "The simple linear structures of LDR make it uniquely suited for incremental learning: the distribution of features ùíÅ j \\bm{Z}_{j} bold_italic_Z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT of each previously learned class can be explicitly and concisely represented by a principal subspace ùíÆ j \\mathcal{S}_{j} caligraphic_S start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT in the feature space. To preserve the memory of an old class j j italic_j , we only need to preserve the subspace while learning new classes. To this end, we simply sample m m italic_m representative prototype features on the s"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS1.SSS0.Px2.p1", "title": "Notice that, with the learned auto-encoding ( 5.2.9 ), one can replay and use the images, say ùëø ^ o ‚Äã l ‚Äã d = g ‚Äã ( ùíÅ o ‚Äã l ‚Äã d , ùúº ) \\hat{\\bm{X}}_{old}=g(\\bm{Z}_{old},\\bm{\\eta}) over^ start_ARG bold_", "snippet": "Notice that, with the learned auto-encoding ( 5.2.9 ), one can replay and use the images, say ùëø ^ o ‚Äã l ‚Äã d = g ‚Äã ( ùíÅ o ‚Äã l ‚Äã d , ùúº ) \\hat{\\bm{X}}_{old}=g(\\bm{Z}_{old},\\bm{\\eta}) over^ start_ARG bold_italic_X end_ARG start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT = italic_g ( bold_italic_Z start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT , bold_italic_Œ∑ ) , associated with the memory features to avoid forgetting while learning new classes. This is typically how generative models have been used for prior incremental learning methods. However, with the closed-lo"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS1.SSS0.Px2.p2", "title": "Consider the task of incrementally learning a new class of objects. 13 13 13 Of course, one may also consider the more general setting where the task contains a small batch of new classes, without ser", "snippet": "Consider the task of incrementally learning a new class of objects. 13 13 13 Of course, one may also consider the more general setting where the task contains a small batch of new classes, without serious modification. We denote a corresponding new sample set as ùëø n ‚Äã e ‚Äã w \\bm{X}_{new} bold_italic_X start_POSTSUBSCRIPT italic_n italic_e italic_w end_POSTSUBSCRIPT . The features of ùëø n ‚Äã e ‚Äã w \\bm{X}_{new} bold_italic_X start_POSTSUBSCRIPT italic_n italic_e italic_w end_POSTSUBSCRIPT are denoted as ùíÅ n ‚Äã e ‚Äã w ‚Äã ( ùúΩ ) = f ‚Äã ( ùëø n ‚Äã e ‚Äã w , ùúΩ ) \\bm{Z}_{new}(\\bm{\\theta})=f(\\bm{X}_{new},\\bm{\\thet"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS1.SSS0.Px2.p3", "title": "Mirroring the motivation for the multi-class CTRL objective ( 5.2.20 ), we would like the features of the new class ùíÅ n ‚Äã e ‚Äã w \\bm{Z}_{new} bold_italic_Z start_POSTSUBSCRIPT italic_n italic_e italic_", "snippet": "Mirroring the motivation for the multi-class CTRL objective ( 5.2.20 ), we would like the features of the new class ùíÅ n ‚Äã e ‚Äã w \\bm{Z}_{new} bold_italic_Z start_POSTSUBSCRIPT italic_n italic_e italic_w end_POSTSUBSCRIPT to be incoherent to all of the old ones ùíÅ o ‚Äã l ‚Äã d \\bm{Z}_{old} bold_italic_Z start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT . As ùíÅ n ‚Äã e ‚Äã w \\bm{Z}_{new} bold_italic_Z start_POSTSUBSCRIPT italic_n italic_e italic_w end_POSTSUBSCRIPT is the only new class whose features needs to be learned, the objective ( 5.2.20 ) reduces to the case where k = 1 k=1 italic_k"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS1.SSS0.Px2.p4", "title": "In practice, the constrained minimax program can be solved by alternating minimization and maximization between the encoder f ‚Äã ( ‚ãÖ , ùúΩ ) f(\\cdot,\\bm{\\theta}) italic_f ( ‚ãÖ , bold_italic_Œ∏ ) and decode", "snippet": "In practice, the constrained minimax program can be solved by alternating minimization and maximization between the encoder f ‚Äã ( ‚ãÖ , ùúΩ ) f(\\cdot,\\bm{\\theta}) italic_f ( ‚ãÖ , bold_italic_Œ∏ ) and decoder g ‚Äã ( ‚ãÖ , ùúº ) g(\\cdot,\\bm{\\eta}) italic_g ( ‚ãÖ , bold_italic_Œ∑ ) as follows: max ùúΩ \\displaystyle\\max_{\\bm{\\theta}} roman_max start_POSTSUBSCRIPT bold_italic_Œ∏ end_POSTSUBSCRIPT Œî ‚Äã R œµ ‚Äã ( ùíÅ ) + Œî ‚Äã R œµ ‚Äã ( ùíÅ ^ ) + Œª ‚ãÖ Œî ‚Äã R œµ ‚Äã ( ùíÅ n ‚Äã e ‚Äã w , ùíÅ ^ n ‚Äã e ‚Äã w ) ‚àí Œ≥ ‚ãÖ Œî ‚Äã R œµ ‚Äã ( ùíÅ o ‚Äã l ‚Äã d , ùíÅ ^ o ‚Äã l ‚Äã d ) , \\displaystyle\\Delta{R_{\\epsilon}(\\bm{Z})}\\!+\\!\\Delta{R_{\\epsilon}(\\hat{\\bm{Z}})}\\!+\\!\\la"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS1.SSS0.Px3.p1", "title": "As we will see, the above constrained minimax program can already achieve state of the art performance for incremental learning. Nevertheless, developing an optimal memory for all classes cannot rely ", "snippet": "As we will see, the above constrained minimax program can already achieve state of the art performance for incremental learning. Nevertheless, developing an optimal memory for all classes cannot rely on graceful forgetting alone. Even for humans, if an object class is learned only once, we should expect the learned memory to fade as we continue to learn new others, unless the memory can be consolidated by reviewing old object classes."}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS1.SSS0.Px3.p2", "title": "To emulate this phase of memory forming, after incrementally learning a whole dataset, we may go back to review all classes again, one class at a time. We refer to going through all classes once as on", "snippet": "To emulate this phase of memory forming, after incrementally learning a whole dataset, we may go back to review all classes again, one class at a time. We refer to going through all classes once as one reviewing ‚Äúcycle‚Äù. 14 14 14 to distinguish from the term ‚Äúepoch‚Äù used in the conventional joint learning setting. If needed, multiple reviewing cycles can be conducted. It is quite expected that reviewing can improve the learned (LDR) memory. But somewhat surprisingly, the closed-loop framework allows us to review even in a ‚Äúclass-unsupervised‚Äù manner: when reviewing data of an old class say ùëø j"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS1.SSS0.Px4.p1", "title": "We show some experimental results on the following datasets: MNIST [ LBB+98a ] and CIFAR-10 [ KNH14 ] . All experiments are conducted for the more challenging class-IL setting. For both MNIST and CIFA", "snippet": "We show some experimental results on the following datasets: MNIST [ LBB+98a ] and CIFAR-10 [ KNH14 ] . All experiments are conducted for the more challenging class-IL setting. For both MNIST and CIFAR-10, the 10 classes are split into 5 tasks with 2 classes each or 10 tasks with 1 class each. For the encoder f f italic_f and decoder g g italic_g , we adopt a very simple network architecture modified from DCGAN [ RMC16 ] , which is merely a four-layer convolutional network. Here we only show some qualitative visual results and more experiments and analytical analysis can be found in the work ["}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS1.SSS0.Px5.p1", "title": "We begin by qualitatively visualizing some representative images ùëø \\bm{X} bold_italic_X and the corresponding replayed ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG on MNIST and CIFAR-10. The", "snippet": "We begin by qualitatively visualizing some representative images ùëø \\bm{X} bold_italic_X and the corresponding replayed ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG on MNIST and CIFAR-10. The model is learned incrementally with the datasets split into 5 tasks. Results are shown in Figure 5.11 , where we observe that the reconstructed ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG preserves the main visual characteristics of ùëø \\bm{X} bold_italic_X including shapes and textures. For a simpler dataset like MNIST, the replayed ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS1.SSS0.Px6.p1", "title": "Most generative memory-based methods utilize autoencoders, VAEs, or GANs for replay purposes. The structure or distribution of the learned features ùíÅ j \\bm{Z}_{j} bold_italic_Z start_POSTSUBSCRIPT ita", "snippet": "Most generative memory-based methods utilize autoencoders, VAEs, or GANs for replay purposes. The structure or distribution of the learned features ùíÅ j \\bm{Z}_{j} bold_italic_Z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT for each class is unclear in the feature space. The features ùíÅ j \\bm{Z}_{j} bold_italic_Z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT of the LDR memory, on the other hand, have a clear linear structure. Figure 5.12 visualizes correlations among all learned features | ùíÅ ‚ä§ ‚Äã ùíÅ | |\\bm{Z}^{\\top}\\bm{Z}| | bold_italic_Z start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_Z"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS1.SSS0.Px7.p1", "title": "Since features of each class can be modeled as a principal subspace, we further visualize the individual principal components within each of those subspaces. Figure 5.13 shows the images replayed from", "snippet": "Since features of each class can be modeled as a principal subspace, we further visualize the individual principal components within each of those subspaces. Figure 5.13 shows the images replayed from sampled features along the top-4 principal components for different classes, on MNIST and CIFAR-10 respectively. Each row represents samples along one principal component and they clearly show similar visual characteristics but distinctively different from those in other rows. We see that the model remembers different poses of ‚Äò4‚Äô after having learned all remaining classes. For CIFAR-10, the incr"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS1.SSS0.Px8.p1", "title": "We verify how the incrementally learned LDR memory can be further consolidated with an unsupervised incremental reviewing phase described before. Experiments are conducted on CIFAR-10, with 10 steps. ", "snippet": "We verify how the incrementally learned LDR memory can be further consolidated with an unsupervised incremental reviewing phase described before. Experiments are conducted on CIFAR-10, with 10 steps. Figure 5.14 left shows replayed images of the first class ‚Äòairplane‚Äô at the end of incremental learning of all ten classes, sampled along the top-3 principal components ‚Äì every two rows (16 images) are along one principal direction. Their visual quality remains very decent ‚Äì observed almost no forgetting. The right figure shows replayed images after reviewing the first class once. We notice a sign"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS2.p1", "title": "As we know, the closed-loop CTRL formulation can already learn a decent autoencoding, even without class information, with the CTRL-Binary program: max ùúΩ ‚Å° min ùúº Œî ‚Äã R œµ ‚Äã ( ùíÅ , ùíÅ ^ ) \\displaystyle\\ma", "snippet": "As we know, the closed-loop CTRL formulation can already learn a decent autoencoding, even without class information, with the CTRL-Binary program: max ùúΩ ‚Å° min ùúº Œî ‚Äã R œµ ‚Äã ( ùíÅ , ùíÅ ^ ) \\displaystyle\\max_{\\bm{\\theta}}\\min_{\\bm{\\eta}}\\quad\\Delta R_{\\epsilon}(\\bm{Z},\\hat{\\bm{Z}}) roman_max start_POSTSUBSCRIPT bold_italic_Œ∏ end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT bold_italic_Œ∑ end_POSTSUBSCRIPT roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z , over^ start_ARG bold_italic_Z end_ARG ) (5.3.6) However, note that ( 5.3.6 ) is practically limited because it only a"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS2.SSS0.Px1.p1", "title": "To improve discriminative and generative properties of representations learned in the unsupervised setting, we propose two additional mechanisms for the above CTRL-Binary maximin game ( 5.3.6 ). For s", "snippet": "To improve discriminative and generative properties of representations learned in the unsupervised setting, we propose two additional mechanisms for the above CTRL-Binary maximin game ( 5.3.6 ). For simplicity and uniformity, here these will be formulated as equality constraints over rate reduction measures, but in practice they can be enforced softly during optimization."}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS2.SSS0.Px2.p1", "title": "First, to address the issue that CTRL-Binary does not learn a sample-wise consistent autoencoding, we need to promote ùíô ^ \\hat{\\bm{x}} over^ start_ARG bold_italic_x end_ARG to be close to ùíô \\bm{x} bol", "snippet": "First, to address the issue that CTRL-Binary does not learn a sample-wise consistent autoencoding, we need to promote ùíô ^ \\hat{\\bm{x}} over^ start_ARG bold_italic_x end_ARG to be close to ùíô \\bm{x} bold_italic_x for each sample. In the CTRL framework, this can be achieved by enforcing their corresponding features ùíõ = f ‚Äã ( ùíô ) \\bm{z}=f(\\bm{x}) bold_italic_z = italic_f ( bold_italic_x ) and ùíõ ^ = f ‚Äã ( ùíô ^ ) \\hat{\\bm{z}}=f(\\hat{\\bm{x}}) over^ start_ARG bold_italic_z end_ARG = italic_f ( over^ start_ARG bold_italic_x end_ARG ) to be close. To promote sample-wise self-consistency, where ùíô ^ = g ‚Äã "}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS2.SSS0.Px3.p1", "title": "Since we do not know any class label information between samples in the unsupervised settings, the best we can do is to view every sample and its augmentations (say via translation, rotation, occlusio", "snippet": "Since we do not know any class label information between samples in the unsupervised settings, the best we can do is to view every sample and its augmentations (say via translation, rotation, occlusion etc) as one ‚Äúclass‚Äù‚Äîa basic idea behind almost all self-supervised learning methods. In the rate reduction framework, it is natural to compress the features of each sample and its augmentations. In this work, we adopt the standard transformations in SimCLR [ CKN+20 ] and denote such a transformation as œÑ \\tau italic_œÑ . We denote each augmented sample ùíô a = œÑ ‚Äã ( ùíô ) \\bm{x}_{a}=\\tau(\\bm{x}) bold"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS2.SSS0.Px4.p1", "title": "So far, we know the CTRL-Binary objective Œî ‚Äã R œµ ‚Äã ( ùíÅ , ùíÅ ^ ) \\Delta R_{\\epsilon}(\\bm{Z},\\hat{\\bm{Z}}) roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z , over^ start_A", "snippet": "So far, we know the CTRL-Binary objective Œî ‚Äã R œµ ‚Äã ( ùíÅ , ùíÅ ^ ) \\Delta R_{\\epsilon}(\\bm{Z},\\hat{\\bm{Z}}) roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z , over^ start_ARG bold_italic_Z end_ARG ) in ( 5.3.6 ) helps align the distributions while sample-wise self-consistency ( 5.3.7 ) and sample-wise augmentation ( 5.3.8 ) help align and compress features associated with each sample. Besides consistency, we also want learned representations are maximally discriminative for different samples (here viewed as different ‚Äúclasses‚Äù). Notice that the rate distortion term "}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS2.SSS0.Px5.p1", "title": "Putting these elements together, we propose to learn a representation via the following constrained maximin program, which we refer to as unsupervised CTRL (u-CTRL): max Œ∏ ‚Å° min Œ∑ \\displaystyle\\max_{\\", "snippet": "Putting these elements together, we propose to learn a representation via the following constrained maximin program, which we refer to as unsupervised CTRL (u-CTRL): max Œ∏ ‚Å° min Œ∑ \\displaystyle\\max_{\\theta}\\min_{\\eta}\\quad roman_max start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT italic_Œ∑ end_POSTSUBSCRIPT R œµ ‚Äã ( ùíÅ ) + Œî ‚Äã R œµ ‚Äã ( ùíÅ , ùíÅ ^ ) \\displaystyle R_{\\epsilon}(\\bm{Z})+\\Delta R_{\\epsilon}(\\bm{Z},\\hat{\\bm{Z}}) italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ) + roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_ital"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS2.SSS0.Px5.p2", "title": "In practice, the above program can be optimized by alternating maximization and minimization between the encoder f ‚Äã ( ‚ãÖ , ùúΩ ) f(\\cdot,\\bm{\\theta}) italic_f ( ‚ãÖ , bold_italic_Œ∏ ) and the decoder g ‚Äã (", "snippet": "In practice, the above program can be optimized by alternating maximization and minimization between the encoder f ‚Äã ( ‚ãÖ , ùúΩ ) f(\\cdot,\\bm{\\theta}) italic_f ( ‚ãÖ , bold_italic_Œ∏ ) and the decoder g ‚Äã ( ‚ãÖ , ùúº ) g(\\cdot,\\bm{\\eta}) italic_g ( ‚ãÖ , bold_italic_Œ∑ ) . We adopt the following optimization strategy that works well in practice, which is used for all subsequent experiments on real image datasets: max Œ∏ ‚Å° R œµ ‚Äã ( ùíÅ ) + Œî ‚Äã R œµ ‚Äã ( ùíÅ , ùíÅ ^ ) ‚àí Œª 1 ‚Äã ‚àë i ‚àà N Œî ‚Äã R œµ ‚Äã ( ùíõ i , ùíõ a i ) ‚àí Œª 2 ‚Äã ‚àë i ‚àà N Œî ‚Äã R œµ ‚Äã ( ùíõ i , ùíõ ^ i ) ; \\displaystyle\\max_{\\theta}\\;R_{\\epsilon}(\\bm{Z})+\\Delta{R_{\\epsilo"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS2.SSS0.Px5.p3", "title": "The above representation is learned without class information. In order to facilitate discriminative or generative tasks, it must be highly structured. It has been verified experimentally that this is", "snippet": "The above representation is learned without class information. In order to facilitate discriminative or generative tasks, it must be highly structured. It has been verified experimentally that this is indeed the case and u-CTRL demonstrates significant advantages over other incremental or unsupervised learning methods [ TDC+24 ] . We here only illustrate some qualitative results with the experiment on the CIFAR-10 dataset [ KNH14 ] , with standard augmentations for self-supervised learning [ CKN+20 ] . One may refer to [ TDC+24 ] for experiments on more and larger datasets and their quantitati"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS2.SSS0.Px5.p4", "title": "As one can see from the experiments, specific and unique structure indeed emerges naturally in the representations learned using u-CTRL: globally, features of images in the same class tend to be clust", "snippet": "As one can see from the experiments, specific and unique structure indeed emerges naturally in the representations learned using u-CTRL: globally, features of images in the same class tend to be clustered well together and separated from other classes, as shown in Figure 5.16 ; locally, features around individual samples exhibit approximately piecewise linear low-dimensional structures, as shown in Figure 5.17 ."}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS2.SSS0.Px6.p1", "title": "The highly-structured feature distribution also suggests that the learned representation can be very useful for generative purposes. For example, we can organize the sample features into meaningful cl", "snippet": "The highly-structured feature distribution also suggests that the learned representation can be very useful for generative purposes. For example, we can organize the sample features into meaningful clusters, and model them with low-dimensional (Gaussian) distributions or subspaces. By sampling from these compact models, we can conditionally regenerate meaningful samples from computed clusters. This is known as unsupervised conditional image generation [ HKJ+21 ] ."}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.SS2.SSS0.Px6.p2", "title": "To cluster features, we exploit the fact that the rate reduction framework ( 3.4.12 ) is inspired by unsupervised clustering via compression [ MDH+07a ] , which provides a principled way to find the m", "snippet": "To cluster features, we exploit the fact that the rate reduction framework ( 3.4.12 ) is inspired by unsupervised clustering via compression [ MDH+07a ] , which provides a principled way to find the membership ùö∑ \\bm{\\Pi} bold_Œ† . Concretely, we maximize the same rate reduction objective ( 3.4.12 ) over ùö∑ \\bm{\\Pi} bold_Œ† , but fix the learned representation ùíÅ \\bm{Z} bold_italic_Z instead. We simply view the membership ùö∑ \\bm{\\Pi} bold_Œ† as a nonlinear function of the features ùíÅ \\bm{Z} bold_italic_Z , say h ùùÖ ‚Äã ( ‚ãÖ , Œæ ) : ùíÅ ‚Ü¶ ùö∑ h_{\\bm{\\pi}}(\\cdot,\\xi):\\bm{Z}\\mapsto\\bm{\\Pi} italic_h start_POSTSUB"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S4.p1", "title": "Historically, autoencoding has been one of the important drivers of research innovation in neural networks for learning, although the most practically impressive demonstrations of deep learning have p", "snippet": "Historically, autoencoding has been one of the important drivers of research innovation in neural networks for learning, although the most practically impressive demonstrations of deep learning have probably been in other domains (such as discriminative classification, with AlexNet [ KSH12 ] , or generative modeling with GPT architectures [ BMR+20 ] ). Works we have featured throughout the chapter, especially the work of [ HS06 ] , served as catalysts of research interest in neural networks during times when they were otherwise not prominent in the machine learning research landscape. In moder"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S4.p2", "title": "Materials presented in the second half of this chapter are based on a series of recent work on the topic of closed-loop transcription: [ DTL+22 ] , [ PPC+23 ] , [ TDW+23 ] , and [ TDC+24 ] . In partic", "snippet": "Materials presented in the second half of this chapter are based on a series of recent work on the topic of closed-loop transcription: [ DTL+22 ] , [ PPC+23 ] , [ TDW+23 ] , and [ TDC+24 ] . In particular, Section 5.2.1 is based on the pioneering work of [ DTL+22 ] . After that, the work of [ PPC+23 ] has provided strong theoretical justifications for the closed-loop framework, at least for an ideal case. Section 5.3.1 and Section 5.3.2 are based on the works of [ TDW+23 ] and [ TDC+24 ] , respectively. They demonstrate that the closed-loop framework naturally supports incremental and continuo"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S4.SS0.SSS0.Px1.p1", "title": "In Section 5.1.2 , we discussed Cybenko‚Äôs universal approximation theorem and how it states that in principle, a neural network with a single hidden layer (and suitable elementwise nonlinearities) is ", "snippet": "In Section 5.1.2 , we discussed Cybenko‚Äôs universal approximation theorem and how it states that in principle, a neural network with a single hidden layer (and suitable elementwise nonlinearities) is sufficient to approximate any suitably regular target function. Of course, in practice, the major architectural reason for the dominance of neural networks in practice has been the refinement of techniques for training deeper neural networks. Why is depth necessary? From a fundamental point of view, the issue of depth separations, which construct settings where a deeper neural network can approxim"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#Thmexercise1.p1", "title": "Consider data lying on a curved manifold ‚Ñ≥ \\mathcal{M} caligraphic_M embedded in ‚Ñù D \\mathbb{R}^{D} blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT (like a curved surface in 3 3 3 -dim", "snippet": "Consider data lying on a curved manifold ‚Ñ≥ \\mathcal{M} caligraphic_M embedded in ‚Ñù D \\mathbb{R}^{D} blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT (like a curved surface in 3 3 3 -dimensional space), as discussed in the manifold flattening subsection of Section 5.1.2 . In this exercise, we will describe the basic ingredients of the manifold flattening algorithm from [ PPR+24 ] . A manifold is called flat if it is an open set in Euclidean space (or more generally, an open set in a subspace)."}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#Thmexercise1.p2", "title": "1. Suppose the manifold ‚Ñ≥ \\mathcal{M} caligraphic_M is a graph : this means that ‚Ñ≥ ‚äÇ ‚Ñù D 1 √ó ‚Ñù D 2 \\mathcal{M}\\subset\\mathbb{R}^{D_{1}}\\times\\mathbb{R}^{D_{2}} caligraphic_M ‚äÇ blackboard_R start_POSTS", "snippet": "1. Suppose the manifold ‚Ñ≥ \\mathcal{M} caligraphic_M is a graph : this means that ‚Ñ≥ ‚äÇ ‚Ñù D 1 √ó ‚Ñù D 2 \\mathcal{M}\\subset\\mathbb{R}^{D_{1}}\\times\\mathbb{R}^{D_{2}} caligraphic_M ‚äÇ blackboard_R start_POSTSUPERSCRIPT italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT √ó blackboard_R start_POSTSUPERSCRIPT italic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT (say), and that there is a function F : ‚Ñù D 1 ‚Üí ‚Ñù D 2 F:\\mathbb{R}^{D_{1}}\\to\\mathbb{R}^{D_{2}} italic_F : blackboard_R start_POSTSUPERSCRIPT italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ‚Üí b"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S5.I1.i1.p1", "title": "Suppose the manifold ‚Ñ≥ \\mathcal{M} caligraphic_M is a graph : this means that ‚Ñ≥ ‚äÇ ‚Ñù D 1 √ó ‚Ñù D 2 \\mathcal{M}\\subset\\mathbb{R}^{D_{1}}\\times\\mathbb{R}^{D_{2}} caligraphic_M ‚äÇ blackboard_R start_POSTSUPE", "snippet": "Suppose the manifold ‚Ñ≥ \\mathcal{M} caligraphic_M is a graph : this means that ‚Ñ≥ ‚äÇ ‚Ñù D 1 √ó ‚Ñù D 2 \\mathcal{M}\\subset\\mathbb{R}^{D_{1}}\\times\\mathbb{R}^{D_{2}} caligraphic_M ‚äÇ blackboard_R start_POSTSUPERSCRIPT italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT √ó blackboard_R start_POSTSUPERSCRIPT italic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT (say), and that there is a function F : ‚Ñù D 1 ‚Üí ‚Ñù D 2 F:\\mathbb{R}^{D_{1}}\\to\\mathbb{R}^{D_{2}} italic_F : blackboard_R start_POSTSUPERSCRIPT italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ‚Üí blac"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S5.I1.i2.p1", "title": "Now suppose that ‚Ñ≥ \\mathcal{M} caligraphic_M is a general smooth manifold. Smooth manifolds have the property that they are locally well-approximated by subspaces near each point. Describe in intuitiv", "snippet": "Now suppose that ‚Ñ≥ \\mathcal{M} caligraphic_M is a general smooth manifold. Smooth manifolds have the property that they are locally well-approximated by subspaces near each point. Describe in intuitive terms how to flatten the manifold ‚Ñ≥ \\mathcal{M} caligraphic_M locally at each point, by relating it to a graph. (The algorithm of [ PPR+24 ] performs a refined version of this local flattening process in a way that allows them to be glued together to form a global flattening.)"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#Thmexercise2.p1", "title": "Implement a closed-loop transcription pipeline for representation learning on the CIFAR-10 dataset following the methodology in Section 5.2.1 . Reference [ DTL+22 ] for useful hyperparameters and arch", "snippet": "Implement a closed-loop transcription pipeline for representation learning on the CIFAR-10 dataset following the methodology in Section 5.2.1 . Reference [ DTL+22 ] for useful hyperparameters and architecture settings. Reproduce the result in Figure 5.8 ."}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#Thmdefinition1", "title": "Definition 5.1 (Consistent Representations) .", "snippet": "Definition 5.1 (Consistent Representations) . Given data ùëø \\bm{X} bold_italic_X , an consistent representation is a pair of functions ( f : ùí≥ ‚Üí ùíµ , g : ùíµ ‚Üí ùí≥ ) (f\\colon\\mathcal{X}\\to\\mathcal{Z},g\\colon\\mathcal{Z}\\to\\mathcal{X}) ( italic_f : caligraphic_X ‚Üí caligraphic_Z , italic_g : caligraphic_Z ‚Üí caligraphic_X ) , such that the features ùíÅ = f ‚Äã ( ùëø ) \\bm{Z}=f(\\bm{X}) bold_italic_Z = italic_f ( bold_italic_X ) are compact and structured, and the autoencoding ùëø ^ ‚âê g ‚Äã ( ùíÅ ) = g ‚Äã ( f ‚Äã ( ùëø ) ) \\hat{\\bm{X}}\\doteq g(\\bm{Z})=g(f(\\bm{X})) over^ start_ARG bold_italic_X end_ARG ‚âê italic_g ( bold_it"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#Thmexample1", "title": "Example 5.1 (Normalized Hebbian learning scheme for PCA) .", "snippet": "Example 5.1 (Normalized Hebbian learning scheme for PCA) . Consider a sequence of i.i.d. random vectors ùíô 1 , ‚Ä¶ , ùíô i , ‚Ä¶ ‚àà ‚Ñù n \\bm{x}_{1},\\ldots,\\bm{x}_{i},\\ldots\\in\\mathbb{R}^{n} bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , ‚Ä¶ ‚àà blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT with covariance ùö∫ ‚àà ‚Ñù n √ó n \\bm{\\Sigma}\\in\\mathbb{R}^{n\\times n} bold_Œ£ ‚àà blackboard_R start_POSTSUPERSCRIPT italic_n √ó italic_n end_POSTSUPERSCRIPT . Let ùíñ 0 ‚àà ‚Ñù n \\bm{u}_{0}\\in\\mathbb{R}^{n} bold_italic_u start_POSTSUBSCRIPT 0"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#Thmdefinition2", "title": "Definition 5.2 (Self-Consistent Representation) .", "snippet": "Definition 5.2 (Self-Consistent Representation) . Given data ùëø \\bm{X} bold_italic_X , we call an self-consistent representation to be a pair of functions ( f : ùí≥ ‚Üí ùíµ , g : ùíµ ‚Üí ùí≥ ) (f\\colon\\mathcal{X}\\to\\mathcal{Z},g\\colon\\mathcal{Z}\\to\\mathcal{X}) ( italic_f : caligraphic_X ‚Üí caligraphic_Z , italic_g : caligraphic_Z ‚Üí caligraphic_X ) , such that the features ùíÅ = f ‚Äã ( ùëø ) \\bm{Z}=f(\\bm{X}) bold_italic_Z = italic_f ( bold_italic_X ) are compact and structured, and the autoencoding features ùíÅ ^ ‚âê f ‚àò g ‚Äã ( ùíÅ ) \\hat{\\bm{Z}}\\doteq f\\circ g(\\bm{Z}) over^ start_ARG bold_italic_Z end_ARG ‚âê italic_f ‚àò "}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#Thmexample2", "title": "Example 5.2 .", "snippet": "Example 5.2 . One may wonder why we need the mapping f ‚Äã ( ‚ãÖ , ùúΩ ) f(\\cdot,\\bm{\\theta}) italic_f ( ‚ãÖ , bold_italic_Œ∏ ) to function as a discriminator between ùëø \\bm{X} bold_italic_X and ùëø ^ \\hat{\\bm{X}} over^ start_ARG bold_italic_X end_ARG by maximizing max ùúΩ ‚Å° Œî ‚Äã R œµ ‚Äã ( f ‚Äã ( ùëø , ùúΩ ) , f ‚Äã ( ùëø ^ , ùúΩ ) ) \\max_{\\bm{\\theta}}\\Delta R_{\\epsilon}\\big{(}f(\\bm{X},\\bm{\\theta}),f(\\hat{\\bm{X}},\\bm{\\theta})\\big{)} roman_max start_POSTSUBSCRIPT bold_italic_Œ∏ end_POSTSUBSCRIPT roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( italic_f ( bold_italic_X , bold_italic_Œ∏ ) , italic_f ( over^ s"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#Thmtheorem1", "title": "Theorem 5.1 ( [ PPC+23 ] , Abridged) .", "snippet": "Theorem 5.1 ( [ PPC+23 ] , Abridged) . Suppose that ùêó \\bm{X} bold_italic_X is distributed on a mixture of subspaces. Under certain realistic yet technical conditions, it holds that all sequential equilibria of ( 5.2.22 ) obey: ‚Ä¢ The ùíÅ k \\bm{Z}_{k} bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT lie on orthogonal subspaces and are isotropic on those subspaces, i.e., maximizing the information gain. ‚Ä¢ The autoencoding is self-consistent, i.e., the subspaces spanned by ùíÅ k \\bm{Z}_{k} bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT and ùíÅ ^ k \\hat{\\bm{Z}}_{k} over^ start_A"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#Thmexercise1", "title": "Exercise 5.1 (Conceptual Understanding of Manifold Flattening) .", "snippet": "Exercise 5.1 (Conceptual Understanding of Manifold Flattening) . Consider data lying on a curved manifold ‚Ñ≥ \\mathcal{M} caligraphic_M embedded in ‚Ñù D \\mathbb{R}^{D} blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT (like a curved surface in 3 3 3 -dimensional space), as discussed in the manifold flattening subsection of Section 5.1.2 . In this exercise, we will describe the basic ingredients of the manifold flattening algorithm from [ PPR+24 ] . A manifold is called flat if it is an open set in Euclidean space (or more generally, an open set in a subspace). 1. Suppose the manifol"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#Thmexercise2", "title": "Exercise 5.2 (Reproduce Closed-Loop Transcription) .", "snippet": "Exercise 5.2 (Reproduce Closed-Loop Transcription) . Implement a closed-loop transcription pipeline for representation learning on the CIFAR-10 dataset following the methodology in Section 5.2.1 . Reference [ DTL+22 ] for useful hyperparameters and architecture settings. Reproduce the result in Figure 5.8 ."}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S0.E1", "title": "f : ùëø ‚Üí f 0 ùíÅ 0 ‚Üí ‚ãØ ‚Üí ùíÅ ‚Ñì ‚Üí f ‚Ñì ùíÅ ‚Ñì + 1 ‚Üí ‚ãØ ‚Üí ùíÅ L = ùíÅ . f\\colon\\bm{X}\\xrightarrow{\\hskip 2.84526ptf^{0}\\hskip 2.84526pt}\\bm{Z}^{0}\\rightarrow\\cdots\\rightarrow\\bm{Z}^{\\ell}\\xrightarrow{\\hskip 2.84526pt", "snippet": "f : ùëø ‚Üí f 0 ùíÅ 0 ‚Üí ‚ãØ ‚Üí ùíÅ ‚Ñì ‚Üí f ‚Ñì ùíÅ ‚Ñì + 1 ‚Üí ‚ãØ ‚Üí ùíÅ L = ùíÅ . f\\colon\\bm{X}\\xrightarrow{\\hskip 2.84526ptf^{0}\\hskip 2.84526pt}\\bm{Z}^{0}\\rightarrow\\cdots\\rightarrow\\bm{Z}^{\\ell}\\xrightarrow{\\hskip 2.84526ptf^{\\ell}\\hskip 2.84526pt}\\bm{Z}^{\\ell+1}\\rightarrow\\cdots\\to\\bm{Z}^{L}=\\bm{Z}. italic_f : bold_italic_X start_ARROW start_OVERACCENT italic_f start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT end_OVERACCENT ‚Üí end_ARROW bold_italic_Z start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT ‚Üí ‚ãØ ‚Üí bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT start_ARROW start_OVERACCENT italic_f start_POSTSUPERSCRIPT"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S0.E2", "title": "ùëø ‚Üí ‚Ñ∞ = f ùíÅ ‚Üí ùíü = g ùëø ^ \\bm{X}\\xrightarrow{\\hskip 2.84526pt\\mathcal{E}=f\\hskip 2.84526pt}\\bm{Z}\\xrightarrow{\\hskip 2.84526pt\\mathcal{D}=g\\hskip 2.84526pt}\\hat{\\bm{X}} bold_italic_X start_ARROW start_O", "snippet": "ùëø ‚Üí ‚Ñ∞ = f ùíÅ ‚Üí ùíü = g ùëø ^ \\bm{X}\\xrightarrow{\\hskip 2.84526pt\\mathcal{E}=f\\hskip 2.84526pt}\\bm{Z}\\xrightarrow{\\hskip 2.84526pt\\mathcal{D}=g\\hskip 2.84526pt}\\hat{\\bm{X}} bold_italic_X start_ARROW start_OVERACCENT caligraphic_E = italic_f end_OVERACCENT ‚Üí end_ARROW bold_italic_Z start_ARROW start_OVERACCENT caligraphic_D = italic_g end_OVERACCENT ‚Üí end_ARROW over^ start_ARG bold_italic_X end_ARG (5.0.2)"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S0.E3", "title": "d ‚Äã ( ùëø , ùëø ^ ) . d(\\bm{X},\\hat{\\bm{X}}). italic_d ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG ) . (5.0.3)", "snippet": "d ‚Äã ( ùëø , ùëø ^ ) . d(\\bm{X},\\hat{\\bm{X}}). italic_d ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG ) . (5.0.3)"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S0.E4", "title": "ùëø ‚Üí ‚Ñ∞ = f ùíÅ ‚Üí ùíü = g ùëø ^ ‚Üí ‚Ñ∞ = f ùíÅ ^ ‚Äã ? \\bm{X}\\xrightarrow{\\hskip 2.84526pt\\mathcal{E}=f\\hskip 2.84526pt}\\bm{Z}\\xrightarrow{\\hskip 2.84526pt\\mathcal{D}=g\\hskip 2.84526pt}\\hat{\\bm{X}}\\xrightarrow{\\hski", "snippet": "ùëø ‚Üí ‚Ñ∞ = f ùíÅ ‚Üí ùíü = g ùëø ^ ‚Üí ‚Ñ∞ = f ùíÅ ^ ‚Äã ? \\bm{X}\\xrightarrow{\\hskip 2.84526pt\\mathcal{E}=f\\hskip 2.84526pt}\\bm{Z}\\xrightarrow{\\hskip 2.84526pt\\mathcal{D}=g\\hskip 2.84526pt}\\hat{\\bm{X}}\\xrightarrow{\\hskip 2.84526pt\\mathcal{E}=f\\hskip 2.84526pt}\\hat{\\bm{Z}}? bold_italic_X start_ARROW start_OVERACCENT caligraphic_E = italic_f end_OVERACCENT ‚Üí end_ARROW bold_italic_Z start_ARROW start_OVERACCENT caligraphic_D = italic_g end_OVERACCENT ‚Üí end_ARROW over^ start_ARG bold_italic_X end_ARG start_ARROW start_OVERACCENT caligraphic_E = italic_f end_OVERACCENT ‚Üí end_ARROW over^ start_ARG bold_italic_Z end_AR"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.E1", "title": "ùëø ^ ‚âê g ‚Äã ( ùíÅ ) = g ‚Äã ( f ‚Äã ( ùëø ) ) \\hat{\\bm{X}}\\doteq g(\\bm{Z})=g(f(\\bm{X})) over^ start_ARG bold_italic_X end_ARG ‚âê italic_g ( bold_italic_Z ) = italic_g ( italic_f ( bold_italic_X ) ) (5.1.1)", "snippet": "ùëø ^ ‚âê g ‚Äã ( ùíÅ ) = g ‚Äã ( f ‚Äã ( ùëø ) ) \\hat{\\bm{X}}\\doteq g(\\bm{Z})=g(f(\\bm{X})) over^ start_ARG bold_italic_X end_ARG ‚âê italic_g ( bold_italic_Z ) = italic_g ( italic_f ( bold_italic_X ) ) (5.1.1)"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.E2", "title": "Œî ‚Äã R œµ ‚Äã ( ùíÅ ) \\Delta R_{\\epsilon}(\\bm{Z}) roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ) (5.1.2)", "snippet": "Œî ‚Äã R œµ ‚Äã ( ùíÅ ) \\Delta R_{\\epsilon}(\\bm{Z}) roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ) (5.1.2)"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.E3", "title": "d ‚Äã ( ùëø , ùëø ^ ) = ùîº ‚Äã [ ‚Äñ ùëø ‚àí ùëø ^ ‚Äñ 2 2 ] . d(\\bm{X},\\hat{\\bm{X}})=\\mathbb{E}[\\|\\bm{X}-\\hat{\\bm{X}}\\|_{2}^{2}]. italic_d ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG ) = blackboard_E [ ‚à• bo", "snippet": "d ‚Äã ( ùëø , ùëø ^ ) = ùîº ‚Äã [ ‚Äñ ùëø ‚àí ùëø ^ ‚Äñ 2 2 ] . d(\\bm{X},\\hat{\\bm{X}})=\\mathbb{E}[\\|\\bm{X}-\\hat{\\bm{X}}\\|_{2}^{2}]. italic_d ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG ) = blackboard_E [ ‚à• bold_italic_X - over^ start_ARG bold_italic_X end_ARG ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] . (5.1.3)"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.E4", "title": "d ‚Äã ( ùëø , ùëø ^ ) = ùíü K ‚Äã L ‚Äã ( ùëø ‚à• ùëø ^ ) . d(\\bm{X},\\hat{\\bm{X}})=\\mathcal{D}_{KL}(\\bm{X}\\|\\hat{\\bm{X}}). italic_d ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG ) = caligraphic_D start_POSTSU", "snippet": "d ‚Äã ( ùëø , ùëø ^ ) = ùíü K ‚Äã L ‚Äã ( ùëø ‚à• ùëø ^ ) . d(\\bm{X},\\hat{\\bm{X}})=\\mathcal{D}_{KL}(\\bm{X}\\|\\hat{\\bm{X}}). italic_d ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG ) = caligraphic_D start_POSTSUBSCRIPT italic_K italic_L end_POSTSUBSCRIPT ( bold_italic_X ‚à• over^ start_ARG bold_italic_X end_ARG ) . (5.1.4)"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.E5", "title": "min f , g ‚Å° [ ‚àí Œî ‚Äã R œµ ‚Äã ( ùíÅ ) + d ‚Äã ( ùëø , ùëø ^ ) ] . \\min_{f,g}[-\\Delta R_{\\epsilon}(\\bm{Z})+d(\\bm{X},\\hat{\\bm{X}})]. roman_min start_POSTSUBSCRIPT italic_f , italic_g end_POSTSUBSCRIPT [ - roman_Œî i", "snippet": "min f , g ‚Å° [ ‚àí Œî ‚Äã R œµ ‚Äã ( ùíÅ ) + d ‚Äã ( ùëø , ùëø ^ ) ] . \\min_{f,g}[-\\Delta R_{\\epsilon}(\\bm{Z})+d(\\bm{X},\\hat{\\bm{X}})]. roman_min start_POSTSUBSCRIPT italic_f , italic_g end_POSTSUBSCRIPT [ - roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ) + italic_d ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG ) ] . (5.1.5)"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.E6", "title": "ùëø ‚Üí ‚Ñ∞ = ùëº ‚ä§ ùíÅ ‚Üí ùíü = ùëº ùëø ^ , \\bm{X}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}=\\bm{U}^{\\top}\\hskip 5.69054pt}\\bm{Z}\\xrightarrow{\\hskip 5.69054pt\\mathcal{D}=\\bm{U}\\hskip 5.69054pt}\\hat{\\bm{X}}, bold_italic", "snippet": "ùëø ‚Üí ‚Ñ∞ = ùëº ‚ä§ ùíÅ ‚Üí ùíü = ùëº ùëø ^ , \\bm{X}\\xrightarrow{\\hskip 5.69054pt\\mathcal{E}=\\bm{U}^{\\top}\\hskip 5.69054pt}\\bm{Z}\\xrightarrow{\\hskip 5.69054pt\\mathcal{D}=\\bm{U}\\hskip 5.69054pt}\\hat{\\bm{X}}, bold_italic_X start_ARROW start_OVERACCENT caligraphic_E = bold_italic_U start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT end_OVERACCENT ‚Üí end_ARROW bold_italic_Z start_ARROW start_OVERACCENT caligraphic_D = bold_italic_U end_OVERACCENT ‚Üí end_ARROW over^ start_ARG bold_italic_X end_ARG , (5.1.6)"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.E7", "title": "min ùëº ‚ä§ ‚Äã ùëº = ùë∞ ‚Å° ùîº ùíô ‚Äã [ ‚Äñ ùíô ‚àí ùëº ‚Äã ùëº ‚ä§ ‚Äã ùíô ‚Äñ 2 2 ] \\min_{\\bm{U}^{\\top}\\bm{U}=\\bm{I}}\\,\\mathbb{E}_{\\bm{x}}\\left[\\left\\|\\bm{x}-\\bm{U}\\bm{U}^{\\top}\\bm{x}\\right\\|_{2}^{2}\\right] roman_min start_POSTSUBSC", "snippet": "min ùëº ‚ä§ ‚Äã ùëº = ùë∞ ‚Å° ùîº ùíô ‚Äã [ ‚Äñ ùíô ‚àí ùëº ‚Äã ùëº ‚ä§ ‚Äã ùíô ‚Äñ 2 2 ] \\min_{\\bm{U}^{\\top}\\bm{U}=\\bm{I}}\\,\\mathbb{E}_{\\bm{x}}\\left[\\left\\|\\bm{x}-\\bm{U}\\bm{U}^{\\top}\\bm{x}\\right\\|_{2}^{2}\\right] roman_min start_POSTSUBSCRIPT bold_italic_U start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_U = bold_italic_I end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ ‚à• bold_italic_x - bold_italic_U bold_italic_U start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_x ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] (5.1.7)"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.E8", "title": "Œ∑ i = ùíñ i T ‚Äã ùíô i \\eta_{i}=\\bm{u}_{i}^{T}\\bm{x}_{i} italic_Œ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_", "snippet": "Œ∑ i = ùíñ i T ‚Äã ùíô i \\eta_{i}=\\bm{u}_{i}^{T}\\bm{x}_{i} italic_Œ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = bold_italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT (5.1.8)"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.E9", "title": "ùíñ i + 1 = ùíñ i + Œ≥ ‚Äã Œ∑ i ‚Äã ùíô i ‚Äñ ùíñ i + Œ≥ ‚Äã Œ∑ i ‚Äã ùíô i ‚Äñ 2 \\bm{u}_{i+1}=\\frac{\\bm{u}_{i}+\\gamma\\eta_{i}\\bm{x}_{i}}{\\|\\bm{u}_{i}+\\gamma\\eta_{i}\\bm{x}_{i}\\|_{2}} bold_italic_u start_POSTSUBSCRIPT italic_i ", "snippet": "ùíñ i + 1 = ùíñ i + Œ≥ ‚Äã Œ∑ i ‚Äã ùíô i ‚Äñ ùíñ i + Œ≥ ‚Äã Œ∑ i ‚Äã ùíô i ‚Äñ 2 \\bm{u}_{i+1}=\\frac{\\bm{u}_{i}+\\gamma\\eta_{i}\\bm{x}_{i}}{\\|\\bm{u}_{i}+\\gamma\\eta_{i}\\bm{x}_{i}\\|_{2}} bold_italic_u start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT = divide start_ARG bold_italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_Œ≥ italic_Œ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG start_ARG ‚à• bold_italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_Œ≥ italic_Œ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT bold_italic_x start_PO"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.E10", "title": "ùíõ = ùëæ 2 ‚Äã œÉ ‚Äã ( ùëæ 1 ‚Äã ùíô + ùíÉ ) , \\bm{z}=\\bm{W}_{2}\\sigma(\\bm{W}_{1}\\bm{x}+\\bm{b}), bold_italic_z = bold_italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_œÉ ( bold_italic_W start_POSTSUBSCRIPT 1 e", "snippet": "ùíõ = ùëæ 2 ‚Äã œÉ ‚Äã ( ùëæ 1 ‚Äã ùíô + ùíÉ ) , \\bm{z}=\\bm{W}_{2}\\sigma(\\bm{W}_{1}\\bm{x}+\\bm{b}), bold_italic_z = bold_italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_œÉ ( bold_italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_italic_x + bold_italic_b ) , (5.1.10)"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.E11", "title": "œÉ ‚Äã ( x ) = 1 1 + e ‚àí x . \\sigma(x)=\\frac{1}{1+e^{-x}}. italic_œÉ ( italic_x ) = divide start_ARG 1 end_ARG start_ARG 1 + italic_e start_POSTSUPERSCRIPT - italic_x end_POSTSUPERSCRIPT end_ARG . (5.1.11", "snippet": "œÉ ‚Äã ( x ) = 1 1 + e ‚àí x . \\sigma(x)=\\frac{1}{1+e^{-x}}. italic_œÉ ( italic_x ) = divide start_ARG 1 end_ARG start_ARG 1 + italic_e start_POSTSUPERSCRIPT - italic_x end_POSTSUPERSCRIPT end_ARG . (5.1.11)"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.E12", "title": "min ùúΩ ‚Å° ùîº ‚Äã [ ‚Äñ ùíô ‚àí ùíô ^ ‚Äã ( ùúΩ ) ‚Äñ 2 2 ] . \\min_{\\bm{\\theta}}\\mathbb{E}[\\|\\bm{x}-\\hat{\\bm{x}}(\\bm{\\theta})\\|_{2}^{2}]. roman_min start_POSTSUBSCRIPT bold_italic_Œ∏ end_POSTSUBSCRIPT blackboard_E [ ‚à• bol", "snippet": "min ùúΩ ‚Å° ùîº ‚Äã [ ‚Äñ ùíô ‚àí ùíô ^ ‚Äã ( ùúΩ ) ‚Äñ 2 2 ] . \\min_{\\bm{\\theta}}\\mathbb{E}[\\|\\bm{x}-\\hat{\\bm{x}}(\\bm{\\theta})\\|_{2}^{2}]. roman_min start_POSTSUBSCRIPT bold_italic_Œ∏ end_POSTSUBSCRIPT blackboard_E [ ‚à• bold_italic_x - over^ start_ARG bold_italic_x end_ARG ( bold_italic_Œ∏ ) ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] . (5.1.12)"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.E13", "title": "min f , g ‚Å° [ ‚Äñ ùíÅ ‚Äñ 0 ‚àí Œî ‚Äã R œµ ‚Äã ( ùíÅ ) + d ‚Äã ( ùëø , ùëø ^ ) ] , \\min_{f,g}[\\|\\bm{Z}\\|_{0}-\\Delta R_{\\epsilon}(\\bm{Z})+d(\\bm{X},\\hat{\\bm{X}})], roman_min start_POSTSUBSCRIPT italic_f , italic_g end_POSTS", "snippet": "min f , g ‚Å° [ ‚Äñ ùíÅ ‚Äñ 0 ‚àí Œî ‚Äã R œµ ‚Äã ( ùíÅ ) + d ‚Äã ( ùëø , ùëø ^ ) ] , \\min_{f,g}[\\|\\bm{Z}\\|_{0}-\\Delta R_{\\epsilon}(\\bm{Z})+d(\\bm{X},\\hat{\\bm{X}})], roman_min start_POSTSUBSCRIPT italic_f , italic_g end_POSTSUBSCRIPT [ ‚à• bold_italic_Z ‚à• start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT - roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ) + italic_d ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG ) ] , (5.1.13)"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.Ex1", "title": "max ùúΩ ‚Å° ùîº ùíô ‚Äã [ log ‚Å° p ‚Äã ( ùíô ; ùúΩ ) ] . \\max_{\\bm{\\theta}}\\,\\mathbb{E}_{\\bm{x}}[\\log p(\\bm{x};\\,\\bm{\\theta})]. roman_max start_POSTSUBSCRIPT bold_italic_Œ∏ end_POSTSUBSCRIPT blackboard_E start_POSTSUBS", "snippet": "max ùúΩ ‚Å° ùîº ùíô ‚Äã [ log ‚Å° p ‚Äã ( ùíô ; ùúΩ ) ] . \\max_{\\bm{\\theta}}\\,\\mathbb{E}_{\\bm{x}}[\\log p(\\bm{x};\\,\\bm{\\theta})]. roman_max start_POSTSUBSCRIPT bold_italic_Œ∏ end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ roman_log italic_p ( bold_italic_x ; bold_italic_Œ∏ ) ] ."}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.Ex6", "title": "ùíõ ‚à£ ùíô ‚àº ùí© ‚Äã ( f 1 ‚Äã ( ùíô ) , diag ‚Å° ( e f 2 ‚Äã ( ùíô ) ) ‚Äã ùë∞ ) . \\bm{z}\\mid\\bm{x}\\sim\\mathcal{N}(f_{1}(\\bm{x}),\\operatorname{diag}(e^{f_{2}(\\bm{x})})\\bm{I}). bold_italic_z ‚à£ bold_italic_x ‚àº caligraphic_N ", "snippet": "ùíõ ‚à£ ùíô ‚àº ùí© ‚Äã ( f 1 ‚Äã ( ùíô ) , diag ‚Å° ( e f 2 ‚Äã ( ùíô ) ) ‚Äã ùë∞ ) . \\bm{z}\\mid\\bm{x}\\sim\\mathcal{N}(f_{1}(\\bm{x}),\\operatorname{diag}(e^{f_{2}(\\bm{x})})\\bm{I}). bold_italic_z ‚à£ bold_italic_x ‚àº caligraphic_N ( italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( bold_italic_x ) , roman_diag ( italic_e start_POSTSUPERSCRIPT italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( bold_italic_x ) end_POSTSUPERSCRIPT ) bold_italic_I ) ."}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.Ex9", "title": "log ‚Å° p ‚Äã ( ùíô ; ùúΩ ) ‚â• ùîº ùíõ ‚àº q ( ‚ãÖ ‚à£ ùíô ; ùúº ) ‚Äã [ log ‚Å° p ‚Äã ( ùíô , ùíõ ; ùúΩ ) q ‚Äã ( ùíõ ‚à£ ùíô ; ùúº ) ] . \\log p(\\bm{x};\\,\\bm{\\theta})\\geq\\mathbb{E}_{\\bm{z}\\sim q(\\,\\cdot\\,\\mid\\bm{x};\\,\\bm{\\eta})}\\left[\\log\\frac{", "snippet": "log ‚Å° p ‚Äã ( ùíô ; ùúΩ ) ‚â• ùîº ùíõ ‚àº q ( ‚ãÖ ‚à£ ùíô ; ùúº ) ‚Äã [ log ‚Å° p ‚Äã ( ùíô , ùíõ ; ùúΩ ) q ‚Äã ( ùíõ ‚à£ ùíô ; ùúº ) ] . \\log p(\\bm{x};\\,\\bm{\\theta})\\geq\\mathbb{E}_{\\bm{z}\\sim q(\\,\\cdot\\,\\mid\\bm{x};\\,\\bm{\\eta})}\\left[\\log\\frac{p(\\bm{x},\\bm{z};\\,\\bm{\\theta})}{q(\\bm{z}\\mid\\bm{x};\\,\\bm{\\eta})}\\right]. roman_log italic_p ( bold_italic_x ; bold_italic_Œ∏ ) ‚â• blackboard_E start_POSTSUBSCRIPT bold_italic_z ‚àº italic_q ( ‚ãÖ ‚à£ bold_italic_x ; bold_italic_Œ∑ ) end_POSTSUBSCRIPT [ roman_log divide start_ARG italic_p ( bold_italic_x , bold_italic_z ; bold_italic_Œ∏ ) end_ARG start_ARG italic_q ( bold_italic_z ‚à£ bold_italic_x ; bold_ital"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.E14", "title": "max ùúΩ , ùúº ‚Å° ùîº ùíô ‚Äã ùîº ùíõ ‚àº q ( ‚ãÖ ‚à£ ùíô ; ùúº ) ‚Äã [ log ‚Å° p ‚Äã ( ùíô , ùíõ ; ùúΩ ) q ‚Äã ( ùíõ ‚à£ ùíô ; ùúº ) ] . \\max_{\\bm{\\theta},\\bm{\\eta}}\\,\\mathbb{E}_{\\bm{x}}\\mathbb{E}_{\\bm{z}\\sim q(\\,\\cdot\\,\\mid\\bm{x};\\,\\bm{\\eta})}\\le", "snippet": "max ùúΩ , ùúº ‚Å° ùîº ùíô ‚Äã ùîº ùíõ ‚àº q ( ‚ãÖ ‚à£ ùíô ; ùúº ) ‚Äã [ log ‚Å° p ‚Äã ( ùíô , ùíõ ; ùúΩ ) q ‚Äã ( ùíõ ‚à£ ùíô ; ùúº ) ] . \\max_{\\bm{\\theta},\\bm{\\eta}}\\,\\mathbb{E}_{\\bm{x}}\\mathbb{E}_{\\bm{z}\\sim q(\\,\\cdot\\,\\mid\\bm{x};\\,\\bm{\\eta})}\\left[\\log\\frac{p(\\bm{x},\\bm{z};\\,\\bm{\\theta})}{q(\\bm{z}\\mid\\bm{x};\\,\\bm{\\eta})}\\right]. roman_max start_POSTSUBSCRIPT bold_italic_Œ∏ , bold_italic_Œ∑ end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT bold_italic_z ‚àº italic_q ( ‚ãÖ ‚à£ bold_italic_x ; bold_italic_Œ∑ ) end_POSTSUBSCRIPT [ roman_log divide start_ARG italic_p ( bold_italic_x , b"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.Ex16", "title": "ùíõ = d f 1 ‚Äã ( ùíô ) + diag ‚Å° ( e 1 2 ‚Äã f 2 ‚Äã ( ùíô ) ) ‚Äã ùíà , ùíà ‚àº ùí© ‚Äã ( 0 , ùë∞ ) , \\bm{z}=_{\\mathrm{d}}f_{1}(\\bm{x})+\\operatorname{diag}(e^{\\tfrac{1}{2}f_{2}(\\bm{x})})\\bm{g},\\quad\\bm{g}\\sim\\mathcal{N}(0,\\bm", "snippet": "ùíõ = d f 1 ‚Äã ( ùíô ) + diag ‚Å° ( e 1 2 ‚Äã f 2 ‚Äã ( ùíô ) ) ‚Äã ùíà , ùíà ‚àº ùí© ‚Äã ( 0 , ùë∞ ) , \\bm{z}=_{\\mathrm{d}}f_{1}(\\bm{x})+\\operatorname{diag}(e^{\\tfrac{1}{2}f_{2}(\\bm{x})})\\bm{g},\\quad\\bm{g}\\sim\\mathcal{N}(0,\\bm{I}), bold_italic_z = start_POSTSUBSCRIPT roman_d end_POSTSUBSCRIPT italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( bold_italic_x ) + roman_diag ( italic_e start_POSTSUPERSCRIPT divide start_ARG 1 end_ARG start_ARG 2 end_ARG italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( bold_italic_x ) end_POSTSUPERSCRIPT ) bold_italic_g , bold_italic_g ‚àº caligraphic_N ( 0 , bold_italic_I ) ,"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.E1", "title": "computable ‚üπ tractable ‚üπ scalable . \\mbox{{computable}}\\;\\Longrightarrow\\;\\mbox{{tractable}}\\;\\Longrightarrow\\;\\mbox{{scalable}}. computable ‚üπ tractable ‚üπ scalable . (5.2.1)", "snippet": "computable ‚üπ tractable ‚üπ scalable . \\mbox{{computable}}\\;\\Longrightarrow\\;\\mbox{{tractable}}\\;\\Longrightarrow\\;\\mbox{{scalable}}. computable ‚üπ tractable ‚üπ scalable . (5.2.1)"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.E2", "title": "ùëø ‚Üí ‚Ñ∞ = f ùíÅ ‚Üí ùíü = g ùëø ^ . \\bm{X}\\xrightarrow{\\hskip 2.84526pt\\mathcal{E}=f\\hskip 2.84526pt}\\bm{Z}\\xrightarrow{\\hskip 2.84526pt\\mathcal{D}=g\\hskip 2.84526pt}\\hat{\\bm{X}}. bold_italic_X start_ARROW star", "snippet": "ùëø ‚Üí ‚Ñ∞ = f ùíÅ ‚Üí ùíü = g ùëø ^ . \\bm{X}\\xrightarrow{\\hskip 2.84526pt\\mathcal{E}=f\\hskip 2.84526pt}\\bm{Z}\\xrightarrow{\\hskip 2.84526pt\\mathcal{D}=g\\hskip 2.84526pt}\\hat{\\bm{X}}. bold_italic_X start_ARROW start_OVERACCENT caligraphic_E = italic_f end_OVERACCENT ‚Üí end_ARROW bold_italic_Z start_ARROW start_OVERACCENT caligraphic_D = italic_g end_OVERACCENT ‚Üí end_ARROW over^ start_ARG bold_italic_X end_ARG . (5.2.2)"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.E3", "title": "ùíÅ ‚Üí g ‚Äã ( ùíõ , Œ∑ ) ùëø ^ , ùëø ‚Üí d ‚Äã ( ùíô , Œ∏ ) { ùüé , ùüè } . \\bm{Z}\\xrightarrow{\\hskip 5.69054ptg(\\bm{z},\\eta)\\hskip 5.69054pt}\\hat{\\bm{X}},\\,\\bm{X}\\xrightarrow{\\hskip 5.69054ptd(\\bm{x},\\theta)\\hskip 5.69054", "snippet": "ùíÅ ‚Üí g ‚Äã ( ùíõ , Œ∑ ) ùëø ^ , ùëø ‚Üí d ‚Äã ( ùíô , Œ∏ ) { ùüé , ùüè } . \\bm{Z}\\xrightarrow{\\hskip 5.69054ptg(\\bm{z},\\eta)\\hskip 5.69054pt}\\hat{\\bm{X}},\\,\\bm{X}\\xrightarrow{\\hskip 5.69054ptd(\\bm{x},\\theta)\\hskip 5.69054pt}\\{\\mathbf{0},\\mathbf{1}\\}. bold_italic_Z start_ARROW start_OVERACCENT italic_g ( bold_italic_z , italic_Œ∑ ) end_OVERACCENT ‚Üí end_ARROW over^ start_ARG bold_italic_X end_ARG , bold_italic_X start_ARROW start_OVERACCENT italic_d ( bold_italic_x , italic_Œ∏ ) end_OVERACCENT ‚Üí end_ARROW { bold_0 , bold_1 } . (5.2.3)"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.E4", "title": "max Œ∏ ‚Å° min Œ∑ ‚Å° ùîº p ‚Äã ( ùíô ) ‚Äã [ log ‚Å° d ‚Äã ( ùíô , Œ∏ ) ] + ùîº p ‚Äã ( ùíõ ) ‚Äã [ 1 ‚àí log ‚Å° d ‚Äã ( g ‚Äã ( ùíõ , Œ∑ ) ‚èü ùíô ^ ‚àº p g , Œ∏ ) ] . \\max_{\\theta}\\min_{\\eta}\\mathbb{E}_{p({\\bm{x}})}\\big{[}\\log d(\\bm{x},\\theta)", "snippet": "max Œ∏ ‚Å° min Œ∑ ‚Å° ùîº p ‚Äã ( ùíô ) ‚Äã [ log ‚Å° d ‚Äã ( ùíô , Œ∏ ) ] + ùîº p ‚Äã ( ùíõ ) ‚Äã [ 1 ‚àí log ‚Å° d ‚Äã ( g ‚Äã ( ùíõ , Œ∑ ) ‚èü ùíô ^ ‚àº p g , Œ∏ ) ] . \\max_{\\theta}\\min_{\\eta}\\mathbb{E}_{p({\\bm{x}})}\\big{[}\\log d(\\bm{x},\\theta)\\big{]}+\\mathbb{E}_{p({\\bm{z}})}\\big{[}1-\\log d(\\underbrace{g(\\bm{z},\\eta)}_{\\hat{\\bm{x}}\\,\\sim\\,p_{g}},\\theta)\\big{]}. roman_max start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT italic_Œ∑ end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_p ( bold_italic_x ) end_POSTSUBSCRIPT [ roman_log italic_d ( bold_italic_x , italic_Œ∏ ) ] + blackboard_E start_POSTSUBSCRIPT i"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.E5", "title": "ùíü J ‚Äã S ‚Äã ( p ‚Äã ( ùíô ) , p g ‚Äã ( ùíô ^ ) ) = ùíü K ‚Äã L ‚Äã ( p ‚à• ( p + p g ) / 2 ) + ùíü K ‚Äã L ‚Äã ( p g ‚à• ( p + p g ) / 2 ) . \\mathcal{D}_{JS}(p(\\bm{x}),p_{g}(\\hat{\\bm{x}}))=\\mathcal{D}_{KL}\\big{(}p\\|(p+p_{g})/", "snippet": "ùíü J ‚Äã S ‚Äã ( p ‚Äã ( ùíô ) , p g ‚Äã ( ùíô ^ ) ) = ùíü K ‚Äã L ‚Äã ( p ‚à• ( p + p g ) / 2 ) + ùíü K ‚Äã L ‚Äã ( p g ‚à• ( p + p g ) / 2 ) . \\mathcal{D}_{JS}(p(\\bm{x}),p_{g}(\\hat{\\bm{x}}))=\\mathcal{D}_{KL}\\big{(}p\\|(p+p_{g})/{2}\\big{)}+\\mathcal{D}_{KL}\\big{(}p_{g}\\|(p+p_{g})/{2}\\big{)}. caligraphic_D start_POSTSUBSCRIPT italic_J italic_S end_POSTSUBSCRIPT ( italic_p ( bold_italic_x ) , italic_p start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ( over^ start_ARG bold_italic_x end_ARG ) ) = caligraphic_D start_POSTSUBSCRIPT italic_K italic_L end_POSTSUBSCRIPT ( italic_p ‚à• ( italic_p + italic_p start_POSTSUBSCRIPT italic_g "}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.E6", "title": "ùëø ‚Üí ‚Ñ∞ = f ùíÅ ‚Üí ùíü = g ùëø ^ ‚Üí ‚Ñ∞ = f ùíÅ ^ ‚Äã ? \\bm{X}\\xrightarrow{\\hskip 2.84526pt\\mathcal{E}=f\\hskip 2.84526pt}\\bm{Z}\\xrightarrow{\\hskip 2.84526pt\\mathcal{D}=g\\hskip 2.84526pt}\\hat{\\bm{X}}\\xrightarrow{\\hski", "snippet": "ùëø ‚Üí ‚Ñ∞ = f ùíÅ ‚Üí ùíü = g ùëø ^ ‚Üí ‚Ñ∞ = f ùíÅ ^ ‚Äã ? \\bm{X}\\xrightarrow{\\hskip 2.84526pt\\mathcal{E}=f\\hskip 2.84526pt}\\bm{Z}\\xrightarrow{\\hskip 2.84526pt\\mathcal{D}=g\\hskip 2.84526pt}\\hat{\\bm{X}}\\xrightarrow{\\hskip 2.84526pt\\mathcal{E}=f\\hskip 2.84526pt}\\hat{\\bm{Z}}? bold_italic_X start_ARROW start_OVERACCENT caligraphic_E = italic_f end_OVERACCENT ‚Üí end_ARROW bold_italic_Z start_ARROW start_OVERACCENT caligraphic_D = italic_g end_OVERACCENT ‚Üí end_ARROW over^ start_ARG bold_italic_X end_ARG start_ARROW start_OVERACCENT caligraphic_E = italic_f end_OVERACCENT ‚Üí end_ARROW over^ start_ARG bold_italic_Z end_AR"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.E7", "title": "ùëø ‚Üí f ‚Äã ( ùíô , ùúΩ ) ùíÅ , \\bm{X}\\xrightarrow{\\hskip 5.69054ptf(\\bm{x},\\bm{\\theta})\\hskip 5.69054pt}\\bm{Z}, bold_italic_X start_ARROW start_OVERACCENT italic_f ( bold_italic_x , bold_italic_Œ∏ ) end_OVERACC", "snippet": "ùëø ‚Üí f ‚Äã ( ùíô , ùúΩ ) ùíÅ , \\bm{X}\\xrightarrow{\\hskip 5.69054ptf(\\bm{x},\\bm{\\theta})\\hskip 5.69054pt}\\bm{Z}, bold_italic_X start_ARROW start_OVERACCENT italic_f ( bold_italic_x , bold_italic_Œ∏ ) end_OVERACCENT ‚Üí end_ARROW bold_italic_Z , (5.2.7)"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.E8", "title": "max ùíÅ ‚Å° Œî ‚Äã R œµ ‚Äã ( ùíÅ ‚à£ ùö∑ ) ‚âê 1 2 ‚Äã log ‚Äã det ( ùë∞ + Œ± ‚Äã ùíÅ ‚Äã ùíÅ ‚ä§ ) ‚èü R œµ ‚Äã ( ùíÅ ) ‚àí ‚àë k = 1 K Œ≥ k 2 ‚Äã log ‚Äã det ( ùë∞ + Œ± k ‚Äã ùíÅ ‚Äã ùö∑ j ‚Äã ùíÅ ‚ä§ ) ‚èü R œµ c ‚Äã ( ùíÅ ‚à£ ùö∑ ) , \\begin{split}\\max_{\\bm{Z}}\\;\\Delta R_{\\e", "snippet": "max ùíÅ ‚Å° Œî ‚Äã R œµ ‚Äã ( ùíÅ ‚à£ ùö∑ ) ‚âê 1 2 ‚Äã log ‚Äã det ( ùë∞ + Œ± ‚Äã ùíÅ ‚Äã ùíÅ ‚ä§ ) ‚èü R œµ ‚Äã ( ùíÅ ) ‚àí ‚àë k = 1 K Œ≥ k 2 ‚Äã log ‚Äã det ( ùë∞ + Œ± k ‚Äã ùíÅ ‚Äã ùö∑ j ‚Äã ùíÅ ‚ä§ ) ‚èü R œµ c ‚Äã ( ùíÅ ‚à£ ùö∑ ) , \\begin{split}\\max_{\\bm{Z}}\\;\\Delta R_{\\epsilon}(\\bm{Z}\\mid\\bm{\\Pi})&\\doteq\\underbrace{\\frac{1}{2}\\log\\det\\Big{(}\\bm{I}+{\\alpha}\\bm{Z}\\bm{Z}^{\\top}\\Big{)}}_{R_{\\epsilon}(\\bm{Z})}\\;-\\;\\underbrace{\\sum_{k=1}^{K}\\frac{\\gamma_{k}}{2}\\log\\det\\Big{(}\\bm{I}+{\\alpha_{k}}\\bm{Z}\\bm{\\Pi}^{j}\\bm{Z}^{\\top}\\Big{)}}_{R^{c}_{\\epsilon}(\\bm{Z}\\mid\\bm{\\Pi})},\\end{split} start_ROW start_CELL roman_max start_POSTSUBSCRIPT bold_italic_Z end_POSTSUBSCRIPT roma"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.E9", "title": "ùëø ‚Üí f ‚Äã ( ùíô , ùúΩ ) ùíÅ ‚Üí g ‚Äã ( ùíõ , ùúº ) ùëø ^ , \\bm{X}\\xrightarrow{\\hskip 5.69054ptf(\\bm{x},\\bm{\\theta})\\hskip 5.69054pt}\\bm{Z}\\xrightarrow{\\hskip 5.69054ptg(\\bm{z},\\bm{\\eta})\\hskip 5.69054pt}\\hat{\\bm{X}}, ", "snippet": "ùëø ‚Üí f ‚Äã ( ùíô , ùúΩ ) ùíÅ ‚Üí g ‚Äã ( ùíõ , ùúº ) ùëø ^ , \\bm{X}\\xrightarrow{\\hskip 5.69054ptf(\\bm{x},\\bm{\\theta})\\hskip 5.69054pt}\\bm{Z}\\xrightarrow{\\hskip 5.69054ptg(\\bm{z},\\bm{\\eta})\\hskip 5.69054pt}\\hat{\\bm{X}}, bold_italic_X start_ARROW start_OVERACCENT italic_f ( bold_italic_x , bold_italic_Œ∏ ) end_OVERACCENT ‚Üí end_ARROW bold_italic_Z start_ARROW start_OVERACCENT italic_g ( bold_italic_z , bold_italic_Œ∑ ) end_OVERACCENT ‚Üí end_ARROW over^ start_ARG bold_italic_X end_ARG , (5.2.9)"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.E10", "title": "Œî ‚Äã R œµ ‚Äã ( ùíÅ k , ùíÅ ^ k ) ‚âê R œµ ‚Äã ( ùíÅ k ‚à™ ùíÅ ^ k ) ‚àí 1 2 ‚Äã ( R œµ ‚Äã ( ùíÅ k ) + R œµ ‚Äã ( ùíÅ ^ k ) ) . \\Delta R_{\\epsilon}\\big{(}\\bm{Z}_{k},\\hat{\\bm{Z}}_{k}\\big{)}\\doteq R_{\\epsilon}\\big{(}\\bm{Z}_{k}\\cup\\hat", "snippet": "Œî ‚Äã R œµ ‚Äã ( ùíÅ k , ùíÅ ^ k ) ‚âê R œµ ‚Äã ( ùíÅ k ‚à™ ùíÅ ^ k ) ‚àí 1 2 ‚Äã ( R œµ ‚Äã ( ùíÅ k ) + R œµ ‚Äã ( ùíÅ ^ k ) ) . \\Delta R_{\\epsilon}\\big{(}\\bm{Z}_{k},\\hat{\\bm{Z}}_{k}\\big{)}\\doteq R_{\\epsilon}\\big{(}\\bm{Z}_{k}\\cup\\hat{\\bm{Z}}_{k}\\big{)}-\\frac{1}{2}\\big{(}R_{\\epsilon}\\big{(}\\bm{Z}_{k})+R_{\\epsilon}\\big{(}\\hat{\\bm{Z}}_{k})\\big{)}. roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , over^ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) ‚âê italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_itali"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.E11", "title": "d ‚Äã ( ùíÅ , ùíÅ ^ ) ‚âê ‚àë k = 1 K Œî ‚Äã R œµ ‚Äã ( ùíÅ k , ùíÅ ^ k ) = ‚àë k = 1 K Œî ‚Äã R œµ ‚Äã ( ùíÅ k , f ‚Äã ( g ‚Äã ( ùíÅ k , ùúº ) , ùúΩ ) ) . d(\\bm{Z},\\hat{\\bm{Z}})\\doteq\\sum_{k=1}^{K}\\Delta R_{\\epsilon}\\big{(}\\bm{Z}_{k},\\hat{", "snippet": "d ‚Äã ( ùíÅ , ùíÅ ^ ) ‚âê ‚àë k = 1 K Œî ‚Äã R œµ ‚Äã ( ùíÅ k , ùíÅ ^ k ) = ‚àë k = 1 K Œî ‚Äã R œµ ‚Äã ( ùíÅ k , f ‚Äã ( g ‚Äã ( ùíÅ k , ùúº ) , ùúΩ ) ) . d(\\bm{Z},\\hat{\\bm{Z}})\\doteq\\sum_{k=1}^{K}\\Delta R_{\\epsilon}\\big{(}\\bm{Z}_{k},\\hat{\\bm{Z}}_{k}\\big{)}=\\sum_{k=1}^{K}\\Delta R_{\\epsilon}\\big{(}\\bm{Z}_{k},f(g(\\bm{Z}_{k},\\bm{\\eta}),\\bm{\\theta})\\big{)}. italic_d ( bold_italic_Z , over^ start_ARG bold_italic_Z end_ARG ) ‚âê ‚àë start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_k e"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.E12", "title": "max f ‚Å° d ‚Äã ( ùíÅ , ùíÅ ^ ) = max ùúΩ ‚Äã ‚àë k = 1 K Œî ‚Äã R œµ ‚Äã ( ùíÅ k , ùíÅ ^ k ) = max ùúΩ ‚Äã ‚àë k = 1 K Œî ‚Äã R œµ ‚Äã ( f ‚Äã ( ùëø k , ùúΩ ) , f ‚Äã ( ùëø ^ k , ùúΩ ) ) . \\max_{f}d(\\bm{Z},\\hat{\\bm{Z}})=\\max_{\\bm{\\theta}}\\sum_{k=1", "snippet": "max f ‚Å° d ‚Äã ( ùíÅ , ùíÅ ^ ) = max ùúΩ ‚Äã ‚àë k = 1 K Œî ‚Äã R œµ ‚Äã ( ùíÅ k , ùíÅ ^ k ) = max ùúΩ ‚Äã ‚àë k = 1 K Œî ‚Äã R œµ ‚Äã ( f ‚Äã ( ùëø k , ùúΩ ) , f ‚Äã ( ùëø ^ k , ùúΩ ) ) . \\max_{f}d(\\bm{Z},\\hat{\\bm{Z}})=\\max_{\\bm{\\theta}}\\sum_{k=1}^{K}\\Delta R_{\\epsilon}\\big{(}\\bm{Z}_{k},\\hat{\\bm{Z}}_{k}\\big{)}=\\max_{\\bm{\\theta}}\\sum_{k=1}^{K}\\Delta R_{\\epsilon}\\big{(}f(\\bm{X}_{k},\\bm{\\theta}),f(\\hat{\\bm{X}}_{k},\\bm{\\theta})\\big{)}. roman_max start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT italic_d ( bold_italic_Z , over^ start_ARG bold_italic_Z end_ARG ) = roman_max start_POSTSUBSCRIPT bold_italic_Œ∏ end_POSTSUBSCRIPT ‚àë start_POSTSUBSCRIPT "}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.E13", "title": "ùíÅ ‚Üí g ‚Äã ( ùíõ , ùúº ) ùëø ^ , ùëø ‚Üí f ‚Äã ( ùíô , ùúΩ ) { ùíÅ ^ , ùíÅ } . \\bm{Z}\\xrightarrow{\\hskip 5.69054ptg(\\bm{z},\\bm{\\eta})\\hskip 5.69054pt}\\hat{\\bm{X}},\\,\\bm{X}\\xrightarrow{\\hskip 5.69054ptf(\\bm{x},\\bm{\\theta})\\h", "snippet": "ùíÅ ‚Üí g ‚Äã ( ùíõ , ùúº ) ùëø ^ , ùëø ‚Üí f ‚Äã ( ùíô , ùúΩ ) { ùíÅ ^ , ùíÅ } . \\bm{Z}\\xrightarrow{\\hskip 5.69054ptg(\\bm{z},\\bm{\\eta})\\hskip 5.69054pt}\\hat{\\bm{X}},\\,\\bm{X}\\xrightarrow{\\hskip 5.69054ptf(\\bm{x},\\bm{\\theta})\\hskip 5.69054pt}\\{\\hat{\\bm{Z}},\\bm{Z}\\}. bold_italic_Z start_ARROW start_OVERACCENT italic_g ( bold_italic_z , bold_italic_Œ∑ ) end_OVERACCENT ‚Üí end_ARROW over^ start_ARG bold_italic_X end_ARG , bold_italic_X start_ARROW start_OVERACCENT italic_f ( bold_italic_x , bold_italic_Œ∏ ) end_OVERACCENT ‚Üí end_ARROW { over^ start_ARG bold_italic_Z end_ARG , bold_italic_Z } . (5.2.13)"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.E14", "title": "ùëø ‚Üí f ‚Äã ( ùíô , ùúΩ ) ùíÅ ‚Üí g ‚Äã ( ùíõ , ùúº ) ùëø ^ ‚Üí f ‚Äã ( ùíô , ùúΩ ) ùíÅ ^ , \\bm{X}\\xrightarrow{\\hskip 5.69054ptf(\\bm{x},\\bm{\\theta})\\hskip 5.69054pt}\\bm{Z}\\xrightarrow{\\hskip 5.69054ptg(\\bm{z},\\bm{\\eta})\\hskip 5.69", "snippet": "ùëø ‚Üí f ‚Äã ( ùíô , ùúΩ ) ùíÅ ‚Üí g ‚Äã ( ùíõ , ùúº ) ùëø ^ ‚Üí f ‚Äã ( ùíô , ùúΩ ) ùíÅ ^ , \\bm{X}\\xrightarrow{\\hskip 5.69054ptf(\\bm{x},\\bm{\\theta})\\hskip 5.69054pt}\\bm{Z}\\xrightarrow{\\hskip 5.69054ptg(\\bm{z},\\bm{\\eta})\\hskip 5.69054pt}\\hat{\\bm{X}}\\xrightarrow{\\hskip 5.69054ptf(\\bm{x},\\bm{\\theta})\\hskip 5.69054pt}\\ \\hat{\\bm{Z}}, bold_italic_X start_ARROW start_OVERACCENT italic_f ( bold_italic_x , bold_italic_Œ∏ ) end_OVERACCENT ‚Üí end_ARROW bold_italic_Z start_ARROW start_OVERACCENT italic_g ( bold_italic_z , bold_italic_Œ∑ ) end_OVERACCENT ‚Üí end_ARROW over^ start_ARG bold_italic_X end_ARG start_ARROW start_OVERACCENT italic"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.E15", "title": "min g ‚Å° d ‚Äã ( ùíÅ , ùíÅ ^ ) ‚âê min Œ∑ ‚Äã ‚àë k = 1 K Œî ‚Äã R ‚Äã ( ùíÅ k , ùíÅ ^ k ) = min ùúº ‚Äã ‚àë k = 1 K Œî ‚Äã R ‚Äã ( ùíÅ k , f ‚Äã ( g ‚Äã ( ùíÅ k , ùúº ) , ùúΩ ) ) , \\min_{g}d(\\bm{Z},\\hat{\\bm{Z}})\\doteq\\min_{\\eta}\\sum_{k=1}^{K}\\De", "snippet": "min g ‚Å° d ‚Äã ( ùíÅ , ùíÅ ^ ) ‚âê min Œ∑ ‚Äã ‚àë k = 1 K Œî ‚Äã R ‚Äã ( ùíÅ k , ùíÅ ^ k ) = min ùúº ‚Äã ‚àë k = 1 K Œî ‚Äã R ‚Äã ( ùíÅ k , f ‚Äã ( g ‚Äã ( ùíÅ k , ùúº ) , ùúΩ ) ) , \\min_{g}d(\\bm{Z},\\hat{\\bm{Z}})\\doteq\\min_{\\eta}\\sum_{k=1}^{K}\\Delta R\\big{(}\\bm{Z}_{k},\\hat{\\bm{Z}}_{k}\\big{)}=\\min_{\\bm{\\eta}}\\sum_{k=1}^{K}\\Delta R\\big{(}\\bm{Z}_{k},f(g(\\bm{Z}_{k},\\bm{\\eta}),\\bm{\\theta})\\big{)}, roman_min start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT italic_d ( bold_italic_Z , over^ start_ARG bold_italic_Z end_ARG ) ‚âê roman_min start_POSTSUBSCRIPT italic_Œ∑ end_POSTSUBSCRIPT ‚àë start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPE"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.E16", "title": "h ‚Äã ( ùíô , ùúΩ , ùúº ) ‚âê f ‚Äã ( g ‚Äã ( f ‚Äã ( ùíô , ùúΩ ) , ùúº ) , ùúΩ ) : ùíô ‚Ü¶ ùíõ ^ . h(\\bm{x},\\bm{\\theta},\\bm{\\eta})\\doteq f\\big{(}g\\big{(}f(\\bm{x},\\bm{\\theta}),\\bm{\\eta}\\big{)},\\bm{\\theta}\\big{)}:\\;\\bm{x}\\mapsto\\ha", "snippet": "h ‚Äã ( ùíô , ùúΩ , ùúº ) ‚âê f ‚Äã ( g ‚Äã ( f ‚Äã ( ùíô , ùúΩ ) , ùúº ) , ùúΩ ) : ùíô ‚Ü¶ ùíõ ^ . h(\\bm{x},\\bm{\\theta},\\bm{\\eta})\\doteq f\\big{(}g\\big{(}f(\\bm{x},\\bm{\\theta}),\\bm{\\eta}\\big{)},\\bm{\\theta}\\big{)}:\\;\\bm{x}\\mapsto\\hat{\\bm{z}}. italic_h ( bold_italic_x , bold_italic_Œ∏ , bold_italic_Œ∑ ) ‚âê italic_f ( italic_g ( italic_f ( bold_italic_x , bold_italic_Œ∏ ) , bold_italic_Œ∑ ) , bold_italic_Œ∏ ) : bold_italic_x ‚Ü¶ over^ start_ARG bold_italic_z end_ARG . (5.2.16)"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.E17", "title": "ùíü ‚Äã ( ùëø , ùëø ^ ) ‚âê max ùúΩ ‚Å° min ùúº ‚Äã ‚àë k = 1 K Œî ‚Äã R œµ ‚Äã ( f ‚Äã ( ùëø k , ùúΩ ) , h ‚Äã ( ùëø k , ùúΩ , ùúº ) ) . \\mathcal{D}(\\bm{X},\\hat{\\bm{X}})\\doteq\\max_{\\bm{\\theta}}\\min_{\\bm{\\eta}}\\sum_{k=1}^{K}\\Delta R_{\\epsil", "snippet": "ùíü ‚Äã ( ùëø , ùëø ^ ) ‚âê max ùúΩ ‚Å° min ùúº ‚Äã ‚àë k = 1 K Œî ‚Äã R œµ ‚Äã ( f ‚Äã ( ùëø k , ùúΩ ) , h ‚Äã ( ùëø k , ùúΩ , ùúº ) ) . \\mathcal{D}(\\bm{X},\\hat{\\bm{X}})\\doteq\\max_{\\bm{\\theta}}\\min_{\\bm{\\eta}}\\sum_{k=1}^{K}\\Delta R_{\\epsilon}\\big{(}f(\\bm{X}_{k},\\bm{\\theta}),h(\\bm{X}_{k},\\bm{\\theta},\\bm{\\eta})\\big{)}. caligraphic_D ( bold_italic_X , over^ start_ARG bold_italic_X end_ARG ) ‚âê roman_max start_POSTSUBSCRIPT bold_italic_Œ∏ end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT bold_italic_Œ∑ end_POSTSUBSCRIPT ‚àë start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_Œî italic_R s"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.E21", "title": "max ùúΩ ‚Å° min ùúº ‚Å° ùíØ ùëø b ‚Äã ( ùúΩ , ùúº ) ‚âê Œî ‚Äã R œµ ‚Äã ( f ‚Äã ( ùëø , ùúΩ ) , h ‚Äã ( ùëø , ùúΩ , ùúº ) ) = Œî ‚Äã R œµ ‚Äã ( ùíÅ ‚Äã ( ùúΩ ) , ùíÅ ^ ‚Äã ( ùúΩ , ùúº ) ) , \\max_{\\bm{\\theta}}\\min_{\\bm{\\eta}}\\mathcal{T}^{b}_{\\bm{X}}(\\bm{\\theta}", "snippet": "max ùúΩ ‚Å° min ùúº ‚Å° ùíØ ùëø b ‚Äã ( ùúΩ , ùúº ) ‚âê Œî ‚Äã R œµ ‚Äã ( f ‚Äã ( ùëø , ùúΩ ) , h ‚Äã ( ùëø , ùúΩ , ùúº ) ) = Œî ‚Äã R œµ ‚Äã ( ùíÅ ‚Äã ( ùúΩ ) , ùíÅ ^ ‚Äã ( ùúΩ , ùúº ) ) , \\max_{\\bm{\\theta}}\\min_{\\bm{\\eta}}\\mathcal{T}^{b}_{\\bm{X}}(\\bm{\\theta},\\bm{\\eta})\\doteq\\Delta R_{\\epsilon}\\big{(}f(\\bm{X},\\bm{\\theta}),h(\\bm{X},\\bm{\\theta},\\bm{\\eta})\\big{)}=\\Delta R_{\\epsilon}\\big{(}\\bm{Z}(\\bm{\\theta}),\\hat{\\bm{Z}}(\\bm{\\theta},\\bm{\\eta})\\big{)}, roman_max start_POSTSUBSCRIPT bold_italic_Œ∏ end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT bold_italic_Œ∑ end_POSTSUBSCRIPT caligraphic_T start_POSTSUPERSCRIPT italic_b end_POSTSUPERSCRIPT start_POSTSUBSCRI"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S2.E22", "title": "max ùúΩ ‚Å° min ùúº ‚Å° { Œî ‚Äã R œµ ‚Äã ( ùíÅ ‚Äã ( ùúΩ ) ) + ‚àë k = 1 K Œî ‚Äã R œµ ‚Äã ( ùíÅ k ‚Äã ( ùúΩ ) , ùíÅ ^ k ‚Äã ( ùúΩ , ùúº ) ) } \\max_{\\bm{\\theta}}\\min_{\\bm{\\eta}}\\left\\{\\Delta R_{\\epsilon}(\\bm{Z}(\\bm{\\theta}))+\\sum_{k=1}^{K}\\D", "snippet": "max ùúΩ ‚Å° min ùúº ‚Å° { Œî ‚Äã R œµ ‚Äã ( ùíÅ ‚Äã ( ùúΩ ) ) + ‚àë k = 1 K Œî ‚Äã R œµ ‚Äã ( ùíÅ k ‚Äã ( ùúΩ ) , ùíÅ ^ k ‚Äã ( ùúΩ , ùúº ) ) } \\max_{\\bm{\\theta}}\\min_{\\bm{\\eta}}\\left\\{\\Delta R_{\\epsilon}(\\bm{Z}(\\bm{\\theta}))+\\sum_{k=1}^{K}\\Delta R_{\\epsilon}(\\bm{Z}_{k}(\\bm{\\theta}),\\hat{\\bm{Z}}_{k}(\\bm{\\theta},\\bm{\\eta}))\\right\\} roman_max start_POSTSUBSCRIPT bold_italic_Œ∏ end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT bold_italic_Œ∑ end_POSTSUBSCRIPT { roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ( bold_italic_Œ∏ ) ) + ‚àë start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.E1", "title": "min ùúº ‚Å° max ùúΩ ‚Å° Œî ‚Äã R œµ ‚Äã ( ùíÅ ) + Œî ‚Äã R œµ ‚Äã ( ùíÅ ^ ) + Œî ‚Äã R œµ ‚Äã ( ùíÅ n ‚Äã e ‚Äã w , ùíÅ ^ n ‚Äã e ‚Äã w ) . \\min_{\\bm{\\eta}}\\max_{\\bm{\\theta}}\\Delta{R_{\\epsilon}(\\bm{Z})}+\\Delta{R_{\\epsilon}(\\hat{\\bm{Z}})}+\\Del", "snippet": "min ùúº ‚Å° max ùúΩ ‚Å° Œî ‚Äã R œµ ‚Äã ( ùíÅ ) + Œî ‚Äã R œµ ‚Äã ( ùíÅ ^ ) + Œî ‚Äã R œµ ‚Äã ( ùíÅ n ‚Äã e ‚Äã w , ùíÅ ^ n ‚Äã e ‚Äã w ) . \\min_{\\bm{\\eta}}\\max_{\\bm{\\theta}}\\Delta{R_{\\epsilon}(\\bm{Z})}+\\Delta{R_{\\epsilon}(\\hat{\\bm{Z}})}+\\Delta{R_{\\epsilon}(\\bm{Z}_{new},\\hat{\\bm{Z}}_{new})}. roman_min start_POSTSUBSCRIPT bold_italic_Œ∑ end_POSTSUBSCRIPT roman_max start_POSTSUBSCRIPT bold_italic_Œ∏ end_POSTSUBSCRIPT roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ) + roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( over^ start_ARG bold_italic_Z end_ARG ) + roman_Œî italic_R start_POSTSUBSCR"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.E2", "title": "ùíÅ o ‚Äã l ‚Äã d ‚Üí g ‚Äã ( ùíõ , ùúº ) ùëø ^ o ‚Äã l ‚Äã d ‚Üí f ‚Äã ( ùíô , ùúΩ ) ùíÅ ^ o ‚Äã l ‚Äã d . \\bm{Z}_{old}\\xrightarrow{\\hskip 5.69054ptg(\\bm{z},\\bm{\\eta})\\hskip 5.69054pt}\\hat{\\bm{X}}_{old}\\xrightarrow{\\hskip 5.69054ptf(", "snippet": "ùíÅ o ‚Äã l ‚Äã d ‚Üí g ‚Äã ( ùíõ , ùúº ) ùëø ^ o ‚Äã l ‚Äã d ‚Üí f ‚Äã ( ùíô , ùúΩ ) ùíÅ ^ o ‚Äã l ‚Äã d . \\bm{Z}_{old}\\xrightarrow{\\hskip 5.69054ptg(\\bm{z},\\bm{\\eta})\\hskip 5.69054pt}\\hat{\\bm{X}}_{old}\\xrightarrow{\\hskip 5.69054ptf(\\bm{x},\\bm{\\theta})\\hskip 5.69054pt}\\ \\hat{\\bm{Z}}_{old}. bold_italic_Z start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT start_ARROW start_OVERACCENT italic_g ( bold_italic_z , bold_italic_Œ∑ ) end_OVERACCENT ‚Üí end_ARROW over^ start_ARG bold_italic_X end_ARG start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT start_ARROW start_OVERACCENT italic_f ( bold_italic_x , bold_"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S3.Ex1", "title": "Œî ‚Äã R œµ ‚Äã ( ùíÅ o ‚Äã l ‚Äã d , ùíÅ ^ o ‚Äã l ‚Äã d ) ‚âê ‚àë j = 1 t Œî ‚Äã R œµ ‚Äã ( ùíÅ j , o ‚Äã l ‚Äã d , ùíÅ ^ j , o ‚Äã l ‚Äã d ) = 0 . \\Delta R_{\\epsilon}(\\bm{Z}_{old},\\hat{\\bm{Z}}_{old})\\doteq\\sum_{j=1}^{t}\\Delta R_{\\epsilon", "snippet": "Œî ‚Äã R œµ ‚Äã ( ùíÅ o ‚Äã l ‚Äã d , ùíÅ ^ o ‚Äã l ‚Äã d ) ‚âê ‚àë j = 1 t Œî ‚Äã R œµ ‚Äã ( ùíÅ j , o ‚Äã l ‚Äã d , ùíÅ ^ j , o ‚Äã l ‚Äã d ) = 0 . \\Delta R_{\\epsilon}(\\bm{Z}_{old},\\hat{\\bm{Z}}_{old})\\doteq\\sum_{j=1}^{t}\\Delta R_{\\epsilon}(\\bm{Z}_{j,old},\\hat{\\bm{Z}}_{j,old})=0. roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT , over^ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT ) ‚âê ‚àë start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT "}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S5.E1", "title": "‚Ñ≥ = { ( ùíô , F ‚Äã ( ùíô ) ‚à£ ùíô ‚àà ‚Ñù D 1 ) } . \\mathcal{M}=\\{(\\bm{x},F(\\bm{x})\\mid\\bm{x}\\in\\mathbb{R}^{D_{1}})\\}. caligraphic_M = { ( bold_italic_x , italic_F ( bold_italic_x ) ‚à£ bold_italic_x ‚àà blackboard_R", "snippet": "‚Ñ≥ = { ( ùíô , F ‚Äã ( ùíô ) ‚à£ ùíô ‚àà ‚Ñù D 1 ) } . \\mathcal{M}=\\{(\\bm{x},F(\\bm{x})\\mid\\bm{x}\\in\\mathbb{R}^{D_{1}})\\}. caligraphic_M = { ( bold_italic_x , italic_F ( bold_italic_x ) ‚à£ bold_italic_x ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ) } . (5.5.1)"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#A2.S3.EGx66", "title": "p ‚Äã ( ùíô ; ùúΩ ) \\displaystyle p(\\bm{x};\\bm{\\theta}) italic_p ( bold_italic_x ; bold_italic_Œ∏ ) = ‚à´ p ‚Äã ( ùíõ ; ùúΩ ) ‚Äã p ‚Äã ( ùíô ‚à£ ùíõ ; ùúΩ ) ‚Äã d ùíõ \\displaystyle=\\int p(\\bm{z};\\,\\bm{\\theta})p(\\bm{x}\\mid\\bm{z};\\,", "snippet": "p ‚Äã ( ùíô ; ùúΩ ) \\displaystyle p(\\bm{x};\\bm{\\theta}) italic_p ( bold_italic_x ; bold_italic_Œ∏ ) = ‚à´ p ‚Äã ( ùíõ ; ùúΩ ) ‚Äã p ‚Äã ( ùíô ‚à£ ùíõ ; ùúΩ ) ‚Äã d ùíõ \\displaystyle=\\int p(\\bm{z};\\,\\bm{\\theta})p(\\bm{x}\\mid\\bm{z};\\,\\bm{\\theta})\\mathrm{d}\\bm{z} = ‚à´ italic_p ( bold_italic_z ; bold_italic_Œ∏ ) italic_p ( bold_italic_x ‚à£ bold_italic_z ; bold_italic_Œ∏ ) roman_d bold_italic_z = ùîº ùíõ ‚àº p ‚Äã ( ‚ãÖ ; ùúΩ ) ‚Äã [ p ‚Äã ( ùíô ‚à£ ùíõ ; ùúΩ ) ] . \\displaystyle=\\mathbb{E}_{\\bm{z}\\sim p(\\,\\cdot\\,;\\,\\bm{\\theta})}[p(\\bm{x}\\mid\\bm{z};\\,\\bm{\\theta})]. = blackboard_E start_POSTSUBSCRIPT bold_italic_z ‚àº italic_p ( ‚ãÖ ; bold_italic_Œ∏ ) end_POSTSUBS"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#A2.S3.EGx67", "title": "ùíõ \\displaystyle\\bm{z} bold_italic_z ‚àº ùí© ‚Äã ( ùüé , ùë∞ ) , \\displaystyle\\sim\\mathcal{N}(\\mathbf{0},\\bm{I}), ‚àº caligraphic_N ( bold_0 , bold_italic_I ) , ùíô ‚à£ ùíõ \\displaystyle\\bm{x}\\mid\\bm{z} bold_italic_x ‚à£ ", "snippet": "ùíõ \\displaystyle\\bm{z} bold_italic_z ‚àº ùí© ‚Äã ( ùüé , ùë∞ ) , \\displaystyle\\sim\\mathcal{N}(\\mathbf{0},\\bm{I}), ‚àº caligraphic_N ( bold_0 , bold_italic_I ) , ùíô ‚à£ ùíõ \\displaystyle\\bm{x}\\mid\\bm{z} bold_italic_x ‚à£ bold_italic_z ‚àº ùí© ‚Äã ( g 1 ‚Äã ( ùíõ ) , diag ‚Å° ( e g 2 ‚Äã ( ùíõ ) ) ‚Äã ùë∞ ) , \\displaystyle\\sim\\mathcal{N}(g_{1}(\\bm{z}),\\operatorname{diag}(e^{g_{2}(\\bm{z})})\\bm{I}), ‚àº caligraphic_N ( italic_g start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( bold_italic_z ) , roman_diag ( italic_e start_POSTSUPERSCRIPT italic_g start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( bold_italic_z ) end_POSTSUPERSCRIPT ) bold_italic_I ) ,"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#A2.S3.EGx68", "title": "log ‚Å° p ‚Äã ( ùíô ; ùúΩ ) \\displaystyle\\log p(\\bm{x};\\,\\bm{\\theta}) roman_log italic_p ( bold_italic_x ; bold_italic_Œ∏ ) = log ‚Å° p ‚Äã ( ùíô , ùíõ ; ùúΩ ) p ‚Äã ( ùíõ ‚à£ ùíô ; ùúΩ ) \\displaystyle=\\log\\frac{p(\\bm{x},\\bm{z};\\", "snippet": "log ‚Å° p ‚Äã ( ùíô ; ùúΩ ) \\displaystyle\\log p(\\bm{x};\\,\\bm{\\theta}) roman_log italic_p ( bold_italic_x ; bold_italic_Œ∏ ) = log ‚Å° p ‚Äã ( ùíô , ùíõ ; ùúΩ ) p ‚Äã ( ùíõ ‚à£ ùíô ; ùúΩ ) \\displaystyle=\\log\\frac{p(\\bm{x},\\bm{z};\\,\\bm{\\theta})}{p(\\bm{z}\\mid\\bm{x};\\,\\bm{\\theta})} = roman_log divide start_ARG italic_p ( bold_italic_x , bold_italic_z ; bold_italic_Œ∏ ) end_ARG start_ARG italic_p ( bold_italic_z ‚à£ bold_italic_x ; bold_italic_Œ∏ ) end_ARG = log ‚Å° p ‚Äã ( ùíô , ùíõ ; ùúΩ ) q ‚Äã ( ùíõ ‚à£ ùíô ; ùúº ) + log ‚Å° q ‚Äã ( ùíõ ‚à£ ùíô ; ùúº ) p ‚Äã ( ùíõ ‚à£ ùíô ; ùúΩ ) , \\displaystyle=\\log\\frac{p(\\bm{x},\\bm{z};\\,\\bm{\\theta})}{q(\\bm{z}\\mid\\bm{x};\\,\\bm{\\eta})"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#A2.S3.EGx69", "title": "max ùúΩ , ùúº ‚Å° ùîº ùíô ‚Äã ùîº ùíõ ‚àº q ( ‚ãÖ ‚à£ ùíô ; ùúº ) ‚Äã [ log ‚Å° p ‚Äã ( ùíô , ùíõ ; ùúΩ ) q ‚Äã ( ùíõ ‚à£ ùíô ; ùúº ) ] \\displaystyle\\max_{\\bm{\\theta},\\bm{\\eta}}\\,\\mathbb{E}_{\\bm{x}}\\mathbb{E}_{\\bm{z}\\sim q(\\,\\cdot\\,\\mid\\bm{x};\\,\\bm", "snippet": "max ùúΩ , ùúº ‚Å° ùîº ùíô ‚Äã ùîº ùíõ ‚àº q ( ‚ãÖ ‚à£ ùíô ; ùúº ) ‚Äã [ log ‚Å° p ‚Äã ( ùíô , ùíõ ; ùúΩ ) q ‚Äã ( ùíõ ‚à£ ùíô ; ùúº ) ] \\displaystyle\\max_{\\bm{\\theta},\\bm{\\eta}}\\,\\mathbb{E}_{\\bm{x}}\\mathbb{E}_{\\bm{z}\\sim q(\\,\\cdot\\,\\mid\\bm{x};\\,\\bm{\\eta})}\\left[\\log\\frac{p(\\bm{x},\\bm{z};\\,\\bm{\\theta})}{q(\\bm{z}\\mid\\bm{x};\\,\\bm{\\eta})}\\right] roman_max start_POSTSUBSCRIPT bold_italic_Œ∏ , bold_italic_Œ∑ end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT bold_italic_z ‚àº italic_q ( ‚ãÖ ‚à£ bold_italic_x ; bold_italic_Œ∑ ) end_POSTSUBSCRIPT [ roman_log divide start_ARG italic_p ( bold_it"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#A2.S3.EGx70", "title": "ùîº ùíô ‚Äã ùîº ùíõ ‚àº q ( ‚ãÖ ‚à£ ùíô ; ùúº ) ‚Äã [ log ‚Å° p ‚Äã ( ùíô ‚à£ ùíõ ; ùúΩ ) ] \\displaystyle\\mathbb{E}_{\\bm{x}}\\mathbb{E}_{\\bm{z}\\sim q(\\,\\cdot\\,\\mid\\bm{x};\\,\\bm{\\eta})}\\left[\\log p(\\bm{x}\\mid\\bm{z};\\,\\bm{\\theta})\\right] ", "snippet": "ùîº ùíô ‚Äã ùîº ùíõ ‚àº q ( ‚ãÖ ‚à£ ùíô ; ùúº ) ‚Äã [ log ‚Å° p ‚Äã ( ùíô ‚à£ ùíõ ; ùúΩ ) ] \\displaystyle\\mathbb{E}_{\\bm{x}}\\mathbb{E}_{\\bm{z}\\sim q(\\,\\cdot\\,\\mid\\bm{x};\\,\\bm{\\eta})}\\left[\\log p(\\bm{x}\\mid\\bm{z};\\,\\bm{\\theta})\\right] blackboard_E start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT bold_italic_z ‚àº italic_q ( ‚ãÖ ‚à£ bold_italic_x ; bold_italic_Œ∑ ) end_POSTSUBSCRIPT [ roman_log italic_p ( bold_italic_x ‚à£ bold_italic_z ; bold_italic_Œ∏ ) ] ‚Üí ùîº ùíô ‚Äã [ log ‚Å° p ‚Äã ( ùíô ‚à£ f 1 ‚Äã ( ùíô ) ; ùúΩ ) ] \\displaystyle\\to\\mathbb{E}_{\\bm{x}}\\left[\\log p(\\bm{x}\\mid f_{1}(\\bm{x});\\,\\bm{\\theta})\\right] ‚Üí black"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#A2.S3.EGx71", "title": "max ùúΩ ‚Å° min ùúº ‚Å° ùíØ ùëø ‚Äã ( ùúΩ , ùúº ) \\displaystyle\\max_{\\bm{\\theta}}\\min_{\\bm{\\eta}}\\mathcal{T}_{\\bm{X}}(\\bm{\\theta},\\bm{\\eta}) roman_max start_POSTSUBSCRIPT bold_italic_Œ∏ end_POSTSUBSCRIPT roman_min start", "snippet": "max ùúΩ ‚Å° min ùúº ‚Å° ùíØ ùëø ‚Äã ( ùúΩ , ùúº ) \\displaystyle\\max_{\\bm{\\theta}}\\min_{\\bm{\\eta}}\\mathcal{T}_{\\bm{X}}(\\bm{\\theta},\\bm{\\eta}) roman_max start_POSTSUBSCRIPT bold_italic_Œ∏ end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT bold_italic_Œ∑ end_POSTSUBSCRIPT caligraphic_T start_POSTSUBSCRIPT bold_italic_X end_POSTSUBSCRIPT ( bold_italic_Œ∏ , bold_italic_Œ∑ ) (5.2.18) ‚âê \\displaystyle\\doteq ‚âê Œî ‚Äã R œµ ‚Äã ( f ‚Äã ( ùëø , ùúΩ ) ) ‚èü Expansive encode + Œî ‚Äã R œµ ‚Äã ( h ‚Äã ( ùëø , ùúΩ , ùúº ) ) ‚èü Compressive decode + ‚àë k = 1 K Œî ‚Äã R œµ ‚Äã ( f ‚Äã ( ùëø k , ùúΩ ) , h ‚Äã ( ùëø k , ùúΩ , ùúº ) ) ‚èü Contrastive encode & Contractive decode \\displaystyl"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#A2.S3.EGx72", "title": "min ùúº ‚Å° max ùúΩ \\displaystyle\\min_{\\bm{\\eta}}\\max_{\\bm{\\theta}} roman_min start_POSTSUBSCRIPT bold_italic_Œ∑ end_POSTSUBSCRIPT roman_max start_POSTSUBSCRIPT bold_italic_Œ∏ end_POSTSUBSCRIPT Œî ‚Äã R œµ ‚Äã ( ùíÅ ", "snippet": "min ùúº ‚Å° max ùúΩ \\displaystyle\\min_{\\bm{\\eta}}\\max_{\\bm{\\theta}} roman_min start_POSTSUBSCRIPT bold_italic_Œ∑ end_POSTSUBSCRIPT roman_max start_POSTSUBSCRIPT bold_italic_Œ∏ end_POSTSUBSCRIPT Œî ‚Äã R œµ ‚Äã ( ùíÅ ) + Œî ‚Äã R œµ ‚Äã ( ùíÅ ^ ) + Œî ‚Äã R œµ ‚Äã ( ùíÅ n ‚Äã e ‚Äã w , ùíÅ ^ n ‚Äã e ‚Äã w ) \\displaystyle\\Delta{R_{\\epsilon}(\\bm{Z})}+\\Delta{R_{\\epsilon}(\\hat{\\bm{Z}})}+\\Delta{R_{\\epsilon}(\\bm{Z}_{new},\\hat{\\bm{Z}}_{new})} roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ) + roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( over^ start_ARG bold_italic_Z end_ARG ) + roman_Œî ita"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#A2.S3.EGx73", "title": "max ùúΩ \\displaystyle\\max_{\\bm{\\theta}} roman_max start_POSTSUBSCRIPT bold_italic_Œ∏ end_POSTSUBSCRIPT Œî ‚Äã R œµ ‚Äã ( ùíÅ ) + Œî ‚Äã R œµ ‚Äã ( ùíÅ ^ ) + Œª ‚ãÖ Œî ‚Äã R œµ ‚Äã ( ùíÅ n ‚Äã e ‚Äã w , ùíÅ ^ n ‚Äã e ‚Äã w ) ‚àí Œ≥ ‚ãÖ Œî ‚Äã R œµ ‚Äã ", "snippet": "max ùúΩ \\displaystyle\\max_{\\bm{\\theta}} roman_max start_POSTSUBSCRIPT bold_italic_Œ∏ end_POSTSUBSCRIPT Œî ‚Äã R œµ ‚Äã ( ùíÅ ) + Œî ‚Äã R œµ ‚Äã ( ùíÅ ^ ) + Œª ‚ãÖ Œî ‚Äã R œµ ‚Äã ( ùíÅ n ‚Äã e ‚Äã w , ùíÅ ^ n ‚Äã e ‚Äã w ) ‚àí Œ≥ ‚ãÖ Œî ‚Äã R œµ ‚Äã ( ùíÅ o ‚Äã l ‚Äã d , ùíÅ ^ o ‚Äã l ‚Äã d ) , \\displaystyle\\Delta{R_{\\epsilon}(\\bm{Z})}\\!+\\!\\Delta{R_{\\epsilon}(\\hat{\\bm{Z}})}\\!+\\!\\lambda\\cdot\\Delta{R_{\\epsilon}(\\bm{Z}_{new},\\hat{\\bm{Z}}_{new})}-\\gamma\\cdot\\Delta{R_{\\epsilon}(\\bm{Z}_{old},\\hat{\\bm{Z}}_{old})}, roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ) + roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ("}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#A2.S3.EGx74", "title": "max ùúΩ ‚Å° min ùúº Œî ‚Äã R œµ ‚Äã ( ùíÅ , ùíÅ ^ ) \\displaystyle\\max_{\\bm{\\theta}}\\min_{\\bm{\\eta}}\\quad\\Delta R_{\\epsilon}(\\bm{Z},\\hat{\\bm{Z}}) roman_max start_POSTSUBSCRIPT bold_italic_Œ∏ end_POSTSUBSCRIPT roman_min", "snippet": "max ùúΩ ‚Å° min ùúº Œî ‚Äã R œµ ‚Äã ( ùíÅ , ùíÅ ^ ) \\displaystyle\\max_{\\bm{\\theta}}\\min_{\\bm{\\eta}}\\quad\\Delta R_{\\epsilon}(\\bm{Z},\\hat{\\bm{Z}}) roman_max start_POSTSUBSCRIPT bold_italic_Œ∏ end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT bold_italic_Œ∑ end_POSTSUBSCRIPT roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z , over^ start_ARG bold_italic_Z end_ARG ) (5.3.6)"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#A2.S3.EGx75", "title": "‚àë i ‚àà N Œî ‚Äã R œµ ‚Äã ( ùíõ i , ùíõ ^ i ) = 0 . \\displaystyle\\sum_{i\\in N}\\Delta R_{\\epsilon}(\\bm{z}^{i},\\hat{\\bm{z}}^{i})=0.\\vspace{-2mm} ‚àë start_POSTSUBSCRIPT italic_i ‚àà italic_N end_POSTSUBSCRIPT roman_Œî i", "snippet": "‚àë i ‚àà N Œî ‚Äã R œµ ‚Äã ( ùíõ i , ùíõ ^ i ) = 0 . \\displaystyle\\sum_{i\\in N}\\Delta R_{\\epsilon}(\\bm{z}^{i},\\hat{\\bm{z}}^{i})=0.\\vspace{-2mm} ‚àë start_POSTSUBSCRIPT italic_i ‚àà italic_N end_POSTSUBSCRIPT roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , over^ start_ARG bold_italic_z end_ARG start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ) = 0 . (5.3.7)"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#A2.S3.EGx76", "title": "‚àë i ‚àà N Œî ‚Äã R œµ ‚Äã ( ùíõ i , ùíõ a i ) = 0 . \\displaystyle\\sum_{i\\in N}\\Delta R_{\\epsilon}(\\bm{z}^{i},\\bm{z}_{a}^{i})=0.\\vspace{-3mm} ‚àë start_POSTSUBSCRIPT italic_i ‚àà italic_N end_POSTSUBSCRIPT roman_Œî ita", "snippet": "‚àë i ‚àà N Œî ‚Äã R œµ ‚Äã ( ùíõ i , ùíõ a i ) = 0 . \\displaystyle\\sum_{i\\in N}\\Delta R_{\\epsilon}(\\bm{z}^{i},\\bm{z}_{a}^{i})=0.\\vspace{-3mm} ‚àë start_POSTSUBSCRIPT italic_i ‚àà italic_N end_POSTSUBSCRIPT roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , bold_italic_z start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ) = 0 . (5.3.8)"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#A2.S3.EGx77", "title": "max Œ∏ ‚Å° min Œ∑ \\displaystyle\\max_{\\theta}\\min_{\\eta}\\quad roman_max start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT italic_Œ∑ end_POSTSUBSCRIPT R œµ ‚Äã ( ùíÅ ) + Œî ‚Äã R œµ ‚Äã ( ùíÅ ,", "snippet": "max Œ∏ ‚Å° min Œ∑ \\displaystyle\\max_{\\theta}\\min_{\\eta}\\quad roman_max start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT roman_min start_POSTSUBSCRIPT italic_Œ∑ end_POSTSUBSCRIPT R œµ ‚Äã ( ùíÅ ) + Œî ‚Äã R œµ ‚Äã ( ùíÅ , ùíÅ ^ ) \\displaystyle R_{\\epsilon}(\\bm{Z})+\\Delta R_{\\epsilon}(\\bm{Z},\\hat{\\bm{Z}}) italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ) + roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z , over^ start_ARG bold_italic_Z end_ARG ) (5.3.9) subject to ‚àë i ‚àà N Œî ‚Äã R œµ ‚Äã ( ùíõ i , ùíõ ^ i ) = 0 , and ‚Äã ‚àë i ‚àà N Œî ‚Äã R œµ ‚Äã ( ùíõ i , ùíõ a i ) = 0 . \\displaystyle\\"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#A2.S3.EGx78", "title": "max Œ∏ ‚Å° R œµ ‚Äã ( ùíÅ ) + Œî ‚Äã R œµ ‚Äã ( ùíÅ , ùíÅ ^ ) ‚àí Œª 1 ‚Äã ‚àë i ‚àà N Œî ‚Äã R œµ ‚Äã ( ùíõ i , ùíõ a i ) ‚àí Œª 2 ‚Äã ‚àë i ‚àà N Œî ‚Äã R œµ ‚Äã ( ùíõ i , ùíõ ^ i ) ; \\displaystyle\\max_{\\theta}\\;R_{\\epsilon}(\\bm{Z})+\\Delta{R_{\\epsilon}(\\", "snippet": "max Œ∏ ‚Å° R œµ ‚Äã ( ùíÅ ) + Œî ‚Äã R œµ ‚Äã ( ùíÅ , ùíÅ ^ ) ‚àí Œª 1 ‚Äã ‚àë i ‚àà N Œî ‚Äã R œµ ‚Äã ( ùíõ i , ùíõ a i ) ‚àí Œª 2 ‚Äã ‚àë i ‚àà N Œî ‚Äã R œµ ‚Äã ( ùíõ i , ùíõ ^ i ) ; \\displaystyle\\max_{\\theta}\\;R_{\\epsilon}(\\bm{Z})+\\Delta{R_{\\epsilon}(\\bm{Z},\\hat{\\bm{Z}})-\\lambda_{1}\\sum_{i\\in N}\\Delta R_{\\epsilon}(\\bm{z}^{i},\\bm{z}_{a}^{i})}-\\lambda_{2}\\sum_{i\\in N}\\Delta R_{\\epsilon}(\\bm{z}^{i},\\hat{\\bm{z}}^{i}); roman_max start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ) + roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z , over^ start_ARG bol"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#A2.S3.EGx79", "title": "ùö∑ ^ = arg ‚Å° max Œæ ‚Å° Œî ‚Äã R œµ ‚Äã ( ùíÅ | ùö∑ ‚Äã ( Œæ ) ) . \\displaystyle\\hat{\\bm{\\Pi}}=\\arg\\max_{\\xi}\\Delta R_{\\epsilon}(\\bm{Z}|\\bm{\\Pi}(\\xi)). over^ start_ARG bold_Œ† end_ARG = roman_arg roman_max start_POSTSU", "snippet": "ùö∑ ^ = arg ‚Å° max Œæ ‚Å° Œî ‚Äã R œµ ‚Äã ( ùíÅ | ùö∑ ‚Äã ( Œæ ) ) . \\displaystyle\\hat{\\bm{\\Pi}}=\\arg\\max_{\\xi}\\Delta R_{\\epsilon}(\\bm{Z}|\\bm{\\Pi}(\\xi)). over^ start_ARG bold_Œ† end_ARG = roman_arg roman_max start_POSTSUBSCRIPT italic_Œæ end_POSTSUBSCRIPT roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z | bold_Œ† ( italic_Œæ ) ) . (5.3.12)"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.SS4.SSSx1", "title": "Probabilistic Perspective on Autoencoding", "snippet": "Probabilistic Perspective on Autoencoding In the manifold model for the data distribution, the key mathematical objects are the support of the data distribution, namely the manifold ‚Ñ≥ \\mathcal{M} caligraphic_M , and the density of the data on the support, say p p italic_p . When we formulate autoencoding from the probabilistic perspective, we often think of the high-dimensional input ùíô \\bm{x} bold_italic_x as having a density p p italic_p with support on ‚Ñù D \\mathbb{R}^{D} blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT ; one can think of adding a very small amount of noise to "}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.SS4.SSSx2", "title": "VAE Training as Probabilistic Autoencoding", "snippet": "VAE Training as Probabilistic Autoencoding There is a general methodology for maximizing the ELBO objective in Equation 5.1.14 using stochastic gradient descent and various tractable Monte Carlo estimators for the associated gradients. However, the task is simpler under the Gaussian assumptions we have made above. In this case, the ELBO reads max ùúΩ , ùúº ‚Å° ùîº ùíô ‚Äã ùîº ùíõ ‚àº q ( ‚ãÖ ‚à£ ùíô ; ùúº ) ‚Äã [ log ‚Å° p ‚Äã ( ùíô , ùíõ ; ùúΩ ) q ‚Äã ( ùíõ ‚à£ ùíô ; ùúº ) ] \\displaystyle\\max_{\\bm{\\theta},\\bm{\\eta}}\\,\\mathbb{E}_{\\bm{x}}\\mathbb{E}_{\\bm{z}\\sim q(\\,\\cdot\\,\\mid\\bm{x};\\,\\bm{\\eta})}\\left[\\log\\frac{p(\\bm{x},\\bm{z};\\,\\bm{\\theta})}"}, {"page": "Chapter 5 Consistent and Self-Consistent Representations", "href": "Ch5.html#S1.SS4.SSSx3", "title": "Training a VAE", "snippet": "Training a VAE VAEs are typically trained by alternating stochastic gradient ascent on the ELBO objective ( Equation 5.1.15 ), given individual samples ùíô \\bm{x} bold_italic_x from the true data distribution and from ùíõ ‚àº q ( ‚ãÖ ‚à£ ùíô ; ùúº ) \\bm{z}\\sim q(\\,\\cdot\\,\\mid\\bm{x};\\,\\bm{\\eta}) bold_italic_z ‚àº italic_q ( ‚ãÖ ‚à£ bold_italic_x ; bold_italic_Œ∑ ) . In particular, it is standard to collect and train on many independently-generated samples ùíõ i \\bm{z}^{i} bold_italic_z start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT for each sample ùíô \\bm{x} bold_italic_x . To take gradients of Equation 5.1.15 with"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#top", "title": "Chapter 6 Inference with Low-Dimensional Distributions", "snippet": ""}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1", "title": "6.1 Bayesian Inference and Constrained Optimization", "snippet": "6.1 Bayesian Inference and Constrained Optimization Leveraging Low-dimensionality for Stable and Robust Inference. Generally speaking, a good representation or autoencoding should enable us to utilize the learned low-dimensional distribution of the data ùíô \\bm{x} bold_italic_x and its representation ùíõ \\bm{z} bold_italic_z for various subsequent classification, estimation, and generation tasks under different conditions. As we have alluded to earlier in Chapter 1 Section 1.2.2 , the importance of the low-dimensionality of the distribution is the key for us to conduct stable and robust inference "}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S2", "title": "6.2 Conditional Inference with a Known Data Distribution", "snippet": "6.2 Conditional Inference with a Known Data Distribution Notice that in the setting we have discussed in previous Chapters, the autoencoding network is trained to reconstruct a set of samples of the random vector ùíô \\bm{x} bold_italic_x . This would allow us to regenerate samples from the learned (low-dimensional) distribution. In practice, the low-dimensionality of the distribution, once given or learned, can be exploited for stable and robust recovery, completion, or prediction tasks. That is, under rather mild conditions, one can recover ùíô \\bm{x} bold_italic_x , from highly compressive, part"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3", "title": "6.3 Conditional Inference with a Learned Data Representation", "snippet": "6.3 Conditional Inference with a Learned Data Representation In the previous subsection, the reason we can infer ùíô \\bm{x} bold_italic_x from the partial observation ùíö \\bm{y} bold_italic_y is because (support of) the distribution of ùëø \\bm{X} bold_italic_X is known or specified apriori , say as the set of all low-rank matrices. For many practical dataset, we do not have its distribution in an analytical form as the low-rank matrices, say the set to of all natural images. Nevertheless, if we have sufficient samples of the data ùíô \\bm{x} bold_italic_x , we should be able to learn its low-dimensiona"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4", "title": "6.4 Conditional Inference with Paired Data and Measurements", "snippet": "6.4 Conditional Inference with Paired Data and Measurements In many practical applications, we do not know either the distribution of the data ùíô \\bm{x} bold_italic_x of interest nor the explicit relationship between the data and certain observed attributes ùíö \\bm{y} bold_italic_y of the data. We only have a (large) set of paired samples ( ùëø , ùíÄ ) = { ( ùíô 1 , ùíö 1 ) , ‚Ä¶ , ( ùíô N , ùíö N ) } (\\bm{X},\\bm{Y})=\\{(\\bm{x}_{1},\\bm{y}_{1}),\\ldots,(\\bm{x}_{N},\\bm{y}_{N})\\} ( bold_italic_X , bold_italic_Y ) = { ( bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_italic_y start_POSTSUBSCRIPT 1 end_P"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5", "title": "6.5 Conditional Inference with Measurement Self-Consistency", "snippet": "6.5 Conditional Inference with Measurement Self-Consistency In this last section, we consider the more extreme, but actually ubiquitous, case for distribution learning in which we only have a set of observed samples ùíÄ = { ùíö 1 , ‚Ä¶ , ùíö N } \\bm{Y}=\\{\\bm{y}_{1},\\ldots,\\bm{y}_{N}\\} bold_italic_Y = { bold_italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_y start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT } of the data ùíô \\bm{x} bold_italic_x , but no samples of ùíô \\bm{x} bold_italic_x directly! In general, the observation ùíö ‚àà ‚Ñù d \\bm{y}\\in\\mathbb{R}^{d} bold_italic_y ‚àà blackboard_R star"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S6", "title": "6.6 Summary and Notes", "snippet": "6.6 Summary and Notes Measurement matching without clean samples. In our development of conditional sampling, we considered measurement matching under an observation model ( 6.3.9 ), where we assume that we have paired data ( ùíô , ùíö ) (\\bm{x},\\bm{y}) ( bold_italic_x , bold_italic_y ) ‚Äîi.e., ground truth for each observation ùíö \\bm{y} bold_italic_y . In many practically-relevant inverse problems, this is not the case: one of the most fundamental examples is in the context of compressed sensing, which we recalled in Chapter 2 , where we need to reconstruct ùíô \\bm{x} bold_italic_x from ùíö \\bm{y} bold"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S7", "title": "6.7 Exercises and Extensions", "snippet": "6.7 Exercises and Extensions Exercise 6.1 (Posterior Variance Correction to DPS) . 1. Using the code provided in the book GitHub for implementing Figure 6.9 , implement the posterior variance correction proposed by [ RAL+24 ] . 2. Verify that it ameliorates the posterior collapse at low noise variance issue observed in Figure 6.9 . 3. Discuss any issues of sampling correctness that are retained or introduced by the corrected method, as well as its efficiency, relative to diffusion posterior sampling (DPS). Exercise 6.2 (Conditional Sampling on MNIST) . 1. Train a simple classifier for the MNIS"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.SS1", "title": "6.3.1 Image Completion with Masked Auto-Encoding", "snippet": "6.3.1 Image Completion with Masked Auto-Encoding For a general image ùëø \\bm{X} bold_italic_X such as the one shown on the left of Figure 6.4 , we can no longer view it as a low-rank matrix. However, humans still demonstrate remarkable ability to complete a scene and recognize familiar objects despite severe occlusion. This suggests that our brain has learned the low-dimensional distribution of natural images and can use it for completion, hence recognition. However, the distribution of all natural images is not as simple as a low-dimensional linear subspace. Hence a natural question is whether "}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.SS2", "title": "6.3.2 Conditional Sampling with Measurement Matching", "snippet": "6.3.2 Conditional Sampling with Measurement Matching The above (masked) autoencoding problem aims to generate a sample image that is consistent with certain observations or conditions. But let us examine the approach more closely: Given the visual part of an image ùëø v = ùí´ Œ© ‚Äã ( ùëø ) \\bm{X}_{v}=\\mathcal{P}_{\\Omega}(\\bm{X}) bold_italic_X start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT = caligraphic_P start_POSTSUBSCRIPT roman_Œ© end_POSTSUBSCRIPT ( bold_italic_X ) , we try to estimate the masked part ùëø m = ùí´ Œ© c ‚Äã ( ùëø ) \\bm{X}_{m}=\\mathcal{P}_{\\Omega^{c}}(\\bm{X}) bold_italic_X start_POSTSUBSCRIPT i"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.SS3", "title": "6.3.3 Body Pose Generation Conditioned on Head and Hands", "snippet": "6.3.3 Body Pose Generation Conditioned on Head and Hands The type of conditional estimation or generation problems arise rather naturally in many practical applications. A typical problem of this kind is how to estimate and generate body pose and hand gesture conditioned on a given head pose and egocentric images, as illustrated in Figure 6.10 . This is often the problem we need to solve when one is wearing a head-mounted device such as the Vision-pro from Apple or the Project Aria from Meta. The pose of the whole body and the gesture of the hands need to be inferred so that we can use the inf"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.SS1", "title": "6.4.1 Class Conditioned Image Generation", "snippet": "6.4.1 Class Conditioned Image Generation While a learned classifier allows us to classify a given image ùíô \\bm{x} bold_italic_x to its corresponding class, we often like to generate an image of a given class, by sampling the learned distribution of natural images. To some extent, this can be viewed as the ‚Äúinverse‚Äù problem to image classification. Let p ùíô p_{\\bm{x}} italic_p start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT denote the distribution of natural images, say modeled by a diffusion-denoising process. Given a class label random variable y ‚àà [ K ] y\\in[K] italic_y ‚àà [ italic_K ] with"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.SS2", "title": "6.4.2 Caption Conditioned Image Generation", "snippet": "6.4.2 Caption Conditioned Image Generation In the previous subsection, we have formulated denoisers for class-conditional denoising with classifier-free guidance, a ubiquitous practical methodology used in the largest-scale diffusion models, and shown how to parameterize them (in Example 6.3 ) in the special case of a low-rank Gaussian mixture model data distribution. One interesting byproduct of this example is that it highlights the crucial role of embeddings of the class label y y italic_y into a common space with the image ùíô \\bm{x} bold_italic_x in order to provide a concise and unified sc"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.SS1", "title": "6.5.1 Linear Measurement Models", "snippet": "6.5.1 Linear Measurement Models First, for simplicity, let us consider the measurement is a linear function of the data ùíô \\bm{x} bold_italic_x of interest: ùíö = ùë® ‚Äã ùíô . \\bm{y}=\\bm{A}\\bm{x}. bold_italic_y = bold_italic_A bold_italic_x . (6.5.2) Here the matrix ùë® ‚àà ‚Ñù m √ó n \\bm{A}\\in\\mathbb{R}^{m\\times n} bold_italic_A ‚àà blackboard_R start_POSTSUPERSCRIPT italic_m √ó italic_n end_POSTSUPERSCRIPT is of full row rank and m m italic_m is typically smaller than n n italic_n . We assume ùë® \\bm{A} bold_italic_A is known for now. We are interested in how to learn the distribution of ùíô \\bm{x} bold_italic_x "}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.SS2", "title": "6.5.2 3D Visual Model from Calibrated Images", "snippet": "6.5.2 3D Visual Model from Calibrated Images In practice, the measurement model is often nonlinear or only partially known. A typical problem of this kind is actually behind how we can learn a working model of the external world from the images perceived, say through our eyes, telescopes or microscopes. In particular, humans and animals are able to build a model of the 3D world ((or 4D for a dynamical world) through a sequence of its 2D projections ‚Äì a sequence of 2D images (or stereo image pairs). The mathematical or geometric model of the projection is generally known: ùíö i = h ‚Äã ( ùíô , Œ∏ i ) "}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.SS0.SSS0.Px1", "title": "Leveraging Low-dimensionality for Stable and Robust Inference.", "snippet": "Leveraging Low-dimensionality for Stable and Robust Inference. Generally speaking, a good representation or autoencoding should enable us to utilize the learned low-dimensional distribution of the data ùíô \\bm{x} bold_italic_x and its representation ùíõ \\bm{z} bold_italic_z for various subsequent classification, estimation, and generation tasks under different conditions. As we have alluded to earlier in Chapter 1 Section 1.2.2 , the importance of the low-dimensionality of the distribution is the key for us to conduct stable and robust inference related to the data ùíô \\bm{x} bold_italic_x , as illu"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.SS0.SSS0.Px2", "title": "Statistical interpretation via Bayes‚Äô rule.", "snippet": "Statistical interpretation via Bayes‚Äô rule. Generally speaking, to accomplish such tasks well, we need to get ahold of the conditional distribution p ‚Äã ( ùíô ‚à£ ùíö ) p(\\bm{x}\\mid\\bm{y}) italic_p ( bold_italic_x ‚à£ bold_italic_y ) . If we had this, then we would be able to find the maximal likelihood estimate (prediction): ùíô ^ = arg ‚Äã max ùíô ‚Å° p ‚Äã ( ùíô ‚à£ ùíö ) ; \\hat{\\bm{x}}=\\operatorname*{arg\\ max}_{\\bm{x}}p(\\bm{x}\\mid\\bm{y}); over^ start_ARG bold_italic_x end_ARG = start_OPERATOR roman_arg roman_max end_OPERATOR start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT italic_p ( bold_italic_x ‚à£ bold_italic"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.SS0.SSS0.Px3", "title": "Geometric interpretation as constrained optimization.", "snippet": "Geometric interpretation as constrained optimization. As the support ùíÆ ùíô \\mathcal{S}_{\\bm{x}} caligraphic_S start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT of the distribution of ùíô \\bm{x} bold_italic_x is low-dimensional, we may assume that there exists a function F F italic_F such that F ‚Äã ( ùíô ) = ùüé ‚áî ùíô ‚àà ùíÆ ùíô F(\\bm{x})=\\bm{0}\\qquad\\iff\\qquad\\bm{x}\\in\\mathcal{S}_{\\bm{x}} italic_F ( bold_italic_x ) = bold_0 ‚áî bold_italic_x ‚àà caligraphic_S start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT (6.1.8) such that ùíÆ ùíô = F ‚àí 1 ‚Äã ( { ùüé } ) \\mathcal{S}_{\\bm{x}}=F^{-1}(\\{\\bm{0}\\}) caligraphic_S start_"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.SS0.SSS0.Px4", "title": "Several representative practical settings for inference.", "snippet": "Several representative practical settings for inference. In practice, however, initial information about the distributions of ùíô \\bm{x} bold_italic_x and the relationship between ùíô \\bm{x} bold_italic_x and ùíö \\bm{y} bold_italic_y can be given in many different ways or forms. In general, they can mostly be categorized into four cases, which are, conceptually, increasingly more challenging: ‚Ä¢ Case 1: Both a model for the distribution of ùíô \\bm{x} bold_italic_x and the observation model ùíö = h ‚Äã ( ùíô ) \\bm{y}=h(\\bm{x}) bold_italic_y = italic_h ( bold_italic_x ) ( + ùíò ) (+\\bm{w}) ( + bold_italic_w ) is"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S2.SS0.SSS0.Px1", "title": "Low-rank matrix completion.", "snippet": "Low-rank matrix completion. The low-rank matrix completion problem is a classical problem for data completion when its distribution is low-dimensional and known. Consider a random sample of a matrix ùëø o = [ ùíô 1 , ‚Ä¶ , ùíô n ] ‚àà ‚Ñù m √ó n \\bm{X}_{o}=[\\bm{x}_{1},\\ldots,\\bm{x}_{n}]\\in\\mathbb{R}^{m\\times n} bold_italic_X start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ] ‚àà blackboard_R start_POSTSUPERSCRIPT italic_m √ó italic_n end_POSTSUPERSCRIPT from the space of all matrices of r"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S2.SS0.SSS0.Px2", "title": "Further extensions.", "snippet": "Further extensions. It has been shown that images (or more accurately textures) and 3D scenes with low-rank structures can be very effectively completed via solving optimization programs of the above kind, even if there is additional corruption and distortion [ ZLG+10 , LRZ+12 , YZB+23 ] : ùíÄ ‚àò œÑ = ùëø o + ùë¨ , \\bm{Y}\\circ\\tau=\\bm{X}_{o}+\\bm{E}, bold_italic_Y ‚àò italic_œÑ = bold_italic_X start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT + bold_italic_E , (6.2.9) where œÑ \\tau italic_œÑ is some unknown nonlinear distortion of the image and ùë¨ \\bm{E} bold_italic_E is an unknown matrix that models some (spar"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.SS2.SSS0.Px1", "title": "General linear measurements.", "snippet": "General linear measurements. In fact, we may even consider recovering ùëø \\bm{X} bold_italic_X from a more general linear observation model: ùíÄ = ùë® ‚Äã ùëø 0 , ùëø t = ùëø 0 + œÉ t ‚Äã ùëÆ , \\bm{Y}=\\bm{A}\\bm{X}_{0},\\quad\\bm{X}_{t}=\\bm{X}_{0}+\\sigma_{t}\\bm{G}, bold_italic_Y = bold_italic_A bold_italic_X start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , bold_italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_X start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + italic_œÉ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_G , (6.3.5) where ùë® \\bm{A} bold_italic_A is a linear operator on matrix space 5 5 5"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.SS2.SSS0.Px2", "title": "General nonlinear measurements.", "snippet": "General nonlinear measurements. To generalize the above (image) completion problems and make things more rigorous, we may consider that a random vector ùíô ‚àº p \\bm{x}\\sim p bold_italic_x ‚àº italic_p is partially observed through a more general observation function: ùíö = h ‚Äã ( ùíô ) + ùíò , \\bm{y}=h(\\bm{x})+\\bm{w}, bold_italic_y = italic_h ( bold_italic_x ) + bold_italic_w , (6.3.9) where ùíò \\bm{w} bold_italic_w usually stands for some random measurement noise, say of a Gaussian distribution ùíò ‚àº ùí© ‚Äã ( ùüé , œÉ 2 ‚Äã ùë∞ ) \\bm{w}\\sim\\mathcal{N}(\\mathbf{0},\\sigma^{2}\\bm{I}) bold_italic_w ‚àº caligraphic_N ( bold_0"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.SS2.SSS0.Px1", "title": "Visual World Model from Uncalibrated Image Sequences", "snippet": "Visual World Model from Uncalibrated Image Sequences In the above derivation, we have assumed that the measurement model h ‚Äã ( ‚ãÖ ) h(\\cdot) italic_h ( ‚ãÖ ) is fully known. In the case of stereo vision, this is rather reasonable as the relative pose (and calibration) of the two camera views (or two eyes 15 15 15 The relative pose of our two eyes is well known to our brain. ) are usually known in advance. Hence, through the stereo image pairs, in principle we should be able to learn the distribution of 3D scenes, at least the ego-centric distribution of 3D scenes. However, the low-dimensional str"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S6.SS0.SSS0.Px1", "title": "Measurement matching without clean samples.", "snippet": "Measurement matching without clean samples. In our development of conditional sampling, we considered measurement matching under an observation model ( 6.3.9 ), where we assume that we have paired data ( ùíô , ùíö ) (\\bm{x},\\bm{y}) ( bold_italic_x , bold_italic_y ) ‚Äîi.e., ground truth for each observation ùíö \\bm{y} bold_italic_y . In many practically-relevant inverse problems, this is not the case: one of the most fundamental examples is in the context of compressed sensing, which we recalled in Chapter 2 , where we need to reconstruct ùíô \\bm{x} bold_italic_x from ùíö \\bm{y} bold_italic_y using prior "}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S6.SS0.SSS0.Px2", "title": "Corrections to the Diffusion Posterior Sampling (DPS) approximation.", "snippet": "Corrections to the Diffusion Posterior Sampling (DPS) approximation. In Example 6.2 and in particular in Figure 6.9 , we pointed out a limitation of the DPS approximation Equation 6.3.26 at small levels of measurement noise. This limitation is well-understood, and a principled approach to ameliorating it has been proposed by Rozet et al. [ RAL+24 ] . The approach involves incorporating an additional estimate for the variance of the noisy posterior p ùíô ‚à£ ùíô t p_{\\bm{x}\\mid\\bm{x}_{t}} italic_p start_POSTSUBSCRIPT bold_italic_x ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POS"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S6.SS0.SSS0.Px3", "title": "More about measurement matching and diffusion models for inverse problems.", "snippet": "More about measurement matching and diffusion models for inverse problems. Diffusion models have become an extremely popular tool for solving inverse problems arising in scientific applications. Many more methods beyond the simple DPS algorithm we have presented in Algorithm 6.1 have been developed and continue to be developed, as the area is evolving rapidly. Popular and performant classes of approaches beyond DPS, which we have presented due to its generality, include variable splitting approaches like DAPS [ ZCB+24 ] , which allow for specific measurement constraints to be enforced much mor"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#p1", "title": "‚Äú Mathematics is the art of giving the same name to different things .‚Äù ‚Äî Henri Poincar√©", "snippet": "‚Äú Mathematics is the art of giving the same name to different things .‚Äù ‚Äî Henri Poincar√©"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#p2", "title": "In the previous chapters of this book, we have studied how to effectively and efficiently learn a representation for a variable ùíô \\bm{x} bold_italic_x in the world with a distribution p ‚Äã ( ùíô ) p(\\bm{", "snippet": "In the previous chapters of this book, we have studied how to effectively and efficiently learn a representation for a variable ùíô \\bm{x} bold_italic_x in the world with a distribution p ‚Äã ( ùíô ) p(\\bm{x}) italic_p ( bold_italic_x ) that has a low-dimensional support in a high-dimensional space. So far, we have mainly developed the methodology for learning representation and autoencoding in a general, distribution or task-agnostic fashion. With such a learned representation, one can already used it to perform some generic and basic tasks such as classification (if the encoding is supervised with"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#p3", "title": "More generally, however, the universality and scalability of the theoretical and computational framework presented in this book have enabled us to learn the distribution of a variety of important real", "snippet": "More generally, however, the universality and scalability of the theoretical and computational framework presented in this book have enabled us to learn the distribution of a variety of important real-world high-dimensional data such as natural languages, human poses, natural images, videos, and even 3D scenes. Once the intrinsic rich and low-dimensional structures of these real data can be learned and represented correctly, they start to enable a broad family of powerful, often seemingly miraculous, tasks. Hence, here onwards, we will start to show how to connect and tailor the general method"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.SS0.SSS0.Px1.p1", "title": "Generally speaking, a good representation or autoencoding should enable us to utilize the learned low-dimensional distribution of the data ùíô \\bm{x} bold_italic_x and its representation ùíõ \\bm{z} bold_i", "snippet": "Generally speaking, a good representation or autoencoding should enable us to utilize the learned low-dimensional distribution of the data ùíô \\bm{x} bold_italic_x and its representation ùíõ \\bm{z} bold_italic_z for various subsequent classification, estimation, and generation tasks under different conditions. As we have alluded to earlier in Chapter 1 Section 1.2.2 , the importance of the low-dimensionality of the distribution is the key for us to conduct stable and robust inference related to the data ùíô \\bm{x} bold_italic_x , as illustrated by the few simple examples in Figure 1.9 , from incompl"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.SS0.SSS0.Px1.p2", "title": "Despite a dazzling variety of applications in the practice of machine learning with data such as languages, images, videos and many other modalities, almost all practical applications can be viewed as", "snippet": "Despite a dazzling variety of applications in the practice of machine learning with data such as languages, images, videos and many other modalities, almost all practical applications can be viewed as a special case of the following inference problem: given an observation ùíö \\bm{y} bold_italic_y that depends on ùíô \\bm{x} bold_italic_x , say ùíö = h ‚Äã ( ùíô ) + ùíò , \\bm{y}=h(\\bm{x})+\\bm{w}, bold_italic_y = italic_h ( bold_italic_x ) + bold_italic_w , (6.1.1) where h ‚Äã ( ‚ãÖ ) h(\\cdot) italic_h ( ‚ãÖ ) represents measurements of a part of ùíô \\bm{x} bold_italic_x or its certain observed attributes and ùíò \\bm{"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#Thmexample1.p1", "title": "The popular natural image completion and natural language prediction are two typical tasks that require us to recover a full data ùíô \\bm{x} bold_italic_x from its partial observations ùíö \\bm{y} bold_ita", "snippet": "The popular natural image completion and natural language prediction are two typical tasks that require us to recover a full data ùíô \\bm{x} bold_italic_x from its partial observations ùíö \\bm{y} bold_italic_y , with parts of ùíô \\bm{x} bold_italic_x masked out and to be completed based on the rest. Figure 6.2 shows some examples of such tasks. In fact, it is precisely these tasks which have inspired how to train modern large models for text generation (such as GPT) and image completion (such as the masked autoencoder) that we will study in greater details later."}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#Thmexample1.p2", "title": "‚ñ† \\blacksquare ‚ñ†", "snippet": "‚ñ† \\blacksquare ‚ñ†"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.SS0.SSS0.Px2.p1", "title": "Generally speaking, to accomplish such tasks well, we need to get ahold of the conditional distribution p ‚Äã ( ùíô ‚à£ ùíö ) p(\\bm{x}\\mid\\bm{y}) italic_p ( bold_italic_x ‚à£ bold_italic_y ) . If we had this, t", "snippet": "Generally speaking, to accomplish such tasks well, we need to get ahold of the conditional distribution p ‚Äã ( ùíô ‚à£ ùíö ) p(\\bm{x}\\mid\\bm{y}) italic_p ( bold_italic_x ‚à£ bold_italic_y ) . If we had this, then we would be able to find the maximal likelihood estimate (prediction): ùíô ^ = arg ‚Äã max ùíô ‚Å° p ‚Äã ( ùíô ‚à£ ùíö ) ; \\hat{\\bm{x}}=\\operatorname*{arg\\ max}_{\\bm{x}}p(\\bm{x}\\mid\\bm{y}); over^ start_ARG bold_italic_x end_ARG = start_OPERATOR roman_arg roman_max end_OPERATOR start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT italic_p ( bold_italic_x ‚à£ bold_italic_y ) ; (6.1.2) compute the conditional expec"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.SS0.SSS0.Px2.p2", "title": "Notice that from Bayes‚Äô rule, we have p ‚Äã ( ùíô ‚à£ ùíö ) = p ‚Äã ( ùíö ‚à£ ùíô ) ‚Äã p ‚Äã ( ùíô ) p ‚Äã ( ùíö ) . p(\\bm{x}\\mid\\bm{y})=\\frac{p(\\bm{y}\\mid\\bm{x})p(\\bm{x})}{p(\\bm{y})}. italic_p ( bold_italic_x ‚à£ bold_italic_y", "snippet": "Notice that from Bayes‚Äô rule, we have p ‚Äã ( ùíô ‚à£ ùíö ) = p ‚Äã ( ùíö ‚à£ ùíô ) ‚Äã p ‚Äã ( ùíô ) p ‚Äã ( ùíö ) . p(\\bm{x}\\mid\\bm{y})=\\frac{p(\\bm{y}\\mid\\bm{x})p(\\bm{x})}{p(\\bm{y})}. italic_p ( bold_italic_x ‚à£ bold_italic_y ) = divide start_ARG italic_p ( bold_italic_y ‚à£ bold_italic_x ) italic_p ( bold_italic_x ) end_ARG start_ARG italic_p ( bold_italic_y ) end_ARG . (6.1.5) For instance, the maximal likelihood estimate can be computed by solving the following (maximal log likelihood) program: ùíô ^ = arg ‚Äã max ùíô ‚Å° [ log ‚Å° p ‚Äã ( ùíö ‚à£ ùíô ) + log ‚Å° p ‚Äã ( ùíô ) ] , \\hat{\\bm{x}}=\\operatorname*{arg\\ max}_{\\bm{x}}[\\log p(\\bm{y}"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#Thmremark1.p1", "title": "In the modern practices of data-driven machine learning, for certain popular tasks people often directly learn the conditional distribution p ‚Äã ( ùíô ‚à£ ùíö ) p(\\bm{x}\\mid\\bm{y}) italic_p ( bold_italic_x ‚à£", "snippet": "In the modern practices of data-driven machine learning, for certain popular tasks people often directly learn the conditional distribution p ‚Äã ( ùíô ‚à£ ùíö ) p(\\bm{x}\\mid\\bm{y}) italic_p ( bold_italic_x ‚à£ bold_italic_y ) or a (probabilistic) mapping or a regressor. Such a mapping is often modeled by some deep networks and trained end-to-end with sufficient paired samples ( ùíô , ùíö ) (\\bm{x},\\bm{y}) ( bold_italic_x , bold_italic_y ) . Such an approach is very different from the above Bayesian approach in which both the distribution of ùíô ‚àº p ‚Äã ( ùíô ) \\bm{x}\\sim p(\\bm{x}) bold_italic_x ‚àº italic_p ( bold"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.SS0.SSS0.Px3.p1", "title": "As the support ùíÆ ùíô \\mathcal{S}_{\\bm{x}} caligraphic_S start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT of the distribution of ùíô \\bm{x} bold_italic_x is low-dimensional, we may assume that there exi", "snippet": "As the support ùíÆ ùíô \\mathcal{S}_{\\bm{x}} caligraphic_S start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT of the distribution of ùíô \\bm{x} bold_italic_x is low-dimensional, we may assume that there exists a function F F italic_F such that F ‚Äã ( ùíô ) = ùüé ‚áî ùíô ‚àà ùíÆ ùíô F(\\bm{x})=\\bm{0}\\qquad\\iff\\qquad\\bm{x}\\in\\mathcal{S}_{\\bm{x}} italic_F ( bold_italic_x ) = bold_0 ‚áî bold_italic_x ‚àà caligraphic_S start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT (6.1.8) such that ùíÆ ùíô = F ‚àí 1 ‚Äã ( { ùüé } ) \\mathcal{S}_{\\bm{x}}=F^{-1}(\\{\\bm{0}\\}) caligraphic_S start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT = italic"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.SS0.SSS0.Px3.p2", "title": "The above program may be interpreted in two different ways. Firstly, one may view the first term as the conditional probability of ùíö \\bm{y} bold_italic_y given ùíô \\bm{x} bold_italic_x , and the second ", "snippet": "The above program may be interpreted in two different ways. Firstly, one may view the first term as the conditional probability of ùíö \\bm{y} bold_italic_y given ùíô \\bm{x} bold_italic_x , and the second term as a probability density for ùíô \\bm{x} bold_italic_x : p ‚Äã ( ùíö ‚à£ ùíô ) ‚àù exp ‚Å° ( ‚àí 1 2 ‚Äã ‚Äñ h ‚Äã ( ùíô ) ‚àí ùíö ‚Äñ 2 2 ) , p ‚Äã ( ùíô ) ‚àù exp ‚Å° ( ‚àí Œº 2 ‚Äã ‚Äñ F ‚Äã ( ùíô ) ‚àí ùíÑ ‚Äñ 2 2 ) . p(\\bm{y}\\mid\\bm{x})\\propto\\exp\\Big{(}-\\frac{1}{2}\\|h(\\bm{x})-\\bm{y}\\|_{2}^{2}\\Big{)},\\quad p(\\bm{x})\\propto\\exp\\Big{(}-\\frac{\\mu}{2}\\|F(\\bm{x})-\\bm{c}\\|_{2}^{2}\\Big{)}. italic_p ( bold_italic_y ‚à£ bold_italic_x ) ‚àù roman_exp ( - d"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.SS0.SSS0.Px3.p3", "title": "Secondly, notice that the above program ( 6.1.12 ) is equivalent to: min ùíô ‚Å° 1 2 ‚Äã ‚Äñ h ‚Äã ( ùíô ) ‚àí ùíö ‚Äñ 2 2 + Œº 2 ‚Äã ‚Äñ F ‚Äã ( ùíô ) ‚àí ùùÄ / Œº ‚Äñ 2 2 , \\min_{\\bm{x}}\\frac{1}{2}\\|h(\\bm{x})-\\bm{y}\\|_{2}^{2}+\\frac{", "snippet": "Secondly, notice that the above program ( 6.1.12 ) is equivalent to: min ùíô ‚Å° 1 2 ‚Äã ‚Äñ h ‚Äã ( ùíô ) ‚àí ùíö ‚Äñ 2 2 + Œº 2 ‚Äã ‚Äñ F ‚Äã ( ùíô ) ‚àí ùùÄ / Œº ‚Äñ 2 2 , \\min_{\\bm{x}}\\frac{1}{2}\\|h(\\bm{x})-\\bm{y}\\|_{2}^{2}+\\frac{\\mu}{2}\\big{\\|}F(\\bm{x})-\\bm{\\lambda}/\\mu\\big{\\|}_{2}^{2}, roman_min start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG 2 end_ARG ‚à• italic_h ( bold_italic_x ) - bold_italic_y ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + divide start_ARG italic_Œº end_ARG start_ARG 2 end_ARG ‚à• italic_F ( bold_italic_x ) - bold_italic_Œª "}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.SS0.SSS0.Px4.p1", "title": "In practice, however, initial information about the distributions of ùíô \\bm{x} bold_italic_x and the relationship between ùíô \\bm{x} bold_italic_x and ùíö \\bm{y} bold_italic_y can be given in many differen", "snippet": "In practice, however, initial information about the distributions of ùíô \\bm{x} bold_italic_x and the relationship between ùíô \\bm{x} bold_italic_x and ùíö \\bm{y} bold_italic_y can be given in many different ways or forms. In general, they can mostly be categorized into four cases, which are, conceptually, increasingly more challenging: ‚Ä¢ Case 1: Both a model for the distribution of ùíô \\bm{x} bold_italic_x and the observation model ùíö = h ‚Äã ( ùíô ) \\bm{y}=h(\\bm{x}) bold_italic_y = italic_h ( bold_italic_x ) ( + ùíò ) (+\\bm{w}) ( + bold_italic_w ) is known, even with an analytical form. This is typically t"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.I1.i1.p1", "title": "Case 1: Both a model for the distribution of ùíô \\bm{x} bold_italic_x and the observation model ùíö = h ‚Äã ( ùíô ) \\bm{y}=h(\\bm{x}) bold_italic_y = italic_h ( bold_italic_x ) ( + ùíò ) (+\\bm{w}) ( + bold_itali", "snippet": "Case 1: Both a model for the distribution of ùíô \\bm{x} bold_italic_x and the observation model ùíö = h ‚Äã ( ùíô ) \\bm{y}=h(\\bm{x}) bold_italic_y = italic_h ( bold_italic_x ) ( + ùíò ) (+\\bm{w}) ( + bold_italic_w ) is known, even with an analytical form. This is typically the case for many classic signal processing problems, such as signal denoising, the sparse vector recovery problem we saw in Chapter 2 and the low-rank matrix recovery problem to be introduced below."}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.I1.i2.p1", "title": "Case 2: We do not have a model for the distribution but only samples ùëø = { ùíô 1 , ‚Ä¶ , ùíô N } \\bm{X}=\\{\\bm{x}_{1},\\ldots,\\bm{x}_{N}\\} bold_italic_X = { bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRI", "snippet": "Case 2: We do not have a model for the distribution but only samples ùëø = { ùíô 1 , ‚Ä¶ , ùíô N } \\bm{X}=\\{\\bm{x}_{1},\\ldots,\\bm{x}_{N}\\} bold_italic_X = { bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT } of ùíô \\bm{x} bold_italic_x , and the observation model ùíö = h ‚Äã ( ùíô ) \\bm{y}=h(\\bm{x}) bold_italic_y = italic_h ( bold_italic_x ) ( + ùíò ) (+\\bm{w}) ( + bold_italic_w ) is known. 3 3 3 In the literature, this setting is sometimes referred to as the empirical Bayesian inference . A model for the distribution p ‚Äã ( ùíô ) p(\\bm{x}) it"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.I1.i3.p1", "title": "Case 3: We only have the paired samples: ( ùëø , ùíÄ ) = { ( ùíô 1 , ùíö 1 ) , ‚Ä¶ , ( ùíô N , ùíö N ) } (\\bm{X},\\bm{Y})=\\{(\\bm{x}_{1},\\bm{y}_{1}),\\ldots,(\\bm{x}_{N},\\bm{y}_{N})\\} ( bold_italic_X , bold_italic_Y ) ", "snippet": "Case 3: We only have the paired samples: ( ùëø , ùíÄ ) = { ( ùíô 1 , ùíö 1 ) , ‚Ä¶ , ( ùíô N , ùíö N ) } (\\bm{X},\\bm{Y})=\\{(\\bm{x}_{1},\\bm{y}_{1}),\\ldots,(\\bm{x}_{N},\\bm{y}_{N})\\} ( bold_italic_X , bold_italic_Y ) = { ( bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , ‚Ä¶ , ( bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT , bold_italic_y start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ) } of the two variables ( ùíô , ùíö ) (\\bm{x},\\bm{y}) ( bold_italic_x , bold_italic_y ) . The distributions of ùíô \\bm{x} bold_italic_x and ùíö \\bm{y} bold_i"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.I1.i4.p1", "title": "Case 4: We only have the samples ùíÄ = { ùíö 1 , ‚Ä¶ , ùíö N } \\bm{Y}=\\{\\bm{y}_{1},\\ldots,\\bm{y}_{N}\\} bold_italic_Y = { bold_italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_y start_POSTSUB", "snippet": "Case 4: We only have the samples ùíÄ = { ùíö 1 , ‚Ä¶ , ùíö N } \\bm{Y}=\\{\\bm{y}_{1},\\ldots,\\bm{y}_{N}\\} bold_italic_Y = { bold_italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_y start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT } of the observations ùíö \\bm{y} bold_italic_y , and the observation model h ‚Äã ( ‚ãÖ ) h(\\cdot) italic_h ( ‚ãÖ ) needs to be known, at least in some parametric family h ‚Äã ( ‚ãÖ , ùúΩ ) h(\\cdot,\\bm{\\theta}) italic_h ( ‚ãÖ , bold_italic_Œ∏ ) . The distribution p ‚Äã ( ùíô ) p(\\bm{x}) italic_p ( bold_italic_x ) and p ‚Äã ( ùíô ‚à£ ùíö ) p(\\bm{x}\\mid\\bm{y}) italic_p ( bold_italic_x ‚à£ bold_ita"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S2.p1", "title": "Notice that in the setting we have discussed in previous Chapters, the autoencoding network is trained to reconstruct a set of samples of the random vector ùíô \\bm{x} bold_italic_x . This would allow us", "snippet": "Notice that in the setting we have discussed in previous Chapters, the autoencoding network is trained to reconstruct a set of samples of the random vector ùíô \\bm{x} bold_italic_x . This would allow us to regenerate samples from the learned (low-dimensional) distribution. In practice, the low-dimensionality of the distribution, once given or learned, can be exploited for stable and robust recovery, completion, or prediction tasks. That is, under rather mild conditions, one can recover ùíô \\bm{x} bold_italic_x , from highly compressive, partial, noisy or even corrupted measures of x x italic_x of "}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S2.p2", "title": "Here to put the classic work in a more general modern setting, we illustrate the basic idea and facts through the arguably simplest task of data (and particularlly image) completion. That is, we consi", "snippet": "Here to put the classic work in a more general modern setting, we illustrate the basic idea and facts through the arguably simplest task of data (and particularlly image) completion. That is, we consider the problem of recovering a sample ùíô \\bm{x} bold_italic_x when parts of it are missing (or even corrupted). We want to recover or predict the rest of ùíô \\bm{x} bold_italic_x from observing only a fraction of it: f : ùí´ Œ© ‚Äã ( ùíô ) ‚Ü¶ ùíô ^ , f:\\mathcal{P}_{\\Omega}(\\bm{x})\\mapsto\\hat{\\bm{x}}, italic_f : caligraphic_P start_POSTSUBSCRIPT roman_Œ© end_POSTSUBSCRIPT ( bold_italic_x ) ‚Ü¶ over^ start_ARG bol"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S2.p3", "title": "In this section and the next, we will study the completion task under two different scenarios: One is when the distribution of the data ùíô \\bm{x} bold_italic_x of interest is already given apriori , ev", "snippet": "In this section and the next, we will study the completion task under two different scenarios: One is when the distribution of the data ùíô \\bm{x} bold_italic_x of interest is already given apriori , even in a certain analytical form. This is the case that prevails in classic signal processing where the structures of the signals are assumed to be known, for example, band-limited, sparse or low-rank. The other is when only raw samples of ùíô \\bm{x} bold_italic_x are available and we need to learn the low-dimensional distribution from the samples in order to solve the completion task well. This is t"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S2.SS0.SSS0.Px1.p1", "title": "The low-rank matrix completion problem is a classical problem for data completion when its distribution is low-dimensional and known. Consider a random sample of a matrix ùëø o = [ ùíô 1 , ‚Ä¶ , ùíô n ] ‚àà ‚Ñù m", "snippet": "The low-rank matrix completion problem is a classical problem for data completion when its distribution is low-dimensional and known. Consider a random sample of a matrix ùëø o = [ ùíô 1 , ‚Ä¶ , ùíô n ] ‚àà ‚Ñù m √ó n \\bm{X}_{o}=[\\bm{x}_{1},\\ldots,\\bm{x}_{n}]\\in\\mathbb{R}^{m\\times n} bold_italic_X start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT = [ bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ] ‚àà blackboard_R start_POSTSUPERSCRIPT italic_m √ó italic_n end_POSTSUPERSCRIPT from the space of all matrices of rank r r italic_r . In genera"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S2.SS0.SSS0.Px1.p2", "title": "Now, let Œ© \\Omega roman_Œ© indicate a set indices of observed entries of the matrix ùëø o \\bm{X}_{o} bold_italic_X start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT . Let the observed entries to be: ùíÄ = ùí´ Œ©", "snippet": "Now, let Œ© \\Omega roman_Œ© indicate a set indices of observed entries of the matrix ùëø o \\bm{X}_{o} bold_italic_X start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT . Let the observed entries to be: ùíÄ = ùí´ Œ© ‚Äã ( ùëø o ) . \\bm{Y}=\\mathcal{P}_{\\Omega}(\\bm{X}_{o}). bold_italic_Y = caligraphic_P start_POSTSUBSCRIPT roman_Œ© end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT ) . (6.2.4) The remaining entries supported on Œ© c \\Omega^{c} roman_Œ© start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT are unobserved or missing. The problem is whether we can recover from ùíÄ \\bm{Y} bold_it"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S2.SS0.SSS0.Px1.p3", "title": "Notice that the fundamental reason why such a matrix can be completed is because columns and rows of the matrix are highly correlated and they all lie on a low-dimensional subspace. For the example sh", "snippet": "Notice that the fundamental reason why such a matrix can be completed is because columns and rows of the matrix are highly correlated and they all lie on a low-dimensional subspace. For the example shown in Figure 6.3 , the dimension or the rank of the matrix completed is only two. Hence the fundamental idea to recover such a matrix is to seek a matrix that has the lowest rank among all matrices that have entries agreeing with the observed ones: min ùëø ‚Å° rank ‚Äã ( ùëø ) subject to ùíÄ = ùí´ Œ© ‚Äã ( ùëø ) . \\min_{\\bm{X}}\\mbox{rank}(\\bm{X})\\quad\\mbox{subject to}\\quad\\bm{Y}=\\mathcal{P}_{\\Omega}(\\bm{X}). roma"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S2.SS0.SSS0.Px1.p4", "title": "Based on our knowledge about compression from Chapter 3 , we could promote the low-rankness of the recovered matrix ùëø \\bm{X} bold_italic_X by enforcing the lossy coding rate (or the volume spanned by ", "snippet": "Based on our knowledge about compression from Chapter 3 , we could promote the low-rankness of the recovered matrix ùëø \\bm{X} bold_italic_X by enforcing the lossy coding rate (or the volume spanned by ùëø \\bm{X} bold_italic_X ) of the data in ùëø \\bm{X} bold_italic_X to be small: min ‚Å° R œµ ‚Äã ( ùëø ) = 1 2 ‚Äã log ‚Äã det ( ùë∞ + Œ± ‚Äã ùëø ‚Äã ùëø ‚ä§ ) subject to ùíÄ = ùí´ Œ© ‚Äã ( ùëø ) . \\min R_{\\epsilon}(\\bm{X})=\\frac{1}{2}\\log\\det\\left(\\bm{I}+\\alpha\\bm{X}\\bm{X}^{\\top}\\right)\\quad\\mbox{subject to}\\quad\\bm{Y}=\\mathcal{P}_{\\Omega}(\\bm{X}). roman_min italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_X ) ="}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S2.SS0.SSS0.Px1.p5", "title": "The rate distortion function is a nonconvex function, and its gradient descent does not always guarantee finding the globally optimal solution. Nevertheless, since the underlying structure sought for ", "snippet": "The rate distortion function is a nonconvex function, and its gradient descent does not always guarantee finding the globally optimal solution. Nevertheless, since the underlying structure sought for ùëø \\bm{X} bold_italic_X is piecewise linear, the rank function admits a rather effective convex relaxation: the nuclear norm‚Äîthe sum of all singular values of the matrix ùëø \\bm{X} bold_italic_X . As shown in the compressive sensing literature, under fairly broad conditions, 4 4 4 Typically, such conditions specify the necessary and sufficient amount of entries needed for the completion to be computa"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S2.SS0.SSS0.Px2.p1", "title": "It has been shown that images (or more accurately textures) and 3D scenes with low-rank structures can be very effectively completed via solving optimization programs of the above kind, even if there ", "snippet": "It has been shown that images (or more accurately textures) and 3D scenes with low-rank structures can be very effectively completed via solving optimization programs of the above kind, even if there is additional corruption and distortion [ ZLG+10 , LRZ+12 , YZB+23 ] : ùíÄ ‚àò œÑ = ùëø o + ùë¨ , \\bm{Y}\\circ\\tau=\\bm{X}_{o}+\\bm{E}, bold_italic_Y ‚àò italic_œÑ = bold_italic_X start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT + bold_italic_E , (6.2.9) where œÑ \\tau italic_œÑ is some unknown nonlinear distortion of the image and ùë¨ \\bm{E} bold_italic_E is an unknown matrix that models some (sparse) occlusion and co"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.p1", "title": "In the previous subsection, the reason we can infer ùíô \\bm{x} bold_italic_x from the partial observation ùíö \\bm{y} bold_italic_y is because (support of) the distribution of ùëø \\bm{X} bold_italic_X is kno", "snippet": "In the previous subsection, the reason we can infer ùíô \\bm{x} bold_italic_x from the partial observation ùíö \\bm{y} bold_italic_y is because (support of) the distribution of ùëø \\bm{X} bold_italic_X is known or specified apriori , say as the set of all low-rank matrices. For many practical dataset, we do not have its distribution in an analytical form as the low-rank matrices, say the set to of all natural images. Nevertheless, if we have sufficient samples of the data ùíô \\bm{x} bold_italic_x , we should be able to learn its low-dimensional distribution first and leverage it for future inference tas"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.SS1.p1", "title": "For a general image ùëø \\bm{X} bold_italic_X such as the one shown on the left of Figure 6.4 , we can no longer view it as a low-rank matrix. However, humans still demonstrate remarkable ability to comp", "snippet": "For a general image ùëø \\bm{X} bold_italic_X such as the one shown on the left of Figure 6.4 , we can no longer view it as a low-rank matrix. However, humans still demonstrate remarkable ability to complete a scene and recognize familiar objects despite severe occlusion. This suggests that our brain has learned the low-dimensional distribution of natural images and can use it for completion, hence recognition. However, the distribution of all natural images is not as simple as a low-dimensional linear subspace. Hence a natural question is whether we can learn the more sophisticated distribution "}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.SS1.p2", "title": "One empirical approach to the image completion task is to find an encoding and decoding scheme by solving the following masked autoencoding (MAE) program that minimizes the reconstruction loss: min f ", "snippet": "One empirical approach to the image completion task is to find an encoding and decoding scheme by solving the following masked autoencoding (MAE) program that minimizes the reconstruction loss: min f , g ‚Å° L MAE ‚Äã ( f , g ) ‚âê ùîº ‚Äã [ ‚Äñ ( g ‚àò f ) ‚Äã ( ùí´ Œ© ‚Äã ( ùëø ) ) ‚àí ùëø ‚Äñ 2 2 ] . \\min_{f,g}L_{\\mathrm{MAE}}(f,g)\\doteq\\mathbb{E}\\big{[}\\|(g\\circ f)(\\mathcal{P}_{\\Omega}(\\bm{X}))-\\bm{X}\\|_{2}^{2}]. roman_min start_POSTSUBSCRIPT italic_f , italic_g end_POSTSUBSCRIPT italic_L start_POSTSUBSCRIPT roman_MAE end_POSTSUBSCRIPT ( italic_f , italic_g ) ‚âê blackboard_E [ ‚à• ( italic_g ‚àò italic_f ) ( caligraphic_P "}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.SS1.p3", "title": "For a general natural image, we can no longer assume its columns or rows are sampled from a low-dimensional subspace or a low-rank Gaussian. However, it is reasonable to assume that the image consists", "snippet": "For a general natural image, we can no longer assume its columns or rows are sampled from a low-dimensional subspace or a low-rank Gaussian. However, it is reasonable to assume that the image consists of multiple regions. Image patches in each region are similar and can be modeled as one (low-rank) Gaussian or subspace. Hence, to exploit the low-dimensionality of the distribution, the objective of the encoder f f italic_f is to transform ùëø \\bm{X} bold_italic_X to a representation ùíÅ \\bm{Z} bold_italic_Z : f : ùëø ‚Ü¶ ùíÅ f:\\bm{X}\\mapsto\\bm{Z} italic_f : bold_italic_X ‚Ü¶ bold_italic_Z (6.3.2) such that"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.SS1.p4", "title": "As we have shown in the previous Chapter 4 , the encoder f f italic_f that minimizes the above objective can be constructed by a sequence of transformer-like operators. As shown in the work of [ PBW+2", "snippet": "As we have shown in the previous Chapter 4 , the encoder f f italic_f that minimizes the above objective can be constructed by a sequence of transformer-like operators. As shown in the work of [ PBW+24 ] , the decoder g g italic_g can be viewed and hence constructed explicitly as the inverse process of the encoder f f italic_f . Figure 6.5 illustrates the overall architectures of both the encoder and the corresponding decoder at each layer. The parameters of the encoder f f italic_f and decoder g g italic_g can be learned by optimizing the reconstruction loss ( 6.3.1 ) via gradient descent."}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.SS1.p5", "title": "Figure 6.6 shows some representative results of the so-designed masked auto-encoder. More implementation details and results of the masked autoencoder for natural image completion can be found in Chap", "snippet": "Figure 6.6 shows some representative results of the so-designed masked auto-encoder. More implementation details and results of the masked autoencoder for natural image completion can be found in Chapter 7 ."}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.SS2.p1", "title": "The above (masked) autoencoding problem aims to generate a sample image that is consistent with certain observations or conditions. But let us examine the approach more closely: Given the visual part ", "snippet": "The above (masked) autoencoding problem aims to generate a sample image that is consistent with certain observations or conditions. But let us examine the approach more closely: Given the visual part of an image ùëø v = ùí´ Œ© ‚Äã ( ùëø ) \\bm{X}_{v}=\\mathcal{P}_{\\Omega}(\\bm{X}) bold_italic_X start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT = caligraphic_P start_POSTSUBSCRIPT roman_Œ© end_POSTSUBSCRIPT ( bold_italic_X ) , we try to estimate the masked part ùëø m = ùí´ Œ© c ‚Äã ( ùëø ) \\bm{X}_{m}=\\mathcal{P}_{\\Omega^{c}}(\\bm{X}) bold_italic_X start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT = caligraphic_P start_POSTS"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.SS2.p2", "title": "For many practical purposes, we would like to learn (a representation of) the conditional distribution p ùëø m ‚à£ ùëø v p_{\\bm{X}_{m}\\mid\\bm{X}_{v}} italic_p start_POSTSUBSCRIPT bold_italic_X start_POSTSUB", "snippet": "For many practical purposes, we would like to learn (a representation of) the conditional distribution p ùëø m ‚à£ ùëø v p_{\\bm{X}_{m}\\mid\\bm{X}_{v}} italic_p start_POSTSUBSCRIPT bold_italic_X start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ‚à£ bold_italic_X start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT end_POSTSUBSCRIPT , or equivalently p ùëø ‚à£ ùëø v p_{\\bm{X}\\mid\\bm{X}_{v}} italic_p start_POSTSUBSCRIPT bold_italic_X ‚à£ bold_italic_X start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT end_POSTSUBSCRIPT , and then get a clear (most likely) sample from this distribution directly. Notice that, when the distribut"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.SS2.p3", "title": "Hence, instead of solving the completion task as a conditional estimation problem, we should address it as a conditional sampling problem. To that end, we should first learn the (low-dimensional) dist", "snippet": "Hence, instead of solving the completion task as a conditional estimation problem, we should address it as a conditional sampling problem. To that end, we should first learn the (low-dimensional) distribution of all natural images ùëø \\bm{X} bold_italic_X . If we have sufficient samples of natural images, we can learn the distribution via a denoising process ùëø t \\bm{X}_{t} bold_italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT described in Chapter 3 . Then the problem of recovering ùëø \\bm{X} bold_italic_X from its partial observation ùíÄ = ùí´ Œ© ‚Äã ( ùíô ) + ùíò \\bm{Y}=\\mathcal{P}_{\\Omega}(\\bm{x})+\\"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.SS2.SSS0.Px1.p1", "title": "In fact, we may even consider recovering ùëø \\bm{X} bold_italic_X from a more general linear observation model: ùíÄ = ùë® ‚Äã ùëø 0 , ùëø t = ùëø 0 + œÉ t ‚Äã ùëÆ , \\bm{Y}=\\bm{A}\\bm{X}_{0},\\quad\\bm{X}_{t}=\\bm{X}_{0}+\\si", "snippet": "In fact, we may even consider recovering ùëø \\bm{X} bold_italic_X from a more general linear observation model: ùíÄ = ùë® ‚Äã ùëø 0 , ùëø t = ùëø 0 + œÉ t ‚Äã ùëÆ , \\bm{Y}=\\bm{A}\\bm{X}_{0},\\quad\\bm{X}_{t}=\\bm{X}_{0}+\\sigma_{t}\\bm{G}, bold_italic_Y = bold_italic_A bold_italic_X start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , bold_italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_X start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + italic_œÉ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_G , (6.3.5) where ùë® \\bm{A} bold_italic_A is a linear operator on matrix space 5 5 5 i.e., if we imagine unrollin"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.SS2.SSS0.Px2.p1", "title": "To generalize the above (image) completion problems and make things more rigorous, we may consider that a random vector ùíô ‚àº p \\bm{x}\\sim p bold_italic_x ‚àº italic_p is partially observed through a more", "snippet": "To generalize the above (image) completion problems and make things more rigorous, we may consider that a random vector ùíô ‚àº p \\bm{x}\\sim p bold_italic_x ‚àº italic_p is partially observed through a more general observation function: ùíö = h ‚Äã ( ùíô ) + ùíò , \\bm{y}=h(\\bm{x})+\\bm{w}, bold_italic_y = italic_h ( bold_italic_x ) + bold_italic_w , (6.3.9) where ùíò \\bm{w} bold_italic_w usually stands for some random measurement noise, say of a Gaussian distribution ùíò ‚àº ùí© ‚Äã ( ùüé , œÉ 2 ‚Äã ùë∞ ) \\bm{w}\\sim\\mathcal{N}(\\mathbf{0},\\sigma^{2}\\bm{I}) bold_italic_w ‚àº caligraphic_N ( bold_0 , italic_œÉ start_POSTSUPERSCRIP"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.SS2.SSS0.Px2.p2", "title": "Like image/matrix completion, we are often faced with a setting where ùíö \\bm{y} bold_italic_y denotes a degraded or otherwise ‚Äúlossy‚Äù observation of the input ùíô \\bm{x} bold_italic_x . This can manifest", "snippet": "Like image/matrix completion, we are often faced with a setting where ùíö \\bm{y} bold_italic_y denotes a degraded or otherwise ‚Äúlossy‚Äù observation of the input ùíô \\bm{x} bold_italic_x . This can manifest in quite different forms. For example, in various scientific or medical imaging problems, the measured data ùíö \\bm{y} bold_italic_y may be a compressed and corrupted observation of the underlying data ùíô \\bm{x} bold_italic_x ; whereas in 3D vision tasks, ùíö \\bm{y} bold_italic_y may represent an image captured by a camera of a physical object with an unknown (low-dimensional) pose ùíô \\bm{x} bold_itali"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.SS2.SSS0.Px2.p3", "title": "At a technical level, we want the learned representation of the data to facilitate us to sample the conditional distribution p ùíô ‚à£ ùíö p_{\\bm{x}\\mid\\bm{y}} italic_p start_POSTSUBSCRIPT bold_italic_x ‚à£ b", "snippet": "At a technical level, we want the learned representation of the data to facilitate us to sample the conditional distribution p ùíô ‚à£ ùíö p_{\\bm{x}\\mid\\bm{y}} italic_p start_POSTSUBSCRIPT bold_italic_x ‚à£ bold_italic_y end_POSTSUBSCRIPT , also known as the posterior, effectively and efficiently. More precisely, write ùùÇ \\bm{\\nu} bold_italic_ŒΩ to denote a realization of the random variable ùíö \\bm{y} bold_italic_y . We want to generate samples ùíô ^ \\hat{\\bm{x}} over^ start_ARG bold_italic_x end_ARG such that: ùíô ^ ‚àº p ùíô ‚à£ ùíö ( ‚ãÖ ‚à£ ùíö = ùùÇ ) . \\hat{\\bm{x}}\\sim p_{\\bm{x}\\mid\\bm{y}}(\\,\\cdot\\,\\mid\\bm{y}=\\bm{\\nu}"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.SS2.SSS0.Px2.p4", "title": "Recall that in Section 3.2 , we have developed a natural and effective way to produce unconditional samples of the data distribution p p italic_p . The ingredients are the denoisers ùíô ¬Ø ‚àó ‚Äã ( t , ùùÉ ) ", "snippet": "Recall that in Section 3.2 , we have developed a natural and effective way to produce unconditional samples of the data distribution p p italic_p . The ingredients are the denoisers ùíô ¬Ø ‚àó ‚Äã ( t , ùùÉ ) = ùîº ‚Äã [ ùíô ‚à£ ùíô t = ùùÉ ] \\bar{\\bm{x}}^{\\ast}(t,\\bm{\\xi})=\\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi}] over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT ( italic_t , bold_italic_Œæ ) = blackboard_E [ bold_italic_x ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_Œæ ] , or their learned approximations ùíô ¬Ø Œ∏ ‚Äã ( t , ùùÉ ) \\bar{\\bm{x}}_{\\theta}(t,\\bm{\\"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.SS2.SSS0.Px2.p5", "title": "Fortunately, it turns out that this is not necessary. Consider the alternate statistical dependency diagram in Figure 6.8 (b), which corresponds to the random variables in the usual denoising-diffusio", "snippet": "Fortunately, it turns out that this is not necessary. Consider the alternate statistical dependency diagram in Figure 6.8 (b), which corresponds to the random variables in the usual denoising-diffusion process, together with the measurement ùíö \\bm{y} bold_italic_y . Because our assumed observation model ( 6.3.9 ) implies that ùíô t \\bm{x}_{t} bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and ùíö \\bm{y} bold_italic_y are independent conditioned on ùíô \\bm{x} bold_italic_x , we have for any realization ùùÇ \\bm{\\nu} bold_italic_ŒΩ of ùíö \\bm{y} bold_italic_y p ùíô t c ‚à£ ùíö ( ‚ãÖ ‚à£ ùùÇ ) = ‚à´ p ùíô t c ‚à£"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.SS2.SSS0.Px2.p6", "title": "The key remaining issue in making this procedure computational is to prescribe how to compute the measurement matching correction, since in general we do not have a closed-form expression for the like", "snippet": "The key remaining issue in making this procedure computational is to prescribe how to compute the measurement matching correction, since in general we do not have a closed-form expression for the likelihood p ùíö ‚à£ ùíô t p_{\\bm{y}\\mid\\bm{x}_{t}} italic_p start_POSTSUBSCRIPT bold_italic_y ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT except for when t = 0 t=0 italic_t = 0 . Before taking up this problem, we discuss an illustrative concrete example of the entire process, continuing from those we have developed in Section 3.2 ."}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#Thmexample2.p1", "title": "Consider the case where the data distribution is Gaussian with mean ùùÅ ‚àà ‚Ñù D \\bm{\\mu}\\in\\mathbb{R}^{D} bold_italic_Œº ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT and covariance ùö∫ ‚àà", "snippet": "Consider the case where the data distribution is Gaussian with mean ùùÅ ‚àà ‚Ñù D \\bm{\\mu}\\in\\mathbb{R}^{D} bold_italic_Œº ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT and covariance ùö∫ ‚àà ‚Ñù D √ó D \\bm{\\Sigma}\\in\\mathbb{R}^{D\\times D} bold_Œ£ ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D √ó italic_D end_POSTSUPERSCRIPT , i.e., ùíô ‚àº ùí© ‚Äã ( ùùÅ , ùö∫ ) \\bm{x}\\sim\\mathcal{N}(\\bm{\\mu},\\bm{\\Sigma}) bold_italic_x ‚àº caligraphic_N ( bold_italic_Œº , bold_Œ£ ) . Assume that ùö∫ ‚™∞ ùüé \\bm{\\Sigma}\\succeq\\mathbf{0} bold_Œ£ ‚™∞ bold_0 is nonzero. Moreover, in the measurement model ( 6.3.9 ), suppose we obtain li"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#Thmexample2.p2", "title": "Equation 6.3.26 is, of course, a direct consequence of our calculations above. However, notice that if we directly interpret this approximation, it is ab initio tractable: the likelihood p ùíö ‚à£ ùíô = ùí© ‚Äã", "snippet": "Equation 6.3.26 is, of course, a direct consequence of our calculations above. However, notice that if we directly interpret this approximation, it is ab initio tractable: the likelihood p ùíö ‚à£ ùíô = ùí© ‚Äã ( ùë® ‚Äã ùíô , œÉ 2 ‚Äã ùë∞ ) p_{\\bm{y}\\mid\\bm{x}}=\\mathcal{N}(\\bm{A}\\bm{x},\\sigma^{2}\\bm{I}) italic_p start_POSTSUBSCRIPT bold_italic_y ‚à£ bold_italic_x end_POSTSUBSCRIPT = caligraphic_N ( bold_italic_A bold_italic_x , italic_œÉ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) is a simple Gaussian distribution centered at the observation, and the approximation to the measurement matching term tha"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#Thmexample2.p3", "title": "To gain insight into the effect of the convenient approximation ( 6.3.26 ), we implement and simulate a simple numerical experiment in the Gaussian setting in Figure 6.9 . The sampler we implement is ", "snippet": "To gain insight into the effect of the convenient approximation ( 6.3.26 ), we implement and simulate a simple numerical experiment in the Gaussian setting in Figure 6.9 . The sampler we implement is a direct implementation of the simple scheme ( 3.2.66 ) we have developed in Chapter 3 and recalled above, using the true conditional posterior denoiser, i.e. Equation 6.3.16 (top row of Figure 6.9 ), and the convenient approximation to this denoiser made with the decomposition ( 6.3.14 ), the posterior denoiser ( 6.3.17 ), and the measurement matching approximation ( 6.3.26 ) (bottom row of Figur"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#Thmexample2.p4", "title": "‚ñ† \\blacksquare ‚ñ†", "snippet": "‚ñ† \\blacksquare ‚ñ†"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.SS2.SSS0.Px2.p7", "title": "Example 6.2 suggests a convenient approximation for the measurement matching term ( 6.3.26 ), which can be made beyond the Gaussian setting of the example. To motivate this approximation in greater ge", "snippet": "Example 6.2 suggests a convenient approximation for the measurement matching term ( 6.3.26 ), which can be made beyond the Gaussian setting of the example. To motivate this approximation in greater generality, notice that by conditional independence of ùíö \\bm{y} bold_italic_y and ùíô t \\bm{x}_{t} bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT given ùíô \\bm{x} bold_italic_x , we can write p ùíö ‚à£ ùíô t ‚Äã ( ùùÇ ‚à£ ùùÉ ) = ‚à´ p ùíö ‚à£ ùíô ‚Äã ( ùùÇ ‚à£ ùùÉ ‚Ä≤ ) ‚Äã p ùíô ‚à£ ùíô t ‚Äã ( ùùÉ ‚Ä≤ ‚à£ ùùÉ ) ‚Äã d ùùÉ ‚Ä≤ . p_{\\bm{y}\\mid\\bm{x}_{t}}(\\bm{\\nu}\\mid\\bm{\\xi})=\\int p_{\\bm{y}\\mid\\bm{x}}(\\bm{\\nu}\\mid\\bm{\\xi}^{\\prime})p_{\\bm{x}\\mid"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.SS2.SSS0.Px2.p8", "title": "Thus, with the DPS approximation, we arrive at the following approximation for the conditional posterior denoisers ùîº ‚Äã [ ùíô ‚à£ ùíö , ùíô t ] \\mathbb{E}[\\bm{x}\\mid\\bm{y},\\bm{x}_{t}] blackboard_E [ bold_itali", "snippet": "Thus, with the DPS approximation, we arrive at the following approximation for the conditional posterior denoisers ùîº ‚Äã [ ùíô ‚à£ ùíö , ùíô t ] \\mathbb{E}[\\bm{x}\\mid\\bm{y},\\bm{x}_{t}] blackboard_E [ bold_italic_x ‚à£ bold_italic_y , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ] , via Equation 6.3.14 : ùîº [ ùíô ‚à£ ùíô t = ùùÉ , ùíö = ùùÇ ] ‚âà ùîº [ ùíô ‚à£ ùíô t = ùùÉ ] + t 2 ‚àá ùùÉ log p ùíö ‚à£ ùíô ( ùùÇ ‚à£ ùîº [ ùíô ‚à£ ùíô t = ùùÉ ] ) . \\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi},\\bm{y}=\\bm{\\nu}]\\approx\\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi}]+t^{2}\\nabla_{\\bm{\\xi}}\\log p_{\\bm{y}\\mid\\bm{x}}(\\bm{\\nu}\\mid\\mathbb{E}[\\bm{x}\\mid\\bm{x}_{"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.I1.i1.p1", "title": "A pretrained denoiser ùíô ¬Ø Œ∏ ‚Äã ( t , ùùÉ ) \\bar{\\bm{x}}_{\\theta}(t,\\bm{\\xi}) over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( italic_t , bold_italic_Œæ ) for the data", "snippet": "A pretrained denoiser ùíô ¬Ø Œ∏ ‚Äã ( t , ùùÉ ) \\bar{\\bm{x}}_{\\theta}(t,\\bm{\\xi}) over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( italic_t , bold_italic_Œæ ) for the data distribution p p italic_p (of ùíô ) \\bm{x}) bold_italic_x ) , learned as in Section 3.2 via Algorithm 3.2 ;"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.I1.i2.p1", "title": "Forward and backward pass access to the forward model h h italic_h for the measurements ( 6.3.9 );", "snippet": "Forward and backward pass access to the forward model h h italic_h for the measurements ( 6.3.9 );"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.I1.i3.p1", "title": "A forward and backward pass through ùíô ¬Ø Œ∏ ‚Äã ( t , ùùÉ ) \\bar{\\bm{x}}_{\\theta}(t,\\bm{\\xi}) over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( italic_t , bold_italic_Œæ ", "snippet": "A forward and backward pass through ùíô ¬Ø Œ∏ ‚Äã ( t , ùùÉ ) \\bar{\\bm{x}}_{\\theta}(t,\\bm{\\xi}) over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( italic_t , bold_italic_Œæ ) , which can be evaluated efficiently using (say) backpropagation."}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.SS2.SSS0.Px2.p9", "title": "Combining this scheme with the basic implementation of unconditional sampling we developed in Section 3.2 , we obtain a practical algorithm for conditional sampling of the posterior p ùíô ‚à£ ùíö p_{\\bm{x}\\", "snippet": "Combining this scheme with the basic implementation of unconditional sampling we developed in Section 3.2 , we obtain a practical algorithm for conditional sampling of the posterior p ùíô ‚à£ ùíö p_{\\bm{x}\\mid\\bm{y}} italic_p start_POSTSUBSCRIPT bold_italic_x ‚à£ bold_italic_y end_POSTSUBSCRIPT given measurements following ( 6.3.9 ). Algorithm 6.1 records this scheme for the case of Gaussian observation noise with known standard deviation œÉ \\sigma italic_œÉ , with minor modifications to extend to a general noising process, as in Equation 3.2.69 and the surrounding discussion in Chapter 3 (our discussio"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.SS3.p1", "title": "The type of conditional estimation or generation problems arise rather naturally in many practical applications. A typical problem of this kind is how to estimate and generate body pose and hand gestu", "snippet": "The type of conditional estimation or generation problems arise rather naturally in many practical applications. A typical problem of this kind is how to estimate and generate body pose and hand gesture conditioned on a given head pose and egocentric images, as illustrated in Figure 6.10 . This is often the problem we need to solve when one is wearing a head-mounted device such as the Vision-pro from Apple or the Project Aria from Meta. The pose of the whole body and the gesture of the hands need to be inferred so that we can use the information to control virtual objects that the person inter"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.SS3.p2", "title": "Notice that in this case, one only has the head pose provided by the device and a very limited field of view for part of one‚Äôs hands and upper limbs. The pose of the rest of the body needs to ‚Äúinferre", "snippet": "Notice that in this case, one only has the head pose provided by the device and a very limited field of view for part of one‚Äôs hands and upper limbs. The pose of the rest of the body needs to ‚Äúinferred‚Äù or ‚Äúcompleted‚Äù based on such partial information. The only way one can estimate the body pose over time is by learning the joint distribution of the head and body pose sequences in advance and then sample this prior distribution conditioned on the real-time partial inputs. Figure 6.11 outlines a system called EgoAllo [ YYZ+24 ] to solve this problem based on a learned conditional diffusion-deno"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.SS3.p3", "title": "Figure 6.12 compares some ground truth motion sequences with sampled results generated by the EgoAllo. Although the figure shows one result for each input head pose sequence, different runs can genera", "snippet": "Figure 6.12 compares some ground truth motion sequences with sampled results generated by the EgoAllo. Although the figure shows one result for each input head pose sequence, different runs can generate different body pose sequences that are consistent with the given head pose, all drawing from the distribution of natural full-body motion sequences."}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.SS3.p4", "title": "Strictly speaking, the solution proposed in EgoAllo [ YYZ+24 ] does not enforce measurement matching using the techniques introduced above. Instead it heuristically enforces the condition by utilizing", "snippet": "Strictly speaking, the solution proposed in EgoAllo [ YYZ+24 ] does not enforce measurement matching using the techniques introduced above. Instead it heuristically enforces the condition by utilizing the cross-attention mechanism in a transformer architecture. As we will describe with more precision in the paired data setting in Section 6.4.2 , there is reason to believe that the cross-attention mechanism is in a way approximately realizing the conditional sampling of the denoising a posteriori. We believe the more principled techniques introduced here, if properly implemented, can lead to be"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.p1", "title": "In many practical applications, we do not know either the distribution of the data ùíô \\bm{x} bold_italic_x of interest nor the explicit relationship between the data and certain observed attributes ùíö \\", "snippet": "In many practical applications, we do not know either the distribution of the data ùíô \\bm{x} bold_italic_x of interest nor the explicit relationship between the data and certain observed attributes ùíö \\bm{y} bold_italic_y of the data. We only have a (large) set of paired samples ( ùëø , ùíÄ ) = { ( ùíô 1 , ùíö 1 ) , ‚Ä¶ , ( ùíô N , ùíö N ) } (\\bm{X},\\bm{Y})=\\{(\\bm{x}_{1},\\bm{y}_{1}),\\ldots,(\\bm{x}_{N},\\bm{y}_{N})\\} ( bold_italic_X , bold_italic_Y ) = { ( bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , ‚Ä¶ , ( bold_italic_x start_POSTSUBSCRIPT ita"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.p2", "title": "The problem of image classification can be viewed as one such an example. In a sense, the classification problem is to learn a (extremely lossy) compressive encoder for natural images. Say, given a ra", "snippet": "The problem of image classification can be viewed as one such an example. In a sense, the classification problem is to learn a (extremely lossy) compressive encoder for natural images. Say, given a random sample of an image ùíô \\bm{x} bold_italic_x , we would like to predict its class label ùíö \\bm{y} bold_italic_y that best correlates the content in ùíô \\bm{x} bold_italic_x . We know the distribution of natural images of objects is low-dimensional compared to the dimension of the pixel space. From the previous chapters, we have learned that given sufficient samples, in principle, we can learn a str"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.p3", "title": "From our study in previous chapters, the mapping f f italic_f is usually learned as a sequence of compression or denoising operators in the same space. Hence to leverage on such family of operations, ", "snippet": "From our study in previous chapters, the mapping f f italic_f is usually learned as a sequence of compression or denoising operators in the same space. Hence to leverage on such family of operations, we may introduce an auxiliary vector ùíò \\bm{w} bold_italic_w that can be viewed as an initial random guess of the class label ùíö \\bm{y} bold_italic_y . In this way, we can learn a compression or denoising mapping: f : ( ùíô , ùíò ) ‚Ü¶ ( ùíõ , ùíö ) f:(\\bm{x},\\bm{w})\\mapsto(\\bm{z},\\bm{y}) italic_f : ( bold_italic_x , bold_italic_w ) ‚Ü¶ ( bold_italic_z , bold_italic_y ) (6.4.4) within a common space. In fact, t"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.SS1.p1", "title": "While a learned classifier allows us to classify a given image ùíô \\bm{x} bold_italic_x to its corresponding class, we often like to generate an image of a given class, by sampling the learned distribut", "snippet": "While a learned classifier allows us to classify a given image ùíô \\bm{x} bold_italic_x to its corresponding class, we often like to generate an image of a given class, by sampling the learned distribution of natural images. To some extent, this can be viewed as the ‚Äúinverse‚Äù problem to image classification. Let p ùíô p_{\\bm{x}} italic_p start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT denote the distribution of natural images, say modeled by a diffusion-denoising process. Given a class label random variable y ‚àà [ K ] y\\in[K] italic_y ‚àà [ italic_K ] with realization ŒΩ \\nu italic_ŒΩ , say an ‚ÄúApp"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.SS1.p2", "title": "In Section 6.3.2 , we have seen how to use the denoising-diffusion paradigm for conditional sampling from the posterior p ùíô ‚à£ ùíö p_{\\bm{x}\\mid\\bm{y}} italic_p start_POSTSUBSCRIPT bold_italic_x ‚à£ bold_i", "snippet": "In Section 6.3.2 , we have seen how to use the denoising-diffusion paradigm for conditional sampling from the posterior p ùíô ‚à£ ùíö p_{\\bm{x}\\mid\\bm{y}} italic_p start_POSTSUBSCRIPT bold_italic_x ‚à£ bold_italic_y end_POSTSUBSCRIPT given model-based measurements ùíö = h ‚Äã ( ùíô ) + ùíò \\bm{y}=h(\\bm{x})+\\bm{w} bold_italic_y = italic_h ( bold_italic_x ) + bold_italic_w ( Equation 6.3.9 ), culminating in the DPS algorithm ( 6.1 ). This is a powerful framework, but it does not apply to the class (or text) conditioned image generation problem here, where an explicit generative model h h italic_h for the observ"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.SS1.p3", "title": "Thus, we now assume only that we have access to samples from the joint distribution of ( ùíô , y ) (\\bm{x},y) ( bold_italic_x , italic_y ) : ( ùíô , y ) ‚àº p ùíô , y . (\\bm{x},y)\\sim p_{\\bm{x},y}. ( bold_ita", "snippet": "Thus, we now assume only that we have access to samples from the joint distribution of ( ùíô , y ) (\\bm{x},y) ( bold_italic_x , italic_y ) : ( ùíô , y ) ‚àº p ùíô , y . (\\bm{x},y)\\sim p_{\\bm{x},y}. ( bold_italic_x , italic_y ) ‚àº italic_p start_POSTSUBSCRIPT bold_italic_x , italic_y end_POSTSUBSCRIPT . (6.4.6) As in the previous section, we define ùíô t = Œ± t ‚Äã ùíô + œÉ t ‚Äã ùíà \\bm{x}_{t}=\\alpha_{t}\\bm{x}+\\sigma_{t}\\bm{g} bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_Œ± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_x + italic_œÉ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.SS1.p4", "title": "To proceed, we note that our development of conditional sampling under measurements ùíö = h ‚Äã ( ùíô ) + ùíò \\bm{y}=h(\\bm{x})+\\bm{w} bold_italic_y = italic_h ( bold_italic_x ) + bold_italic_w only explicitly", "snippet": "To proceed, we note that our development of conditional sampling under measurements ùíö = h ‚Äã ( ùíô ) + ùíò \\bm{y}=h(\\bm{x})+\\bm{w} bold_italic_y = italic_h ( bold_italic_x ) + bold_italic_w only explicitly used the forward model h h italic_h in making the DPS approximation ( 6.3.26 ). In particular, the conditional posterior denoiser decomposition ( 6.3.14 ) still holds in the paired data setting , by virtue of Bayes‚Äô rule and conditional independence of ùíö \\bm{y} bold_italic_y and ùíô t \\bm{x}_{t} bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT given ùíô \\bm{x} bold_italic_x (recall Figure"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.SS1.p5", "title": "However, this straightforward methodology has two key drawbacks (which is why we label it as ‚Äúnaive‚Äù). The first is that, empirically, such a trained deep network classifier frequently does not provid", "snippet": "However, this straightforward methodology has two key drawbacks (which is why we label it as ‚Äúnaive‚Äù). The first is that, empirically, such a trained deep network classifier frequently does not provide a strong enough guidance signal (in Equation 6.4.7 ) to ensure that generated samples reflect the conditioning information y y italic_y . This was first emphasized by [ DN21 ] , who noted that in the setting of class-conditional ImageNet generation, the learned deep network classifier‚Äôs probability outputs for the class y y italic_y being conditioned on were frequently around 0.5 0.5 0.5 ‚Äîlarge "}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.SS1.p6", "title": "Nevertheless, classifier guidance does not address the second key drawback of the naive methodology: it is both cumbersome and wasteful to have to train an auxiliary classifier f Œ∏ c f_{\\theta_{\\mathr", "snippet": "Nevertheless, classifier guidance does not address the second key drawback of the naive methodology: it is both cumbersome and wasteful to have to train an auxiliary classifier f Œ∏ c f_{\\theta_{\\mathrm{c}}} italic_f start_POSTSUBSCRIPT italic_Œ∏ start_POSTSUBSCRIPT roman_c end_POSTSUBSCRIPT end_POSTSUBSCRIPT in addition to the unconditional denoiser ùíô ¬Ø Œ∏ d \\bar{\\bm{x}}_{\\theta_{\\mathrm{d}}} over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_Œ∏ start_POSTSUBSCRIPT roman_d end_POSTSUBSCRIPT end_POSTSUBSCRIPT , given that it is not possible to directly adapt a pretrained classifier d"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.SS1.p7", "title": "This state of affairs, empirically-motivated as it is, led [ HS22 ] in subsequent work to propose a more empirically pragmatic methodology, known as classifier-free guidance (CFG). Instead of represen", "snippet": "This state of affairs, empirically-motivated as it is, led [ HS22 ] in subsequent work to propose a more empirically pragmatic methodology, known as classifier-free guidance (CFG). Instead of representing the conditional denoiser ( 6.4.7 ) as a weighted sum of an unconditional denoiser for ùíô t \\bm{x}_{t} bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT with a log-likelihood correction term (with possibly modified weights, as in classifier guidance), they accept the apparent necessity of training a conditional denoiser for ùíô t \\bm{x}_{t} bold_italic_x start_POSTSUBSCRIPT italic_t en"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.SS1.p8", "title": "However, following [ HS22 ] and the common practice of training deep network denoisers, it is standard to use the same deep network to represent both the conditional and unconditional denoisers, by in", "snippet": "However, following [ HS22 ] and the common practice of training deep network denoisers, it is standard to use the same deep network to represent both the conditional and unconditional denoisers, by introducing an additional label, which we will denote by ‚àÖ \\varnothing ‚àÖ , to denote the ‚Äúunconditional‚Äù case. This leads to the form of the CFG denoiser: ùíô ¬Ø Œ∏ CFG ‚Äã ( t , ùíô t , y ) = ( 1 ‚àí Œ≥ ) ‚Äã ùíô ¬Ø Œ∏ ‚Äã ( t , ùíô t , ‚àÖ ) + Œ≥ ‚Äã ùíô ¬Ø Œ∏ ‚Äã ( t , ùíô t , y ) . \\bar{\\bm{x}}_{\\theta}^{\\mathrm{CFG}}(t,\\bm{x}_{t},y)=(1-\\gamma)\\bar{\\bm{x}}_{\\theta}(t,\\bm{x}_{t},\\varnothing)+\\gamma\\bar{\\bm{x}}_{\\theta}(t,\\bm{x}_{"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.I1.i1.p1", "title": "When we sample from the dataset, we sample a pair ( ùíô , y ) (\\bm{x},y) ( bold_italic_x , italic_y ) rather than just a sample ùíô \\bm{x} bold_italic_x .", "snippet": "When we sample from the dataset, we sample a pair ( ùíô , y ) (\\bm{x},y) ( bold_italic_x , italic_y ) rather than just a sample ùíô \\bm{x} bold_italic_x ."}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.I1.i2.p1", "title": "Every time we sample a pair from the dataset, we sample the augmented label y + y^{+} italic_y start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT via y + = { ‚àÖ with probability ‚Äã p uncond ; y else . y^{+}=\\b", "snippet": "Every time we sample a pair from the dataset, we sample the augmented label y + y^{+} italic_y start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT via y + = { ‚àÖ with probability ‚Äã p uncond ; y else . y^{+}=\\begin{cases}\\varnothing&\\text{with probability }p_{\\mathrm{uncond}};\\\\ y&\\text{else}.\\end{cases} italic_y start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT = { start_ROW start_CELL ‚àÖ end_CELL start_CELL with probability italic_p start_POSTSUBSCRIPT roman_uncond end_POSTSUBSCRIPT ; end_CELL end_ROW start_ROW start_CELL italic_y end_CELL start_CELL else . end_CELL end_ROW (6.4.16) Here, p uncond ‚àà [ 0 , 1 "}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.SS1.p9", "title": "[ HS22 ] report strong empirical performance for class-conditional image generation with classifier-free guidance, and it has become a mainstay of the largest-scale practical diffusion models, such as", "snippet": "[ HS22 ] report strong empirical performance for class-conditional image generation with classifier-free guidance, and it has become a mainstay of the largest-scale practical diffusion models, such as Stable Diffusion [ RBL+22 ] and its derivatives. At the same time, its derivation is rather opaque and empirically-motivated, giving little insight into the mechanisms behind its strong performance. A number of theoretical works have studied this, providing explanations for some parts of the overall CFG methodology [ BN24a , LWQ25 , WCL+24 ] ‚Äîitself encompassing denoiser parameterization and trai"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#Thmexample3.p1", "title": "Let us recall the low-rank mixture of Gaussians data generating process we studied in Example 3.2 (and specifically, the form in Equation 3.2.42 ). Given K ‚àà ‚Ñï K\\in\\mathbb{N} italic_K ‚àà blackboard_N c", "snippet": "Let us recall the low-rank mixture of Gaussians data generating process we studied in Example 3.2 (and specifically, the form in Equation 3.2.42 ). Given K ‚àà ‚Ñï K\\in\\mathbb{N} italic_K ‚àà blackboard_N classes, we assume that ùíô ‚àº 1 K ‚Äã ‚àë k = 1 K ùí© ‚Å° ( ùüé , ùëº k ‚Äã ùëº k ‚ä§ ) , \\bm{x}\\sim\\frac{1}{K}\\sum_{k=1}^{K}\\operatorname{\\mathcal{N}}(\\bm{0},\\bm{U}_{k}\\bm{U}_{k}^{\\top}), bold_italic_x ‚àº divide start_ARG 1 end_ARG start_ARG italic_K end_ARG ‚àë start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT caligraphic_N ( bold_0 , bold_italic_U start_POSTSUBSCRIPT"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.I2.i1.p1", "title": "Well-correlated regime: If ùíô t \\bm{x}_{t} bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT correlates well with ùëº y \\bm{U}_{y} bold_italic_U start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT ", "snippet": "Well-correlated regime: If ùíô t \\bm{x}_{t} bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT correlates well with ùëº y \\bm{U}_{y} bold_italic_U start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT , then the normalized weight corresponding to the k = y k=y italic_k = italic_y summand in the unconditional denoiser is near to 1 1 1 . Then Œ≥ + ( 1 ‚àí Œ≥ ) ‚Äã exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ‚Äñ ùëº y ‚ä§ ‚Äã ùíô t ‚Äñ 2 2 ) ‚àë i = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ‚Äñ ùëº i ‚ä§ ‚Äã ùíô t ‚Äñ 2 2 ) ‚âà 1 , \\gamma+(1-\\gamma)\\frac{\\exp\\left(\\frac{1}{2t^{2}(1+t^{2})}\\|\\bm{U}_{y}^{\\top}\\bm{x}_{t}\\|_{2}^{2}\\right)}{\\sum_{i="}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.I2.i2.p1", "title": "Poorly-correlated regime: In contrast, if ùíô t \\bm{x}_{t} bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT does not correlate well with ùëº y \\bm{U}_{y} bold_italic_U start_POSTSUBSCRIPT ital", "snippet": "Poorly-correlated regime: In contrast, if ùíô t \\bm{x}_{t} bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT does not correlate well with ùëº y \\bm{U}_{y} bold_italic_U start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT (say because t t italic_t is large), then the normalized weight corresponding to the k = y k=y italic_k = italic_y summand in the unconditional denoiser is near to 0 . As a result, Œ≥ + ( 1 ‚àí Œ≥ ) ‚Äã exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ‚Äñ ùëº y ‚ä§ ‚Äã ùíô t ‚Äñ 2 2 ) ‚àë i = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ‚Äñ ùëº i ‚ä§ ‚Äã ùíô t ‚Äñ 2 2 ) ‚âà Œ≥ , \\gamma+(1-\\gamma)\\frac{\\exp\\left(\\frac{1}{2t^{2}(1+t"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#Thmexample3.p2", "title": "We now perform a further analysis of the form of this guided denoiser in order to make some inferences about the role of CFG. Many of these insights will be relevant to general data distributions with", "snippet": "We now perform a further analysis of the form of this guided denoiser in order to make some inferences about the role of CFG. Many of these insights will be relevant to general data distributions with low-dimensional geometric structure, as well. First, notice that the CFG denoiser ( 6.4.20 ) takes a simple form in the setting where ùíô t \\bm{x}_{t} bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT correlates significantly more strongly with a single subspace ùëº y \\bm{U}_{y} bold_italic_U start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT than any other ùëº y ‚Ä≤ \\bm{U}_{y^{\\prime}} bold_itali"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#Thmexample3.p3", "title": "Next, we consider the problem of parameterizing a learnable denoiser ùíô Œ∏ CFG \\bm{x}_{\\theta}^{\\mathrm{CFG}} bold_italic_x start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_CFG", "snippet": "Next, we consider the problem of parameterizing a learnable denoiser ùíô Œ∏ CFG \\bm{x}_{\\theta}^{\\mathrm{CFG}} bold_italic_x start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_CFG end_POSTSUPERSCRIPT to represent the optimal denoiser ( 6.4.20 ). Here, it may initially seem that the setting of classification of a mixture distribution is too much of a special case relative to learning practical data distributions, as the ideal denoiser ( 6.4.20 ) has in this setting the simple form of a hard assignment of the noisy signal ùíô t \\bm{x}_{t} bold_italic_x start_POSTSUBSCRIPT ital"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.SS1.p10", "title": "Example 6.3 shows that in the special case of a low-rank mixture of Gaussians data distribution for ùíô \\bm{x} bold_italic_x with incoherent components, operators of the form ( ùíô t , ùíó ) ‚Ü¶ ‚àë k = 1 K exp", "snippet": "Example 6.3 shows that in the special case of a low-rank mixture of Gaussians data distribution for ùíô \\bm{x} bold_italic_x with incoherent components, operators of the form ( ùíô t , ùíó ) ‚Ü¶ ‚àë k = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ùíô t ‚ä§ ‚Äã ùëº k ‚Äã ùëº k ‚ä§ ‚Äã ùíó ) ‚àë i = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ùíô t ‚ä§ ‚Äã ùëº i ‚Äã ùëº i ‚ä§ ‚Äã ùíó ) ‚Äã ùëº k ‚Äã ùëº k ‚ä§ ‚Äã ùíô t (\\bm{x}_{t},\\bm{v})\\mapsto\\sum_{k=1}^{K}\\frac{\\exp\\left(\\frac{1}{2t^{2}(1+t^{2})}\\bm{x}_{t}^{\\top}\\bm{U}_{k}\\bm{U}_{k}^{\\top}\\bm{v}\\right)}{\\sum_{i=1}^{K}\\exp\\left(\\frac{1}{2t^{2}(1+t^{2})}\\bm{x}_{t}^{\\top}\\bm{U}_{i}\\bm{U}_{i}^{\\top}\\bm{v}\\right)}\\bm{U}"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.SS2.p1", "title": "In the previous subsection, we have formulated denoisers for class-conditional denoising with classifier-free guidance, a ubiquitous practical methodology used in the largest-scale diffusion models, a", "snippet": "In the previous subsection, we have formulated denoisers for class-conditional denoising with classifier-free guidance, a ubiquitous practical methodology used in the largest-scale diffusion models, and shown how to parameterize them (in Example 6.3 ) in the special case of a low-rank Gaussian mixture model data distribution. One interesting byproduct of this example is that it highlights the crucial role of embeddings of the class label y y italic_y into a common space with the image ùíô \\bm{x} bold_italic_x in order to provide a concise and unified scheme for parameterizing the optimal denoise"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.SS2.p2", "title": "Stable Diffusion follows the conditional generation methodology we outline in Section 6.4.1 , with two key modifications: (i) The conditioning signal is a tokenized text prompt ùíÄ \\bm{Y} bold_italic_Y ", "snippet": "Stable Diffusion follows the conditional generation methodology we outline in Section 6.4.1 , with two key modifications: (i) The conditioning signal is a tokenized text prompt ùíÄ \\bm{Y} bold_italic_Y , rather than a class label; (ii) Image denoising is performed in ‚Äúlatent‚Äù space rather than on raw pixels, using a specialized, pretrained variational autoencoder pair f : ‚Ñù D img ‚Üí ‚Ñù d img f:\\mathbb{R}^{D_{\\mathrm{img}}}\\to\\mathbb{R}^{d_{\\mathrm{img}}} italic_f : blackboard_R start_POSTSUPERSCRIPT italic_D start_POSTSUBSCRIPT roman_img end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ‚Üí blackboard_R start_P"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.I3.i1.p1", "title": "At training time , the encoder f : ùíô ‚Ü¶ ùíõ f:\\bm{x}\\mapsto\\bm{z} italic_f : bold_italic_x ‚Ü¶ bold_italic_z is used to generate the denoising targets, and all denoising is performed on the encoded represe", "snippet": "At training time , the encoder f : ùíô ‚Ü¶ ùíõ f:\\bm{x}\\mapsto\\bm{z} italic_f : bold_italic_x ‚Ü¶ bold_italic_z is used to generate the denoising targets, and all denoising is performed on the encoded representations ùíõ t ‚àà ‚Ñù d img \\bm{z}_{t}\\in\\mathbb{R}^{d_{\\mathrm{img}}} bold_italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT roman_img end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ;"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.I3.i2.p1", "title": "At generation time , sampling is performed on the representations ùíõ ^ t \\hat{\\bm{z}}_{t} over^ start_ARG bold_italic_z end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , and the final image is g", "snippet": "At generation time , sampling is performed on the representations ùíõ ^ t \\hat{\\bm{z}}_{t} over^ start_ARG bold_italic_z end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , and the final image is generated by applying the decoder g ‚Äã ( ùíõ ^ 0 ) g(\\hat{\\bm{z}}_{0}) italic_g ( over^ start_ARG bold_italic_z end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) ."}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.SS2.p3", "title": "Notice that, by the definition of the self-attention operation, cross attention outputs linear combinations of the value-projected text embeddings, weighted by correlations between the image features ", "snippet": "Notice that, by the definition of the self-attention operation, cross attention outputs linear combinations of the value-projected text embeddings, weighted by correlations between the image features and the text embeddings. In the denoiser architecture used by [ RBL+22 ] , self-attention residual blocks in the denoiser architecture, applied to the image representation at the current layer and defined analogously to those in Equation 7.2.13 for the vision transformer, are followed by cross attention residual blocks of the form ( 6.4.31 ). Such a structure requires the text encoder œÑ \\tau itali"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.SS2.p4", "title": "This same basic design has been further scaled to even larger model and dataset sizes, in particular in modern instantiations of Stable Diffusion [ EKB+24 ] , as well as in competing models such as FL", "snippet": "This same basic design has been further scaled to even larger model and dataset sizes, in particular in modern instantiations of Stable Diffusion [ EKB+24 ] , as well as in competing models such as FLUX.1 [ LBB+25 ] , Imagen [ SCS+22 ] , and DALL-E [ RDN+22 ] . The conditioning mechanism of cross attention has also become ubiquitous in other applications, as in EgoAllo ( Section 6.3.3 ) for conditioned pose generation and in Michelangelo [ ZLC+23 ] for conditional 3D shape generation based on images or texts."}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.p1", "title": "In this last section, we consider the more extreme, but actually ubiquitous, case for distribution learning in which we only have a set of observed samples ùíÄ = { ùíö 1 , ‚Ä¶ , ùíö N } \\bm{Y}=\\{\\bm{y}_{1},\\l", "snippet": "In this last section, we consider the more extreme, but actually ubiquitous, case for distribution learning in which we only have a set of observed samples ùíÄ = { ùíö 1 , ‚Ä¶ , ùíö N } \\bm{Y}=\\{\\bm{y}_{1},\\ldots,\\bm{y}_{N}\\} bold_italic_Y = { bold_italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_y start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT } of the data ùíô \\bm{x} bold_italic_x , but no samples of ùíô \\bm{x} bold_italic_x directly! In general, the observation ùíö ‚àà ‚Ñù d \\bm{y}\\in\\mathbb{R}^{d} bold_italic_y ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT is of lower d"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.p2", "title": "Let us first try to understand the problem conceptually with the simple case when the measurement function h h italic_h is known and the observed ùíö = h ‚Äã ( ùíô ) + ùíò \\bm{y}=h(\\bm{x})+\\bm{w} bold_italic_", "snippet": "Let us first try to understand the problem conceptually with the simple case when the measurement function h h italic_h is known and the observed ùíö = h ‚Äã ( ùíô ) + ùíò \\bm{y}=h(\\bm{x})+\\bm{w} bold_italic_y = italic_h ( bold_italic_x ) + bold_italic_w is informative about ùíô \\bm{x} bold_italic_x . That is, we assume that h h italic_h is surjective from the space of ùíô \\bm{x} bold_italic_x to that of ùíö \\bm{y} bold_italic_y and the support of the distribution ùíö 0 = h ‚Äã ( ùíô 0 ) \\bm{y}_{0}=h(\\bm{x}_{0}) bold_italic_y start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = italic_h ( bold_italic_x start_POSTSUBSCRIPT 0"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.SS1.p1", "title": "First, for simplicity, let us consider the measurement is a linear function of the data ùíô \\bm{x} bold_italic_x of interest: ùíö = ùë® ‚Äã ùíô . \\bm{y}=\\bm{A}\\bm{x}. bold_italic_y = bold_italic_A bold_italic_x", "snippet": "First, for simplicity, let us consider the measurement is a linear function of the data ùíô \\bm{x} bold_italic_x of interest: ùíö = ùë® ‚Äã ùíô . \\bm{y}=\\bm{A}\\bm{x}. bold_italic_y = bold_italic_A bold_italic_x . (6.5.2) Here the matrix ùë® ‚àà ‚Ñù m √ó n \\bm{A}\\in\\mathbb{R}^{m\\times n} bold_italic_A ‚àà blackboard_R start_POSTSUPERSCRIPT italic_m √ó italic_n end_POSTSUPERSCRIPT is of full row rank and m m italic_m is typically smaller than n n italic_n . We assume ùë® \\bm{A} bold_italic_A is known for now. We are interested in how to learn the distribution of ùíô \\bm{x} bold_italic_x from such measurements. Since we"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.SS1.p2", "title": "Without loss of generality, we assume ùë® \\bm{A} bold_italic_A is of full row rank, i.e., under-determined. Let us define the corresponding process ùíô t \\bm{x}_{t} bold_italic_x start_POSTSUBSCRIPT itali", "snippet": "Without loss of generality, we assume ùë® \\bm{A} bold_italic_A is of full row rank, i.e., under-determined. Let us define the corresponding process ùíô t \\bm{x}_{t} bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT as one that satisfies: ùíö t = ùë® ‚Äã ùíô t . \\bm{y}_{t}=\\bm{A}\\bm{x}_{t}. bold_italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_A bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . (6.5.4)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.SS1.p3", "title": "From the denoising process of ùíö t \\bm{y}_{t} bold_italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , we have ùíö t ‚àí s ‚âà ùíö t + s ‚Äã t ‚Äã ‚àá log ‚Å° p t ‚Äã ( ùíö t ) . \\bm{y}_{t-s}\\approx\\bm{y}_{t}+st\\nab", "snippet": "From the denoising process of ùíö t \\bm{y}_{t} bold_italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , we have ùíö t ‚àí s ‚âà ùíö t + s ‚Äã t ‚Äã ‚àá log ‚Å° p t ‚Äã ( ùíö t ) . \\bm{y}_{t-s}\\approx\\bm{y}_{t}+st\\nabla\\log p_{t}(\\bm{y}_{t}). bold_italic_y start_POSTSUBSCRIPT italic_t - italic_s end_POSTSUBSCRIPT ‚âà bold_italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_s italic_t ‚àá roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) . (6.5.5) Then we have: ùë® ‚Äã ùíô t ‚àí s ‚âà ùë® ‚Äã ùíô t + s ‚Äã t ‚Äã ‚àá log ‚Å° p t ‚Äã ( ùë® ‚Äã ùíô t ) , \\b"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.SS2.p1", "title": "In practice, the measurement model is often nonlinear or only partially known. A typical problem of this kind is actually behind how we can learn a working model of the external world from the images ", "snippet": "In practice, the measurement model is often nonlinear or only partially known. A typical problem of this kind is actually behind how we can learn a working model of the external world from the images perceived, say through our eyes, telescopes or microscopes. In particular, humans and animals are able to build a model of the 3D world ((or 4D for a dynamical world) through a sequence of its 2D projections ‚Äì a sequence of 2D images (or stereo image pairs). The mathematical or geometric model of the projection is generally known: ùíö i = h ‚Äã ( ùíô , Œ∏ i ) + ùíò i , \\bm{y}^{i}=h(\\bm{x},\\theta^{i})+\\bm{w"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.SS2.p2", "title": "In general, we would like to learn the distribution p ‚Äã ( ùíô ) p(\\bm{x}) italic_p ( bold_italic_x ) of the 3D (or 4D) world scene ùíô \\bm{x} bold_italic_x 14 14 14 Here by abuse of notation, we use ùíô \\bm", "snippet": "In general, we would like to learn the distribution p ‚Äã ( ùíô ) p(\\bm{x}) italic_p ( bold_italic_x ) of the 3D (or 4D) world scene ùíô \\bm{x} bold_italic_x 14 14 14 Here by abuse of notation, we use ùíô \\bm{x} bold_italic_x to represent either a point in 3D or a sample of an entire 3D object or a scene which consists of many points. from the perceived 2D images of the world so far. The primary function of such a (visual) world model is to allow us to recognize places where we had been before or predict what the current scene would look alike in a future time at a new viewpoint."}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.SS2.p3", "title": "Let us first examine the special but important case of stereo vision. In this case, we have two calibrated views of the 3D scene ùíô \\bm{x} bold_italic_x : ùíö 0 = h ‚Äã ( ùíô , Œ∏ 0 ) + ùíò 0 , ùíö 1 = h ‚Äã ( ùíô , ", "snippet": "Let us first examine the special but important case of stereo vision. In this case, we have two calibrated views of the 3D scene ùíô \\bm{x} bold_italic_x : ùíö 0 = h ‚Äã ( ùíô , Œ∏ 0 ) + ùíò 0 , ùíö 1 = h ‚Äã ( ùíô , Œ∏ 1 ) + ùíò 1 , \\bm{y}^{0}=h(\\bm{x},\\theta^{0})+\\bm{w}^{0},\\quad\\bm{y}^{1}=h(\\bm{x},\\theta^{1})+\\bm{w}^{1}, bold_italic_y start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT = italic_h ( bold_italic_x , italic_Œ∏ start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT ) + bold_italic_w start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT , bold_italic_y start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT = italic_h ( bold_italic_x , ita"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.SS2.p4", "title": "The main question now is: How to learn (a representation for) the distribution of the 3D scene ùíô \\bm{x} bold_italic_x from its two projections with known relationships? People might question the ratio", "snippet": "The main question now is: How to learn (a representation for) the distribution of the 3D scene ùíô \\bm{x} bold_italic_x from its two projections with known relationships? People might question the rationale for doing this: why this is necessary if the function h ‚Äã ( ‚ãÖ ) h(\\cdot) italic_h ( ‚ãÖ ) is largely invertible? That is, the observation ùíö \\bm{y} bold_italic_y can largely determine the unknown ùíô \\bm{x} bold_italic_x , which is kind of the case for stereo ‚Äì in general, two (calibrated) images contain sufficient information about the scene depth, from the given vintage point. However, 2D images"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.SS2.p5", "title": "Consider the (inverse) denoising processing for the diffusion: ùíö t = ùíö + t ‚Äã ùíà \\bm{y}_{t}=\\bm{y}+t\\bm{g} bold_italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_y + italic_t bold_it", "snippet": "Consider the (inverse) denoising processing for the diffusion: ùíö t = ùíö + t ‚Äã ùíà \\bm{y}_{t}=\\bm{y}+t\\bm{g} bold_italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_y + italic_t bold_italic_g in ( 6.5.11 ), where ùíà \\bm{g} bold_italic_g is standard Gaussian. From the denoising process of ( 6.5.11 ), we have ùíö t ‚àí s = ùíö t + s ‚Äã t ‚Äã ‚àá ùíö log ‚Å° p t ‚Äã ( ùíö t ) . \\bm{y}_{t-s}=\\bm{y}_{t}+st\\nabla_{\\bm{y}}\\log p_{t}(\\bm{y}_{t}). bold_italic_y start_POSTSUBSCRIPT italic_t - italic_s end_POSTSUBSCRIPT = bold_italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_s italic_t ‚àá start"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.SS2.p6", "title": "Then we have: h ‚Äã ( ùíô t ‚àí s ) ‚âà h ‚Äã ( ùíô t ) + s ‚Äã t ‚Äã ‚àá ùíö log ‚Å° p t ‚Äã ( h ‚Äã ( ùíô t ) ) , h(\\bm{x}_{t-s})\\approx h(\\bm{x}_{t})+st\\nabla_{\\bm{y}}\\log p_{t}(h(\\bm{x}_{t})), italic_h ( bold_italic_x start_", "snippet": "Then we have: h ‚Äã ( ùíô t ‚àí s ) ‚âà h ‚Äã ( ùíô t ) + s ‚Äã t ‚Äã ‚àá ùíö log ‚Å° p t ‚Äã ( h ‚Äã ( ùíô t ) ) , h(\\bm{x}_{t-s})\\approx h(\\bm{x}_{t})+st\\nabla_{\\bm{y}}\\log p_{t}(h(\\bm{x}_{t})), italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t - italic_s end_POSTSUBSCRIPT ) ‚âà italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) + italic_s italic_t ‚àá start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ) , (6.5.14) for a small s > 0 s>0 italic_s > 0 . Supp"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#Thmremark2.p1", "title": "There is something very interesting about the above equation ( 6.5.17 ). It seems to suggest we could try to learn the distribution of ùíô \\bm{x} bold_italic_x through a process that coupled with (many ", "snippet": "There is something very interesting about the above equation ( 6.5.17 ). It seems to suggest we could try to learn the distribution of ùíô \\bm{x} bold_italic_x through a process that coupled with (many of) its (partial) observations: ùíö i = h i ‚Äã ( ùíô ) + ùíò i , i = 1 , ‚Ä¶ , K . \\bm{y}^{i}=h^{i}(\\bm{x})+\\bm{w}^{i},i=1,\\ldots,K. bold_italic_y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT = italic_h start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ( bold_italic_x ) + bold_italic_w start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , italic_i = 1 , ‚Ä¶ , italic_K . (6.5.18) In this case, we obtain"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.SS2.SSS0.Px1.p1", "title": "In the above derivation, we have assumed that the measurement model h ‚Äã ( ‚ãÖ ) h(\\cdot) italic_h ( ‚ãÖ ) is fully known. In the case of stereo vision, this is rather reasonable as the relative pose (and ", "snippet": "In the above derivation, we have assumed that the measurement model h ‚Äã ( ‚ãÖ ) h(\\cdot) italic_h ( ‚ãÖ ) is fully known. In the case of stereo vision, this is rather reasonable as the relative pose (and calibration) of the two camera views (or two eyes 15 15 15 The relative pose of our two eyes is well known to our brain. ) are usually known in advance. Hence, through the stereo image pairs, in principle we should be able to learn the distribution of 3D scenes, at least the ego-centric distribution of 3D scenes. However, the low-dimensional structures of the so-called learned distribution contain"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#Thmremark3.p1", "title": "Note that the above goal aligns well with Klein‚Äôs Erlangen Program for modern geometry, which is to study invariants of a manifold under a group of transformations. Here, we may view the manifold of i", "snippet": "Note that the above goal aligns well with Klein‚Äôs Erlangen Program for modern geometry, which is to study invariants of a manifold under a group of transformations. Here, we may view the manifold of interest as the distribution of ego-centric representations of 3D scenes. We have learned that it admits a group of three-dimensional rigid-body motion acting on it. It is remarkable that our brain has learned to effectively decouple such transformations from the observed 3D world."}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.SS2.SSS0.Px1.p2", "title": "Notice that we have studied learning representations that are invariant to translation and rotation in a limited setting in Chapter 4 . We know that the associated compression operators take the neces", "snippet": "Notice that we have studied learning representations that are invariant to translation and rotation in a limited setting in Chapter 4 . We know that the associated compression operators take the necessary form of (multi-channel) convolutions, hence leading to the (deep) convolution neural networks. Nevertheless, operators that are associated with compression or denoising that are invariant to more general transformation groups remain elusive to characterize [ CW16a ] . For the 3D Vision problem in its most general setting, we know the change of our viewpoints can be well modeled as a rigid-bod"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.SS2.SSS0.Px1.p3", "title": "To this end, not that we can model a sequence of stereo pairs as: ùíö k = h ‚Äã ( ùíô k , Œ∏ k ) , k = 1 , ‚Ä¶ , K , \\bm{y}^{k}=h(\\bm{x}^{k},\\theta^{k}),\\quad k=1,\\ldots,K, bold_italic_y start_POSTSUPERSCRIPT ", "snippet": "To this end, not that we can model a sequence of stereo pairs as: ùíö k = h ‚Äã ( ùíô k , Œ∏ k ) , k = 1 , ‚Ä¶ , K , \\bm{y}^{k}=h(\\bm{x}^{k},\\theta^{k}),\\quad k=1,\\ldots,K, bold_italic_y start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT = italic_h ( bold_italic_x start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT , italic_Œ∏ start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ) , italic_k = 1 , ‚Ä¶ , italic_K , (6.5.21) where h ‚Äã ( ‚ãÖ ) h(\\cdot) italic_h ( ‚ãÖ ) represents the projection map from 3D to 2D. Œ∏ k \\theta^{k} italic_Œ∏ start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT denotes the rigid-body motio"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S6.SS0.SSS0.Px1.p1", "title": "In our development of conditional sampling, we considered measurement matching under an observation model ( 6.3.9 ), where we assume that we have paired data ( ùíô , ùíö ) (\\bm{x},\\bm{y}) ( bold_italic_x ", "snippet": "In our development of conditional sampling, we considered measurement matching under an observation model ( 6.3.9 ), where we assume that we have paired data ( ùíô , ùíö ) (\\bm{x},\\bm{y}) ( bold_italic_x , bold_italic_y ) ‚Äîi.e., ground truth for each observation ùíö \\bm{y} bold_italic_y . In many practically-relevant inverse problems, this is not the case: one of the most fundamental examples is in the context of compressed sensing, which we recalled in Chapter 2 , where we need to reconstruct ùíô \\bm{x} bold_italic_x from ùíö \\bm{y} bold_italic_y using prior knowledge about ùíô \\bm{x} bold_italic_x (i.e."}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S6.SS0.SSS0.Px1.p2", "title": "For intuition as to why this might be possible, we recall a classical example from statistics known as Stein‚Äôs unbiased risk estimator (SURE). Under an observation model ùíô t = ùíô + t ‚Äã ùíà \\bm{x}_{t}=\\bm", "snippet": "For intuition as to why this might be possible, we recall a classical example from statistics known as Stein‚Äôs unbiased risk estimator (SURE). Under an observation model ùíô t = ùíô + t ‚Äã ùíà \\bm{x}_{t}=\\bm{x}+t\\bm{g} bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_x + italic_t bold_italic_g with ùíà ‚àº ùí© ‚Äã ( ùüé , ùë∞ ) \\bm{g}\\sim\\mathcal{N}(\\mathbf{0},\\bm{I}) bold_italic_g ‚àº caligraphic_N ( bold_0 , bold_italic_I ) and t > 0 t>0 italic_t > 0 , it turns out that for any weakly differentiable f : ‚Ñù D ‚Üí ‚Ñù D f:\\mathbb{R}^{D}\\to\\mathbb{R}^{D} italic_f : blackboard_R start_POSTSUPERS"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S6.SS0.SSS0.Px1.p3", "title": "As a fun aside, we point out that Equation 6.6.1 leads to an alternate proof of Tweedie‚Äôs formula ( Theorem 3.3 ). At a high level, one takes expectations over ùíô \\bm{x} bold_italic_x and expresses the", "snippet": "As a fun aside, we point out that Equation 6.6.1 leads to an alternate proof of Tweedie‚Äôs formula ( Theorem 3.3 ). At a high level, one takes expectations over ùíô \\bm{x} bold_italic_x and expresses the main part of the RHS of Equation 6.6.1 equivalently, via integration by parts, as ùîº ùíô t ‚Äã [ ‚Äñ ùíô t ‚àí f ‚Äã ( ùíô t ) ‚Äñ 2 2 + 2 ‚Äã t 2 ‚Äã ‚àá ‚ãÖ f ‚Äã ( ùíô t ) ] = ùîº ùíô t ‚Äã [ ‚Äñ ùíô t ‚àí f ‚Äã ( ùíô t ) ‚Äñ 2 2 ] ‚àí 2 ‚Äã t 2 ‚Äã ‚à´ ‚ü® ‚àá p ùíô t ‚Äã ( ùùÉ ) , f ‚Äã ( ùùÉ ) ‚ü© ‚Äã d ùùÉ . \\mathbb{E}_{\\bm{x}_{t}}\\left[\\left\\|\\bm{x}_{t}-f(\\bm{x}_{t})\\right\\|_{2}^{2}+2t^{2}\\nabla\\cdot f(\\bm{x}_{t})\\right]=\\mathbb{E}_{\\bm{x}_{t}}\\left[\\left\\|\\bm{x"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S6.SS0.SSS0.Px2.p1", "title": "In Example 6.2 and in particular in Figure 6.9 , we pointed out a limitation of the DPS approximation Equation 6.3.26 at small levels of measurement noise. This limitation is well-understood, and a pr", "snippet": "In Example 6.2 and in particular in Figure 6.9 , we pointed out a limitation of the DPS approximation Equation 6.3.26 at small levels of measurement noise. This limitation is well-understood, and a principled approach to ameliorating it has been proposed by Rozet et al. [ RAL+24 ] . The approach involves incorporating an additional estimate for the variance of the noisy posterior p ùíô ‚à£ ùíô t p_{\\bm{x}\\mid\\bm{x}_{t}} italic_p start_POSTSUBSCRIPT bold_italic_x ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT to Equation 6.3.26 ‚Äîwe refer to the paper for details. Nat"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S6.SS0.SSS0.Px3.p1", "title": "Diffusion models have become an extremely popular tool for solving inverse problems arising in scientific applications. Many more methods beyond the simple DPS algorithm we have presented in Algorithm", "snippet": "Diffusion models have become an extremely popular tool for solving inverse problems arising in scientific applications. Many more methods beyond the simple DPS algorithm we have presented in Algorithm 6.1 have been developed and continue to be developed, as the area is evolving rapidly. Popular and performant classes of approaches beyond DPS, which we have presented due to its generality, include variable splitting approaches like DAPS [ ZCB+24 ] , which allow for specific measurement constraints to be enforced much more strongly than in DPS, and exact approaches that can avoid the use of appr"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#Thmexercise1.p1", "title": "1. Using the code provided in the book GitHub for implementing Figure 6.9 , implement the posterior variance correction proposed by [ RAL+24 ] . 2. Verify that it ameliorates the posterior collapse at", "snippet": "1. Using the code provided in the book GitHub for implementing Figure 6.9 , implement the posterior variance correction proposed by [ RAL+24 ] . 2. Verify that it ameliorates the posterior collapse at low noise variance issue observed in Figure 6.9 . 3. Discuss any issues of sampling correctness that are retained or introduced by the corrected method, as well as its efficiency, relative to diffusion posterior sampling (DPS)."}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S7.I1.i1.p1", "title": "Using the code provided in the book GitHub for implementing Figure 6.9 , implement the posterior variance correction proposed by [ RAL+24 ] .", "snippet": "Using the code provided in the book GitHub for implementing Figure 6.9 , implement the posterior variance correction proposed by [ RAL+24 ] ."}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S7.I1.i2.p1", "title": "Verify that it ameliorates the posterior collapse at low noise variance issue observed in Figure 6.9 .", "snippet": "Verify that it ameliorates the posterior collapse at low noise variance issue observed in Figure 6.9 ."}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S7.I1.i3.p1", "title": "Discuss any issues of sampling correctness that are retained or introduced by the corrected method, as well as its efficiency, relative to diffusion posterior sampling (DPS).", "snippet": "Discuss any issues of sampling correctness that are retained or introduced by the corrected method, as well as its efficiency, relative to diffusion posterior sampling (DPS)."}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#Thmexercise2.p1", "title": "1. Train a simple classifier for the MNIST dataset, using an architecture of your choice. Additionally train a denoiser suitable for use in conditional sampling ( Algorithm 6.2 , since this denoiser c", "snippet": "1. Train a simple classifier for the MNIST dataset, using an architecture of your choice. Additionally train a denoiser suitable for use in conditional sampling ( Algorithm 6.2 , since this denoiser can be used for unconditional denoising as well). 2. Integrate the classifier into a conditional sampler based on classifier guidance, as described in the first part of Section 6.4.1 . Evaluate the resulting samples in terms of faithfulness to the conditioning class (visually; in terms of nearest neighbor; in terms of the output of the classifier). 3. Integrate the classifier into a conditional sam"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S7.I2.i1.p1", "title": "Train a simple classifier for the MNIST dataset, using an architecture of your choice. Additionally train a denoiser suitable for use in conditional sampling ( Algorithm 6.2 , since this denoiser can ", "snippet": "Train a simple classifier for the MNIST dataset, using an architecture of your choice. Additionally train a denoiser suitable for use in conditional sampling ( Algorithm 6.2 , since this denoiser can be used for unconditional denoising as well)."}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S7.I2.i2.p1", "title": "Integrate the classifier into a conditional sampler based on classifier guidance, as described in the first part of Section 6.4.1 . Evaluate the resulting samples in terms of faithfulness to the condi", "snippet": "Integrate the classifier into a conditional sampler based on classifier guidance, as described in the first part of Section 6.4.1 . Evaluate the resulting samples in terms of faithfulness to the conditioning class (visually; in terms of nearest neighbor; in terms of the output of the classifier)."}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S7.I2.i3.p1", "title": "Integrate the classifier into a conditional sampler based on classifier-free guidance, as described in Section 6.4.1 and Algorithm 6.2 . Perform the same evaluation as in the previous step, and compar", "snippet": "Integrate the classifier into a conditional sampler based on classifier-free guidance, as described in Section 6.4.1 and Algorithm 6.2 . Perform the same evaluation as in the previous step, and compare the results."}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S7.I2.i4.p1", "title": "Repeat the experiment on the CIFAR-10 dataset.", "snippet": "Repeat the experiment on the CIFAR-10 dataset."}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#Thmexample1", "title": "Example 6.1 (Image Completion and Text Prediction) .", "snippet": "Example 6.1 (Image Completion and Text Prediction) . The popular natural image completion and natural language prediction are two typical tasks that require us to recover a full data ùíô \\bm{x} bold_italic_x from its partial observations ùíö \\bm{y} bold_italic_y , with parts of ùíô \\bm{x} bold_italic_x masked out and to be completed based on the rest. Figure 6.2 shows some examples of such tasks. In fact, it is precisely these tasks which have inspired how to train modern large models for text generation (such as GPT) and image completion (such as the masked autoencoder) that we will study in greate"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#Thmremark1", "title": "Remark 6.1 (End-to-End versus Bayesian) .", "snippet": "Remark 6.1 (End-to-End versus Bayesian) . In the modern practices of data-driven machine learning, for certain popular tasks people often directly learn the conditional distribution p ‚Äã ( ùíô ‚à£ ùíö ) p(\\bm{x}\\mid\\bm{y}) italic_p ( bold_italic_x ‚à£ bold_italic_y ) or a (probabilistic) mapping or a regressor. Such a mapping is often modeled by some deep networks and trained end-to-end with sufficient paired samples ( ùíô , ùíö ) (\\bm{x},\\bm{y}) ( bold_italic_x , bold_italic_y ) . Such an approach is very different from the above Bayesian approach in which both the distribution of ùíô ‚àº p ‚Äã ( ùíô ) \\bm{x}\\sim"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#Thmexample2", "title": "Example 6.2 .", "snippet": "Example 6.2 . Consider the case where the data distribution is Gaussian with mean ùùÅ ‚àà ‚Ñù D \\bm{\\mu}\\in\\mathbb{R}^{D} bold_italic_Œº ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT and covariance ùö∫ ‚àà ‚Ñù D √ó D \\bm{\\Sigma}\\in\\mathbb{R}^{D\\times D} bold_Œ£ ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D √ó italic_D end_POSTSUPERSCRIPT , i.e., ùíô ‚àº ùí© ‚Äã ( ùùÅ , ùö∫ ) \\bm{x}\\sim\\mathcal{N}(\\bm{\\mu},\\bm{\\Sigma}) bold_italic_x ‚àº caligraphic_N ( bold_italic_Œº , bold_Œ£ ) . Assume that ùö∫ ‚™∞ ùüé \\bm{\\Sigma}\\succeq\\mathbf{0} bold_Œ£ ‚™∞ bold_0 is nonzero. Moreover, in the measurement model ( 6.3.9 ), suppos"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#Thmexample3", "title": "Example 6.3 .", "snippet": "Example 6.3 . Let us recall the low-rank mixture of Gaussians data generating process we studied in Example 3.2 (and specifically, the form in Equation 3.2.42 ). Given K ‚àà ‚Ñï K\\in\\mathbb{N} italic_K ‚àà blackboard_N classes, we assume that ùíô ‚àº 1 K ‚Äã ‚àë k = 1 K ùí© ‚Å° ( ùüé , ùëº k ‚Äã ùëº k ‚ä§ ) , \\bm{x}\\sim\\frac{1}{K}\\sum_{k=1}^{K}\\operatorname{\\mathcal{N}}(\\bm{0},\\bm{U}_{k}\\bm{U}_{k}^{\\top}), bold_italic_x ‚àº divide start_ARG 1 end_ARG start_ARG italic_K end_ARG ‚àë start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT caligraphic_N ( bold_0 , bold_italic_U start"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#Thmremark2", "title": "Remark 6.2 (Parallel Sensing and Distributed Denoising.) .", "snippet": "Remark 6.2 (Parallel Sensing and Distributed Denoising.) . There is something very interesting about the above equation ( 6.5.17 ). It seems to suggest we could try to learn the distribution of ùíô \\bm{x} bold_italic_x through a process that coupled with (many of) its (partial) observations: ùíö i = h i ‚Äã ( ùíô ) + ùíò i , i = 1 , ‚Ä¶ , K . \\bm{y}^{i}=h^{i}(\\bm{x})+\\bm{w}^{i},i=1,\\ldots,K. bold_italic_y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT = italic_h start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ( bold_italic_x ) + bold_italic_w start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , ita"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#Thmremark3", "title": "Remark 6.3 .", "snippet": "Remark 6.3 . Note that the above goal aligns well with Klein‚Äôs Erlangen Program for modern geometry, which is to study invariants of a manifold under a group of transformations. Here, we may view the manifold of interest as the distribution of ego-centric representations of 3D scenes. We have learned that it admits a group of three-dimensional rigid-body motion acting on it. It is remarkable that our brain has learned to effectively decouple such transformations from the observed 3D world."}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#Thmexercise1", "title": "Exercise 6.1 (Posterior Variance Correction to DPS) .", "snippet": "Exercise 6.1 (Posterior Variance Correction to DPS) . 1. Using the code provided in the book GitHub for implementing Figure 6.9 , implement the posterior variance correction proposed by [ RAL+24 ] . 2. Verify that it ameliorates the posterior collapse at low noise variance issue observed in Figure 6.9 . 3. Discuss any issues of sampling correctness that are retained or introduced by the corrected method, as well as its efficiency, relative to diffusion posterior sampling (DPS)."}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#Thmexercise2", "title": "Exercise 6.2 (Conditional Sampling on MNIST) .", "snippet": "Exercise 6.2 (Conditional Sampling on MNIST) . 1. Train a simple classifier for the MNIST dataset, using an architecture of your choice. Additionally train a denoiser suitable for use in conditional sampling ( Algorithm 6.2 , since this denoiser can be used for unconditional denoising as well). 2. Integrate the classifier into a conditional sampler based on classifier guidance, as described in the first part of Section 6.4.1 . Evaluate the resulting samples in terms of faithfulness to the conditioning class (visually; in terms of nearest neighbor; in terms of the output of the classifier). 3. "}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.E1", "title": "ùíö = h ‚Äã ( ùíô ) + ùíò , \\bm{y}=h(\\bm{x})+\\bm{w}, bold_italic_y = italic_h ( bold_italic_x ) + bold_italic_w , (6.1.1)", "snippet": "ùíö = h ‚Äã ( ùíô ) + ùíò , \\bm{y}=h(\\bm{x})+\\bm{w}, bold_italic_y = italic_h ( bold_italic_x ) + bold_italic_w , (6.1.1)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.E2", "title": "ùíô ^ = arg ‚Äã max ùíô ‚Å° p ‚Äã ( ùíô ‚à£ ùíö ) ; \\hat{\\bm{x}}=\\operatorname*{arg\\ max}_{\\bm{x}}p(\\bm{x}\\mid\\bm{y}); over^ start_ARG bold_italic_x end_ARG = start_OPERATOR roman_arg roman_max end_OPERATOR start_POS", "snippet": "ùíô ^ = arg ‚Äã max ùíô ‚Å° p ‚Äã ( ùíô ‚à£ ùíö ) ; \\hat{\\bm{x}}=\\operatorname*{arg\\ max}_{\\bm{x}}p(\\bm{x}\\mid\\bm{y}); over^ start_ARG bold_italic_x end_ARG = start_OPERATOR roman_arg roman_max end_OPERATOR start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT italic_p ( bold_italic_x ‚à£ bold_italic_y ) ; (6.1.2)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.E3", "title": "ùíô ^ = ùîº ‚Äã [ ùíô ‚à£ ùíö ] = ‚à´ ùíô ‚Äã p ‚Äã ( ùíô ‚à£ ùíö ) ‚Äã d ùíô ; \\hat{\\bm{x}}=\\mathbb{E}[\\bm{x}\\mid\\bm{y}]=\\int\\bm{x}p(\\bm{x}\\mid\\bm{y})\\mathrm{d}\\bm{x}; over^ start_ARG bold_italic_x end_ARG = blackboard_E [ bold_i", "snippet": "ùíô ^ = ùîº ‚Äã [ ùíô ‚à£ ùíö ] = ‚à´ ùíô ‚Äã p ‚Äã ( ùíô ‚à£ ùíö ) ‚Äã d ùíô ; \\hat{\\bm{x}}=\\mathbb{E}[\\bm{x}\\mid\\bm{y}]=\\int\\bm{x}p(\\bm{x}\\mid\\bm{y})\\mathrm{d}\\bm{x}; over^ start_ARG bold_italic_x end_ARG = blackboard_E [ bold_italic_x ‚à£ bold_italic_y ] = ‚à´ bold_italic_x italic_p ( bold_italic_x ‚à£ bold_italic_y ) roman_d bold_italic_x ; (6.1.3)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.E4", "title": "ùíô ^ ‚àº p ‚Äã ( ùíô ‚à£ ùíö ) . \\hat{\\bm{x}}\\sim p(\\bm{x}\\mid\\bm{y}). over^ start_ARG bold_italic_x end_ARG ‚àº italic_p ( bold_italic_x ‚à£ bold_italic_y ) . (6.1.4)", "snippet": "ùíô ^ ‚àº p ‚Äã ( ùíô ‚à£ ùíö ) . \\hat{\\bm{x}}\\sim p(\\bm{x}\\mid\\bm{y}). over^ start_ARG bold_italic_x end_ARG ‚àº italic_p ( bold_italic_x ‚à£ bold_italic_y ) . (6.1.4)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.E5", "title": "p ‚Äã ( ùíô ‚à£ ùíö ) = p ‚Äã ( ùíö ‚à£ ùíô ) ‚Äã p ‚Äã ( ùíô ) p ‚Äã ( ùíö ) . p(\\bm{x}\\mid\\bm{y})=\\frac{p(\\bm{y}\\mid\\bm{x})p(\\bm{x})}{p(\\bm{y})}. italic_p ( bold_italic_x ‚à£ bold_italic_y ) = divide start_ARG italic_p ( bold_", "snippet": "p ‚Äã ( ùíô ‚à£ ùíö ) = p ‚Äã ( ùíö ‚à£ ùíô ) ‚Äã p ‚Äã ( ùíô ) p ‚Äã ( ùíö ) . p(\\bm{x}\\mid\\bm{y})=\\frac{p(\\bm{y}\\mid\\bm{x})p(\\bm{x})}{p(\\bm{y})}. italic_p ( bold_italic_x ‚à£ bold_italic_y ) = divide start_ARG italic_p ( bold_italic_y ‚à£ bold_italic_x ) italic_p ( bold_italic_x ) end_ARG start_ARG italic_p ( bold_italic_y ) end_ARG . (6.1.5)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.E6", "title": "ùíô ^ = arg ‚Äã max ùíô ‚Å° [ log ‚Å° p ‚Äã ( ùíö ‚à£ ùíô ) + log ‚Å° p ‚Äã ( ùíô ) ] , \\hat{\\bm{x}}=\\operatorname*{arg\\ max}_{\\bm{x}}[\\log p(\\bm{y}\\mid\\bm{x})+\\log p(\\bm{x})], over^ start_ARG bold_italic_x end_ARG = start_O", "snippet": "ùíô ^ = arg ‚Äã max ùíô ‚Å° [ log ‚Å° p ‚Äã ( ùíö ‚à£ ùíô ) + log ‚Å° p ‚Äã ( ùíô ) ] , \\hat{\\bm{x}}=\\operatorname*{arg\\ max}_{\\bm{x}}[\\log p(\\bm{y}\\mid\\bm{x})+\\log p(\\bm{x})], over^ start_ARG bold_italic_x end_ARG = start_OPERATOR roman_arg roman_max end_OPERATOR start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ roman_log italic_p ( bold_italic_y ‚à£ bold_italic_x ) + roman_log italic_p ( bold_italic_x ) ] , (6.1.6)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.E7", "title": "ùíô k + 1 = ùíô k + Œ± ‚ãÖ ( ‚àá ùíô log ‚Å° p ‚Äã ( ùíö ‚à£ ùíô ) + ‚àá ùíô log ‚Å° p ‚Äã ( ùíô ) ) . \\bm{x}_{k+1}=\\bm{x}_{k}+\\alpha\\cdot\\big{(}\\nabla_{\\bm{x}}\\log p(\\bm{y}\\mid\\bm{x})+\\nabla_{\\bm{x}}\\log p(\\bm{x})\\big{)}. bold_ita", "snippet": "ùíô k + 1 = ùíô k + Œ± ‚ãÖ ( ‚àá ùíô log ‚Å° p ‚Äã ( ùíö ‚à£ ùíô ) + ‚àá ùíô log ‚Å° p ‚Äã ( ùíô ) ) . \\bm{x}_{k+1}=\\bm{x}_{k}+\\alpha\\cdot\\big{(}\\nabla_{\\bm{x}}\\log p(\\bm{y}\\mid\\bm{x})+\\nabla_{\\bm{x}}\\log p(\\bm{x})\\big{)}. bold_italic_x start_POSTSUBSCRIPT italic_k + 1 end_POSTSUBSCRIPT = bold_italic_x start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT + italic_Œ± ‚ãÖ ( ‚àá start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT roman_log italic_p ( bold_italic_y ‚à£ bold_italic_x ) + ‚àá start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT roman_log italic_p ( bold_italic_x ) ) . (6.1.7)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.E8", "title": "F ‚Äã ( ùíô ) = ùüé ‚áî ùíô ‚àà ùíÆ ùíô F(\\bm{x})=\\bm{0}\\qquad\\iff\\qquad\\bm{x}\\in\\mathcal{S}_{\\bm{x}} italic_F ( bold_italic_x ) = bold_0 ‚áî bold_italic_x ‚àà caligraphic_S start_POSTSUBSCRIPT bold_italic_x end_POSTSUBS", "snippet": "F ‚Äã ( ùíô ) = ùüé ‚áî ùíô ‚àà ùíÆ ùíô F(\\bm{x})=\\bm{0}\\qquad\\iff\\qquad\\bm{x}\\in\\mathcal{S}_{\\bm{x}} italic_F ( bold_italic_x ) = bold_0 ‚áî bold_italic_x ‚àà caligraphic_S start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT (6.1.8)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.E9", "title": "F ‚Äã ( ùíô ) = min ùíô p ‚àà ùíÆ ùíô ‚Å° ‚Äñ ùíô ‚àí ùíô p ‚Äñ 2 . F(\\bm{x})=\\min_{\\bm{x}_{p}\\in\\mathcal{S}_{\\bm{x}}}\\|\\bm{x}-\\bm{x}_{p}\\|_{2}. italic_F ( bold_italic_x ) = roman_min start_POSTSUBSCRIPT bold_italic_x start_", "snippet": "F ‚Äã ( ùíô ) = min ùíô p ‚àà ùíÆ ùíô ‚Å° ‚Äñ ùíô ‚àí ùíô p ‚Äñ 2 . F(\\bm{x})=\\min_{\\bm{x}_{p}\\in\\mathcal{S}_{\\bm{x}}}\\|\\bm{x}-\\bm{x}_{p}\\|_{2}. italic_F ( bold_italic_x ) = roman_min start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ‚àà caligraphic_S start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT end_POSTSUBSCRIPT ‚à• bold_italic_x - bold_italic_x start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT . (6.1.9)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.E10", "title": "max ùíô ‚àí 1 2 ‚Äã ‚Äñ h ‚Äã ( ùíô ) ‚àí ùíö ‚Äñ 2 2 s.t. F ‚Äã ( ùíô ) = ùüé . \\max_{\\bm{x}}-\\frac{1}{2}\\|h(\\bm{x})-\\bm{y}\\|_{2}^{2}\\quad\\mbox{s.t.}\\quad F(\\bm{x})=\\bm{0}. roman_max start_POSTSUBSCRIPT bold_italic_x end_PO", "snippet": "max ùíô ‚àí 1 2 ‚Äã ‚Äñ h ‚Äã ( ùíô ) ‚àí ùíö ‚Äñ 2 2 s.t. F ‚Äã ( ùíô ) = ùüé . \\max_{\\bm{x}}-\\frac{1}{2}\\|h(\\bm{x})-\\bm{y}\\|_{2}^{2}\\quad\\mbox{s.t.}\\quad F(\\bm{x})=\\bm{0}. roman_max start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT - divide start_ARG 1 end_ARG start_ARG 2 end_ARG ‚à• italic_h ( bold_italic_x ) - bold_italic_y ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT s.t. italic_F ( bold_italic_x ) = bold_0 . (6.1.10)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.E11", "title": "max ùíô ‚Å° [ ‚àí 1 2 ‚Äã ‚Äñ h ‚Äã ( ùíô ) ‚àí ùíö ‚Äñ 2 2 + ùùÄ ‚ä§ ‚Äã F ‚Äã ( ùíô ) ‚àí Œº 2 ‚Äã ‚Äñ F ‚Äã ( ùíô ) ‚Äñ 2 2 ] \\max_{\\bm{x}}\\left[-\\frac{1}{2}\\|h(\\bm{x})-\\bm{y}\\|_{2}^{2}+\\bm{\\lambda}^{\\top}F(\\bm{x})-\\frac{\\mu}{2}\\|F(\\bm{x})\\", "snippet": "max ùíô ‚Å° [ ‚àí 1 2 ‚Äã ‚Äñ h ‚Äã ( ùíô ) ‚àí ùíö ‚Äñ 2 2 + ùùÄ ‚ä§ ‚Äã F ‚Äã ( ùíô ) ‚àí Œº 2 ‚Äã ‚Äñ F ‚Äã ( ùíô ) ‚Äñ 2 2 ] \\max_{\\bm{x}}\\left[-\\frac{1}{2}\\|h(\\bm{x})-\\bm{y}\\|_{2}^{2}+\\bm{\\lambda}^{\\top}F(\\bm{x})-\\frac{\\mu}{2}\\|F(\\bm{x})\\|_{2}^{2}\\right] roman_max start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ - divide start_ARG 1 end_ARG start_ARG 2 end_ARG ‚à• italic_h ( bold_italic_x ) - bold_italic_y ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + bold_italic_Œª start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT italic_F ( bold_italic_x ) - divide start_ARG italic_Œº end_ARG start_ARG 2 end_"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.E12", "title": "max ùíô ‚Å° [ log ‚Å° exp ‚Å° ( ‚àí 1 2 ‚Äã ‚Äñ h ‚Äã ( ùíô ) ‚àí ùíö ‚Äñ 2 2 ) + log ‚Å° exp ‚Å° ( ‚àí Œº 2 ‚Äã ‚Äñ F ‚Äã ( ùíô ) ‚àí ùùÄ / Œº ‚Äñ 2 2 ) ] , \\max_{\\bm{x}}\\left[\\log\\exp\\Big{(}-\\frac{1}{2}\\|h(\\bm{x})-\\bm{y}\\|_{2}^{2}\\Big{)}+\\log\\e", "snippet": "max ùíô ‚Å° [ log ‚Å° exp ‚Å° ( ‚àí 1 2 ‚Äã ‚Äñ h ‚Äã ( ùíô ) ‚àí ùíö ‚Äñ 2 2 ) + log ‚Å° exp ‚Å° ( ‚àí Œº 2 ‚Äã ‚Äñ F ‚Äã ( ùíô ) ‚àí ùùÄ / Œº ‚Äñ 2 2 ) ] , \\max_{\\bm{x}}\\left[\\log\\exp\\Big{(}-\\frac{1}{2}\\|h(\\bm{x})-\\bm{y}\\|_{2}^{2}\\Big{)}+\\log\\exp\\Big{(}-\\frac{\\mu}{2}\\big{\\|}F(\\bm{x})-\\bm{\\lambda}/\\mu\\big{\\|}_{2}^{2}\\Big{)}\\right], roman_max start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT [ roman_log roman_exp ( - divide start_ARG 1 end_ARG start_ARG 2 end_ARG ‚à• italic_h ( bold_italic_x ) - bold_italic_y ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) + roman_log roman_exp ( - divide start_ARG "}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.E13", "title": "p ‚Äã ( ùíö ‚à£ ùíô ) ‚àù exp ‚Å° ( ‚àí 1 2 ‚Äã ‚Äñ h ‚Äã ( ùíô ) ‚àí ùíö ‚Äñ 2 2 ) , p ‚Äã ( ùíô ) ‚àù exp ‚Å° ( ‚àí Œº 2 ‚Äã ‚Äñ F ‚Äã ( ùíô ) ‚àí ùíÑ ‚Äñ 2 2 ) . p(\\bm{y}\\mid\\bm{x})\\propto\\exp\\Big{(}-\\frac{1}{2}\\|h(\\bm{x})-\\bm{y}\\|_{2}^{2}\\Big{)},\\qu", "snippet": "p ‚Äã ( ùíö ‚à£ ùíô ) ‚àù exp ‚Å° ( ‚àí 1 2 ‚Äã ‚Äñ h ‚Äã ( ùíô ) ‚àí ùíö ‚Äñ 2 2 ) , p ‚Äã ( ùíô ) ‚àù exp ‚Å° ( ‚àí Œº 2 ‚Äã ‚Äñ F ‚Äã ( ùíô ) ‚àí ùíÑ ‚Äñ 2 2 ) . p(\\bm{y}\\mid\\bm{x})\\propto\\exp\\Big{(}-\\frac{1}{2}\\|h(\\bm{x})-\\bm{y}\\|_{2}^{2}\\Big{)},\\quad p(\\bm{x})\\propto\\exp\\Big{(}-\\frac{\\mu}{2}\\|F(\\bm{x})-\\bm{c}\\|_{2}^{2}\\Big{)}. italic_p ( bold_italic_y ‚à£ bold_italic_x ) ‚àù roman_exp ( - divide start_ARG 1 end_ARG start_ARG 2 end_ARG ‚à• italic_h ( bold_italic_x ) - bold_italic_y ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) , italic_p ( bold_italic_x ) ‚àù roman_exp ( - divide start_ARG italic_Œº end_ARG s"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.E14", "title": "‚àá ùíô log ‚Å° p ‚Äã ( ùíö ‚à£ ùíô ) + ‚àá ùíô log ‚Å° p ‚Äã ( ùíô ) = ‚àÇ h ‚àÇ ùíô ‚Äã ( ùíô ) ‚Äã ( ùíö ‚àí h ‚Äã ( ùíô ) ) + Œº ‚Äã ‚àÇ F ‚àÇ ùíô ‚Äã ( ùíô ) ‚Äã ( ùíÑ ‚àí F ‚Äã ( ùíô ) ) , \\nabla_{\\bm{x}}\\log p(\\bm{y}\\mid\\bm{x})+\\nabla_{\\bm{x}}\\log p(\\bm{x})=\\f", "snippet": "‚àá ùíô log ‚Å° p ‚Äã ( ùíö ‚à£ ùíô ) + ‚àá ùíô log ‚Å° p ‚Äã ( ùíô ) = ‚àÇ h ‚àÇ ùíô ‚Äã ( ùíô ) ‚Äã ( ùíö ‚àí h ‚Äã ( ùíô ) ) + Œº ‚Äã ‚àÇ F ‚àÇ ùíô ‚Äã ( ùíô ) ‚Äã ( ùíÑ ‚àí F ‚Äã ( ùíô ) ) , \\nabla_{\\bm{x}}\\log p(\\bm{y}\\mid\\bm{x})+\\nabla_{\\bm{x}}\\log p(\\bm{x})=\\frac{\\partial h}{\\partial\\bm{x}}(\\bm{x})\\big{(}\\bm{y}-h(\\bm{x})\\big{)}+\\mu\\frac{\\partial F}{\\partial\\bm{x}}(\\bm{x})\\big{(}\\bm{c}-F(\\bm{x})\\big{)}, ‚àá start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT roman_log italic_p ( bold_italic_y ‚à£ bold_italic_x ) + ‚àá start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT roman_log italic_p ( bold_italic_x ) = divide start_ARG ‚àÇ italic_h end_ARG start_ARG ‚àÇ bold"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S1.E15", "title": "min ùíô ‚Å° 1 2 ‚Äã ‚Äñ h ‚Äã ( ùíô ) ‚àí ùíö ‚Äñ 2 2 + Œº 2 ‚Äã ‚Äñ F ‚Äã ( ùíô ) ‚àí ùùÄ / Œº ‚Äñ 2 2 , \\min_{\\bm{x}}\\frac{1}{2}\\|h(\\bm{x})-\\bm{y}\\|_{2}^{2}+\\frac{\\mu}{2}\\big{\\|}F(\\bm{x})-\\bm{\\lambda}/\\mu\\big{\\|}_{2}^{2}, roman_min ", "snippet": "min ùíô ‚Å° 1 2 ‚Äã ‚Äñ h ‚Äã ( ùíô ) ‚àí ùíö ‚Äñ 2 2 + Œº 2 ‚Äã ‚Äñ F ‚Äã ( ùíô ) ‚àí ùùÄ / Œº ‚Äñ 2 2 , \\min_{\\bm{x}}\\frac{1}{2}\\|h(\\bm{x})-\\bm{y}\\|_{2}^{2}+\\frac{\\mu}{2}\\big{\\|}F(\\bm{x})-\\bm{\\lambda}/\\mu\\big{\\|}_{2}^{2}, roman_min start_POSTSUBSCRIPT bold_italic_x end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG 2 end_ARG ‚à• italic_h ( bold_italic_x ) - bold_italic_y ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + divide start_ARG italic_Œº end_ARG start_ARG 2 end_ARG ‚à• italic_F ( bold_italic_x ) - bold_italic_Œª / italic_Œº ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERS"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S2.E1", "title": "ùíö = h ‚Äã ( ùíô ) + ùíò , \\bm{y}=h(\\bm{x})+\\bm{w}, bold_italic_y = italic_h ( bold_italic_x ) + bold_italic_w , (6.2.1)", "snippet": "ùíö = h ‚Äã ( ùíô ) + ùíò , \\bm{y}=h(\\bm{x})+\\bm{w}, bold_italic_y = italic_h ( bold_italic_x ) + bold_italic_w , (6.2.1)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S2.E2", "title": "f : ùí´ Œ© ‚Äã ( ùíô ) ‚Ü¶ ùíô ^ , f:\\mathcal{P}_{\\Omega}(\\bm{x})\\mapsto\\hat{\\bm{x}}, italic_f : caligraphic_P start_POSTSUBSCRIPT roman_Œ© end_POSTSUBSCRIPT ( bold_italic_x ) ‚Ü¶ over^ start_ARG bold_italic_x end_", "snippet": "f : ùí´ Œ© ‚Äã ( ùíô ) ‚Ü¶ ùíô ^ , f:\\mathcal{P}_{\\Omega}(\\bm{x})\\mapsto\\hat{\\bm{x}}, italic_f : caligraphic_P start_POSTSUBSCRIPT roman_Œ© end_POSTSUBSCRIPT ( bold_italic_x ) ‚Ü¶ over^ start_ARG bold_italic_x end_ARG , (6.2.2)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S2.E3", "title": "rank ‚Äã ( ùëø o ) = r < min ‚Å° { m , n } . \\mbox{rank}(\\bm{X}_{o})=r<\\min\\{m,n\\}. rank ( bold_italic_X start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT ) = italic_r < roman_min { italic_m , italic_n } . (6.", "snippet": "rank ‚Äã ( ùëø o ) = r < min ‚Å° { m , n } . \\mbox{rank}(\\bm{X}_{o})=r<\\min\\{m,n\\}. rank ( bold_italic_X start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT ) = italic_r < roman_min { italic_m , italic_n } . (6.2.3)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S2.E4", "title": "ùíÄ = ùí´ Œ© ‚Äã ( ùëø o ) . \\bm{Y}=\\mathcal{P}_{\\Omega}(\\bm{X}_{o}). bold_italic_Y = caligraphic_P start_POSTSUBSCRIPT roman_Œ© end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT ", "snippet": "ùíÄ = ùí´ Œ© ‚Äã ( ùëø o ) . \\bm{Y}=\\mathcal{P}_{\\Omega}(\\bm{X}_{o}). bold_italic_Y = caligraphic_P start_POSTSUBSCRIPT roman_Œ© end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT ) . (6.2.4)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S2.E5", "title": "min ùëø ‚Å° rank ‚Äã ( ùëø ) subject to ùíÄ = ùí´ Œ© ‚Äã ( ùëø ) . \\min_{\\bm{X}}\\mbox{rank}(\\bm{X})\\quad\\mbox{subject to}\\quad\\bm{Y}=\\mathcal{P}_{\\Omega}(\\bm{X}). roman_min start_POSTSUBSCRIPT bold_italic_X end_POSTSU", "snippet": "min ùëø ‚Å° rank ‚Äã ( ùëø ) subject to ùíÄ = ùí´ Œ© ‚Äã ( ùëø ) . \\min_{\\bm{X}}\\mbox{rank}(\\bm{X})\\quad\\mbox{subject to}\\quad\\bm{Y}=\\mathcal{P}_{\\Omega}(\\bm{X}). roman_min start_POSTSUBSCRIPT bold_italic_X end_POSTSUBSCRIPT rank ( bold_italic_X ) subject to bold_italic_Y = caligraphic_P start_POSTSUBSCRIPT roman_Œ© end_POSTSUBSCRIPT ( bold_italic_X ) . (6.2.5)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S2.E6", "title": "min ‚Å° R œµ ‚Äã ( ùëø ) = 1 2 ‚Äã log ‚Äã det ( ùë∞ + Œ± ‚Äã ùëø ‚Äã ùëø ‚ä§ ) subject to ùíÄ = ùí´ Œ© ‚Äã ( ùëø ) . \\min R_{\\epsilon}(\\bm{X})=\\frac{1}{2}\\log\\det\\left(\\bm{I}+\\alpha\\bm{X}\\bm{X}^{\\top}\\right)\\quad\\mbox{subject to}\\qu", "snippet": "min ‚Å° R œµ ‚Äã ( ùëø ) = 1 2 ‚Äã log ‚Äã det ( ùë∞ + Œ± ‚Äã ùëø ‚Äã ùëø ‚ä§ ) subject to ùíÄ = ùí´ Œ© ‚Äã ( ùëø ) . \\min R_{\\epsilon}(\\bm{X})=\\frac{1}{2}\\log\\det\\left(\\bm{I}+\\alpha\\bm{X}\\bm{X}^{\\top}\\right)\\quad\\mbox{subject to}\\quad\\bm{Y}=\\mathcal{P}_{\\Omega}(\\bm{X}). roman_min italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_X ) = divide start_ARG 1 end_ARG start_ARG 2 end_ARG roman_log roman_det ( bold_italic_I + italic_Œ± bold_italic_X bold_italic_X start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ) subject to bold_italic_Y = caligraphic_P start_POSTSUBSCRIPT roman_Œ© end_POSTSUBSCRIPT ( bold_italic_X ) . "}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S2.E7", "title": "min ‚Å° ‚Äñ ùëø ‚Äñ ‚àó subject to ùíÄ = ùí´ Œ© ‚Äã ( ùëø ) , \\min\\|\\bm{X}\\|_{*}\\quad\\mbox{subject to}\\quad\\bm{Y}=\\mathcal{P}_{\\Omega}(\\bm{X}), roman_min ‚à• bold_italic_X ‚à• start_POSTSUBSCRIPT ‚àó end_POSTSUBSCRIPT subject", "snippet": "min ‚Å° ‚Äñ ùëø ‚Äñ ‚àó subject to ùíÄ = ùí´ Œ© ‚Äã ( ùëø ) , \\min\\|\\bm{X}\\|_{*}\\quad\\mbox{subject to}\\quad\\bm{Y}=\\mathcal{P}_{\\Omega}(\\bm{X}), roman_min ‚à• bold_italic_X ‚à• start_POSTSUBSCRIPT ‚àó end_POSTSUBSCRIPT subject to bold_italic_Y = caligraphic_P start_POSTSUBSCRIPT roman_Œ© end_POSTSUBSCRIPT ( bold_italic_X ) , (6.2.7)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S2.E8", "title": "min ‚Å° ‚Äñ ùëø ‚Äñ ‚àó + Œª ‚Äã ‚Äñ ùíÄ ‚àí ùí´ Œ© ‚Äã ( ùëø ) ‚Äñ F 2 , \\min\\|\\bm{X}\\|_{*}+\\lambda\\|\\bm{Y}-\\mathcal{P}_{\\Omega}(\\bm{X})\\|_{F}^{2}, roman_min ‚à• bold_italic_X ‚à• start_POSTSUBSCRIPT ‚àó end_POSTSUBSCRIPT + italic_Œª ", "snippet": "min ‚Å° ‚Äñ ùëø ‚Äñ ‚àó + Œª ‚Äã ‚Äñ ùíÄ ‚àí ùí´ Œ© ‚Äã ( ùëø ) ‚Äñ F 2 , \\min\\|\\bm{X}\\|_{*}+\\lambda\\|\\bm{Y}-\\mathcal{P}_{\\Omega}(\\bm{X})\\|_{F}^{2}, roman_min ‚à• bold_italic_X ‚à• start_POSTSUBSCRIPT ‚àó end_POSTSUBSCRIPT + italic_Œª ‚à• bold_italic_Y - caligraphic_P start_POSTSUBSCRIPT roman_Œ© end_POSTSUBSCRIPT ( bold_italic_X ) ‚à• start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , (6.2.8)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S2.E9", "title": "ùíÄ ‚àò œÑ = ùëø o + ùë¨ , \\bm{Y}\\circ\\tau=\\bm{X}_{o}+\\bm{E}, bold_italic_Y ‚àò italic_œÑ = bold_italic_X start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT + bold_italic_E , (6.2.9)", "snippet": "ùíÄ ‚àò œÑ = ùëø o + ùë¨ , \\bm{Y}\\circ\\tau=\\bm{X}_{o}+\\bm{E}, bold_italic_Y ‚àò italic_œÑ = bold_italic_X start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT + bold_italic_E , (6.2.9)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E1", "title": "min f , g ‚Å° L MAE ‚Äã ( f , g ) ‚âê ùîº ‚Äã [ ‚Äñ ( g ‚àò f ) ‚Äã ( ùí´ Œ© ‚Äã ( ùëø ) ) ‚àí ùëø ‚Äñ 2 2 ] . \\min_{f,g}L_{\\mathrm{MAE}}(f,g)\\doteq\\mathbb{E}\\big{[}\\|(g\\circ f)(\\mathcal{P}_{\\Omega}(\\bm{X}))-\\bm{X}\\|_{2}^{2}]. ro", "snippet": "min f , g ‚Å° L MAE ‚Äã ( f , g ) ‚âê ùîº ‚Äã [ ‚Äñ ( g ‚àò f ) ‚Äã ( ùí´ Œ© ‚Äã ( ùëø ) ) ‚àí ùëø ‚Äñ 2 2 ] . \\min_{f,g}L_{\\mathrm{MAE}}(f,g)\\doteq\\mathbb{E}\\big{[}\\|(g\\circ f)(\\mathcal{P}_{\\Omega}(\\bm{X}))-\\bm{X}\\|_{2}^{2}]. roman_min start_POSTSUBSCRIPT italic_f , italic_g end_POSTSUBSCRIPT italic_L start_POSTSUBSCRIPT roman_MAE end_POSTSUBSCRIPT ( italic_f , italic_g ) ‚âê blackboard_E [ ‚à• ( italic_g ‚àò italic_f ) ( caligraphic_P start_POSTSUBSCRIPT roman_Œ© end_POSTSUBSCRIPT ( bold_italic_X ) ) - bold_italic_X ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] . (6.3.1)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E2", "title": "f : ùëø ‚Ü¶ ùíÅ f:\\bm{X}\\mapsto\\bm{Z} italic_f : bold_italic_X ‚Ü¶ bold_italic_Z (6.3.2)", "snippet": "f : ùëø ‚Ü¶ ùíÅ f:\\bm{X}\\mapsto\\bm{Z} italic_f : bold_italic_X ‚Ü¶ bold_italic_Z (6.3.2)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E3", "title": "ùîº ùíÅ = f ‚Äã ( ùëø ) ‚Äã [ Œî ‚Äã R œµ ‚Äã ( ùíÅ ‚à£ ùëº [ K ] ) ‚àí Œª ‚Äã ‚Äñ ùíÅ ‚Äñ 0 ] = ùîº ùíÅ = f ‚Äã ( ùëø ) ‚Äã [ R œµ ‚Äã ( ùíÅ ) ‚àí R œµ c ‚Äã ( ùíÅ ‚à£ ùëº [ K ] ) ‚àí Œª ‚Äã ‚Äñ ùíÅ ‚Äñ 0 ] , \\mathbb{E}_{\\bm{Z}=f(\\bm{X})}[\\Delta R_{\\epsilon}(\\bm{Z}\\mid", "snippet": "ùîº ùíÅ = f ‚Äã ( ùëø ) ‚Äã [ Œî ‚Äã R œµ ‚Äã ( ùíÅ ‚à£ ùëº [ K ] ) ‚àí Œª ‚Äã ‚Äñ ùíÅ ‚Äñ 0 ] = ùîº ùíÅ = f ‚Äã ( ùëø ) ‚Äã [ R œµ ‚Äã ( ùíÅ ) ‚àí R œµ c ‚Äã ( ùíÅ ‚à£ ùëº [ K ] ) ‚àí Œª ‚Äã ‚Äñ ùíÅ ‚Äñ 0 ] , \\mathbb{E}_{\\bm{Z}=f(\\bm{X})}[\\Delta R_{\\epsilon}(\\bm{Z}\\mid\\bm{U}_{[K]})-\\lambda\\|\\bm{Z}\\|_{0}]=\\mathbb{E}_{\\bm{Z}=f(\\bm{X})}[R_{\\epsilon}(\\bm{Z})-R^{c}_{\\epsilon}(\\bm{Z}\\mid\\bm{U}_{[K]})-\\lambda\\|\\bm{Z}\\|_{0}], blackboard_E start_POSTSUBSCRIPT bold_italic_Z = italic_f ( bold_italic_X ) end_POSTSUBSCRIPT [ roman_Œî italic_R start_POSTSUBSCRIPT italic_œµ end_POSTSUBSCRIPT ( bold_italic_Z ‚à£ bold_italic_U start_POSTSUBSCRIPT [ italic_K ] end_POSTSUBSCRIPT ) - "}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.Ex1", "title": "p ùëø m ‚à£ ùëø v ‚Äã ( ùöµ m ‚à£ ùöµ v ) p_{\\bm{X}_{m}\\mid\\bm{X}_{v}}(\\bm{\\Xi}_{m}\\mid\\bm{\\Xi}_{v}) italic_p start_POSTSUBSCRIPT bold_italic_X start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ‚à£ bold_italic_X start_P", "snippet": "p ùëø m ‚à£ ùëø v ‚Äã ( ùöµ m ‚à£ ùöµ v ) p_{\\bm{X}_{m}\\mid\\bm{X}_{v}}(\\bm{\\Xi}_{m}\\mid\\bm{\\Xi}_{v}) italic_p start_POSTSUBSCRIPT bold_italic_X start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ‚à£ bold_italic_X start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_Œû start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ‚à£ bold_Œû start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT )"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E4", "title": "arg ‚Äã min h = g ‚àò f ‚Å° L MAE ‚Äã ( h ) = ùöµ v ‚Ü¶ ùöµ v + ùîº ‚Äã [ ùëø m ‚à£ ùëø v = ùöµ v ] . \\operatorname*{arg\\ min}_{h=g\\circ f}\\,L_{\\mathrm{MAE}}(h)=\\bm{\\Xi}_{v}\\mapsto\\bm{\\Xi}_{v}+\\mathbb{E}[\\bm{X}_{m}\\mid\\bm{X}_{", "snippet": "arg ‚Äã min h = g ‚àò f ‚Å° L MAE ‚Äã ( h ) = ùöµ v ‚Ü¶ ùöµ v + ùîº ‚Äã [ ùëø m ‚à£ ùëø v = ùöµ v ] . \\operatorname*{arg\\ min}_{h=g\\circ f}\\,L_{\\mathrm{MAE}}(h)=\\bm{\\Xi}_{v}\\mapsto\\bm{\\Xi}_{v}+\\mathbb{E}[\\bm{X}_{m}\\mid\\bm{X}_{v}=\\bm{\\Xi}_{v}]. start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_h = italic_g ‚àò italic_f end_POSTSUBSCRIPT italic_L start_POSTSUBSCRIPT roman_MAE end_POSTSUBSCRIPT ( italic_h ) = bold_Œû start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ‚Ü¶ bold_Œû start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT + blackboard_E [ bold_italic_X start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ‚à£ bol"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E5", "title": "ùíÄ = ùë® ‚Äã ùëø 0 , ùëø t = ùëø 0 + œÉ t ‚Äã ùëÆ , \\bm{Y}=\\bm{A}\\bm{X}_{0},\\quad\\bm{X}_{t}=\\bm{X}_{0}+\\sigma_{t}\\bm{G}, bold_italic_Y = bold_italic_A bold_italic_X start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , bold_ital", "snippet": "ùíÄ = ùë® ‚Äã ùëø 0 , ùëø t = ùëø 0 + œÉ t ‚Äã ùëÆ , \\bm{Y}=\\bm{A}\\bm{X}_{0},\\quad\\bm{X}_{t}=\\bm{X}_{0}+\\sigma_{t}\\bm{G}, bold_italic_Y = bold_italic_A bold_italic_X start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , bold_italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_X start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + italic_œÉ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_G , (6.3.5)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E6", "title": "ùëø ^ ‚àó = arg ‚Äã min ùëø ^ ‚Å° ùîº ‚Äã [ ‚Äñ ùë® ‚Äã ( ùëø ^ ‚Äã ( ùë® ‚Äã ùëø t , ùë® ) ‚àí ùëø 0 ) ‚Äñ 2 ] \\hat{\\bm{X}}_{*}=\\operatorname*{arg\\ min}_{\\hat{\\bm{X}}}\\mathbb{E}[\\|\\bm{A}(\\hat{\\bm{X}}(\\bm{A}\\bm{X}_{t},\\bm{A})-\\bm{X}_{0})\\", "snippet": "ùëø ^ ‚àó = arg ‚Äã min ùëø ^ ‚Å° ùîº ‚Äã [ ‚Äñ ùë® ‚Äã ( ùëø ^ ‚Äã ( ùë® ‚Äã ùëø t , ùë® ) ‚àí ùëø 0 ) ‚Äñ 2 ] \\hat{\\bm{X}}_{*}=\\operatorname*{arg\\ min}_{\\hat{\\bm{X}}}\\mathbb{E}[\\|\\bm{A}(\\hat{\\bm{X}}(\\bm{A}\\bm{X}_{t},\\bm{A})-\\bm{X}_{0})\\|^{2}] over^ start_ARG bold_italic_X end_ARG start_POSTSUBSCRIPT ‚àó end_POSTSUBSCRIPT = start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT over^ start_ARG bold_italic_X end_ARG end_POSTSUBSCRIPT blackboard_E [ ‚à• bold_italic_A ( over^ start_ARG bold_italic_X end_ARG ( bold_italic_A bold_italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_italic_A ) - bold_italic_X start_P"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E7", "title": "ùë® ‚Äã ùëø ^ ‚àó ‚Äã ( ùë® ‚Äã ( ùëø t ) , ùë® ) = ùë® ‚Äã ùîº ‚Äã [ ùëø 0 ‚à£ ùë® ‚Äã ùëø t , ùë® ] . \\bm{A}\\hat{\\bm{X}}_{*}(\\bm{A}(\\bm{X}_{t}),\\bm{A})=\\bm{A}\\mathbb{E}[\\bm{X}_{0}\\mid\\bm{A}\\bm{X}_{t},\\bm{A}]. bold_italic_A over^ start_A", "snippet": "ùë® ‚Äã ùëø ^ ‚àó ‚Äã ( ùë® ‚Äã ( ùëø t ) , ùë® ) = ùë® ‚Äã ùîº ‚Äã [ ùëø 0 ‚à£ ùë® ‚Äã ùëø t , ùë® ] . \\bm{A}\\hat{\\bm{X}}_{*}(\\bm{A}(\\bm{X}_{t}),\\bm{A})=\\bm{A}\\mathbb{E}[\\bm{X}_{0}\\mid\\bm{A}\\bm{X}_{t},\\bm{A}]. bold_italic_A over^ start_ARG bold_italic_X end_ARG start_POSTSUBSCRIPT ‚àó end_POSTSUBSCRIPT ( bold_italic_A ( bold_italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , bold_italic_A ) = bold_italic_A blackboard_E [ bold_italic_X start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ‚à£ bold_italic_A bold_italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_italic_A ] . (6.3.7)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E8", "title": "ùëø t ‚àí s = Œ≥ t ‚Äã ùëø t + ( 1 ‚àí Œ≥ t ) ‚Äã ùîº ‚Äã [ ùëø 0 ‚à£ ùë® ‚Äã ùëø t , ùë® ] . \\bm{X}_{t-s}=\\gamma_{t}\\bm{X}_{t}+(1-\\gamma_{t})\\mathbb{E}[\\bm{X}_{0}\\mid\\bm{A}\\bm{X}_{t},\\bm{A}]. bold_italic_X start_POSTSUBSCRIPT ita", "snippet": "ùëø t ‚àí s = Œ≥ t ‚Äã ùëø t + ( 1 ‚àí Œ≥ t ) ‚Äã ùîº ‚Äã [ ùëø 0 ‚à£ ùë® ‚Äã ùëø t , ùë® ] . \\bm{X}_{t-s}=\\gamma_{t}\\bm{X}_{t}+(1-\\gamma_{t})\\mathbb{E}[\\bm{X}_{0}\\mid\\bm{A}\\bm{X}_{t},\\bm{A}]. bold_italic_X start_POSTSUBSCRIPT italic_t - italic_s end_POSTSUBSCRIPT = italic_Œ≥ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT bold_italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + ( 1 - italic_Œ≥ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) blackboard_E [ bold_italic_X start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ‚à£ bold_italic_A bold_italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_italic_A ] . (6.3.8)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E9", "title": "ùíö = h ‚Äã ( ùíô ) + ùíò , \\bm{y}=h(\\bm{x})+\\bm{w}, bold_italic_y = italic_h ( bold_italic_x ) + bold_italic_w , (6.3.9)", "snippet": "ùíö = h ‚Äã ( ùíô ) + ùíò , \\bm{y}=h(\\bm{x})+\\bm{w}, bold_italic_y = italic_h ( bold_italic_x ) + bold_italic_w , (6.3.9)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E10", "title": "ùíô ^ ‚àº p ùíô ‚à£ ùíö ( ‚ãÖ ‚à£ ùíö = ùùÇ ) . \\hat{\\bm{x}}\\sim p_{\\bm{x}\\mid\\bm{y}}(\\,\\cdot\\,\\mid\\bm{y}=\\bm{\\nu}). over^ start_ARG bold_italic_x end_ARG ‚àº italic_p start_POSTSUBSCRIPT bold_italic_x ‚à£ bold_italic_y en", "snippet": "ùíô ^ ‚àº p ùíô ‚à£ ùíö ( ‚ãÖ ‚à£ ùíö = ùùÇ ) . \\hat{\\bm{x}}\\sim p_{\\bm{x}\\mid\\bm{y}}(\\,\\cdot\\,\\mid\\bm{y}=\\bm{\\nu}). over^ start_ARG bold_italic_x end_ARG ‚àº italic_p start_POSTSUBSCRIPT bold_italic_x ‚à£ bold_italic_y end_POSTSUBSCRIPT ( ‚ãÖ ‚à£ bold_italic_y = bold_italic_ŒΩ ) . (6.3.10)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E11", "title": "p ùíô t c ‚à£ ùíö ( ‚ãÖ ‚à£ ùùÇ ) = ‚à´ p ùíô t c ‚à£ ùíô c ( ‚ãÖ ‚à£ ùùÉ ) ‚èü = ùí© ‚Äã ( ùùÉ , t 2 ‚Äã ùë∞ ) ‚ãÖ p ùíô c ‚à£ ùíö ‚èü = p ùíô ‚à£ ùíö ‚Äã ( ùùÉ ‚à£ ùùÇ ) ‚Äã d ùùÉ = ‚à´ p ùíô t ‚à£ ùíô , ùíö ( ‚ãÖ ‚à£ ùùÉ , ùùÇ ) ‚ãÖ p ùíô ‚à£ ùíö ( ùùÉ ‚à£ ùùÇ ) d ùùÉ = ‚à´ p ùíô t , ùíô ‚à£ ùíö ‚Äã ( ‚ãÖ , ùùÉ ", "snippet": "p ùíô t c ‚à£ ùíö ( ‚ãÖ ‚à£ ùùÇ ) = ‚à´ p ùíô t c ‚à£ ùíô c ( ‚ãÖ ‚à£ ùùÉ ) ‚èü = ùí© ‚Äã ( ùùÉ , t 2 ‚Äã ùë∞ ) ‚ãÖ p ùíô c ‚à£ ùíö ‚èü = p ùíô ‚à£ ùíö ‚Äã ( ùùÉ ‚à£ ùùÇ ) ‚Äã d ùùÉ = ‚à´ p ùíô t ‚à£ ùíô , ùíö ( ‚ãÖ ‚à£ ùùÉ , ùùÇ ) ‚ãÖ p ùíô ‚à£ ùíö ( ùùÉ ‚à£ ùùÇ ) d ùùÉ = ‚à´ p ùíô t , ùíô ‚à£ ùíö ‚Äã ( ‚ãÖ , ùùÉ ‚à£ ùùÇ ) ‚Äã d ùùÉ = p ùíô t ‚à£ ùíö ( ‚ãÖ ‚à£ ùùÇ ) . \\begin{split}p_{\\bm{x}^{\\mathrm{c}}_{t}\\mid\\bm{y}}(\\,\\cdot\\,\\mid\\bm{\\nu})&=\\int\\underbrace{p_{\\bm{x}^{\\mathrm{c}}_{t}\\mid\\bm{x}^{\\mathrm{c}}}(\\,\\cdot\\,\\mid\\bm{\\xi})}_{=\\mathcal{N}(\\bm{\\xi},t^{2}\\bm{I})}\\,\\cdot\\,\\underbrace{p_{\\bm{x}^{\\mathrm{c}}\\mid\\bm{y}}}_{=p_{\\bm{x}\\mid\\bm{y}}}(\\bm{\\xi}\\mid\\bm{\\nu})\\mathrm{d}\\bm{\\xi}\\\\ &=\\int p_{\\bm{x}_{t}\\mid\\bm{x},\\bm{y}}(\\"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E12", "title": "p ùíô t ‚à£ ùíö ‚Äã ( ùùÉ ‚à£ ùùÇ ) = p ùíö ‚à£ ùíô t ‚Äã ( ùùÇ ‚à£ ùùÉ ) ‚Äã p ùíô t ‚Äã ( ùùÉ ) p ùíö ‚Äã ( ùùÇ ) . p_{\\bm{x}_{t}\\mid\\bm{y}}(\\bm{\\xi}\\mid\\bm{\\nu})=\\frac{p_{\\bm{y}\\mid\\bm{x}_{t}}(\\bm{\\nu}\\mid\\bm{\\xi})p_{\\bm{x}_{t}}(\\bm{\\xi})}", "snippet": "p ùíô t ‚à£ ùíö ‚Äã ( ùùÉ ‚à£ ùùÇ ) = p ùíö ‚à£ ùíô t ‚Äã ( ùùÇ ‚à£ ùùÉ ) ‚Äã p ùíô t ‚Äã ( ùùÉ ) p ùíö ‚Äã ( ùùÇ ) . p_{\\bm{x}_{t}\\mid\\bm{y}}(\\bm{\\xi}\\mid\\bm{\\nu})=\\frac{p_{\\bm{y}\\mid\\bm{x}_{t}}(\\bm{\\nu}\\mid\\bm{\\xi})p_{\\bm{x}_{t}}(\\bm{\\xi})}{p_{\\bm{y}}(\\bm{\\nu})}. italic_p start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ‚à£ bold_italic_y end_POSTSUBSCRIPT ( bold_italic_Œæ ‚à£ bold_italic_ŒΩ ) = divide start_ARG italic_p start_POSTSUBSCRIPT bold_italic_y ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_ŒΩ ‚à£ bold_italic_Œæ ) italic_p start_POSTSUBSCRIPT bold_italic_"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E13", "title": "‚àá ùùÉ log ‚Å° p ùíô t ‚à£ ùíö ‚Äã ( ùùÉ ‚à£ ùùÇ ) = ‚àá ùùÉ log ‚Å° p t ‚Äã ( ùùÉ ) ‚èü score matching + ‚àá ùùÉ log ‚Å° p ùíö ‚à£ ùíô t ‚Äã ( ùùÇ ‚à£ ùùÉ ) ‚èü measurement matching , \\nabla_{\\bm{\\xi}}\\log p_{\\bm{x}_{t}\\mid\\bm{y}}(\\bm{\\xi}\\mid\\bm{\\nu})", "snippet": "‚àá ùùÉ log ‚Å° p ùíô t ‚à£ ùíö ‚Äã ( ùùÉ ‚à£ ùùÇ ) = ‚àá ùùÉ log ‚Å° p t ‚Äã ( ùùÉ ) ‚èü score matching + ‚àá ùùÉ log ‚Å° p ùíö ‚à£ ùíô t ‚Äã ( ùùÇ ‚à£ ùùÉ ) ‚èü measurement matching , \\nabla_{\\bm{\\xi}}\\log p_{\\bm{x}_{t}\\mid\\bm{y}}(\\bm{\\xi}\\mid\\bm{\\nu})=\\underbrace{\\nabla_{\\bm{\\xi}}\\log p_{t}(\\bm{\\xi})}_{\\text{score matching}}+\\underbrace{\\nabla_{\\bm{\\xi}}\\log p_{\\bm{y}\\mid\\bm{x}_{t}}(\\bm{\\nu}\\mid\\bm{\\xi})}_{\\text{measurement matching}}, ‚àá start_POSTSUBSCRIPT bold_italic_Œæ end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ‚à£ bold_italic_y end_POSTSUBSCRIPT ( bold_italic_Œæ ‚à£ bold_"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.Ex4", "title": "[ ùíô ùíö ] = d [ ùö∫ 1 / 2 ùüé ùë® ‚Äã ùö∫ 1 / 2 œÉ ‚Äã ùë∞ ] ‚Äã [ ùíà ùíò ] + [ ùùÅ ùë® ‚Äã ùùÅ ] . \\begin{bmatrix}\\bm{x}\\\\ \\bm{y}\\end{bmatrix}=_{d}\\begin{bmatrix}\\bm{\\Sigma}^{1/2}&\\mathbf{0}\\\\ \\bm{A}\\bm{\\Sigma}^{1/2}&\\sigma\\bm{I}", "snippet": "[ ùíô ùíö ] = d [ ùö∫ 1 / 2 ùüé ùë® ‚Äã ùö∫ 1 / 2 œÉ ‚Äã ùë∞ ] ‚Äã [ ùíà ùíò ] + [ ùùÅ ùë® ‚Äã ùùÅ ] . \\begin{bmatrix}\\bm{x}\\\\ \\bm{y}\\end{bmatrix}=_{d}\\begin{bmatrix}\\bm{\\Sigma}^{1/2}&\\mathbf{0}\\\\ \\bm{A}\\bm{\\Sigma}^{1/2}&\\sigma\\bm{I}\\end{bmatrix}\\begin{bmatrix}\\bm{g}\\\\ \\bm{w}\\end{bmatrix}+\\begin{bmatrix}\\bm{\\mu}\\\\ \\bm{A}\\bm{\\mu}\\end{bmatrix}. [ start_ARG start_ROW start_CELL bold_italic_x end_CELL end_ROW start_ROW start_CELL bold_italic_y end_CELL end_ROW end_ARG ] = start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT [ start_ARG start_ROW start_CELL bold_Œ£ start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT end_CELL start_CELL bold_"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.Ex5", "title": "[ ùö∫ 1 / 2 ùüé ùë® ‚Äã ùö∫ 1 / 2 œÉ ‚Äã ùë∞ ] ‚Äã [ ùö∫ 1 / 2 ùüé ùë® ‚Äã ùö∫ 1 / 2 œÉ ‚Äã ùë∞ ] ‚ä§ = [ ùö∫ ùö∫ ‚Äã ùë® ‚ä§ ùë® ‚Äã ùö∫ ùë® ‚Äã ùö∫ ‚Äã ùë® ‚ä§ + œÉ 2 ‚Äã ùë∞ ] . \\begin{bmatrix}\\bm{\\Sigma}^{1/2}&\\mathbf{0}\\\\ \\bm{A}\\bm{\\Sigma}^{1/2}&\\sigma\\bm{I}\\end", "snippet": "[ ùö∫ 1 / 2 ùüé ùë® ‚Äã ùö∫ 1 / 2 œÉ ‚Äã ùë∞ ] ‚Äã [ ùö∫ 1 / 2 ùüé ùë® ‚Äã ùö∫ 1 / 2 œÉ ‚Äã ùë∞ ] ‚ä§ = [ ùö∫ ùö∫ ‚Äã ùë® ‚ä§ ùë® ‚Äã ùö∫ ùë® ‚Äã ùö∫ ‚Äã ùë® ‚ä§ + œÉ 2 ‚Äã ùë∞ ] . \\begin{bmatrix}\\bm{\\Sigma}^{1/2}&\\mathbf{0}\\\\ \\bm{A}\\bm{\\Sigma}^{1/2}&\\sigma\\bm{I}\\end{bmatrix}\\begin{bmatrix}\\bm{\\Sigma}^{1/2}&\\mathbf{0}\\\\ \\bm{A}\\bm{\\Sigma}^{1/2}&\\sigma\\bm{I}\\end{bmatrix}^{\\top}=\\begin{bmatrix}\\bm{\\Sigma}&\\bm{\\Sigma}\\bm{A}^{\\top}\\\\ \\bm{A}\\bm{\\Sigma}&\\bm{A}\\bm{\\Sigma}\\bm{A}^{\\top}+\\sigma^{2}\\bm{I}\\end{bmatrix}. [ start_ARG start_ROW start_CELL bold_Œ£ start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT end_CELL start_CELL bold_0 end_CELL end_ROW start_ROW start_CELL b"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E15", "title": "p ùíô ‚à£ ùíö ( ‚ãÖ ‚à£ ùùÇ ) = ùí© ( ùùÅ + ùö∫ ‚Äã ùë® ‚ä§ ‚Äã ( ùë® ‚Äã ùö∫ ‚Äã ùë® ‚ä§ + œÉ 2 ‚Äã ùë∞ ) ‚àí 1 ‚Äã ( ùùÇ ‚àí ùë® ‚Äã ùùÅ ) ‚èü ùùÅ ùíô ‚à£ ùíö ‚Äã ( ùùÇ ) , ùö∫ ‚àí ùö∫ ‚Äã ùë® ‚ä§ ‚Äã ( ùë® ‚Äã ùö∫ ‚Äã ùë® ‚ä§ + œÉ 2 ‚Äã ùë∞ ) ‚àí 1 ‚Äã ùë® ‚Äã ùö∫ ‚èü ùö∫ ùíô ‚à£ ùíö ) . p_{\\bm{x}\\mid\\bm{y}}(\\,\\cdot\\,", "snippet": "p ùíô ‚à£ ùíö ( ‚ãÖ ‚à£ ùùÇ ) = ùí© ( ùùÅ + ùö∫ ‚Äã ùë® ‚ä§ ‚Äã ( ùë® ‚Äã ùö∫ ‚Äã ùë® ‚ä§ + œÉ 2 ‚Äã ùë∞ ) ‚àí 1 ‚Äã ( ùùÇ ‚àí ùë® ‚Äã ùùÅ ) ‚èü ùùÅ ùíô ‚à£ ùíö ‚Äã ( ùùÇ ) , ùö∫ ‚àí ùö∫ ‚Äã ùë® ‚ä§ ‚Äã ( ùë® ‚Äã ùö∫ ‚Äã ùë® ‚ä§ + œÉ 2 ‚Äã ùë∞ ) ‚àí 1 ‚Äã ùë® ‚Äã ùö∫ ‚èü ùö∫ ùíô ‚à£ ùíö ) . p_{\\bm{x}\\mid\\bm{y}}(\\,\\cdot\\,\\mid\\bm{\\nu})=\\mathcal{N}\\left(\\underbrace{\\bm{\\mu}+\\bm{\\Sigma}\\bm{A}^{\\top}\\left(\\bm{A}\\bm{\\Sigma}\\bm{A}^{\\top}+\\sigma^{2}\\bm{I}\\right)^{-1}(\\bm{\\nu}-\\bm{A}\\bm{\\mu})}_{\\bm{\\mu}_{\\bm{x}\\mid\\bm{y}}(\\bm{\\nu})},\\underbrace{\\bm{\\Sigma}-\\bm{\\Sigma}\\bm{A}^{\\top}\\left(\\bm{A}\\bm{\\Sigma}\\bm{A}^{\\top}+\\sigma^{2}\\bm{I}\\right)^{-1}\\bm{A}\\bm{\\Sigma}}_{\\bm{\\Sigma}_{\\bm{x}\\mid\\bm{y}}}\\right). italic_p start_POST"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E16", "title": "ùîº ‚Äã [ ùíô ‚à£ ùíô t = ùùÉ , ùíö = ùùÇ ] = ùùÅ ùíô ‚à£ ùíö ‚Äã ( ùùÇ ) + ùö∫ ùíô ‚à£ ùíö ‚Äã ( ùö∫ ùíô ‚à£ ùíö + t 2 ‚Äã ùë∞ ) ‚àí 1 ‚Äã ( ùùÉ ‚àí ùùÅ ùíô ‚à£ ùíö ‚Äã ( ùùÇ ) ) . \\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi},\\bm{y}=\\bm{\\nu}]=\\bm{\\mu}_{\\bm{x}\\mid\\bm{y}}(\\b", "snippet": "ùîº ‚Äã [ ùíô ‚à£ ùíô t = ùùÉ , ùíö = ùùÇ ] = ùùÅ ùíô ‚à£ ùíö ‚Äã ( ùùÇ ) + ùö∫ ùíô ‚à£ ùíö ‚Äã ( ùö∫ ùíô ‚à£ ùíö + t 2 ‚Äã ùë∞ ) ‚àí 1 ‚Äã ( ùùÉ ‚àí ùùÅ ùíô ‚à£ ùíö ‚Äã ( ùùÇ ) ) . \\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi},\\bm{y}=\\bm{\\nu}]=\\bm{\\mu}_{\\bm{x}\\mid\\bm{y}}(\\bm{\\nu})+\\bm{\\Sigma}_{\\bm{x}\\mid\\bm{y}}\\left(\\bm{\\Sigma}_{\\bm{x}\\mid\\bm{y}}+t^{2}\\bm{I}\\right)^{-1}\\left(\\bm{\\xi}-\\bm{\\mu}_{\\bm{x}\\mid\\bm{y}}(\\bm{\\nu})\\right). blackboard_E [ bold_italic_x ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_Œæ , bold_italic_y = bold_italic_ŒΩ ] = bold_italic_Œº start_POSTSUBSCRIPT bold_italic_x ‚à£ bold_italic_y end_POSTSUBSCRIPT ( bold_italic_ŒΩ"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E17", "title": "ùîº ‚Äã [ ùíô ‚à£ ùíô t = ùùÉ ] = ùùÅ + ùö∫ ‚Äã ( ùö∫ + t 2 ‚Äã ùë∞ ) ‚àí 1 ‚Äã ( ùùÉ ‚àí ùùÅ ) , \\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi}]=\\bm{\\mu}+\\bm{\\Sigma}\\left(\\bm{\\Sigma}+t^{2}\\bm{I}\\right)^{-1}(\\bm{\\xi}-\\bm{\\mu}), blackboard_E", "snippet": "ùîº ‚Äã [ ùíô ‚à£ ùíô t = ùùÉ ] = ùùÅ + ùö∫ ‚Äã ( ùö∫ + t 2 ‚Äã ùë∞ ) ‚àí 1 ‚Äã ( ùùÉ ‚àí ùùÅ ) , \\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi}]=\\bm{\\mu}+\\bm{\\Sigma}\\left(\\bm{\\Sigma}+t^{2}\\bm{I}\\right)^{-1}(\\bm{\\xi}-\\bm{\\mu}), blackboard_E [ bold_italic_x ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_Œæ ] = bold_italic_Œº + bold_Œ£ ( bold_Œ£ + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( bold_italic_Œæ - bold_italic_Œº ) , (6.3.17)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E18", "title": "[ ùíô ùíô t ùíö ] = d [ ùö∫ 1 / 2 ùüé ùüé ùö∫ 1 / 2 t ‚Äã ùë∞ ùüé ùë® ‚Äã ùö∫ 1 / 2 ùüé œÉ ‚Äã ùë∞ ] ‚Äã [ ùíà ùíà ‚Ä≤ ùíò ] + [ ùùÅ ùùÅ ùë® ‚Äã ùùÅ ] , \\begin{bmatrix}\\bm{x}\\\\ \\bm{x}_{t}\\\\ \\bm{y}\\end{bmatrix}=_{d}\\begin{bmatrix}\\bm{\\Sigma}^{1/2}&\\mathb", "snippet": "[ ùíô ùíô t ùíö ] = d [ ùö∫ 1 / 2 ùüé ùüé ùö∫ 1 / 2 t ‚Äã ùë∞ ùüé ùë® ‚Äã ùö∫ 1 / 2 ùüé œÉ ‚Äã ùë∞ ] ‚Äã [ ùíà ùíà ‚Ä≤ ùíò ] + [ ùùÅ ùùÅ ùë® ‚Äã ùùÅ ] , \\begin{bmatrix}\\bm{x}\\\\ \\bm{x}_{t}\\\\ \\bm{y}\\end{bmatrix}=_{d}\\begin{bmatrix}\\bm{\\Sigma}^{1/2}&\\mathbf{0}&\\mathbf{0}\\\\ \\bm{\\Sigma}^{1/2}&t\\bm{I}&\\mathbf{0}\\\\ \\bm{A}\\bm{\\Sigma}^{1/2}&\\mathbf{0}&\\sigma\\bm{I}\\end{bmatrix}\\begin{bmatrix}\\bm{g}\\\\ \\bm{g}^{\\prime}\\\\ \\bm{w}\\end{bmatrix}+\\begin{bmatrix}\\bm{\\mu}\\\\ \\bm{\\mu}\\\\ \\bm{A}\\bm{\\mu}\\end{bmatrix}, [ start_ARG start_ROW start_CELL bold_italic_x end_CELL end_ROW start_ROW start_CELL bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_CELL "}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.Ex6", "title": "[ ùö∫ 1 / 2 t ‚Äã ùë∞ ùüé ùë® ‚Äã ùö∫ 1 / 2 ùüé œÉ ‚Äã ùë∞ ] ‚Äã [ ùö∫ 1 / 2 t ‚Äã ùë∞ ùüé ùë® ‚Äã ùö∫ 1 / 2 ùüé œÉ ‚Äã ùë∞ ] ‚ä§ = [ ùö∫ + t 2 ‚Äã ùë∞ ùö∫ ‚Äã ùë® ‚ä§ ùë® ‚Äã ùö∫ ùë® ‚Äã ùö∫ ‚Äã ùë® ‚ä§ + œÉ 2 ‚Äã ùë∞ ] . \\begin{bmatrix}\\bm{\\Sigma}^{1/2}&t\\bm{I}&\\mathbf{0}\\\\ \\bm{A}", "snippet": "[ ùö∫ 1 / 2 t ‚Äã ùë∞ ùüé ùë® ‚Äã ùö∫ 1 / 2 ùüé œÉ ‚Äã ùë∞ ] ‚Äã [ ùö∫ 1 / 2 t ‚Äã ùë∞ ùüé ùë® ‚Äã ùö∫ 1 / 2 ùüé œÉ ‚Äã ùë∞ ] ‚ä§ = [ ùö∫ + t 2 ‚Äã ùë∞ ùö∫ ‚Äã ùë® ‚ä§ ùë® ‚Äã ùö∫ ùë® ‚Äã ùö∫ ‚Äã ùë® ‚ä§ + œÉ 2 ‚Äã ùë∞ ] . \\begin{bmatrix}\\bm{\\Sigma}^{1/2}&t\\bm{I}&\\mathbf{0}\\\\ \\bm{A}\\bm{\\Sigma}^{1/2}&\\mathbf{0}&\\sigma\\bm{I}\\end{bmatrix}\\begin{bmatrix}\\bm{\\Sigma}^{1/2}&t\\bm{I}&\\mathbf{0}\\\\ \\bm{A}\\bm{\\Sigma}^{1/2}&\\mathbf{0}&\\sigma\\bm{I}\\end{bmatrix}^{\\top}=\\begin{bmatrix}\\bm{\\Sigma}+t^{2}\\bm{I}&\\bm{\\Sigma}\\bm{A}^{\\top}\\\\ \\bm{A}\\bm{\\Sigma}&\\bm{A}\\bm{\\Sigma}\\bm{A}^{\\top}+\\sigma^{2}\\bm{I}\\end{bmatrix}. [ start_ARG start_ROW start_CELL bold_Œ£ start_POSTSUPERSCRIPT 1 / 2 end_POSTSU"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E19", "title": "p ùíö ‚à£ ùíô t ( ‚ãÖ ‚à£ ùùÉ ) = ùí© ( ùë® ‚Äã ùùÅ + ùë® ‚Äã ùö∫ ‚Äã ( ùö∫ + t 2 ‚Äã ùë∞ ) ‚àí 1 ‚Äã ( ùùÉ ‚àí ùùÅ ) ‚èü ùùÅ ùíö ‚à£ ùíô t ‚Äã ( ùùÉ ) , ùë® ‚Äã ùö∫ ‚Äã ùë® ‚ä§ + œÉ 2 ‚Äã ùë∞ ‚àí ùë® ‚Äã ùö∫ ‚Äã ( ùö∫ + t 2 ‚Äã ùë∞ ) ‚àí 1 ‚Äã ùö∫ ‚Äã ùë® ‚ä§ ‚èü ùö∫ ùíö ‚à£ ùíô t ) . p_{\\bm{y}\\mid\\bm{x}_{t}}(\\", "snippet": "p ùíö ‚à£ ùíô t ( ‚ãÖ ‚à£ ùùÉ ) = ùí© ( ùë® ‚Äã ùùÅ + ùë® ‚Äã ùö∫ ‚Äã ( ùö∫ + t 2 ‚Äã ùë∞ ) ‚àí 1 ‚Äã ( ùùÉ ‚àí ùùÅ ) ‚èü ùùÅ ùíö ‚à£ ùíô t ‚Äã ( ùùÉ ) , ùë® ‚Äã ùö∫ ‚Äã ùë® ‚ä§ + œÉ 2 ‚Äã ùë∞ ‚àí ùë® ‚Äã ùö∫ ‚Äã ( ùö∫ + t 2 ‚Äã ùë∞ ) ‚àí 1 ‚Äã ùö∫ ‚Äã ùë® ‚ä§ ‚èü ùö∫ ùíö ‚à£ ùíô t ) . p_{\\bm{y}\\mid\\bm{x}_{t}}(\\,\\cdot\\,\\mid\\bm{\\xi})=\\mathcal{N}\\left(\\underbrace{\\bm{A}\\bm{\\mu}+\\bm{A}\\bm{\\Sigma}\\left(\\bm{\\Sigma}+t^{2}\\bm{I}\\right)^{-1}\\left(\\bm{\\xi}-\\bm{\\mu}\\right)}_{\\bm{\\mu}_{\\bm{y}\\mid\\bm{x}_{t}}(\\bm{\\xi})},\\underbrace{\\bm{A}\\bm{\\Sigma}\\bm{A}^{\\top}+\\sigma^{2}\\bm{I}-\\bm{A}\\bm{\\Sigma}\\left(\\bm{\\Sigma}+t^{2}\\bm{I}\\right)^{-1}\\bm{\\Sigma}\\bm{A}^{\\top}}_{\\bm{\\Sigma}_{\\bm{y}\\mid\\bm{x}_{t}}}\\right). italic_p st"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E21", "title": "ùö∫ ùíö ‚à£ ùíô t = œÉ 2 ‚Äã ùë∞ + ùë® ‚Äã ùö∫ 1 / 2 ‚Äã ( ùë∞ ‚àí ùö∫ 1 / 2 ‚Äã ( ùö∫ + t 2 ‚Äã ùë∞ ) ‚àí 1 ‚Äã ùö∫ 1 / 2 ) ‚Äã ùö∫ ‚Äã ùë® ‚ä§ . \\bm{\\Sigma}_{\\bm{y}\\mid\\bm{x}_{t}}=\\sigma^{2}\\bm{I}+\\bm{A}\\bm{\\Sigma}^{1/2}\\left(\\bm{I}-\\bm{\\Sigma}^{1/2", "snippet": "ùö∫ ùíö ‚à£ ùíô t = œÉ 2 ‚Äã ùë∞ + ùë® ‚Äã ùö∫ 1 / 2 ‚Äã ( ùë∞ ‚àí ùö∫ 1 / 2 ‚Äã ( ùö∫ + t 2 ‚Äã ùë∞ ) ‚àí 1 ‚Äã ùö∫ 1 / 2 ) ‚Äã ùö∫ ‚Äã ùë® ‚ä§ . \\bm{\\Sigma}_{\\bm{y}\\mid\\bm{x}_{t}}=\\sigma^{2}\\bm{I}+\\bm{A}\\bm{\\Sigma}^{1/2}\\left(\\bm{I}-\\bm{\\Sigma}^{1/2}\\left(\\bm{\\Sigma}+t^{2}\\bm{I}\\right)^{-1}\\bm{\\Sigma}^{1/2}\\right)\\bm{\\Sigma}\\bm{A}^{\\top}. bold_Œ£ start_POSTSUBSCRIPT bold_italic_y ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT = italic_œÉ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I + bold_italic_A bold_Œ£ start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT ( bold_italic_I - bold_Œ£ start_POSTSUPERSCRIPT"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E24", "title": "Œª i ‚Äã t 2 Œª i + t 2 ‚âà 0 . \\frac{\\lambda_{i}t^{2}}{\\lambda_{i}+t^{2}}\\approx 0. divide start_ARG italic_Œª start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSC", "snippet": "Œª i ‚Äã t 2 Œª i + t 2 ‚âà 0 . \\frac{\\lambda_{i}t^{2}}{\\lambda_{i}+t^{2}}\\approx 0. divide start_ARG italic_Œª start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_Œª start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ‚âà 0 . (6.3.24)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E25", "title": "ùö∫ ùíö ‚à£ ùíô t ‚âà œÉ 2 ‚Äã ùë∞ . \\bm{\\Sigma}_{\\bm{y}\\mid\\bm{x}_{t}}\\approx\\sigma^{2}\\bm{I}. bold_Œ£ start_POSTSUBSCRIPT bold_italic_y ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRI", "snippet": "ùö∫ ùíö ‚à£ ùíô t ‚âà œÉ 2 ‚Äã ùë∞ . \\bm{\\Sigma}_{\\bm{y}\\mid\\bm{x}_{t}}\\approx\\sigma^{2}\\bm{I}. bold_Œ£ start_POSTSUBSCRIPT bold_italic_y ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ‚âà italic_œÉ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I . (6.3.25)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E26", "title": "‚àá ùùÉ log p ùíö ‚à£ ùíô t ( ùùÇ ‚à£ ùùÉ ) ‚âà ‚àá ùùÉ log p ùíö ‚à£ ùíô ( ùùÇ ‚à£ ùîº [ ùíô ‚à£ ùíô t = ùùÉ ] ) . \\nabla_{\\bm{\\xi}}\\log p_{\\bm{y}\\mid\\bm{x}_{t}}(\\bm{\\nu}\\mid\\bm{\\xi})\\approx\\nabla_{\\bm{\\xi}}\\log p_{\\bm{y}\\mid\\bm{x}}(\\bm{\\nu}", "snippet": "‚àá ùùÉ log p ùíö ‚à£ ùíô t ( ùùÇ ‚à£ ùùÉ ) ‚âà ‚àá ùùÉ log p ùíö ‚à£ ùíô ( ùùÇ ‚à£ ùîº [ ùíô ‚à£ ùíô t = ùùÉ ] ) . \\nabla_{\\bm{\\xi}}\\log p_{\\bm{y}\\mid\\bm{x}_{t}}(\\bm{\\nu}\\mid\\bm{\\xi})\\approx\\nabla_{\\bm{\\xi}}\\log p_{\\bm{y}\\mid\\bm{x}}(\\bm{\\nu}\\mid\\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi}]). ‚àá start_POSTSUBSCRIPT bold_italic_Œæ end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT bold_italic_y ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_ŒΩ ‚à£ bold_italic_Œæ ) ‚âà ‚àá start_POSTSUBSCRIPT bold_italic_Œæ end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT bold_italic_y ‚à£ bold_italic_x end_P"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E27", "title": "p ùíö ‚à£ ùíô t ‚Äã ( ùùÇ ‚à£ ùùÉ ) = ‚à´ p ùíö ‚à£ ùíô ‚Äã ( ùùÇ ‚à£ ùùÉ ‚Ä≤ ) ‚Äã p ùíô ‚à£ ùíô t ‚Äã ( ùùÉ ‚Ä≤ ‚à£ ùùÉ ) ‚Äã d ùùÉ ‚Ä≤ . p_{\\bm{y}\\mid\\bm{x}_{t}}(\\bm{\\nu}\\mid\\bm{\\xi})=\\int p_{\\bm{y}\\mid\\bm{x}}(\\bm{\\nu}\\mid\\bm{\\xi}^{\\prime})p_{\\bm{x}\\mid", "snippet": "p ùíö ‚à£ ùíô t ‚Äã ( ùùÇ ‚à£ ùùÉ ) = ‚à´ p ùíö ‚à£ ùíô ‚Äã ( ùùÇ ‚à£ ùùÉ ‚Ä≤ ) ‚Äã p ùíô ‚à£ ùíô t ‚Äã ( ùùÉ ‚Ä≤ ‚à£ ùùÉ ) ‚Äã d ùùÉ ‚Ä≤ . p_{\\bm{y}\\mid\\bm{x}_{t}}(\\bm{\\nu}\\mid\\bm{\\xi})=\\int p_{\\bm{y}\\mid\\bm{x}}(\\bm{\\nu}\\mid\\bm{\\xi}^{\\prime})p_{\\bm{x}\\mid\\bm{x}_{t}}(\\bm{\\xi}^{\\prime}\\mid\\bm{\\xi})\\mathrm{d}\\bm{\\xi}^{\\prime}. italic_p start_POSTSUBSCRIPT bold_italic_y ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_ŒΩ ‚à£ bold_italic_Œæ ) = ‚à´ italic_p start_POSTSUBSCRIPT bold_italic_y ‚à£ bold_italic_x end_POSTSUBSCRIPT ( bold_italic_ŒΩ ‚à£ bold_italic_Œæ start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT ) italic_p sta"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E28", "title": "ùîº [ ùíô ‚à£ ùíô t = ùùÉ , ùíö = ùùÇ ] ‚âà ùîº [ ùíô ‚à£ ùíô t = ùùÉ ] + t 2 ‚àá ùùÉ log p ùíö ‚à£ ùíô ( ùùÇ ‚à£ ùîº [ ùíô ‚à£ ùíô t = ùùÉ ] ) . \\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi},\\bm{y}=\\bm{\\nu}]\\approx\\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi}", "snippet": "ùîº [ ùíô ‚à£ ùíô t = ùùÉ , ùíö = ùùÇ ] ‚âà ùîº [ ùíô ‚à£ ùíô t = ùùÉ ] + t 2 ‚àá ùùÉ log p ùíö ‚à£ ùíô ( ùùÇ ‚à£ ùîº [ ùíô ‚à£ ùíô t = ùùÉ ] ) . \\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi},\\bm{y}=\\bm{\\nu}]\\approx\\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi}]+t^{2}\\nabla_{\\bm{\\xi}}\\log p_{\\bm{y}\\mid\\bm{x}}(\\bm{\\nu}\\mid\\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi}]). blackboard_E [ bold_italic_x ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_Œæ , bold_italic_y = bold_italic_ŒΩ ] ‚âà blackboard_E [ bold_italic_x ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_Œæ ] + italic_t start_POSTSUPERSCRIPT 2 end_P"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E29", "title": "ùíô ¬Ø Œ∏ ‚Äã ( t , ùùÉ , ùùÇ ) = ùíô ¬Ø Œ∏ ‚Äã ( t , ùùÉ ) + t 2 ‚Äã ‚àá ùùÉ log ‚Å° p ùíö ‚à£ ùíô ‚Äã ( ùùÇ ‚à£ ùíô ¬Ø Œ∏ ‚Äã ( t , ùùÉ ) ) . \\bar{\\bm{x}}_{\\theta}(t,\\bm{\\xi},\\bm{\\nu})=\\bar{\\bm{x}}_{\\theta}(t,\\bm{\\xi})+t^{2}\\nabla_{\\bm{\\xi}}\\lo", "snippet": "ùíô ¬Ø Œ∏ ‚Äã ( t , ùùÉ , ùùÇ ) = ùíô ¬Ø Œ∏ ‚Äã ( t , ùùÉ ) + t 2 ‚Äã ‚àá ùùÉ log ‚Å° p ùíö ‚à£ ùíô ‚Äã ( ùùÇ ‚à£ ùíô ¬Ø Œ∏ ‚Äã ( t , ùùÉ ) ) . \\bar{\\bm{x}}_{\\theta}(t,\\bm{\\xi},\\bm{\\nu})=\\bar{\\bm{x}}_{\\theta}(t,\\bm{\\xi})+t^{2}\\nabla_{\\bm{\\xi}}\\log p_{\\bm{y}\\mid\\bm{x}}(\\bm{\\nu}\\mid\\bar{\\bm{x}}_{\\theta}(t,\\bm{\\xi})). over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( italic_t , bold_italic_Œæ , bold_italic_ŒΩ ) = over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( italic_t , bold_italic_Œæ ) + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ‚àá start_POSTSUBSCRIPT bold_i"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.E30", "title": "p ùíö ‚à£ ùíô ‚Äã ( ùùÇ ‚à£ ùùÉ ) ‚àù exp ‚Å° ( ‚àí 1 2 ‚Äã œÉ 2 ‚Äã ‚Äñ h ‚Äã ( ùùÉ ) ‚àí ùùÇ ‚Äñ 2 2 ) . p_{\\bm{y}\\mid\\bm{x}}(\\bm{\\nu}\\mid\\bm{\\xi})\\propto\\exp\\left(-\\frac{1}{2\\sigma^{2}}\\left\\|h(\\bm{\\xi})-\\bm{\\nu}\\right\\|_{2}^{2}\\right", "snippet": "p ùíö ‚à£ ùíô ‚Äã ( ùùÇ ‚à£ ùùÉ ) ‚àù exp ‚Å° ( ‚àí 1 2 ‚Äã œÉ 2 ‚Äã ‚Äñ h ‚Äã ( ùùÉ ) ‚àí ùùÇ ‚Äñ 2 2 ) . p_{\\bm{y}\\mid\\bm{x}}(\\bm{\\nu}\\mid\\bm{\\xi})\\propto\\exp\\left(-\\frac{1}{2\\sigma^{2}}\\left\\|h(\\bm{\\xi})-\\bm{\\nu}\\right\\|_{2}^{2}\\right). italic_p start_POSTSUBSCRIPT bold_italic_y ‚à£ bold_italic_x end_POSTSUBSCRIPT ( bold_italic_ŒΩ ‚à£ bold_italic_Œæ ) ‚àù roman_exp ( - divide start_ARG 1 end_ARG start_ARG 2 italic_œÉ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ‚à• italic_h ( bold_italic_Œæ ) - bold_italic_ŒΩ ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) . (6.3.30)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S3.Ex9", "title": "ùíô ^ t ‚Ñì ‚àí 1 ‚âê œÉ t ‚Ñì ‚àí 1 œÉ t ‚Ñì ùíô ^ t ‚Ñì + ( Œ± t ‚Ñì ‚àí 1 ‚àí œÉ t ‚Ñì ‚àí 1 œÉ t ‚Ñì Œ± t ‚Ñì ) ( ùíô ¬Ø Œ∏ ( t ‚Ñì , ùíô ^ t ‚Ñì ) ‚àí œÉ t ‚Ñì 2 2 ‚Äã Œ± t ‚Ñì ‚Äã œÉ 2 ‚àá ùùÉ [ ‚à• h ( ùíô ¬Ø Œ∏ ( t ‚Ñì , ùùÉ ) ) ‚àí ùùÇ ‚à• 2 2 ] | ùùÉ = ùíô ^ t ‚Ñì . ) \\hat{\\bm", "snippet": "ùíô ^ t ‚Ñì ‚àí 1 ‚âê œÉ t ‚Ñì ‚àí 1 œÉ t ‚Ñì ùíô ^ t ‚Ñì + ( Œ± t ‚Ñì ‚àí 1 ‚àí œÉ t ‚Ñì ‚àí 1 œÉ t ‚Ñì Œ± t ‚Ñì ) ( ùíô ¬Ø Œ∏ ( t ‚Ñì , ùíô ^ t ‚Ñì ) ‚àí œÉ t ‚Ñì 2 2 ‚Äã Œ± t ‚Ñì ‚Äã œÉ 2 ‚àá ùùÉ [ ‚à• h ( ùíô ¬Ø Œ∏ ( t ‚Ñì , ùùÉ ) ) ‚àí ùùÇ ‚à• 2 2 ] | ùùÉ = ùíô ^ t ‚Ñì . ) \\hat{\\bm{x}}_{t_{\\ell-1}}\\doteq\\frac{\\sigma_{t_{\\ell-1}}}{\\sigma_{t_{\\ell}}}\\hat{\\bm{x}}_{t_{\\ell}}+\\left(\\alpha_{t_{\\ell-1}}-\\frac{\\sigma_{t_{\\ell-1}}}{\\sigma_{t_{\\ell}}}\\alpha_{t_{\\ell}}\\right)\\left(\\bar{\\bm{x}}_{\\theta}(t_{\\ell},\\hat{\\bm{x}}_{t_{\\ell}})-\\frac{\\sigma_{t_{\\ell}}^{2}}{2\\alpha_{t_{\\ell}}\\sigma^{2}}\\nabla_{\\bm{\\xi}}\\left[\\left\\|h(\\bar{\\bm{x}}_{\\theta}(t_{\\ell},\\bm{\\xi}))-\\bm{\\nu}\\right\\|_{2"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E1", "title": "h : ùíô ‚Ü¶ ùíö . h:\\bm{x}\\mapsto\\bm{y}. italic_h : bold_italic_x ‚Ü¶ bold_italic_y . (6.4.1)", "snippet": "h : ùíô ‚Ü¶ ùíö . h:\\bm{x}\\mapsto\\bm{y}. italic_h : bold_italic_x ‚Ü¶ bold_italic_y . (6.4.1)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E2", "title": "f : ùíô ‚Ü¶ ùíõ . f:\\bm{x}\\mapsto\\bm{z}. italic_f : bold_italic_x ‚Ü¶ bold_italic_z . (6.4.2)", "snippet": "f : ùíô ‚Ü¶ ùíõ . f:\\bm{x}\\mapsto\\bm{z}. italic_f : bold_italic_x ‚Ü¶ bold_italic_z . (6.4.2)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E3", "title": "f : ùíô ‚Ü¶ ( ùíõ , ùíö ) f:\\bm{x}\\mapsto(\\bm{z},\\bm{y}) italic_f : bold_italic_x ‚Ü¶ ( bold_italic_z , bold_italic_y ) (6.4.3)", "snippet": "f : ùíô ‚Ü¶ ( ùíõ , ùíö ) f:\\bm{x}\\mapsto(\\bm{z},\\bm{y}) italic_f : bold_italic_x ‚Ü¶ ( bold_italic_z , bold_italic_y ) (6.4.3)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E4", "title": "f : ( ùíô , ùíò ) ‚Ü¶ ( ùíõ , ùíö ) f:(\\bm{x},\\bm{w})\\mapsto(\\bm{z},\\bm{y}) italic_f : ( bold_italic_x , bold_italic_w ) ‚Ü¶ ( bold_italic_z , bold_italic_y ) (6.4.4)", "snippet": "f : ( ùíô , ùíò ) ‚Ü¶ ( ùíõ , ùíö ) f:(\\bm{x},\\bm{w})\\mapsto(\\bm{z},\\bm{y}) italic_f : ( bold_italic_x , bold_italic_w ) ‚Ü¶ ( bold_italic_z , bold_italic_y ) (6.4.4)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E5", "title": "ùíô ^ ‚àº p ùíô ‚à£ ùíö ( ‚ãÖ ‚à£ ùùÇ ) . \\hat{\\bm{x}}\\sim p_{\\bm{x}\\mid\\bm{y}}(\\,\\cdot\\,\\mid\\bm{\\nu}). over^ start_ARG bold_italic_x end_ARG ‚àº italic_p start_POSTSUBSCRIPT bold_italic_x ‚à£ bold_italic_y end_POSTSUBSC", "snippet": "ùíô ^ ‚àº p ùíô ‚à£ ùíö ( ‚ãÖ ‚à£ ùùÇ ) . \\hat{\\bm{x}}\\sim p_{\\bm{x}\\mid\\bm{y}}(\\,\\cdot\\,\\mid\\bm{\\nu}). over^ start_ARG bold_italic_x end_ARG ‚àº italic_p start_POSTSUBSCRIPT bold_italic_x ‚à£ bold_italic_y end_POSTSUBSCRIPT ( ‚ãÖ ‚à£ bold_italic_ŒΩ ) . (6.4.5)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E6", "title": "( ùíô , y ) ‚àº p ùíô , y . (\\bm{x},y)\\sim p_{\\bm{x},y}. ( bold_italic_x , italic_y ) ‚àº italic_p start_POSTSUBSCRIPT bold_italic_x , italic_y end_POSTSUBSCRIPT . (6.4.6)", "snippet": "( ùíô , y ) ‚àº p ùíô , y . (\\bm{x},y)\\sim p_{\\bm{x},y}. ( bold_italic_x , italic_y ) ‚àº italic_p start_POSTSUBSCRIPT bold_italic_x , italic_y end_POSTSUBSCRIPT . (6.4.6)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E7", "title": "ùîº ‚Äã [ ùíô ‚à£ ùíô t = ùùÉ , y = ŒΩ ] = ùîº ‚Äã [ ùíô ‚à£ ùíô t = ùùÉ ] + œÉ t 2 Œ± t ‚Äã ‚àá ùùÉ log ‚Å° p y ‚à£ ùíô t ‚Äã ( ŒΩ ‚à£ ùùÉ ) . \\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi},y=\\nu]=\\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi}]+\\frac{\\sigma", "snippet": "ùîº ‚Äã [ ùíô ‚à£ ùíô t = ùùÉ , y = ŒΩ ] = ùîº ‚Äã [ ùíô ‚à£ ùíô t = ùùÉ ] + œÉ t 2 Œ± t ‚Äã ‚àá ùùÉ log ‚Å° p y ‚à£ ùíô t ‚Äã ( ŒΩ ‚à£ ùùÉ ) . \\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi},y=\\nu]=\\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi}]+\\frac{\\sigma_{t}^{2}}{\\alpha_{t}}\\nabla_{\\bm{\\xi}}\\log p_{y\\mid\\bm{x}_{t}}(\\nu\\mid\\bm{\\xi}). blackboard_E [ bold_italic_x ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_Œæ , italic_y = italic_ŒΩ ] = blackboard_E [ bold_italic_x ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_Œæ ] + divide start_ARG italic_œÉ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT "}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E8", "title": "f Œ∏ c : ( t , ùíô t ) ‚Ü¶ softmax ‚Å° ( ùëæ head ‚Äã ùíõ ‚Äã ( t , ùíô t ) ) . f_{\\theta_{\\mathrm{c}}}:(t,\\bm{x}_{t})\\mapsto\\operatorname{\\mathrm{softmax}}(\\bm{W}_{\\mathrm{head}}\\bm{z}(t,\\bm{x}_{t})). italic_f start_", "snippet": "f Œ∏ c : ( t , ùíô t ) ‚Ü¶ softmax ‚Å° ( ùëæ head ‚Äã ùíõ ‚Äã ( t , ùíô t ) ) . f_{\\theta_{\\mathrm{c}}}:(t,\\bm{x}_{t})\\mapsto\\operatorname{\\mathrm{softmax}}(\\bm{W}_{\\mathrm{head}}\\bm{z}(t,\\bm{x}_{t})). italic_f start_POSTSUBSCRIPT italic_Œ∏ start_POSTSUBSCRIPT roman_c end_POSTSUBSCRIPT end_POSTSUBSCRIPT : ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ‚Ü¶ roman_softmax ( bold_italic_W start_POSTSUBSCRIPT roman_head end_POSTSUBSCRIPT bold_italic_z ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ) . (6.4.8)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E9", "title": "ùíô ¬Ø Œ∏ naive ‚Äã ( t , ùíô t , y ) = ùíô ¬Ø Œ∏ d ‚Äã ( t , ùíô t ) + œÉ t 2 Œ± t ‚Äã ‚àá ùíô t ‚ü® log ‚Å° f Œ∏ c ‚Äã ( t , ùíô t ) , ùíÜ y ‚ü© \\bar{\\bm{x}}_{\\theta}^{\\mathrm{naive}}(t,\\bm{x}_{t},y)=\\bar{\\bm{x}}_{\\theta_{\\mathrm{d}}}(", "snippet": "ùíô ¬Ø Œ∏ naive ‚Äã ( t , ùíô t , y ) = ùíô ¬Ø Œ∏ d ‚Äã ( t , ùíô t ) + œÉ t 2 Œ± t ‚Äã ‚àá ùíô t ‚ü® log ‚Å° f Œ∏ c ‚Äã ( t , ùíô t ) , ùíÜ y ‚ü© \\bar{\\bm{x}}_{\\theta}^{\\mathrm{naive}}(t,\\bm{x}_{t},y)=\\bar{\\bm{x}}_{\\theta_{\\mathrm{d}}}(t,\\bm{x}_{t})+\\frac{\\sigma_{t}^{2}}{\\alpha_{t}}\\nabla_{\\bm{x}_{t}}\\left\\langle\\log f_{\\theta_{\\mathrm{c}}}(t,\\bm{x}_{t}),\\bm{e}_{y}\\right\\rangle over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_naive end_POSTSUPERSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_y ) = over¬Ø start_ARG bold_italic"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E10", "title": "ùíô ¬Ø Œ∏ CG ‚Äã ( t , ùíô t , y ) = ùíô ¬Ø Œ∏ d ‚Äã ( t , ùíô t ) + Œ≥ ‚Äã œÉ t 2 Œ± t ‚Äã ‚àá ùíô t ‚ü® log ‚Å° f Œ∏ c ‚Äã ( t , ùíô t ) , ùíÜ y ‚ü© \\bar{\\bm{x}}_{\\theta}^{\\mathrm{CG}}(t,\\bm{x}_{t},y)=\\bar{\\bm{x}}_{\\theta_{\\mathrm{d}}}(t,", "snippet": "ùíô ¬Ø Œ∏ CG ‚Äã ( t , ùíô t , y ) = ùíô ¬Ø Œ∏ d ‚Äã ( t , ùíô t ) + Œ≥ ‚Äã œÉ t 2 Œ± t ‚Äã ‚àá ùíô t ‚ü® log ‚Å° f Œ∏ c ‚Äã ( t , ùíô t ) , ùíÜ y ‚ü© \\bar{\\bm{x}}_{\\theta}^{\\mathrm{CG}}(t,\\bm{x}_{t},y)=\\bar{\\bm{x}}_{\\theta_{\\mathrm{d}}}(t,\\bm{x}_{t})+\\gamma\\frac{\\sigma_{t}^{2}}{\\alpha_{t}}\\nabla_{\\bm{x}_{t}}\\left\\langle\\log f_{\\theta_{\\mathrm{c}}}(t,\\bm{x}_{t}),\\bm{e}_{y}\\right\\rangle over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_CG end_POSTSUPERSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_y ) = over¬Ø start_ARG bold_itali"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E12", "title": "ùíô ¬Ø Œ∏ CG , ideal ‚Äã ( t , ùùÉ , ŒΩ ) = ùîº ‚Äã [ ùíô ‚à£ ùíô t = ùùÉ ] + Œ≥ ‚Äã œÉ t 2 Œ± t ‚Äã ‚àá ùùÉ log ‚Å° p y ‚à£ ùíô t ‚Äã ( ŒΩ ‚à£ ùùÉ ) . \\bar{\\bm{x}}_{\\theta}^{\\mathrm{CG,\\,ideal}}(t,\\bm{\\xi},\\nu)=\\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\", "snippet": "ùíô ¬Ø Œ∏ CG , ideal ‚Äã ( t , ùùÉ , ŒΩ ) = ùîº ‚Äã [ ùíô ‚à£ ùíô t = ùùÉ ] + Œ≥ ‚Äã œÉ t 2 Œ± t ‚Äã ‚àá ùùÉ log ‚Å° p y ‚à£ ùíô t ‚Äã ( ŒΩ ‚à£ ùùÉ ) . \\bar{\\bm{x}}_{\\theta}^{\\mathrm{CG,\\,ideal}}(t,\\bm{\\xi},\\nu)=\\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi}]+\\gamma\\frac{\\sigma_{t}^{2}}{\\alpha_{t}}\\nabla_{\\bm{\\xi}}\\log p_{y\\mid\\bm{x}_{t}}(\\nu\\mid\\bm{\\xi}). over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_CG , roman_ideal end_POSTSUPERSCRIPT ( italic_t , bold_italic_Œæ , italic_ŒΩ ) = blackboard_E [ bold_italic_x ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_i"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E13", "title": "log ‚Å° p y ‚à£ ùíô t = log ‚Å° p ùíô t ‚à£ y + log ‚Å° p y ‚àí log ‚Å° p ùíô t , \\log p_{y\\mid\\bm{x}_{t}}=\\log p_{\\bm{x}_{t}\\mid y}+\\log p_{y}-\\log p_{\\bm{x}_{t}}, roman_log italic_p start_POSTSUBSCRIPT italic_y ‚à£ bold_", "snippet": "log ‚Å° p y ‚à£ ùíô t = log ‚Å° p ùíô t ‚à£ y + log ‚Å° p y ‚àí log ‚Å° p ùíô t , \\log p_{y\\mid\\bm{x}_{t}}=\\log p_{\\bm{x}_{t}\\mid y}+\\log p_{y}-\\log p_{\\bm{x}_{t}}, roman_log italic_p start_POSTSUBSCRIPT italic_y ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT = roman_log italic_p start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ‚à£ italic_y end_POSTSUBSCRIPT + roman_log italic_p start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT - roman_log italic_p start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT , (6.4."}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E15", "title": "ùíô ¬Ø Œ∏ CFG ‚Äã ( t , ùíô t , y ) = ( 1 ‚àí Œ≥ ) ‚Äã ùíô ¬Ø Œ∏ ‚Äã ( t , ùíô t , ‚àÖ ) + Œ≥ ‚Äã ùíô ¬Ø Œ∏ ‚Äã ( t , ùíô t , y ) . \\bar{\\bm{x}}_{\\theta}^{\\mathrm{CFG}}(t,\\bm{x}_{t},y)=(1-\\gamma)\\bar{\\bm{x}}_{\\theta}(t,\\bm{x}_{t},\\var", "snippet": "ùíô ¬Ø Œ∏ CFG ‚Äã ( t , ùíô t , y ) = ( 1 ‚àí Œ≥ ) ‚Äã ùíô ¬Ø Œ∏ ‚Äã ( t , ùíô t , ‚àÖ ) + Œ≥ ‚Äã ùíô ¬Ø Œ∏ ‚Äã ( t , ùíô t , y ) . \\bar{\\bm{x}}_{\\theta}^{\\mathrm{CFG}}(t,\\bm{x}_{t},y)=(1-\\gamma)\\bar{\\bm{x}}_{\\theta}(t,\\bm{x}_{t},\\varnothing)+\\gamma\\bar{\\bm{x}}_{\\theta}(t,\\bm{x}_{t},y). over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_CFG end_POSTSUPERSCRIPT ( italic_t , bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_y ) = ( 1 - italic_Œ≥ ) over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( italic_t , bold_"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E16", "title": "y + = { ‚àÖ with probability ‚Äã p uncond ; y else . y^{+}=\\begin{cases}\\varnothing&\\text{with probability }p_{\\mathrm{uncond}};\\\\ y&\\text{else}.\\end{cases} italic_y start_POSTSUPERSCRIPT + end_POSTSUPERS", "snippet": "y + = { ‚àÖ with probability ‚Äã p uncond ; y else . y^{+}=\\begin{cases}\\varnothing&\\text{with probability }p_{\\mathrm{uncond}};\\\\ y&\\text{else}.\\end{cases} italic_y start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT = { start_ROW start_CELL ‚àÖ end_CELL start_CELL with probability italic_p start_POSTSUBSCRIPT roman_uncond end_POSTSUBSCRIPT ; end_CELL end_ROW start_ROW start_CELL italic_y end_CELL start_CELL else . end_CELL end_ROW (6.4.16)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.Ex3", "title": "ùíô ^ t ‚Ñì ‚àí 1 ‚âê œÉ t ‚Ñì ‚àí 1 œÉ t ‚Ñì ‚Äã ùíô ^ t ‚Ñì + ( Œ± t ‚Ñì ‚àí 1 ‚àí œÉ t ‚Ñì ‚àí 1 œÉ t ‚Ñì ‚Äã Œ± t ‚Ñì ) ‚Äã ( ( 1 ‚àí Œ≥ ) ‚Äã ùíô ¬Ø Œ∏ ‚Äã ( t ‚Ñì , ùíô ^ t ‚Ñì , ‚àÖ ) + Œ≥ ‚Äã ùíô ¬Ø Œ∏ ‚Äã ( t ‚Ñì , ùíô ^ t ‚Ñì , ŒΩ ) ) \\hat{\\bm{x}}_{t_{\\ell-1}}\\doteq\\fr", "snippet": "ùíô ^ t ‚Ñì ‚àí 1 ‚âê œÉ t ‚Ñì ‚àí 1 œÉ t ‚Ñì ‚Äã ùíô ^ t ‚Ñì + ( Œ± t ‚Ñì ‚àí 1 ‚àí œÉ t ‚Ñì ‚àí 1 œÉ t ‚Ñì ‚Äã Œ± t ‚Ñì ) ‚Äã ( ( 1 ‚àí Œ≥ ) ‚Äã ùíô ¬Ø Œ∏ ‚Äã ( t ‚Ñì , ùíô ^ t ‚Ñì , ‚àÖ ) + Œ≥ ‚Äã ùíô ¬Ø Œ∏ ‚Äã ( t ‚Ñì , ùíô ^ t ‚Ñì , ŒΩ ) ) \\hat{\\bm{x}}_{t_{\\ell-1}}\\doteq\\frac{\\sigma_{t_{\\ell-1}}}{\\sigma_{t_{\\ell}}}\\hat{\\bm{x}}_{t_{\\ell}}+\\left(\\alpha_{t_{\\ell-1}}-\\frac{\\sigma_{t_{\\ell-1}}}{\\sigma_{t_{\\ell}}}\\alpha_{t_{\\ell}}\\right)\\bigl{(}(1-\\gamma)\\bar{\\bm{x}}_{\\theta}(t_{\\ell},\\hat{\\bm{x}}_{t_{\\ell}},\\varnothing)+\\gamma\\bar{\\bm{x}}_{\\theta}(t_{\\ell},\\hat{\\bm{x}}_{t_{\\ell}},\\nu)\\bigr{)} over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t start_POSTSU"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E17", "title": "ùíô ‚àº 1 K ‚Äã ‚àë k = 1 K ùí© ‚Å° ( ùüé , ùëº k ‚Äã ùëº k ‚ä§ ) , \\bm{x}\\sim\\frac{1}{K}\\sum_{k=1}^{K}\\operatorname{\\mathcal{N}}(\\bm{0},\\bm{U}_{k}\\bm{U}_{k}^{\\top}), bold_italic_x ‚àº divide start_ARG 1 end_ARG start_ARG it", "snippet": "ùíô ‚àº 1 K ‚Äã ‚àë k = 1 K ùí© ‚Å° ( ùüé , ùëº k ‚Äã ùëº k ‚ä§ ) , \\bm{x}\\sim\\frac{1}{K}\\sum_{k=1}^{K}\\operatorname{\\mathcal{N}}(\\bm{0},\\bm{U}_{k}\\bm{U}_{k}^{\\top}), bold_italic_x ‚àº divide start_ARG 1 end_ARG start_ARG italic_K end_ARG ‚àë start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT caligraphic_N ( bold_0 , bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ) , (6.4.17)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E18", "title": "ùîº ‚Äã [ ùíô ‚à£ ùíô t = ùùÉ , y = ŒΩ ] = 1 1 + t 2 ‚Äã ùëº ŒΩ ‚Äã ùëº ŒΩ ‚ä§ ‚Äã ùùÉ \\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi},y=\\nu]=\\frac{1}{1+t^{2}}\\bm{U}_{\\nu}\\bm{U}_{\\nu}^{\\top}\\bm{\\xi} blackboard_E [ bold_italic_x ‚à£ bold_i", "snippet": "ùîº ‚Äã [ ùíô ‚à£ ùíô t = ùùÉ , y = ŒΩ ] = 1 1 + t 2 ‚Äã ùëº ŒΩ ‚Äã ùëº ŒΩ ‚ä§ ‚Äã ùùÉ \\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi},y=\\nu]=\\frac{1}{1+t^{2}}\\bm{U}_{\\nu}\\bm{U}_{\\nu}^{\\top}\\bm{\\xi} blackboard_E [ bold_italic_x ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_Œæ , italic_y = italic_ŒΩ ] = divide start_ARG 1 end_ARG start_ARG 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG bold_italic_U start_POSTSUBSCRIPT italic_ŒΩ end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_ŒΩ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_Œæ (6.4.18)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E19", "title": "ùîº ‚Äã [ ùíô ‚à£ ùíô t = ùùÉ ] = 1 1 + t 2 ‚Äã ‚àë k = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ‚Äñ ùëº k ‚ä§ ‚Äã ùùÉ ‚Äñ 2 2 ) ‚àë i = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ‚Äñ ùëº i ‚ä§ ‚Äã ùùÉ ‚Äñ 2 2 ) ‚Äã ùëº k ‚Äã ùëº k ‚ä§ ‚Äã ùùÉ . \\mathbb{E}[\\bm{x}\\", "snippet": "ùîº ‚Äã [ ùíô ‚à£ ùíô t = ùùÉ ] = 1 1 + t 2 ‚Äã ‚àë k = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ‚Äñ ùëº k ‚ä§ ‚Äã ùùÉ ‚Äñ 2 2 ) ‚àë i = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ‚Äñ ùëº i ‚ä§ ‚Äã ùùÉ ‚Äñ 2 2 ) ‚Äã ùëº k ‚Äã ùëº k ‚ä§ ‚Äã ùùÉ . \\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi}]=\\frac{1}{1+t^{2}}\\sum_{k=1}^{K}\\frac{\\exp\\left(\\frac{1}{2t^{2}(1+t^{2})}\\|\\bm{U}_{k}^{\\top}\\bm{\\xi}\\|_{2}^{2}\\right)}{\\sum_{i=1}^{K}\\exp\\left(\\frac{1}{2t^{2}(1+t^{2})}\\|\\bm{U}_{i}^{\\top}\\bm{\\xi}\\|_{2}^{2}\\right)}\\bm{U}_{k}\\bm{U}_{k}^{\\top}\\bm{\\xi}. blackboard_E [ bold_italic_x ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_Œæ ] = divide start_AR"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E20", "title": "ùíô ¬Ø CFG , ideal ‚Äã ( t , ùíô t , y ) = 1 1 + t 2 ‚Äã ( ( 1 ‚àí Œ≥ ) ‚Äã ‚àë k = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ‚Äñ ùëº k ‚ä§ ‚Äã ùíô t ‚Äñ 2 2 ) ‚àë i = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ‚Äñ ùëº i ‚ä§ ‚Äã ùíô t ‚Äñ 2 2 ) ‚Äã ùëº k ", "snippet": "ùíô ¬Ø CFG , ideal ‚Äã ( t , ùíô t , y ) = 1 1 + t 2 ‚Äã ( ( 1 ‚àí Œ≥ ) ‚Äã ‚àë k = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ‚Äñ ùëº k ‚ä§ ‚Äã ùíô t ‚Äñ 2 2 ) ‚àë i = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ‚Äñ ùëº i ‚ä§ ‚Äã ùíô t ‚Äñ 2 2 ) ‚Äã ùëº k ‚Äã ùëº k ‚ä§ + Œ≥ ‚Äã ùëº y ‚Äã ùëº y ‚ä§ ) ‚Äã ùíô t . \\bar{\\bm{x}}^{\\mathrm{CFG,\\,ideal}}(t,\\bm{x}_{t},y)=\\frac{1}{1+t^{2}}\\left((1-\\gamma)\\sum_{k=1}^{K}\\frac{\\exp\\left(\\frac{1}{2t^{2}(1+t^{2})}\\|\\bm{U}_{k}^{\\top}\\bm{x}_{t}\\|_{2}^{2}\\right)}{\\sum_{i=1}^{K}\\exp\\left(\\frac{1}{2t^{2}(1+t^{2})}\\|\\bm{U}_{i}^{\\top}\\bm{x}_{t}\\|_{2}^{2}\\right)}\\bm{U}_{k}\\bm{U}_{k}^{\\top}+\\gamma\\bm{U}_{y}\\bm{U}_{y}^{\\top}\\right)\\bm{x}_{t}. "}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E21", "title": "ùíô ¬Ø CFG , ideal ‚Äã ( t , ùíô t , y ) = 1 1 + t 2 ( [ Œ≥ + ( 1 ‚àí Œ≥ ) exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ‚Äñ ùëº y ‚ä§ ‚Äã ùíô t ‚Äñ 2 2 ) ‚àë i = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ‚Äñ ùëº i ‚ä§ ‚Äã ùíô t ‚Äñ 2 2 ) ] ùëº y ùëº y ‚ä§ + ", "snippet": "ùíô ¬Ø CFG , ideal ‚Äã ( t , ùíô t , y ) = 1 1 + t 2 ( [ Œ≥ + ( 1 ‚àí Œ≥ ) exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ‚Äñ ùëº y ‚ä§ ‚Äã ùíô t ‚Äñ 2 2 ) ‚àë i = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ‚Äñ ùëº i ‚ä§ ‚Äã ùíô t ‚Äñ 2 2 ) ] ùëº y ùëº y ‚ä§ + ( 1 ‚àí Œ≥ ) ‚àë k ‚â† y exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ‚Äñ ùëº k ‚ä§ ‚Äã ùíô t ‚Äñ 2 2 ) ‚àë i = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ‚Äñ ùëº i ‚ä§ ‚Äã ùíô t ‚Äñ 2 2 ) ùëº k ùëº k ‚ä§ ) ùíô t . \\begin{split}\\bar{\\bm{x}}^{\\mathrm{CFG,\\,ideal}}(t,\\bm{x}_{t},y)=\\frac{1}{1+t^{2}}&\\Biggl{(}\\left[\\gamma+(1-\\gamma)\\frac{\\exp\\left(\\frac{1}{2t^{2}(1+t^{2})}\\|\\bm{U}_{y}^{\\top}\\bm{x}_{t}\\|_{2}^{2}\\right)}{\\sum_{i=1}^{K}\\exp\\left(\\frac{1}{2t^{2"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E22", "title": "‚àë k = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ‚Äñ ùëº k ‚ä§ ‚Äã ùíô t ‚Äñ 2 2 ) ‚àë i = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ‚Äñ ùëº i ‚ä§ ‚Äã ùíô t ‚Äñ 2 2 ) = 1 , \\sum_{k=1}^{K}\\frac{\\exp\\left(\\frac{1}{2t^{2}(1+t^{2})}\\|\\bm{U", "snippet": "‚àë k = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ‚Äñ ùëº k ‚ä§ ‚Äã ùíô t ‚Äñ 2 2 ) ‚àë i = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ‚Äñ ùëº i ‚ä§ ‚Äã ùíô t ‚Äñ 2 2 ) = 1 , \\sum_{k=1}^{K}\\frac{\\exp\\left(\\frac{1}{2t^{2}(1+t^{2})}\\|\\bm{U}_{k}^{\\top}\\bm{x}_{t}\\|_{2}^{2}\\right)}{\\sum_{i=1}^{K}\\exp\\left(\\frac{1}{2t^{2}(1+t^{2})}\\|\\bm{U}_{i}^{\\top}\\bm{x}_{t}\\|_{2}^{2}\\right)}=1, ‚àë start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT divide start_ARG roman_exp ( divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + italic_t start_POSTSUPERSCRI"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E23", "title": "Œ≥ + ( 1 ‚àí Œ≥ ) ‚Äã exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ‚Äñ ùëº y ‚ä§ ‚Äã ùíô t ‚Äñ 2 2 ) ‚àë i = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ‚Äñ ùëº i ‚ä§ ‚Äã ùíô t ‚Äñ 2 2 ) ‚âà 1 , \\gamma+(1-\\gamma)\\frac{\\exp\\left(\\frac{1}{2t^{2}(1+t^{2}", "snippet": "Œ≥ + ( 1 ‚àí Œ≥ ) ‚Äã exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ‚Äñ ùëº y ‚ä§ ‚Äã ùíô t ‚Äñ 2 2 ) ‚àë i = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ‚Äñ ùëº i ‚ä§ ‚Äã ùíô t ‚Äñ 2 2 ) ‚âà 1 , \\gamma+(1-\\gamma)\\frac{\\exp\\left(\\frac{1}{2t^{2}(1+t^{2})}\\|\\bm{U}_{y}^{\\top}\\bm{x}_{t}\\|_{2}^{2}\\right)}{\\sum_{i=1}^{K}\\exp\\left(\\frac{1}{2t^{2}(1+t^{2})}\\|\\bm{U}_{i}^{\\top}\\bm{x}_{t}\\|_{2}^{2}\\right)}\\approx 1, italic_Œ≥ + ( 1 - italic_Œ≥ ) divide start_ARG roman_exp ( divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG ‚à• bold_italic_U start_POS"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E24", "title": "Œ≥ + ( 1 ‚àí Œ≥ ) ‚Äã exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ‚Äñ ùëº y ‚ä§ ‚Äã ùíô t ‚Äñ 2 2 ) ‚àë i = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ‚Äñ ùëº i ‚ä§ ‚Äã ùíô t ‚Äñ 2 2 ) ‚âà Œ≥ , \\gamma+(1-\\gamma)\\frac{\\exp\\left(\\frac{1}{2t^{2}(1+t^{2}", "snippet": "Œ≥ + ( 1 ‚àí Œ≥ ) ‚Äã exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ‚Äñ ùëº y ‚ä§ ‚Äã ùíô t ‚Äñ 2 2 ) ‚àë i = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ‚Äñ ùëº i ‚ä§ ‚Äã ùíô t ‚Äñ 2 2 ) ‚âà Œ≥ , \\gamma+(1-\\gamma)\\frac{\\exp\\left(\\frac{1}{2t^{2}(1+t^{2})}\\|\\bm{U}_{y}^{\\top}\\bm{x}_{t}\\|_{2}^{2}\\right)}{\\sum_{i=1}^{K}\\exp\\left(\\frac{1}{2t^{2}(1+t^{2})}\\|\\bm{U}_{i}^{\\top}\\bm{x}_{t}\\|_{2}^{2}\\right)}\\approx\\gamma, italic_Œ≥ + ( 1 - italic_Œ≥ ) divide start_ARG roman_exp ( divide start_ARG 1 end_ARG start_ARG 2 italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( 1 + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) end_ARG ‚à• bold_italic_U start"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E25", "title": "exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ‚Äñ ùëº y ‚ä§ ‚Äã ùíô t ‚Äñ 2 2 ) exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ‚Äñ ùëº y ‚Ä≤ ‚ä§ ‚Äã ùíô t ‚Äñ 2 2 ) = exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ( ‚Äñ ùëº y ‚ä§ ‚Äã ùíô t ‚Äñ 2 2 ‚àí ‚Äñ ùëº y ‚Ä≤ ‚ä§ ‚Äã ùíô t ‚Äñ 2 2 ) ) ", "snippet": "exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ‚Äñ ùëº y ‚ä§ ‚Äã ùíô t ‚Äñ 2 2 ) exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ‚Äñ ùëº y ‚Ä≤ ‚ä§ ‚Äã ùíô t ‚Äñ 2 2 ) = exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ( ‚Äñ ùëº y ‚ä§ ‚Äã ùíô t ‚Äñ 2 2 ‚àí ‚Äñ ùëº y ‚Ä≤ ‚ä§ ‚Äã ùíô t ‚Äñ 2 2 ) ) , \\frac{\\exp\\left(\\frac{1}{2t^{2}(1+t^{2})}\\|\\bm{U}_{y}^{\\top}\\bm{x}_{t}\\|_{2}^{2}\\right)}{\\exp\\left(\\frac{1}{2t^{2}(1+t^{2})}\\|\\bm{U}_{y^{\\prime}}^{\\top}\\bm{x}_{t}\\|_{2}^{2}\\right)}=\\exp\\left(\\frac{1}{2t^{2}(1+t^{2})}\\left(\\|\\bm{U}_{y}^{\\top}\\bm{x}_{t}\\|_{2}^{2}-\\|\\bm{U}_{y^{\\prime}}^{\\top}\\bm{x}_{t}\\|_{2}^{2}\\right)\\right), divide start_ARG roman_exp ( divide start_ARG 1 end_ARG start_ARG 2 ital"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E26", "title": "ùëº k ‚Äã ùëº k ‚ä§ ‚Äã ùíó k = ùíó k , ùëº k ‚Ä≤ ‚Äã ùëº k ‚Ä≤ ‚ä§ ‚Äã ùíó k = ùüé , k ‚Ä≤ ‚â† k . \\bm{U}_{k}\\bm{U}_{k}^{\\top}\\bm{v}_{k}=\\bm{v}_{k},\\quad\\bm{U}_{k^{\\prime}}\\bm{U}_{k^{\\prime}}^{\\top}\\bm{v}_{k}=\\mathbf{0},\\enspace k^{\\pr", "snippet": "ùëº k ‚Äã ùëº k ‚ä§ ‚Äã ùíó k = ùíó k , ùëº k ‚Ä≤ ‚Äã ùëº k ‚Ä≤ ‚ä§ ‚Äã ùíó k = ùüé , k ‚Ä≤ ‚â† k . \\bm{U}_{k}\\bm{U}_{k}^{\\top}\\bm{v}_{k}=\\bm{v}_{k},\\quad\\bm{U}_{k^{\\prime}}\\bm{U}_{k^{\\prime}}^{\\top}\\bm{v}_{k}=\\mathbf{0},\\enspace k^{\\prime}\\neq k. bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT bold_italic_U start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT bold_italic_v start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT = bold_italic_v start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , bold_italic_U start_POSTSUBSCRIPT italic_k start_POSTSUPERSCRIPT ‚Ä≤ end_POSTSUPERSCRIPT end_POSTS"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E27", "title": "( ùíô t , ùíó ) ‚Ü¶ ‚àë k = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ùíô t ‚ä§ ‚Äã ùëº k ‚Äã ùëº k ‚ä§ ‚Äã ùíó ) ‚àë i = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ùíô t ‚ä§ ‚Äã ùëº i ‚Äã ùëº i ‚ä§ ‚Äã ùíó ) ‚Äã ùëº k ‚Äã ùëº k ‚ä§ ‚Äã ùíô t . (\\bm{x}_{t},\\bm{v})\\maps", "snippet": "( ùíô t , ùíó ) ‚Ü¶ ‚àë k = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ùíô t ‚ä§ ‚Äã ùëº k ‚Äã ùëº k ‚ä§ ‚Äã ùíó ) ‚àë i = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ùíô t ‚ä§ ‚Äã ùëº i ‚Äã ùëº i ‚ä§ ‚Äã ùíó ) ‚Äã ùëº k ‚Äã ùëº k ‚ä§ ‚Äã ùíô t . (\\bm{x}_{t},\\bm{v})\\mapsto\\sum_{k=1}^{K}\\frac{\\exp\\left(\\frac{1}{2t^{2}(1+t^{2})}\\bm{x}_{t}^{\\top}\\bm{U}_{k}\\bm{U}_{k}^{\\top}\\bm{v}\\right)}{\\sum_{i=1}^{K}\\exp\\left(\\frac{1}{2t^{2}(1+t^{2})}\\bm{x}_{t}^{\\top}\\bm{U}_{i}\\bm{U}_{i}^{\\top}\\bm{v}\\right)}\\bm{U}_{k}\\bm{U}_{k}^{\\top}\\bm{x}_{t}. ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_italic_v ) ‚Ü¶ ‚àë start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT st"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E29", "title": "‚àë k = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ùíô t ‚ä§ ‚Äã ùëº k ‚Äã ùëº k ‚ä§ ‚Äã ùíó y ) ‚àë i = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ùíô t ‚ä§ ‚Äã ùëº i ‚Äã ùëº i ‚ä§ ‚Äã ùíó y ) ‚Äã ùëº k ‚Äã ùëº k ‚ä§ ‚Äã ùíô t ‚âà ùëº y ‚Äã ùëº y ‚ä§ ‚Äã ùíô t . \\sum_{k=1}^{K}", "snippet": "‚àë k = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ùíô t ‚ä§ ‚Äã ùëº k ‚Äã ùëº k ‚ä§ ‚Äã ùíó y ) ‚àë i = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ùíô t ‚ä§ ‚Äã ùëº i ‚Äã ùëº i ‚ä§ ‚Äã ùíó y ) ‚Äã ùëº k ‚Äã ùëº k ‚ä§ ‚Äã ùíô t ‚âà ùëº y ‚Äã ùëº y ‚ä§ ‚Äã ùíô t . \\sum_{k=1}^{K}\\frac{\\exp\\left(\\frac{1}{2t^{2}(1+t^{2})}\\bm{x}_{t}^{\\top}\\bm{U}_{k}\\bm{U}_{k}^{\\top}\\bm{v}_{y}\\right)}{\\sum_{i=1}^{K}\\exp\\left(\\frac{1}{2t^{2}(1+t^{2})}\\bm{x}_{t}^{\\top}\\bm{U}_{i}\\bm{U}_{i}^{\\top}\\bm{v}_{y}\\right)}\\bm{U}_{k}\\bm{U}_{k}^{\\top}\\bm{x}_{t}\\approx\\bm{U}_{y}\\bm{U}_{y}^{\\top}\\bm{x}_{t}. ‚àë start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIP"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E30", "title": "( ùíô t , ùíó ) ‚Ü¶ ‚àë k = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ùíô t ‚ä§ ‚Äã ùëº k ‚Äã ùëº k ‚ä§ ‚Äã ùíó ) ‚àë i = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ùíô t ‚ä§ ‚Äã ùëº i ‚Äã ùëº i ‚ä§ ‚Äã ùíó ) ‚Äã ùëº k ‚Äã ùëº k ‚ä§ ‚Äã ùíô t (\\bm{x}_{t},\\bm{v})\\mapsto", "snippet": "( ùíô t , ùíó ) ‚Ü¶ ‚àë k = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ùíô t ‚ä§ ‚Äã ùëº k ‚Äã ùëº k ‚ä§ ‚Äã ùíó ) ‚àë i = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ùíô t ‚ä§ ‚Äã ùëº i ‚Äã ùëº i ‚ä§ ‚Äã ùíó ) ‚Äã ùëº k ‚Äã ùëº k ‚ä§ ‚Äã ùíô t (\\bm{x}_{t},\\bm{v})\\mapsto\\sum_{k=1}^{K}\\frac{\\exp\\left(\\frac{1}{2t^{2}(1+t^{2})}\\bm{x}_{t}^{\\top}\\bm{U}_{k}\\bm{U}_{k}^{\\top}\\bm{v}\\right)}{\\sum_{i=1}^{K}\\exp\\left(\\frac{1}{2t^{2}(1+t^{2})}\\bm{x}_{t}^{\\top}\\bm{U}_{i}\\bm{U}_{i}^{\\top}\\bm{v}\\right)}\\bm{U}_{k}\\bm{U}_{k}^{\\top}\\bm{x}_{t} ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_italic_v ) ‚Ü¶ ‚àë start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S4.E31", "title": "MHCA ‚Äã ( ùíõ t , ùíÄ + ) = ùëº out ‚Äã [ SA ‚Å° ( [ ùëº qry 1 ] ‚ä§ ‚Äã œà ‚Äã ( ùíõ t ) , [ ùëº key 1 ] ‚ä§ ‚Äã œÑ ‚Äã ( ùíÄ + ) , [ ùëº val 1 ] ‚ä§ ‚Äã œÑ ‚Äã ( ùíÄ + ) ) ‚ãÆ SA ‚Å° ( [ ùëº qry K ] ‚ä§ ‚Äã œà ‚Äã ( ùíõ t ) , [ ùëº key K ] ‚ä§ ‚Äã œÑ ‚Äã ( ùíÄ + ) , [", "snippet": "MHCA ‚Äã ( ùíõ t , ùíÄ + ) = ùëº out ‚Äã [ SA ‚Å° ( [ ùëº qry 1 ] ‚ä§ ‚Äã œà ‚Äã ( ùíõ t ) , [ ùëº key 1 ] ‚ä§ ‚Äã œÑ ‚Äã ( ùíÄ + ) , [ ùëº val 1 ] ‚ä§ ‚Äã œÑ ‚Äã ( ùíÄ + ) ) ‚ãÆ SA ‚Å° ( [ ùëº qry K ] ‚ä§ ‚Äã œà ‚Äã ( ùíõ t ) , [ ùëº key K ] ‚ä§ ‚Äã œÑ ‚Äã ( ùíÄ + ) , [ ùëº val K ] ‚ä§ ‚Äã œÑ ‚Äã ( ùíÄ + ) ) ] , \\mathrm{MHCA}(\\bm{z}_{t},\\bm{Y}^{+})=\\bm{U}_{\\mathrm{out}}\\begin{bmatrix}\\operatorname{SA}([\\bm{U}_{\\mathrm{qry}}^{1}]^{\\top}\\psi(\\bm{z}_{t}),[\\bm{U}_{\\mathrm{key}}^{1}]^{\\top}\\tau(\\bm{Y}^{+}),[\\bm{U}_{\\mathrm{val}}^{1}]^{\\top}\\tau(\\bm{Y}^{+}))\\\\ \\vdots\\\\ \\operatorname{SA}([\\bm{U}_{\\mathrm{qry}}^{K}]^{\\top}\\psi(\\bm{z}_{t}),[\\bm{U}_{\\mathrm{key}}^{K}]^{\\top}\\tau(\\bm"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E1", "title": "F ‚Äã ( ùíô ) = ùüé , G ‚Äã ( ùíö ) = ùüé . F(\\bm{x})=\\bm{0},\\quad G(\\bm{y})=\\bm{0}. italic_F ( bold_italic_x ) = bold_0 , italic_G ( bold_italic_y ) = bold_0 . (6.5.1)", "snippet": "F ‚Äã ( ùíô ) = ùüé , G ‚Äã ( ùíö ) = ùüé . F(\\bm{x})=\\bm{0},\\quad G(\\bm{y})=\\bm{0}. italic_F ( bold_italic_x ) = bold_0 , italic_G ( bold_italic_y ) = bold_0 . (6.5.1)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E2", "title": "ùíö = ùë® ‚Äã ùíô . \\bm{y}=\\bm{A}\\bm{x}. bold_italic_y = bold_italic_A bold_italic_x . (6.5.2)", "snippet": "ùíö = ùë® ‚Äã ùíô . \\bm{y}=\\bm{A}\\bm{x}. bold_italic_y = bold_italic_A bold_italic_x . (6.5.2)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E3", "title": "ùíö t = ùíö 0 + t ‚Äã ùíà , ùíö 0 = ùë® ‚Äã ( ùíô 0 ) , \\bm{y}_{t}=\\bm{y}_{0}+t\\bm{g},\\quad\\bm{y}_{0}=\\bm{A}(\\bm{x}_{0}), bold_italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_y start_POSTSUBSCRI", "snippet": "ùíö t = ùíö 0 + t ‚Äã ùíà , ùíö 0 = ùë® ‚Äã ( ùíô 0 ) , \\bm{y}_{t}=\\bm{y}_{0}+t\\bm{g},\\quad\\bm{y}_{0}=\\bm{A}(\\bm{x}_{0}), bold_italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_y start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT + italic_t bold_italic_g , bold_italic_y start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = bold_italic_A ( bold_italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) , (6.5.3)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E4", "title": "ùíö t = ùë® ‚Äã ùíô t . \\bm{y}_{t}=\\bm{A}\\bm{x}_{t}. bold_italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_A bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . (6.5.4)", "snippet": "ùíö t = ùë® ‚Äã ùíô t . \\bm{y}_{t}=\\bm{A}\\bm{x}_{t}. bold_italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_A bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . (6.5.4)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E5", "title": "ùíö t ‚àí s ‚âà ùíö t + s ‚Äã t ‚Äã ‚àá log ‚Å° p t ‚Äã ( ùíö t ) . \\bm{y}_{t-s}\\approx\\bm{y}_{t}+st\\nabla\\log p_{t}(\\bm{y}_{t}). bold_italic_y start_POSTSUBSCRIPT italic_t - italic_s end_POSTSUBSCRIPT ‚âà bold_italic_y st", "snippet": "ùíö t ‚àí s ‚âà ùíö t + s ‚Äã t ‚Äã ‚àá log ‚Å° p t ‚Äã ( ùíö t ) . \\bm{y}_{t-s}\\approx\\bm{y}_{t}+st\\nabla\\log p_{t}(\\bm{y}_{t}). bold_italic_y start_POSTSUBSCRIPT italic_t - italic_s end_POSTSUBSCRIPT ‚âà bold_italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_s italic_t ‚àá roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) . (6.5.5)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E6", "title": "ùë® ‚Äã ùíô t ‚àí s ‚âà ùë® ‚Äã ùíô t + s ‚Äã t ‚Äã ‚àá log ‚Å° p t ‚Äã ( ùë® ‚Äã ùíô t ) , \\bm{A}\\bm{x}_{t-s}\\approx\\bm{A}\\bm{x}_{t}+st\\nabla\\log p_{t}(\\bm{A}\\bm{x}_{t}), bold_italic_A bold_italic_x start_POSTSUBSCRIPT italic_t - i", "snippet": "ùë® ‚Äã ùíô t ‚àí s ‚âà ùë® ‚Äã ùíô t + s ‚Äã t ‚Äã ‚àá log ‚Å° p t ‚Äã ( ùë® ‚Äã ùíô t ) , \\bm{A}\\bm{x}_{t-s}\\approx\\bm{A}\\bm{x}_{t}+st\\nabla\\log p_{t}(\\bm{A}\\bm{x}_{t}), bold_italic_A bold_italic_x start_POSTSUBSCRIPT italic_t - italic_s end_POSTSUBSCRIPT ‚âà bold_italic_A bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_s italic_t ‚àá roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_A bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , (6.5.6)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E7", "title": "ùë® ‚Äã ( ùíô t ‚àí s ‚àí ùíô t ) ‚âà s ‚Äã t ‚Äã ‚àá log ‚Å° p t ‚Äã ( ùë® ‚Äã ùíô t ) . \\bm{A}(\\bm{x}_{t-s}-\\bm{x}_{t})\\approx st\\nabla\\log p_{t}(\\bm{A}\\bm{x}_{t}). bold_italic_A ( bold_italic_x start_POSTSUBSCRIPT italic_t - it", "snippet": "ùë® ‚Äã ( ùíô t ‚àí s ‚àí ùíô t ) ‚âà s ‚Äã t ‚Äã ‚àá log ‚Å° p t ‚Äã ( ùë® ‚Äã ùíô t ) . \\bm{A}(\\bm{x}_{t-s}-\\bm{x}_{t})\\approx st\\nabla\\log p_{t}(\\bm{A}\\bm{x}_{t}). bold_italic_A ( bold_italic_x start_POSTSUBSCRIPT italic_t - italic_s end_POSTSUBSCRIPT - bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ‚âà italic_s italic_t ‚àá roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_A bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) . (6.5.7)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E8", "title": "ùíô t ‚àí s ‚âà ùíô t + s ‚Äã t ‚Äã ùë® ‚Ä† ‚Äã ‚àá log ‚Å° p t ‚Äã ( ùë® ‚Äã ùíô t ) . \\bm{x}_{t-s}\\approx\\bm{x}_{t}+st\\bm{A}^{\\dagger}\\nabla\\log p_{t}(\\bm{A}\\bm{x}_{t}). bold_italic_x start_POSTSUBSCRIPT italic_t - italic_s end_", "snippet": "ùíô t ‚àí s ‚âà ùíô t + s ‚Äã t ‚Äã ùë® ‚Ä† ‚Äã ‚àá log ‚Å° p t ‚Äã ( ùë® ‚Äã ùíô t ) . \\bm{x}_{t-s}\\approx\\bm{x}_{t}+st\\bm{A}^{\\dagger}\\nabla\\log p_{t}(\\bm{A}\\bm{x}_{t}). bold_italic_x start_POSTSUBSCRIPT italic_t - italic_s end_POSTSUBSCRIPT ‚âà bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_s italic_t bold_italic_A start_POSTSUPERSCRIPT ‚Ä† end_POSTSUPERSCRIPT ‚àá roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_A bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) . (6.5.8)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E9", "title": "ùíö i = h ‚Äã ( ùíô , Œ∏ i ) + ùíò i , \\bm{y}^{i}=h(\\bm{x},\\theta^{i})+\\bm{w}^{i}, bold_italic_y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT = italic_h ( bold_italic_x , italic_Œ∏ start_POSTSUPERSCRIPT i", "snippet": "ùíö i = h ‚Äã ( ùíô , Œ∏ i ) + ùíò i , \\bm{y}^{i}=h(\\bm{x},\\theta^{i})+\\bm{w}^{i}, bold_italic_y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT = italic_h ( bold_italic_x , italic_Œ∏ start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ) + bold_italic_w start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , (6.5.9)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E10", "title": "ùíö 0 = h ‚Äã ( ùíô , Œ∏ 0 ) + ùíò 0 , ùíö 1 = h ‚Äã ( ùíô , Œ∏ 1 ) + ùíò 1 , \\bm{y}^{0}=h(\\bm{x},\\theta^{0})+\\bm{w}^{0},\\quad\\bm{y}^{1}=h(\\bm{x},\\theta^{1})+\\bm{w}^{1}, bold_italic_y start_POSTSUPERSCRIPT 0 end_POSTSU", "snippet": "ùíö 0 = h ‚Äã ( ùíô , Œ∏ 0 ) + ùíò 0 , ùíö 1 = h ‚Äã ( ùíô , Œ∏ 1 ) + ùíò 1 , \\bm{y}^{0}=h(\\bm{x},\\theta^{0})+\\bm{w}^{0},\\quad\\bm{y}^{1}=h(\\bm{x},\\theta^{1})+\\bm{w}^{1}, bold_italic_y start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT = italic_h ( bold_italic_x , italic_Œ∏ start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT ) + bold_italic_w start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT , bold_italic_y start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT = italic_h ( bold_italic_x , italic_Œ∏ start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ) + bold_italic_w start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , (6.5.10)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E11", "title": "ùîº ‚Äã [ ùíö ‚à£ ùíö t = ùùÇ ] = ùùÇ + t 2 ‚Äã ‚àá ùùÇ log ‚Å° p t ‚Äã ( ùùÇ ) . \\mathbb{E}[\\bm{y}\\mid\\bm{y}_{t}=\\bm{\\nu}]=\\bm{\\nu}+t^{2}\\nabla_{\\bm{\\nu}}\\log p_{t}(\\bm{\\nu}). blackboard_E [ bold_italic_y ‚à£ bold_italic_y star", "snippet": "ùîº ‚Äã [ ùíö ‚à£ ùíö t = ùùÇ ] = ùùÇ + t 2 ‚Äã ‚àá ùùÇ log ‚Å° p t ‚Äã ( ùùÇ ) . \\mathbb{E}[\\bm{y}\\mid\\bm{y}_{t}=\\bm{\\nu}]=\\bm{\\nu}+t^{2}\\nabla_{\\bm{\\nu}}\\log p_{t}(\\bm{\\nu}). blackboard_E [ bold_italic_y ‚à£ bold_italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_ŒΩ ] = bold_italic_ŒΩ + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ‚àá start_POSTSUBSCRIPT bold_italic_ŒΩ end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_ŒΩ ) . (6.5.11)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E12", "title": "ùíö t ‚àí s = ùíö t + s ‚Äã t ‚Äã ‚àá ùíö log ‚Å° p t ‚Äã ( ùíö t ) . \\bm{y}_{t-s}=\\bm{y}_{t}+st\\nabla_{\\bm{y}}\\log p_{t}(\\bm{y}_{t}). bold_italic_y start_POSTSUBSCRIPT italic_t - italic_s end_POSTSUBSCRIPT = bold_italic", "snippet": "ùíö t ‚àí s = ùíö t + s ‚Äã t ‚Äã ‚àá ùíö log ‚Å° p t ‚Äã ( ùíö t ) . \\bm{y}_{t-s}=\\bm{y}_{t}+st\\nabla_{\\bm{y}}\\log p_{t}(\\bm{y}_{t}). bold_italic_y start_POSTSUBSCRIPT italic_t - italic_s end_POSTSUBSCRIPT = bold_italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_s italic_t ‚àá start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) . (6.5.12)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E13", "title": "ùíö = h ‚Äã ( ùíô ) . \\bm{y}=h(\\bm{x}). bold_italic_y = italic_h ( bold_italic_x ) . (6.5.13)", "snippet": "ùíö = h ‚Äã ( ùíô ) . \\bm{y}=h(\\bm{x}). bold_italic_y = italic_h ( bold_italic_x ) . (6.5.13)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E14", "title": "h ‚Äã ( ùíô t ‚àí s ) ‚âà h ‚Äã ( ùíô t ) + s ‚Äã t ‚Äã ‚àá ùíö log ‚Å° p t ‚Äã ( h ‚Äã ( ùíô t ) ) , h(\\bm{x}_{t-s})\\approx h(\\bm{x}_{t})+st\\nabla_{\\bm{y}}\\log p_{t}(h(\\bm{x}_{t})), italic_h ( bold_italic_x start_POSTSUBSCRIPT ", "snippet": "h ‚Äã ( ùíô t ‚àí s ) ‚âà h ‚Äã ( ùíô t ) + s ‚Äã t ‚Äã ‚àá ùíö log ‚Å° p t ‚Äã ( h ‚Äã ( ùíô t ) ) , h(\\bm{x}_{t-s})\\approx h(\\bm{x}_{t})+st\\nabla_{\\bm{y}}\\log p_{t}(h(\\bm{x}_{t})), italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t - italic_s end_POSTSUBSCRIPT ) ‚âà italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) + italic_s italic_t ‚àá start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ) , (6.5.14)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E15", "title": "h ‚Äã ( ùíô t ‚àí s ) ‚âà h ‚Äã ( ùíô t ) + ‚àÇ h ‚àÇ ùíô ‚Äã ( ùíô t ) ‚ãÖ ùíó ‚Äã s ‚âê h ‚Äã ( ùíô t ) + ùë® ‚Äã ( ùíô t ) ‚Äã ùíó ‚Äã s . h(\\bm{x}_{t-s})\\approx h(\\bm{x}_{t})+\\frac{\\partial h}{\\partial\\bm{x}}(\\bm{x}_{t})\\cdot\\bm{v}s\\doteq h(\\", "snippet": "h ‚Äã ( ùíô t ‚àí s ) ‚âà h ‚Äã ( ùíô t ) + ‚àÇ h ‚àÇ ùíô ‚Äã ( ùíô t ) ‚ãÖ ùíó ‚Äã s ‚âê h ‚Äã ( ùíô t ) + ùë® ‚Äã ( ùíô t ) ‚Äã ùíó ‚Äã s . h(\\bm{x}_{t-s})\\approx h(\\bm{x}_{t})+\\frac{\\partial h}{\\partial\\bm{x}}(\\bm{x}_{t})\\cdot\\bm{v}s\\doteq h(\\bm{x}_{t})+\\bm{A}(\\bm{x}_{t})\\bm{v}s. italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t - italic_s end_POSTSUBSCRIPT ) ‚âà italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) + divide start_ARG ‚àÇ italic_h end_ARG start_ARG ‚àÇ bold_italic_x end_ARG ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ‚ãÖ bold_italic_v italic_s ‚âê italic_h ( bold_italic_x start_POSTSU"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E16", "title": "ùë® ‚Äã ( ùíô t ) ‚Äã ùíó = t ‚Äã ‚àá ùíö log ‚Å° p t ‚Äã ( h ‚Äã ( ùíô t ) ) . \\bm{A}(\\bm{x}_{t})\\bm{v}=t\\nabla_{\\bm{y}}\\log p_{t}(h(\\bm{x}_{t})). bold_italic_A ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT", "snippet": "ùë® ‚Äã ( ùíô t ) ‚Äã ùíó = t ‚Äã ‚àá ùíö log ‚Å° p t ‚Äã ( h ‚Äã ( ùíô t ) ) . \\bm{A}(\\bm{x}_{t})\\bm{v}=t\\nabla_{\\bm{y}}\\log p_{t}(h(\\bm{x}_{t})). bold_italic_A ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) bold_italic_v = italic_t ‚àá start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_h ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ) . (6.5.16)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E17", "title": "ùíô ^ t ‚àí s ‚âà ùíô t + s ‚Äã t ‚Äã ùë® ‚Äã ( ùíô t ) ‚Ä† ‚Äã ‚àá ùíö log ‚Å° p t ‚Äã ( h ‚Äã ( ùíô t ) ) . \\hat{\\bm{x}}_{t-s}\\approx\\bm{x}_{t}+st\\bm{A}(\\bm{x}_{t})^{\\dagger}\\nabla_{\\bm{y}}\\log p_{t}(h(\\bm{x}_{t})). over^ start_ARG ", "snippet": "ùíô ^ t ‚àí s ‚âà ùíô t + s ‚Äã t ‚Äã ùë® ‚Äã ( ùíô t ) ‚Ä† ‚Äã ‚àá ùíö log ‚Å° p t ‚Äã ( h ‚Äã ( ùíô t ) ) . \\hat{\\bm{x}}_{t-s}\\approx\\bm{x}_{t}+st\\bm{A}(\\bm{x}_{t})^{\\dagger}\\nabla_{\\bm{y}}\\log p_{t}(h(\\bm{x}_{t})). over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t - italic_s end_POSTSUBSCRIPT ‚âà bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_s italic_t bold_italic_A ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT ‚Ä† end_POSTSUPERSCRIPT ‚àá start_POSTSUBSCRIPT bold_italic_y end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBS"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E18", "title": "ùíö i = h i ‚Äã ( ùíô ) + ùíò i , i = 1 , ‚Ä¶ , K . \\bm{y}^{i}=h^{i}(\\bm{x})+\\bm{w}^{i},i=1,\\ldots,K. bold_italic_y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT = italic_h start_POSTSUPERSCRIPT italic_i e", "snippet": "ùíö i = h i ‚Äã ( ùíô ) + ùíò i , i = 1 , ‚Ä¶ , K . \\bm{y}^{i}=h^{i}(\\bm{x})+\\bm{w}^{i},i=1,\\ldots,K. bold_italic_y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT = italic_h start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ( bold_italic_x ) + bold_italic_w start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , italic_i = 1 , ‚Ä¶ , italic_K . (6.5.18)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E19", "title": "ùë® i ‚Äã ( ùíô t ) ‚Äã ùíó = t ‚Äã ‚àá ùíö i log ‚Å° p t ‚Äã ( h i ‚Äã ( ùíô t ) ) , \\bm{A}^{i}(\\bm{x}_{t})\\bm{v}=t\\nabla_{\\bm{y}^{i}}\\log p_{t}(h^{i}(\\bm{x}_{t})), bold_italic_A start_POSTSUPERSCRIPT italic_i end_POSTSUPER", "snippet": "ùë® i ‚Äã ( ùíô t ) ‚Äã ùíó = t ‚Äã ‚àá ùíö i log ‚Å° p t ‚Äã ( h i ‚Äã ( ùíô t ) ) , \\bm{A}^{i}(\\bm{x}_{t})\\bm{v}=t\\nabla_{\\bm{y}^{i}}\\log p_{t}(h^{i}(\\bm{x}_{t})), bold_italic_A start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) bold_italic_v = italic_t ‚àá start_POSTSUBSCRIPT bold_italic_y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( italic_h start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ) , ("}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E20", "title": "ùíó i = t ‚Äã ùë® i ‚Äã ( ùíô t ) ‚Ä† ‚Äã [ ‚àá ùíö i log ‚Å° p t ‚Äã ( h i ‚Äã ( ùíô t ) ) ] , i = 1 , ‚Ä¶ , K , \\bm{v}^{i}=t\\bm{A}^{i}(\\bm{x}_{t})^{\\dagger}\\big{[}\\nabla_{\\bm{y}^{i}}\\log p_{t}(h^{i}(\\bm{x}_{t}))\\big{]},\\quad i", "snippet": "ùíó i = t ‚Äã ùë® i ‚Äã ( ùíô t ) ‚Ä† ‚Äã [ ‚àá ùíö i log ‚Å° p t ‚Äã ( h i ‚Äã ( ùíô t ) ) ] , i = 1 , ‚Ä¶ , K , \\bm{v}^{i}=t\\bm{A}^{i}(\\bm{x}_{t})^{\\dagger}\\big{[}\\nabla_{\\bm{y}^{i}}\\log p_{t}(h^{i}(\\bm{x}_{t}))\\big{]},\\quad i=1,\\ldots,K, bold_italic_v start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT = italic_t bold_italic_A start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ( bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT ‚Ä† end_POSTSUPERSCRIPT [ ‚àá start_POSTSUBSCRIPT bold_italic_y start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_POSTSUBSCRIPT roman_log italic_p start_POSTS"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E21", "title": "ùíö k = h ‚Äã ( ùíô k , Œ∏ k ) , k = 1 , ‚Ä¶ , K , \\bm{y}^{k}=h(\\bm{x}^{k},\\theta^{k}),\\quad k=1,\\ldots,K, bold_italic_y start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT = italic_h ( bold_italic_x start_POST", "snippet": "ùíö k = h ‚Äã ( ùíô k , Œ∏ k ) , k = 1 , ‚Ä¶ , K , \\bm{y}^{k}=h(\\bm{x}^{k},\\theta^{k}),\\quad k=1,\\ldots,K, bold_italic_y start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT = italic_h ( bold_italic_x start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT , italic_Œ∏ start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT ) , italic_k = 1 , ‚Ä¶ , italic_K , (6.5.21)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S5.E22", "title": "ùíÄ = H ‚Äã ( ùíô , Œò ) . \\bm{Y}=H(\\bm{x},\\Theta). bold_italic_Y = italic_H ( bold_italic_x , roman_Œò ) . (6.5.22)", "snippet": "ùíÄ = H ‚Äã ( ùíô , Œò ) . \\bm{Y}=H(\\bm{x},\\Theta). bold_italic_Y = italic_H ( bold_italic_x , roman_Œò ) . (6.5.22)"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S6.E1", "title": "ùîº ùíà ‚Äã [ ‚Äñ ùíô ‚àí f ‚Äã ( ùíô + t ‚Äã ùíà ) ‚Äñ 2 2 ] = ùîº ùíà ‚Äã [ ‚Äñ ùíô + t ‚Äã ùíà ‚àí f ‚Äã ( ùíô + t ‚Äã ùíà ) ‚Äñ 2 2 + 2 ‚Äã t 2 ‚Äã ‚àá ‚ãÖ f ‚Äã ( ùíô + t ‚Äã ùíà ) ] ‚àí t 2 ‚Äã D , \\mathbb{E}_{\\bm{g}}\\left[\\left\\|\\bm{x}-f(\\bm{x}+t\\bm{g})\\right\\|", "snippet": "ùîº ùíà ‚Äã [ ‚Äñ ùíô ‚àí f ‚Äã ( ùíô + t ‚Äã ùíà ) ‚Äñ 2 2 ] = ùîº ùíà ‚Äã [ ‚Äñ ùíô + t ‚Äã ùíà ‚àí f ‚Äã ( ùíô + t ‚Äã ùíà ) ‚Äñ 2 2 + 2 ‚Äã t 2 ‚Äã ‚àá ‚ãÖ f ‚Äã ( ùíô + t ‚Äã ùíà ) ] ‚àí t 2 ‚Äã D , \\mathbb{E}_{\\bm{g}}\\left[\\left\\|\\bm{x}-f(\\bm{x}+t\\bm{g})\\right\\|_{2}^{2}\\right]=\\mathbb{E}_{\\bm{g}}\\left[\\left\\|\\bm{x}+t\\bm{g}-f(\\bm{x}+t\\bm{g})\\right\\|_{2}^{2}+2t^{2}\\nabla\\cdot f(\\bm{x}+t\\bm{g})\\right]-t^{2}D, blackboard_E start_POSTSUBSCRIPT bold_italic_g end_POSTSUBSCRIPT [ ‚à• bold_italic_x - italic_f ( bold_italic_x + italic_t bold_italic_g ) ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] = blackboard_E start_POSTSU"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S6.Ex1", "title": "‚àá ‚ãÖ f = ‚àë i = 1 D ‚àÇ i f i . \\nabla\\cdot f=\\sum_{i=1}^{D}\\partial_{i}f_{i}. ‚àá ‚ãÖ italic_f = ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT ‚àÇ star", "snippet": "‚àá ‚ãÖ f = ‚àë i = 1 D ‚àÇ i f i . \\nabla\\cdot f=\\sum_{i=1}^{D}\\partial_{i}f_{i}. ‚àá ‚ãÖ italic_f = ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT ‚àÇ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ."}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#S6.E2", "title": "ùîº ùíô t ‚Äã [ ‚Äñ ùíô t ‚àí f ‚Äã ( ùíô t ) ‚Äñ 2 2 + 2 ‚Äã t 2 ‚Äã ‚àá ‚ãÖ f ‚Äã ( ùíô t ) ] = ùîº ùíô t ‚Äã [ ‚Äñ ùíô t ‚àí f ‚Äã ( ùíô t ) ‚Äñ 2 2 ] ‚àí 2 ‚Äã t 2 ‚Äã ‚à´ ‚ü® ‚àá p ùíô t ‚Äã ( ùùÉ ) , f ‚Äã ( ùùÉ ) ‚ü© ‚Äã d ùùÉ . \\mathbb{E}_{\\bm{x}_{t}}\\left[\\left\\|\\bm{", "snippet": "ùîº ùíô t ‚Äã [ ‚Äñ ùíô t ‚àí f ‚Äã ( ùíô t ) ‚Äñ 2 2 + 2 ‚Äã t 2 ‚Äã ‚àá ‚ãÖ f ‚Äã ( ùíô t ) ] = ùîº ùíô t ‚Äã [ ‚Äñ ùíô t ‚àí f ‚Äã ( ùíô t ) ‚Äñ 2 2 ] ‚àí 2 ‚Äã t 2 ‚Äã ‚à´ ‚ü® ‚àá p ùíô t ‚Äã ( ùùÉ ) , f ‚Äã ( ùùÉ ) ‚ü© ‚Äã d ùùÉ . \\mathbb{E}_{\\bm{x}_{t}}\\left[\\left\\|\\bm{x}_{t}-f(\\bm{x}_{t})\\right\\|_{2}^{2}+2t^{2}\\nabla\\cdot f(\\bm{x}_{t})\\right]=\\mathbb{E}_{\\bm{x}_{t}}\\left[\\left\\|\\bm{x}_{t}-f(\\bm{x}_{t})\\right\\|_{2}^{2}\\right]-2t^{2}\\int\\left\\langle\\nabla p_{\\bm{x}_{t}}(\\bm{\\xi}),f(\\bm{\\xi})\\right\\rangle\\mathrm{d}\\bm{\\xi}. blackboard_E start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ ‚à• bold_italic_x start_POSTSU"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#A2.S3.EGx80", "title": "ùîº ‚Äã [ ùíô ‚à£ ùíô t = ùùÉ , ùíö = ùùÇ ] \\displaystyle\\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi},\\bm{y}=\\bm{\\nu}] blackboard_E [ bold_italic_x ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_it", "snippet": "ùîº ‚Äã [ ùíô ‚à£ ùíô t = ùùÉ , ùíö = ùùÇ ] \\displaystyle\\mathbb{E}[\\bm{x}\\mid\\bm{x}_{t}=\\bm{\\xi},\\bm{y}=\\bm{\\nu}] blackboard_E [ bold_italic_x ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = bold_italic_Œæ , bold_italic_y = bold_italic_ŒΩ ] = ùùÉ + t 2 ‚Äã ‚àá ùùÉ log ‚Å° p t ‚Äã ( ùùÉ ) + t 2 ‚Äã ‚àá ùùÉ log ‚Å° p ùíö ‚à£ ùíô t ‚Äã ( ùùÇ ‚à£ ùùÉ ) \\displaystyle=\\bm{\\xi}+t^{2}\\nabla_{\\bm{\\xi}}\\log p_{t}(\\bm{\\xi})+t^{2}\\nabla_{\\bm{\\xi}}\\log p_{\\bm{y}\\mid\\bm{x}_{t}}(\\bm{\\nu}\\mid\\bm{\\xi}) = bold_italic_Œæ + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ‚àá start_POSTSUBSCRIPT bold_italic_Œæ end_POSTSUBSCRIPT roman_log italic_p s"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#A2.S3.EGx81", "title": "t 2 ‚Äã ‚àá ùùÉ log ‚Å° p ùíö ‚à£ ùíô t ‚Äã ( ùùÇ ‚à£ ùùÉ ) \\displaystyle t^{2}\\nabla_{\\bm{\\xi}}\\log p_{\\bm{y}\\mid\\bm{x}_{t}}(\\bm{\\nu}\\mid\\bm{\\xi}) italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ‚àá start_POSTSUBSCRIPT", "snippet": "t 2 ‚Äã ‚àá ùùÉ log ‚Å° p ùíö ‚à£ ùíô t ‚Äã ( ùùÇ ‚à£ ùùÉ ) \\displaystyle t^{2}\\nabla_{\\bm{\\xi}}\\log p_{\\bm{y}\\mid\\bm{x}_{t}}(\\bm{\\nu}\\mid\\bm{\\xi}) italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ‚àá start_POSTSUBSCRIPT bold_italic_Œæ end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT bold_italic_y ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_ŒΩ ‚à£ bold_italic_Œæ ) = t 2 ‚Äã ‚àá ùùÉ [ ‚àí 1 2 ‚Äã ( ùùÇ ‚àí ùë® ‚Äã ùîº ‚Äã [ ùíô ‚à£ ùíô t = ùùÉ ] ) ‚ä§ ‚Äã ùö∫ ùíö ‚à£ ùíô t ‚àí 1 ‚Äã ( ùùÇ ‚àí ùë® ‚Äã ùîº ‚Äã [ ùíô ‚à£ ùíô t = ùùÉ ] ) ] \\displaystyle=t^{2}\\nabla_{\\bm{\\xi}}\\left[-\\frac{1}{2}(\\bm{\\nu}-\\bm{A}\\mathbb{E}[\\bm{x}\\mid"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#A2.S3.EGx82", "title": "ùö∫ 1 / 2 ‚Äã ( ùë∞ ‚àí ùö∫ 1 / 2 ‚Äã ( ùö∫ + t 2 ‚Äã ùë∞ ) ‚àí 1 ‚Äã ùö∫ 1 / 2 ) ‚Äã ùö∫ 1 / 2 \\displaystyle\\bm{\\Sigma}^{1/2}\\left(\\bm{I}-\\bm{\\Sigma}^{1/2}\\left(\\bm{\\Sigma}+t^{2}\\bm{I}\\right)^{-1}\\bm{\\Sigma}^{1/2}\\right)\\bm{\\Si", "snippet": "ùö∫ 1 / 2 ‚Äã ( ùë∞ ‚àí ùö∫ 1 / 2 ‚Äã ( ùö∫ + t 2 ‚Äã ùë∞ ) ‚àí 1 ‚Äã ùö∫ 1 / 2 ) ‚Äã ùö∫ 1 / 2 \\displaystyle\\bm{\\Sigma}^{1/2}\\left(\\bm{I}-\\bm{\\Sigma}^{1/2}\\left(\\bm{\\Sigma}+t^{2}\\bm{I}\\right)^{-1}\\bm{\\Sigma}^{1/2}\\right)\\bm{\\Sigma}^{1/2} bold_Œ£ start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT ( bold_italic_I - bold_Œ£ start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT ( bold_Œ£ + italic_t start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT bold_italic_I ) start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT bold_Œ£ start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT ) bold_Œ£ start_POSTSUPERSCRIPT 1 / 2 end_POSTSUPERSCRIPT = t 2 ‚Äã ùëΩ ‚Äã ùö≤ 1 / 2 ‚Äã ("}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#A2.S3.EGx83", "title": "Œ≥ ‚Äã œÉ t 2 Œ± t ‚Äã ‚àá ùùÉ log ‚Å° p ùíö ‚à£ ùíô t ‚Äã ( ùùÇ ‚à£ ùùÉ ) \\displaystyle\\gamma\\frac{\\sigma_{t}^{2}}{\\alpha_{t}}\\nabla_{\\bm{\\xi}}\\log p_{\\bm{y}\\mid\\bm{x}_{t}}(\\bm{\\nu}\\mid\\bm{\\xi}) italic_Œ≥ divide start_ARG itali", "snippet": "Œ≥ ‚Äã œÉ t 2 Œ± t ‚Äã ‚àá ùùÉ log ‚Å° p ùíö ‚à£ ùíô t ‚Äã ( ùùÇ ‚à£ ùùÉ ) \\displaystyle\\gamma\\frac{\\sigma_{t}^{2}}{\\alpha_{t}}\\nabla_{\\bm{\\xi}}\\log p_{\\bm{y}\\mid\\bm{x}_{t}}(\\bm{\\nu}\\mid\\bm{\\xi}) italic_Œ≥ divide start_ARG italic_œÉ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_Œ± start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG ‚àá start_POSTSUBSCRIPT bold_italic_Œæ end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT bold_italic_y ‚à£ bold_italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_ŒΩ ‚à£ bold_italic_Œæ ) ="}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#A2.S3.EGx84", "title": "ùíô ¬Ø Œ∏ CG , ideal ‚Äã ( t , ùùÉ , ŒΩ ) \\displaystyle\\bar{\\bm{x}}_{\\theta}^{\\mathrm{CG,\\,ideal}}(t,\\bm{\\xi},\\nu) over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POS", "snippet": "ùíô ¬Ø Œ∏ CG , ideal ‚Äã ( t , ùùÉ , ŒΩ ) \\displaystyle\\bar{\\bm{x}}_{\\theta}^{\\mathrm{CG,\\,ideal}}(t,\\bm{\\xi},\\nu) over¬Ø start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_CG , roman_ideal end_POSTSUPERSCRIPT ( italic_t , bold_italic_Œæ , italic_ŒΩ ) = 1 Œ± t ‚Äã ùùÉ + ( 1 ‚àí Œ≥ ) ‚Äã œÉ t 2 Œ± t ‚Äã ‚àá ùùÉ log ‚Å° p ùíô t ‚Äã ( ùùÉ ) + Œ≥ ‚Äã œÉ t 2 Œ± t ‚Äã ‚àá ùùÉ log ‚Å° p ùíô t ‚à£ y ‚Äã ( ùùÉ ‚à£ ŒΩ ) \\displaystyle=\\frac{1}{\\alpha_{t}}\\bm{\\xi}+(1-\\gamma)\\frac{\\sigma_{t}^{2}}{\\alpha_{t}}\\nabla_{\\bm{\\xi}}\\log p_{\\bm{x}_{t}}(\\bm{\\xi})+\\gamma\\frac{\\sigma_{t}^{2}}{\\alpha_{t}}\\nabla_{\\bm{\\xi}}\\lo"}, {"page": "Chapter 6 Inference with Low-Dimensional Distributions", "href": "Ch6.html#A2.S3.EGx85", "title": "‚àë k = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ùíô t ‚ä§ ‚Äã ùëº k ‚Äã ùëº k ‚ä§ ‚Äã ùíó y ) ‚àë i = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ùíô t ‚ä§ ‚Äã ùëº i ‚Äã ùëº i ‚ä§ ‚Äã ùíó y ) ‚Äã ùëº k ‚Äã ùëº k ‚ä§ ‚Äã ùíô t = exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ", "snippet": "‚àë k = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ùíô t ‚ä§ ‚Äã ùëº k ‚Äã ùëº k ‚ä§ ‚Äã ùíó y ) ‚àë i = 1 K exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ùíô t ‚ä§ ‚Äã ùëº i ‚Äã ùëº i ‚ä§ ‚Äã ùíó y ) ‚Äã ùëº k ‚Äã ùëº k ‚ä§ ‚Äã ùíô t = exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ùíô t ‚ä§ ‚Äã ùíó y ) exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ùíô t ‚ä§ ‚Äã ùíó y ) + K ‚àí 1 ‚Äã ùëº y ‚Äã ùëº y ‚ä§ ‚Äã ùíô t + ‚àë k ‚â† y 1 exp ‚Å° ( 1 2 ‚Äã t 2 ‚Äã ( 1 + t 2 ) ‚Äã ùíô t ‚ä§ ‚Äã ùíó y ) + K ‚àí 1 ‚Äã ùëº k ‚Äã ùëº k ‚ä§ ‚Äã ùíô t . \\displaystyle\\begin{split}\\sum_{k=1}^{K}\\frac{\\exp\\left(\\frac{1}{2t^{2}(1+t^{2})}\\bm{x}_{t}^{\\top}\\bm{U}_{k}\\bm{U}_{k}^{\\top}\\bm{v}_{y}\\right)}{\\sum_{i=1}^{K}\\exp\\left(\\frac{1}{2t^{2}(1+t^{2})}\\bm{x}_{t}^{\\top}\\bm{U}_{i}"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#top", "title": "Chapter 7 Learning Representations for Real-World Data", "snippet": ""}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S1", "title": "7.1 Technical Setup and Outline of the Chapter", "snippet": "7.1 Technical Setup and Outline of the Chapter In previous chapters, we alluded to different setups in which we used representation-learning techniques to process real data at scale. In this chapter, we will describe such setups in great detail. The objective of this section is to get you, the reader, to be able to reproduce any experiment discussed in this section (or indeed the book) using just the description we will give in the book, the principles introduced in previous chapters and expanded on in this chapter, and hyperparameters taken from a smattering of papers whose results are discus"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2", "title": "7.2 Simplified Contrastive Learning", "snippet": "7.2 Simplified Contrastive Learning Learning high-quality and faithful representations of data is a fundamental problem in deep learning, known as self-supervised learning . There have been many approaches proposed for this task, many of which do not evidently use the techniques and principles outlined in this manuscript. One such approach is called contrastive learning , so named because the learning objective is (roughly speaking) about ensuring that features of ‚Äúsimilar‚Äù data are similar, and features of ‚Äúdis-similar‚Äù data are far apart. Contrastive learning solutions are often highly-engin"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3", "title": "7.3 Image Classification", "snippet": "7.3 Image Classification In the previous section, we simplified an overly complex learning objective using our intuition of representation learning through the lens of compression. However, many of the most popular learning procedures are incredibly simple. In these cases, it is difficult to further simplify the objective. Thus, in this and future sections, we will focus on principled ways to modify the deep network architectures for a variety of tasks. Let us first start with arguably the most classical task in machine learning: image classification , which is often used as a standard task to"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4", "title": "7.4 Causal Language Modeling", "snippet": "7.4 Causal Language Modeling We now study causal language modeling , a method for training large language models (LLMs). This is the same setup used to train, among many others, GPT-2 and many more language models. 7.4.1 Data The data we will use to investigate the performance of CRATE for language tasks will be OpenWebText (OWT) [ GC19 ] , an open-source reproduction of the unreleased WebText dataset used by OpenAI to train GPT2. Each sample in OWT is a web document, typically sourced from high-quality web pages, blogs, articles, or online discussions, that is written in well-formed natural l"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S5", "title": "7.5 Scaling White-Box Transformers", "snippet": "7.5 Scaling White-Box Transformers In this section, we will discuss three ways in which various parts of CRATE-type models can be scaled up or made more efficient while still remaining white-box. These developments mix both conceptual and empirical insights, and can be viewed as case studies about how to use white-box understanding to improve deep learning models in practice. The tasks that we use to evaluate the methods will be image classification and next-token-prediction, the data will be ImageNet and OpenWebText respectively, the optimization procedure will be the same backpropagation, an"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6", "title": "7.6 Masked Autoencoding for Imagery Data", "snippet": "7.6 Masked Autoencoding for Imagery Data The second application we discuss is nonlinear image completion , also known as masked autoencoding (MAE), which is a direct generalization of the low-rank matrix completion problem discussed in Chapter 2 . Masked autoencoding, since its introduction in the deep learning context by [ HCX+22 ] has been a staple and simple self-supervised representation learning method, which aims to endow each patch feature within ùíÅ Œ∏ \\bm{Z}_{\\theta} bold_italic_Z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT with aggregate information as well as information about its n"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S7", "title": "7.7 Summary and Notes", "snippet": "7.7 Summary and Notes All work in this chapter is downstream of the Transformer architecture, which was introduced by [ VSP+17 ] . The Transformer architecture is formally described in Section 7.2 . A main empirical innovation in recent years, spurred by the prevalence and performance of the transformer architecture, is to formulate a given learning problem as a sequence-to-sequence problem and apply the transformer architecture. This has enabled the transformer architecture to be essentially ubiquitous in (almost) all deep learning applications. As such, direct improvements to the transformer"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S8", "title": "7.8 Exercises and Extensions", "snippet": "7.8 Exercises and Extensions Exercise 7.1 . Read the DINO paper [ CTM+21 ] . Exercise 7.2 . DINO v2 [ ODM+23 ] uses everything from DINO v1 but also, during the data augmentation phase, randomly masks out patches within each view. This kind of augmentation should enforce that the features of images with similar local information are similar. Formulate an optimization problem which promotes this in the encoder, and implement it. Exercise 7.3 . This exercise considers the implementation of stochastic optimization algorithms to minimize losses involving expectations. (a) Propose an alternative to"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS1", "title": "7.2.1 Data", "snippet": "7.2.1 Data The data that we will use to explore and simplify the DINO methodology is all 2-dimensional image data. For training , we will use the ImageNet-1K and ImageNet-21K datasets. Each sample in the dataset is an RGB image, of varying resolution, and a label indicating the object or scene that the image contains (i.e., the class of the image). The ImageNet-1K dataset contains 1.28M training images and 50K validation images partitioned into 1K classes. The ImageNet-21K dataset contains 14.2M training images and 21.8K classes, but the classes are not disjoint (i.e., some classes are subsets"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS2", "title": "7.2.2 Task and Objective Function", "snippet": "7.2.2 Task and Objective Function Our task is to learn a good representation of the data. Contrastive learning, by and large, does this by defining what properties of the input image we wish the features to reflect, construct images which share these properties but vary others, and set up a loss which promotes that the features of images with shared properties are close and images with different properties are different. The naturally optimal solution to this learning problem is that the learned features preserve the desired properties of the input. However, there are many practical and empiri"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS3", "title": "7.2.3 Architecture: Vision Transformer", "snippet": "7.2.3 Architecture: Vision Transformer For the architecture, we use a standard vision transformer. Here is how such an architecture works, formally, in the context of image data. Recall from Section 7.1 that there are four components to an encoder architecture, namely an embedding, a backbone, a feature extractor, and a task-specific head. We discuss these three parts presently. Figure 7.5 : An example of an image turned into 5 √ó 5 5\\times 5 5 √ó 5 square patches, which are placed in raster order. Each patch is of the same size, and the grid of patches is of shape ( N H , N W ) = ( 5 , 5 ) (N_{"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS4", "title": "7.2.4 Optimization Strategy", "snippet": "7.2.4 Optimization Strategy Figure 7.8 : The DINO pipeline. Student features and teacher features are computed for each input. The objective attempts to align the student features with the teacher features by projecting both sets of features into a high-dimensional probability simplex and computing a cross-entropy loss. Notably, because of the ‚Äústop-grad‚Äù, the gradient is only computed w.r.t. the student parameters‚Äô outputs . Optimizing DINO. We have a loss function and an architecture, so we now discuss the optimization strategy. The optimization strategy for DINO uses two sets of weights for"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS5", "title": "7.2.5 Evaluation Methodology", "snippet": "7.2.5 Evaluation Methodology There are several ways to evaluate a trained transformer model. We highlight two in this section. Let us define the center crop view v cc : ‚Ñê ‚Üí ‚Ñê v_{\\mathrm{cc}}\\colon\\mathcal{I}\\to\\mathcal{I} italic_v start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT : caligraphic_I ‚Üí caligraphic_I which is a a deterministic resized crop : ‚Ä¢ it resizes the image so that the shortest edge is of size S rsz S_{\\mathrm{rsz}} italic_S start_POSTSUBSCRIPT roman_rsz end_POSTSUBSCRIPT (similar to random resized crops with area percentage parameter 1 1 1 ); ‚Ä¢ then it takes the center S cc √ó S"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS6", "title": "7.2.6 Experimental Setup and Results", "snippet": "7.2.6 Experimental Setup and Results Since SimDINO is directly built upon DINO, we compare the optimal settings for DINO as given by their original paper [ CTM+21 ] with the same settings applied to SimDINO for fair comparison. Objective function. We use 10 10 10 local views (i.e., M loc = 10 M_{\\mathrm{loc}}=10 italic_M start_POSTSUBSCRIPT roman_loc end_POSTSUBSCRIPT = 10 ) of resolution 96 √ó 96 96\\times 96 96 √ó 96 (i.e., S loc = 96 S_{\\mathrm{loc}}=96 italic_S start_POSTSUBSCRIPT roman_loc end_POSTSUBSCRIPT = 96 ) and 2 2 2 global views (i.e., M glo = 2 M_{\\mathrm{glo}}=2 italic_M start_POST"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.SS1", "title": "7.3.1 Task and Objective", "snippet": "7.3.1 Task and Objective Unlike before, our task is not just to learn a good representation of the data, but also simultaneously build a classifier. Formally, we have labeled data pairs ( ùëø , ùíö ) (\\bm{X},\\bm{y}) ( bold_italic_X , bold_italic_y ) , where ùíö ‚àà { 0 , 1 } N cls \\bm{y}\\in\\{0,1\\}^{N_{\\mathrm{cls}}} bold_italic_y ‚àà { 0 , 1 } start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT roman_cls end_POSTSUBSCRIPT end_POSTSUPERSCRIPT is a one-hot vector denoting the class membership of ùëø \\bm{X} bold_italic_X . We consider a deterministic center crop view v cc v_{\\mathrm{cc}} italic_v start_POSTSU"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.SS2", "title": "7.3.2 The CRATE Architecture", "snippet": "7.3.2 The CRATE Architecture The architecture that we use is the CRATE architecture, described in some level of detail in Chapter 4 . The overall setup is similar to that of the regular transformer in Section 7.2.3 , with a few changes. While the embedding step is the same as both DINO and SimDINO in Section 7.2.3 , the feature extraction step is the same as SimDINO in Section 7.2.3 as it just extracts the feature corresponding to the class token, and the classification head is described in Section 7.3.1 , the backbone architecture is different. Each layer takes the form ùíÅ Œ∏ ‚Ñì + 1 / 2 ‚Äã ( ùëø ) "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.SS3", "title": "7.3.3 Optimization", "snippet": "7.3.3 Optimization We train our classifier using a simple end-to-end stochastic optimization procedure, where we sub-sample data and views, compute the average loss and its gradient over these samples, and use an optimization algorithm to change the parameters. At each timestep k k italic_k , we: ‚Ä¢ Subsample B B italic_B different labeled samples { ( ùëø b ( k ) , ùíö b ( k ) ) } b = 1 B ‚äÜ ‚Ñê √ó { 0 , 1 } N cls \\{(\\bm{X}_{b}^{(k)},\\bm{y}_{b}^{(k)})\\}_{b=1}^{B}\\subseteq\\mathcal{I}\\times\\{0,1\\}^{N_{\\mathrm{cls}}} { ( bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.SS4", "title": "7.3.4 Evaluation Methodology", "snippet": "7.3.4 Evaluation Methodology We use the same evaluation procedure as Section 7.2.5 . To summarize, for all evaluations (as well as training) we use a center crop view v cc v_{\\mathrm{cc}} italic_v start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT which reshapes the input image and takes a large central crop of size ( C , S cc , S cc ) (C,S_{\\mathrm{cc}},S_{\\mathrm{cc}}) ( italic_C , italic_S start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT ) where C C italic_C is the number of channels in the input image. We can then do linear probing, atten"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.SS5", "title": "7.3.5 Experimental Setup and Results", "snippet": "7.3.5 Experimental Setup and Results Since CRATE is directly based on the transformer, we compare the optimal settings for ViT as given by [ DBK+21 , TCD+20 ] with the same settings applied to CRATE for fair comparison. Model architecture. The center crop resizes the whole image so that the shorter edge is of size 256 256 256 (i.e., S rsz = 256 S_{\\mathrm{rsz}}=256 italic_S start_POSTSUBSCRIPT roman_rsz end_POSTSUBSCRIPT = 256 ) before taking a center crop of size 224 √ó 224 224\\times 224 224 √ó 224 (i.e., S cc = 224 S_{\\mathrm{cc}}=224 italic_S start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT = 2"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS1", "title": "7.4.1 Data", "snippet": "7.4.1 Data The data we will use to investigate the performance of CRATE for language tasks will be OpenWebText (OWT) [ GC19 ] , an open-source reproduction of the unreleased WebText dataset used by OpenAI to train GPT2. Each sample in OWT is a web document, typically sourced from high-quality web pages, blogs, articles, or online discussions, that is written in well-formed natural language. The OpenWebText dataset contains around 8.01M documents of varying lengths, totaling around 41.70GB of text. For evaluation, we will use several datasets, such as WikiText [ MXB+16 ] 3 3 3 For WikiText2 and"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS2", "title": "7.4.2 Task and Objective", "snippet": "7.4.2 Task and Objective For causal language modeling pre-training, the idea is that we want to train the model to output human-like text . The most popular way to do this by far is to use a two-stage training process: 5 5 5 Modern language model training has several additional training steps which demand different data distributions and algorithm approaches. However, training a model to merely mimic human writing only requires these few presented steps. ‚Ä¢ First , we wish to learn a way to optimally encode documents as a sequence of basic (‚Äúbuilding block‚Äù) strings, called tokens . This is cal"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS3", "title": "7.4.3 Architecture: Causal CRATE", "snippet": "7.4.3 Architecture: Causal CRATE For the architecture, we use a standard GPT-2 style transformer, substituting CRATE layers in for the transformer layers. 9 9 9 In direct contravention of the conventions in this book and those of many other communities, the NLP community calls such GPT-2 style transformers (encompassing nearly all current LLMs) as ‚Äúdecoder-only‚Äù transformers. ‚ÄúEncoder-only‚Äù transformers have a different architecture, and ‚Äúencoder-decoder‚Äù transformers concatenate an ‚Äúencoder-only‚Äù transformer with a ‚Äúdecoder-only‚Äù transformer. This despite the fact that ‚Äúdecoder-only‚Äù transfor"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS4", "title": "7.4.4 Optimization Strategy", "snippet": "7.4.4 Optimization Strategy We train our language model using end-to-end stochastic optimization. One remaining issue is that, in practice, different documents in a batch have different lengths (in terms of the number of tokens required for each sequence), but as of the time of writing this book, the main deep learning frameworks by-and-large allow only ‚Äúrectangular‚Äù tensors, which do not accommodate this behavior. To try to resolve this issue, we just insert a special padding token <|pad|> for all shorter samples in the batch, so that we can batch process everything using rectangular tensors."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS5", "title": "7.4.5 Evaluation Methodology", "snippet": "7.4.5 Evaluation Methodology There are several ways to evaluate a trained transformer language model. ‚Ä¢ On a holdout dataset of arbitrary text, we can evaluate ‚Ñí CLM \\mathcal{L}_{\\mathrm{CLM}} caligraphic_L start_POSTSUBSCRIPT roman_CLM end_POSTSUBSCRIPT on it; lower losses are better since they mean the model‚Äôs sampling yields better performance. ‚Ä¢ On a multiple choice question dataset, for each question we can put it as the context and check the estimated probability of the correct answer being generated. ‚Ä¢ We can also test the text generation capabilities. Namely, we can repeatedly sample f"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS6", "title": "7.4.6 Experimental Setup and Results", "snippet": "7.4.6 Experimental Setup and Results Since our causal CRATE architecture is directly built upon GPT-2, we compare the optimal settings for GPT-2 as given by the NanoGPT repository [ Kar22 ] with the same settings applied to CRATE for fair comparison. Model architecture. We use the GPT-2 tokenizer, which has vocabulary size V = 50257 V=50257 italic_V = 50257 , including a special token for <|pad|> . 13 13 13 The <|bos|> token is not included in this setup, although it is very common in modern language models. The context length is N max = 1024 N_{\\max}=1024 italic_N start_POSTSUBSCRIPT roman_ma"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S5.SS1", "title": "7.5.1 Increasing Network Width: CRATE- Œ± \\alpha italic_Œ±", "snippet": "7.5.1 Increasing Network Width: CRATE- Œ± \\alpha italic_Œ± Figure 7.14 : One layer of the CRATE- Œ± \\alpha italic_Œ± backbone. The difference from CRATE is that the ISTA Œ∏ ‚Ñì \\operatorname{ISTA}_{\\theta}^{\\ell} roman_ISTA start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT block is replaced by the ODL Œ∏ ‚Ñì \\operatorname{ODL}_{\\theta}^{\\ell} roman_ODL start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT block, which performs several ISTA \\operatorname{ISTA} roman_ISTA steps with an overcomplete dictionary. One de"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S5.SS2", "title": "7.5.2 Linear Time Complexity Transformers", "snippet": "7.5.2 Linear Time Complexity Transformers In practice, deep learning models suffer bottlenecks to space and time complexity, representing problem sizes which they cannot scale beyond given fixed resources. One such bottleneck, particularly meaningful when dealing with data where each sample is itself high-dimensional and rich (such as long streams of text or videos), is the time complexity of processing long sequences of data. In order to alleviate the time-complexity of processing data using transformers, in Section 4.3.2 we proposed a token statistics self-attention operator TSSA Œ∏ ‚Ñì \\operat"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S5.SS3", "title": "7.5.3 Attention-Only Transformers", "snippet": "7.5.3 Attention-Only Transformers Another bottleneck to remove from deep learning models, specifically transformer-like architectures, is the memory bottleneck which comes from massive matrix multiplications in MLPs, where the internal dimension is far greater than the feature dimension d d italic_d . It thus is an interesting and important question to ask: do we really need the MLP inside a transformer, and how good can the performance get without it? To explore this question, we use the attention-only-transformer (AoT) architecture (see Section 4.3.1 ), depicted in Figure 7.17 . Namely, each"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.SS1", "title": "7.6.1 Task and Objective", "snippet": "7.6.1 Task and Objective As the name suggests, masked autoencoding involves a view v m v_{m} italic_v start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT which, given an input, performs a random resized crop (cf Section 7.2.2 ) to turn the input image into a square image of size ( C , S mask , S mask ) (C,S_{\\mathrm{mask}},S_{\\mathrm{mask}}) ( italic_C , italic_S start_POSTSUBSCRIPT roman_mask end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT roman_mask end_POSTSUBSCRIPT ) , then masks (i.e., sets to zero) a fixed percentage p mask ‚àà [ 0 , 1 ] p_{\\mathrm{mask}}\\in[0,1] italic_p start_POSTSUBSCRIPT r"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.SS2", "title": "7.6.2 Architecture", "snippet": "7.6.2 Architecture Figure 7.19 : One layer of the encoder and decoder in a CRATE autoencoder backbone. The encoder and decoder layers both feed their inputs through multi-head subspace self-attention and a dictionary learning or dictionary encoding step. Note that the encoder and decoder layers are symmetrically designed; the conceptual goal of each decoder layer is to invert an encoder layer, so this symmetry is very much by design (see e.g., Chapter 5 ). We use a CRATE encoder and decoder, depicted in Figure 7.7 , though of course it is possible to use a regular transformer encoder and decod"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.SS3", "title": "7.6.3 Optimization", "snippet": "7.6.3 Optimization As in Section 7.3.3 , we use a simple optimization setup: we sample images and masks, compute the loss on those samples and the gradients of this loss, and update the parameters using a generic optimization algorithm and the aforementioned gradients. For each timestep k k italic_k , we: ‚Ä¢ Subsample B B italic_B different samples { ùëø b ( k ) } b = 1 B ‚äÜ ‚Ñê \\{\\bm{X}_{b}^{(k)}\\}_{b=1}^{B}\\subseteq\\mathcal{I} { bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_b = 1 end_POSTSUBSCRIPT st"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.SS4", "title": "7.6.4 Evaluation", "snippet": "7.6.4 Evaluation This is the first autoencoder network we discuss in this chapter. We use the same center crop view v cc v_{\\mathrm{cc}} italic_v start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT as in Sections 7.2.5 and 7.3.4 , resizing the final image to a square with side length S cc = S mask S_{\\mathrm{cc}}=S_{\\mathrm{mask}} italic_S start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT = italic_S start_POSTSUBSCRIPT roman_mask end_POSTSUBSCRIPT pixels so as to match the shapes of the input images seen during training. On top of evaluating the masked autoencoding loss itself, it is also possible to "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.SS5", "title": "7.6.5 Experiments", "snippet": "7.6.5 Experiments Since CRATE-MAE is directly based on the ViT-MAE, we compare the optimal settings for ViT-MAE as given by [ HCX+22 ] with the same settings applied to CRATE-MAE for fair comparison. Model architecture. During training, the masked crop v m v_{m} italic_v start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT resizes the whole image so that the shorter edge is of size 256 256 256 (i.e., S rsz = 256 S_{\\mathrm{rsz}}=256 italic_S start_POSTSUBSCRIPT roman_rsz end_POSTSUBSCRIPT = 256 ) before taking a random crop of size 224 √ó 224 224\\times 224 224 √ó 224 (i.e., S mask = 224 S_{\\mathrm{mas"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS3.SSS0.Px1", "title": "Embedding.", "snippet": "Embedding. Given imagery data ùëø ‚àà ‚Ñê \\bm{X}\\in\\mathcal{I} bold_italic_X ‚àà caligraphic_I , we embed it as a sequence of tokens in ‚Ñù d \\mathbb{R}^{d} blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT using the map f Œ∏ emb f_{\\theta}^{\\mathrm{emb}} italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT , as follows. The first two steps are depicted in Figure 7.5 , and the latter two are depicted in Figure 7.6 . 1. First, we turn the image data ùëø \\bm{X} bold_italic_X into a sequence of patches of shape ( C , P H , P W ) (C,P_{H},P_{"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS3.SSS0.Px2", "title": "Backbone.", "snippet": "Backbone. Given a sequence of embeddings ùíÅ Œ∏ 1 ‚Äã ( ùëø ) ‚âê f Œ∏ emb ‚Äã ( ùëø ) ‚àà ( ‚Ñù d ) ‚àó \\bm{Z}_{\\theta}^{1}(\\bm{X})\\doteq f_{\\theta}^{\\mathrm{emb}}(\\bm{X})\\in(\\mathbb{R}^{d})^{*} bold_italic_Z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( bold_italic_X ) ‚âê italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT ( bold_italic_X ) ‚àà ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT , we process it using the backbone map f Œ∏ bb f_{\\theta}^{\\"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS3.SSS0.Px3", "title": "Feature extractor.", "snippet": "Feature extractor. We use a post-processing step f Œ∏ ext f_{\\theta}^{\\mathrm{ext}} italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT which extracts the class token feature , which (recall) is the feature meant to contain aggregate information about the input image, and applies an MLP and normalization to it. Namely, we have ùíõ Œ∏ ‚Äã ( ùëø ) ‚âê f Œ∏ ext ‚Äã ( ùíÅ Œ∏ ‚Äã ( ùëø ) ) = f Œ∏ ext ‚Äã ( [ ùíõ Œ∏ 1 ‚Äã ( ùëø ) , ‚Ä¶ , ùíõ Œ∏ n ‚Äã ( ùëø ) ] ) ‚âê MLP Œ∏ ext ‚Å° ( ùíõ Œ∏ 1 ‚Äã ( ùëø ) ) ‚Äñ MLP Œ∏ ext ‚Å° ( ùíõ Œ∏ 1 ‚Äã ( ùëø ) ) ‚Äñ 2 . \\bm{z}_{\\theta}(\\bm{X})\\doteq f_{\\theta}^{\\mathrm{ex"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS3.SSS0.Px4", "title": "Task-specific (‚ÄúDINO‚Äù) head.", "snippet": "Task-specific (‚ÄúDINO‚Äù) head. For DINO, we use the task-specific DINO head h ùëæ , ùùÅ h_{\\bm{W},\\bm{\\mu}} italic_h start_POSTSUBSCRIPT bold_italic_W , bold_italic_Œº end_POSTSUBSCRIPT . For SimDINO, we use no task-specific head at all , as previously described."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS4.SSS0.Px1", "title": "Optimizing DINO.", "snippet": "Optimizing DINO. We have a loss function and an architecture, so we now discuss the optimization strategy. The optimization strategy for DINO uses two sets of weights for the same architecture : student weights Œ∏ s \\theta_{\\mathrm{s}} italic_Œ∏ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT and teacher weights Œ∏ t \\theta_{\\mathrm{t}} italic_Œ∏ start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT . These correspond to two different neural networks, called the teacher network and student network, with the same architecture. The teacher network encodes all global views, while the student network encodes al"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS4.SSS0.Px2", "title": "Optimizing SimDINO.", "snippet": "Optimizing SimDINO. The simplified DINO population-level objective is very similar in spirit but much simpler in execution, i.e., ‚Ñí SimDINO ‚àí st ( Œ∏ s , Œ∏ t ) ‚âê ùîº [ d ‚Ñì 2 ( ùíõ Œ∏ t ( ùëø g ) , ùíõ Œ∏ s ( ùëø c ) ) ] ‚àí Œ≥ 2 log det ( ùë∞ + d Œµ 2 Cov ( ùíõ Œ∏ s ( ùëø g ) ) ) ) . \\mathcal{L}_{\\mathrm{SimDINO}-\\mathrm{s}\\mathrm{t}}(\\theta_{\\mathrm{s}},\\theta_{\\mathrm{t}})\\doteq\\operatorname{\\mathbb{E}}\\left[d_{\\ell^{2}}(\\bm{z}_{\\theta_{\\mathrm{t}}}(\\bm{X}_{g}),\\bm{z}_{\\theta_{\\mathrm{s}}}(\\bm{X}_{c}))\\right]-\\frac{\\gamma}{2}\\log\\det\\left(\\bm{I}+\\frac{d}{\\varepsilon^{2}}\\operatorname{Cov}(\\bm{z}_{\\theta_{\\mathrm{s}"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS5.SSS0.Px1", "title": "Linear probing.", "snippet": "Linear probing. The first, and most architecture-agnostic, way to evaluate an encoder model ùëø ‚Ü¶ ùíõ Œ∏ ‚Äã ( ùëø ) \\bm{X}\\mapsto\\bm{z}_{\\theta}(\\bm{X}) bold_italic_X ‚Ü¶ bold_italic_z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( bold_italic_X ) is to employ linear probing . Linear probing is, in a sentence, running logistic regression on the aggregate features computed by the encoder. This tells us how much semantic information exists in the representations, as well as how easily extractable this information is. (That is: to what extent do the features of images with different semantics live on dif"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS5.SSS0.Px2", "title": "k k italic_k -nearest neighbors.", "snippet": "k k italic_k -nearest neighbors. We can also evaluate the performance of the features on classification tasks without needing to explicitly train a classifier by using the k k italic_k -nearest neighbor algorithm to get an average predicted label. Namely, given a dataset { ùíõ b } b = 1 B ‚äÜ ‚Ñù d \\{\\bm{z}_{b}\\}_{b=1}^{B}\\subseteq\\mathbb{R}^{d} { bold_italic_z start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_b = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT ‚äÜ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , define the k k italic_k"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS5.SSS0.Px3", "title": "Fidelity of the attention maps.", "snippet": "Fidelity of the attention maps. Another way to check the performance of the representations, for a transformer-based encoder, is to examine the fidelity of the attention maps ùë® L , k ‚àà ‚Ñù n √ó n \\bm{A}^{L,k}\\in\\mathbb{R}^{n\\times n} bold_italic_A start_POSTSUPERSCRIPT italic_L , italic_k end_POSTSUPERSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT italic_n √ó italic_n end_POSTSUPERSCRIPT as defined in Equation 7.2.19 , at the last layer L L italic_L , and given by the following pipeline: ùëø ‚Ü¶ ‚ãØ ‚Ü¶ ùíÅ L ‚àí 1 = [ ùíõ 1 L ‚àí 1 ‚èü class token , ùíõ 2 L ‚àí 1 ‚Äã ‚Ä¶ , ùíõ n L ‚àí 1 ‚èü patch tokens ] ‚Ü¶ ùë® k , L = [ ùë® 1 , 1 k ,"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS5.SSS0.Px4", "title": "Object detection and segmentation.", "snippet": "Object detection and segmentation. We can evaluate how the representations capture the fine-grained (i.e., smaller or more detailed) properties of the input by using them for semantic segmentation . Roughly, this means that we use the features to construct bounding boxes for all objects in the input. There are several ways to do this, and several ways to score the resulting bounding boxes compared to a ground-truth. Each combination of methods corresponds to a particular segmentation metric. We do not formally describe them here as they are not particularly insightful, but the DINO paper [ CTM"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS6.SSS0.Px1", "title": "Objective function.", "snippet": "Objective function. We use 10 10 10 local views (i.e., M loc = 10 M_{\\mathrm{loc}}=10 italic_M start_POSTSUBSCRIPT roman_loc end_POSTSUBSCRIPT = 10 ) of resolution 96 √ó 96 96\\times 96 96 √ó 96 (i.e., S loc = 96 S_{\\mathrm{loc}}=96 italic_S start_POSTSUBSCRIPT roman_loc end_POSTSUBSCRIPT = 96 ) and 2 2 2 global views (i.e., M glo = 2 M_{\\mathrm{glo}}=2 italic_M start_POSTSUBSCRIPT roman_glo end_POSTSUBSCRIPT = 2 ) of resolution 224 √ó 224 224\\times 224 224 √ó 224 (i.e., S glo = 224 S_{\\mathrm{glo}}=224 italic_S start_POSTSUBSCRIPT roman_glo end_POSTSUBSCRIPT = 224 ) for all experiments. The corres"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS6.SSS0.Px2", "title": "Model architecture.", "snippet": "Model architecture. For all inputs we set the patch size to be 16 √ó 16 16\\times 16 16 √ó 16 (namely, P H = P W = 16 P_{H}=P_{W}=16 italic_P start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT = italic_P start_POSTSUBSCRIPT italic_W end_POSTSUBSCRIPT = 16 ). We use the small, base, and large models of the ViT [ DBK+21 ] architecture as the embedding and backbone. The feature extractor is a three-layer MLP with a hidden size of 2048 2048 2048 and an output dimension of 256 256 256 , followed by an ‚Ñì 2 \\ell^{2} roman_‚Ñì start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT -normalization, as specified in Section "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS6.SSS0.Px3", "title": "Datasets and optimization.", "snippet": "Datasets and optimization. For pre-training, both our DINO reproduction and SimDINO use the ImageNet-1K dataset across all methods. We use AdamW [ LH17 ] as the optimizer, which is a very standard choice. We follow the following hyperparameter recommendations: ‚Ä¢ The batch size is B = 1024 B=1024 italic_B = 1024 . ‚Ä¢ The learning rate (for AdamW and the student model) has ‚Äúbase‚Äù value 2 √ó 10 ‚àí 3 2\\times 10^{-3} 2 √ó 10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT . In the first 10 10 10 epochs the learning rate linearly increases from 0 to the base value (i.e., at the i th i^{\\mathrm{th}} italic"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS6.SSS0.Px4", "title": "Evaluation results.", "snippet": "Evaluation results. In terms of downsream classification performance, we obtain the performance in Table 7.1 . We observe that the performance of SimDINO is much higher than in DINO under fair comparison. Also, it is much more stable: the prescribed settings of DINO cannot train a ViT-L(arge) model. On the other hand, Figure 7.10 shows visualizations of the average saliency maps in DINO and our simplified DINO, observing that the saliency maps look quite similar across models, indicating that the models learn features which are at least as good at capturing fine-grained details. The segmentati"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.SS5.SSS0.Px1", "title": "Model architecture.", "snippet": "Model architecture. The center crop resizes the whole image so that the shorter edge is of size 256 256 256 (i.e., S rsz = 256 S_{\\mathrm{rsz}}=256 italic_S start_POSTSUBSCRIPT roman_rsz end_POSTSUBSCRIPT = 256 ) before taking a center crop of size 224 √ó 224 224\\times 224 224 √ó 224 (i.e., S cc = 224 S_{\\mathrm{cc}}=224 italic_S start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT = 224 ), both in evaluation and training. We take patch size 16 16 16 (i.e., P H = P W = 16 P_{H}=P_{W}=16 italic_P start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT = italic_P start_POSTSUBSCRIPT italic_W end_POSTSUBSCRIPT = "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.SS5.SSS0.Px2", "title": "Datasets and optimization.", "snippet": "Datasets and optimization. For pre-training, we use the ImageNet-1K dataset. We use the LION optimizer [ CLH+24 ] to pre-train both our ViT replication as well as CRATE. We set the base learning rate as 2.4 √ó 10 ‚àí 4 2.4\\times 10^{-4} 2.4 √ó 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT , the weight decay as 0.5 0.5 0.5 , and batch size as B = 2048 B=2048 italic_B = 2048 . Our learning rate schedule increases the learning rate linearly to the base learning rate over the first 5 5 5 epochs, and decreases to 0 using a cosine schedule over the next 145 145 145 epochs (training all models for 150"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.SS5.SSS0.Px3", "title": "Experiment results.", "snippet": "Experiment results. Table 7.3 demonstrates that CRATE models achieve parity or improvement compared to the popular Vision Transformer (ViT) architecture at similar parameter counts, at least in terms of the linear separability of their features w.r.t. different classes. In terms of attention map fidelity, Figure 7.11 demonstrates a truly extraordinary result: without needing to train on any segmentation or object detection data, not only do the saliency maps effectively capture all relevant parts of the input image, the saliency maps self-organize to each correspond to a discrete set of concep"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS3.SSS0.Px1", "title": "Embedding.", "snippet": "Embedding. We first embed the token sequence ùëø ‚àà [ V ] N \\bm{X}\\in[V]^{N} bold_italic_X ‚àà [ italic_V ] start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT to Euclidean space. This is often done by associating each index in [ V ] [V] [ italic_V ] with a vector in ‚Ñù d \\mathbb{R}^{d} blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT using a massive 10 10 10 By ‚Äúmassive‚Äù we mean that such a structure is often a large fraction of the language model‚Äôs total size. array ùë¨ ‚àà ‚Ñù V √ó d \\bm{E}\\in\\mathbb{R}^{V\\times d} bold_italic_E ‚àà blackboard_R start_POSTSUPERSCRIPT italic_V √ó italic_d end_"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS3.SSS0.Px2", "title": "Backbone.", "snippet": "Backbone. We process the embeddings using a CRATE-like backbone which uses causal masking. To motivate causal masking, consider the causal language modeling loss ‚Ñí CLM \\mathcal{L}_{\\mathrm{CLM}} caligraphic_L start_POSTSUBSCRIPT roman_CLM end_POSTSUBSCRIPT defined in ( 7.4.1 ). The most naive implementation would require us to compute hte forward pass N N italic_N times in order to backpropagate once. Obviously this is extremely inefficient, since N N italic_N can often be in the thousands. In order to scale training with this loss efficiently, we impose a causal constraint, i.e., ùíÅ Œ∏ ‚Äã ( ùëø : "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS3.SSS0.Px3", "title": "Feature extractor.", "snippet": "Feature extractor. We use a post-processing step f Œ∏ ext f_{\\theta}^{\\mathrm{ext}} italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT which extracts the feature vector of the last known token so as to predict the next token. In theory this means that each token ùíÅ Œ∏ ‚Äã ( ùëø ) n \\bm{Z}_{\\theta}(\\bm{X})_{n} bold_italic_Z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( bold_italic_X ) start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT should contain rich information about all tokens that come before or on index n n italic_n , i.e., ùíô 1 , ‚Ä¶ , ùíô n "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS3.SSS0.Px4", "title": "Task-specific head.", "snippet": "Task-specific head. For our classification head h Œ∏ h_{\\theta} italic_h start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT , the GPT-2 architecture uses a simple linear layer and a softmax to get the desired probability vectors: h Œ∏ ‚Äã ( ùíõ ) ‚âê softmax ‚Å° ( ùëæ out ‚Äã ùíõ + ùíÉ out ) , h_{\\theta}(\\bm{z})\\doteq\\operatorname{\\mathrm{softmax}}(\\bm{W}^{\\mathrm{out}}\\bm{z}+\\bm{b}^{\\mathrm{out}}), italic_h start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( bold_italic_z ) ‚âê roman_softmax ( bold_italic_W start_POSTSUPERSCRIPT roman_out end_POSTSUPERSCRIPT bold_italic_z + bold_italic_b start_POSTSUPERSCRIPT roman_out"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS6.SSS0.Px1", "title": "Model architecture.", "snippet": "Model architecture. We use the GPT-2 tokenizer, which has vocabulary size V = 50257 V=50257 italic_V = 50257 , including a special token for <|pad|> . 13 13 13 The <|bos|> token is not included in this setup, although it is very common in modern language models. The context length is N max = 1024 N_{\\max}=1024 italic_N start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT = 1024 . The backbone model follows the GPT2-Base architecture [ RWC+19 ] with the appropriate alterations to have causal CRATE layers, and we compare against GPT2-Small and GPT2-Base."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS6.SSS0.Px2", "title": "Datasets and optimization.", "snippet": "Datasets and optimization. For training causal CRATE, we follow the implementations in the NanoGPT repository [ Kar22 ] . Specifically, we use a batch size of 384 and train for 600,000 steps with the Adam optimizer [ KB14 ] . For the Adam optimizer, we use ( Œ≤ 1 , Œ≤ 2 ) = ( 0.9 , 0.95 ) (\\beta_{1},\\beta_{2})=(0.9,0.95) ( italic_Œ≤ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_Œ≤ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) = ( 0.9 , 0.95 ) and weight decay of 0.1 0.1 0.1 . For the learning rate schedule, we apply a linear warm-up and cosine decay, with a peak value of Œ∑ = 6 √ó 10 ‚àí 4 \\eta=6\\times"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS6.SSS0.Px3", "title": "Experiment results.", "snippet": "Experiment results. Table 7.5 demonstrates that CRATE models achieve reasonable performance on the causal language modeling loss across a variety of datasets compared to GPT-2 models with similar parameter counts and similar architectures. Table 7.5: Zero-shot cross-entropy loss of the CRATE-GPT2-Base model and GPT2-Small, GPT2-Base model evaluated on the test split of the datasets ( ‚Üì \\downarrow ‚Üì lower is better). #parameters OWT LAMBADA WikiText PTB Avg GPT2-Base 124M 2.85 ‚Üì \\downarrow ‚Üì 4.12 ‚Üì \\downarrow ‚Üì 3.89 ‚Üì \\downarrow ‚Üì 4.63 ‚Üì \\downarrow ‚Üì 3.87 ‚Üì \\downarrow ‚Üì GPT2-Small 64M 3.04 4.49"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.SS2.SSS0.Px1", "title": "The encoder.", "snippet": "The encoder. The encoder is the same as the CRATE encoder in Section 7.3.2 , with the caveat that there is no feature extractor f Œ∏ ext f_{\\theta}^{\\mathrm{ext}} italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT . However, both the embedding f Œ∏ emb f_{\\theta}^{\\mathrm{emb}} italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT and the backbone f Œ∏ bb f_{\\theta}^{\\mathrm{bb}} italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_bb end_POSTSUPERSCRIPT ar"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.SS2.SSS0.Px2", "title": "The decoder backbone.", "snippet": "The decoder backbone. The decoder backbone is the CRATE decoder described in Chapter 5 . For completeness‚Äô sake, we describe it now. Given a feature sequence ùíÅ Œ∏ ‚Äã ( ùëø ) ‚âê f Œ∏ ‚Äã ( ùëø ) ‚àà ( ‚Ñù d ) ‚àó \\bm{Z}_{\\theta}(\\bm{X})\\doteq f_{\\theta}(\\bm{X})\\in(\\mathbb{R}^{d})^{*} bold_italic_Z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( bold_italic_X ) ‚âê italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( bold_italic_X ) ‚àà ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT , we can process it using the decoder backbone g Œ∑ bb g_{\\eta}^"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.SS2.SSS0.Px3", "title": "The un-embedding module.", "snippet": "The un-embedding module. To transform ùíÅ ~ Œ∏ , Œ∑ ‚Äã ( ùëø ) \\tilde{\\bm{Z}}_{\\theta,\\eta}(\\bm{X}) over~ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_Œ∏ , italic_Œ∑ end_POSTSUBSCRIPT ( bold_italic_X ) back into an estimate for ùëø \\bm{X} bold_italic_X , we need to undo the effect of the embedding module f Œ∏ emb f_{\\theta}^{\\mathrm{emb}} italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT using the unembedding module g Œ∑ unemb g_{\\eta}^{\\mathrm{unemb}} italic_g start_POSTSUBSCRIPT italic_Œ∑ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.SS5.SSS0.Px1", "title": "Model architecture.", "snippet": "Model architecture. During training, the masked crop v m v_{m} italic_v start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT resizes the whole image so that the shorter edge is of size 256 256 256 (i.e., S rsz = 256 S_{\\mathrm{rsz}}=256 italic_S start_POSTSUBSCRIPT roman_rsz end_POSTSUBSCRIPT = 256 ) before taking a random crop of size 224 √ó 224 224\\times 224 224 √ó 224 (i.e., S mask = 224 S_{\\mathrm{mask}}=224 italic_S start_POSTSUBSCRIPT roman_mask end_POSTSUBSCRIPT = 224 ), and masking p mask = 3 4 p_{\\mathrm{mask}}=\\frac{3}{4} italic_p start_POSTSUBSCRIPT roman_mask end_POSTSUBSCRIPT = divide sta"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.SS5.SSS0.Px2", "title": "Datasets and optimization.", "snippet": "Datasets and optimization. For pre-training, we use the ImageNet-1K dataset. We use the AdamW optimizer to pre-train both our ViT-MAE replication as well as CRATE-MAE. We set the base learning rate as 3 √ó 10 ‚àí 5 3\\times 10^{-5} 3 √ó 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT , the weight decay as 0.1 0.1 0.1 , and batch size as B = 4096 B=4096 italic_B = 4096 . Our learning rate schedule increases the learning rate linearly to the base learning rate over the first 40 40 40 epochs, and decreases to 0 using a cosine schedule over the next 760 760 760 epochs (training all models for 800 800 "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.SS5.SSS0.Px3", "title": "Experiment results.", "snippet": "Experiment results. Table 7.12 demonstrates that CRATE-MAE models achieve, roughly speaking, parity compared to the popular ViT-MAE architecture at similar parameter counts, and also that the feature learning performance (as measured by performance on downstream classification tasks) increases with scale. Meanwhile, Figure 7.20 demonstrates that the encoder saliency maps (and therefore the fine-grained features learned by the encoder) indeed isolate and highlight the key parts of the input image. Model CRATE-MAE-S(mall) CRATE-MAE-B(ase) ViT-MAE-S ViT-MAE-B # parameters 25.4M 44.6M 47.6M 143.8M"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#p1", "title": "‚Äú The best theory is inspired by practice, and the best practice is inspired by theory .‚Äù ‚Äî Donald Knuth", "snippet": "‚Äú The best theory is inspired by practice, and the best practice is inspired by theory .‚Äù ‚Äî Donald Knuth"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#p2", "title": "The previous chapters have presented a systematic introduction to mathematical problems, computational frameworks, and practical algorithms associated with learning low-dimensional distributions from ", "snippet": "The previous chapters have presented a systematic introduction to mathematical problems, computational frameworks, and practical algorithms associated with learning low-dimensional distributions from high-dimensional data. Although most theoretical justifications of these methods have been established for idealistic models of data such as (mixtures of) subspaces and/or Gaussians, the principles and ideas behind these computational methods are nevertheless powerful and general, and they are in fact meant to be applicable for real-world datasets and tasks."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#p3", "title": "To help readers understand the material in this book better and learn how to apply what you have learned so far to real-world data, towards the end of the book, we provide some demonstrations and vign", "snippet": "To help readers understand the material in this book better and learn how to apply what you have learned so far to real-world data, towards the end of the book, we provide some demonstrations and vignettes of several representative applications. Each application proposes a solution to a real-world task with a real-world dataset (such as visual data and text data), using the methods we have introduced in this book. The results presented in this chapter are meant to serve the following two purposes: ‚Ä¢ firstly, to provide additional experimental details and empirical evidence which validate the m"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S0.I1.i1.p1", "title": "firstly, to provide additional experimental details and empirical evidence which validate the methods presented earlier in the book, and demonstrate their significant potential in real-world contexts;", "snippet": "firstly, to provide additional experimental details and empirical evidence which validate the methods presented earlier in the book, and demonstrate their significant potential in real-world contexts;"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S0.I1.i2.p1", "title": "secondly, to introduce the reader to certain modern empirical methods and tasks in deep learning which are not well-documented outside of research or production codebases.", "snippet": "secondly, to introduce the reader to certain modern empirical methods and tasks in deep learning which are not well-documented outside of research or production codebases."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S1.p1", "title": "In previous chapters, we alluded to different setups in which we used representation-learning techniques to process real data at scale. In this chapter, we will describe such setups in great detail. T", "snippet": "In previous chapters, we alluded to different setups in which we used representation-learning techniques to process real data at scale. In this chapter, we will describe such setups in great detail. The objective of this section is to get you, the reader, to be able to reproduce any experiment discussed in this section (or indeed the book) using just the description we will give in the book, the principles introduced in previous chapters and expanded on in this chapter, and hyperparameters taken from a smattering of papers whose results are discussed in this chapter. To this end, we will preci"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S1.p2", "title": "Let us define the set of possible data as ùíü \\mathcal{D} caligraphic_D (eventually this will be the set of images ‚Ñê \\mathcal{I} caligraphic_I , for example, or the set of text ùíØ \\mathcal{T} caligraphic", "snippet": "Let us define the set of possible data as ùíü \\mathcal{D} caligraphic_D (eventually this will be the set of images ‚Ñê \\mathcal{I} caligraphic_I , for example, or the set of text ùíØ \\mathcal{T} caligraphic_T ), and the set of finite sequences of tokens in ‚Ñù d \\mathbb{R}^{d} blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT (i.e., the set of matrices with d d italic_d rows) as ( ‚Ñù d ) ‚àó ‚âê ‚ãÉ T = 1 ‚àû ‚Ñù d √ó T (\\mathbb{R}^{d})^{*}\\doteq\\bigcup_{T=1}^{\\infty}\\mathbb{R}^{d\\times T} ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S1.I1.i1.p1", "title": "An encoder architecture, parameterized by Œ∏ \\theta italic_Œ∏ , which is composed of several components: ‚Äì An embedding f Œ∏ emb : ùíü ‚Üí ( ‚Ñù d ) ‚àó f_{\\theta}^{\\mathrm{emb}}\\colon\\mathcal{D}\\to(\\mathbb{R}^{", "snippet": "An encoder architecture, parameterized by Œ∏ \\theta italic_Œ∏ , which is composed of several components: ‚Äì An embedding f Œ∏ emb : ùíü ‚Üí ( ‚Ñù d ) ‚àó f_{\\theta}^{\\mathrm{emb}}\\colon\\mathcal{D}\\to(\\mathbb{R}^{d})^{*} italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT : caligraphic_D ‚Üí ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT , which converts the input data ùíü \\mathcal{D} caligraphic_D into a series of tokens which are mapped into, or embedded in, D D italic_D -dimensional space"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S1.I1.i1.I1.i1.p1", "title": "An embedding f Œ∏ emb : ùíü ‚Üí ( ‚Ñù d ) ‚àó f_{\\theta}^{\\mathrm{emb}}\\colon\\mathcal{D}\\to(\\mathbb{R}^{d})^{*} italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_emb end_POSTS", "snippet": "An embedding f Œ∏ emb : ùíü ‚Üí ( ‚Ñù d ) ‚àó f_{\\theta}^{\\mathrm{emb}}\\colon\\mathcal{D}\\to(\\mathbb{R}^{d})^{*} italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT : caligraphic_D ‚Üí ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT , which converts the input data ùíü \\mathcal{D} caligraphic_D into a series of tokens which are mapped into, or embedded in, D D italic_D -dimensional space. In the rest of the chapter, we will often identify tokens and embeddings with each other."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S1.I1.i1.I1.i2.p1", "title": "An encoder backbone f Œ∏ bb : ( ‚Ñù d ) ‚àó ‚Üí ( ‚Ñù d ) ‚àó f_{\\theta}^{\\mathrm{bb}}\\colon(\\mathbb{R}^{d})^{*}\\to(\\mathbb{R}^{d})^{*} italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRI", "snippet": "An encoder backbone f Œ∏ bb : ( ‚Ñù d ) ‚àó ‚Üí ( ‚Ñù d ) ‚àó f_{\\theta}^{\\mathrm{bb}}\\colon(\\mathbb{R}^{d})^{*}\\to(\\mathbb{R}^{d})^{*} italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_bb end_POSTSUPERSCRIPT : ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT ‚Üí ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT , which processes the series of embeddings using a sequence-to-sequence operation. This backbone is implemented by the network architectures"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S1.I1.i1.I1.i3.p1", "title": "A aggregate feature extractor f Œ∏ ext : ( ‚Ñù d ) ‚àó ‚Üí ‚Ñù d f_{\\theta}^{\\mathrm{ext}}\\colon(\\mathbb{R}^{d})^{*}\\to\\mathbb{R}^{d} italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRI", "snippet": "A aggregate feature extractor f Œ∏ ext : ( ‚Ñù d ) ‚àó ‚Üí ‚Ñù d f_{\\theta}^{\\mathrm{ext}}\\colon(\\mathbb{R}^{d})^{*}\\to\\mathbb{R}^{d} italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT : ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT ‚Üí blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , which extracts an aggregate representation of the whole sequence. This is used to define a single feature for the entire data sample."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S1.I1.i1.I1.i4.p1", "title": "A task-specific head h Œ∏ : ‚Ñù d ‚Üí ‚Ñù m h_{\\theta}\\colon\\mathbb{R}^{d}\\to\\mathbb{R}^{m} italic_h start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT : blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPER", "snippet": "A task-specific head h Œ∏ : ‚Ñù d ‚Üí ‚Ñù m h_{\\theta}\\colon\\mathbb{R}^{d}\\to\\mathbb{R}^{m} italic_h start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT : blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ‚Üí blackboard_R start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , which extracts an m m italic_m -dimensional output for prediction."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S1.I1.i2.p1", "title": "An autoencoder architecture, which is composed of several components: ‚Äì An embedding f Œ∏ emb : ùíü ‚Üí ( ‚Ñù d ) ‚àó f_{\\theta}^{\\mathrm{emb}}\\colon\\mathcal{D}\\to(\\mathbb{R}^{d})^{*} italic_f start_POSTSUBSCR", "snippet": "An autoencoder architecture, which is composed of several components: ‚Äì An embedding f Œ∏ emb : ùíü ‚Üí ( ‚Ñù d ) ‚àó f_{\\theta}^{\\mathrm{emb}}\\colon\\mathcal{D}\\to(\\mathbb{R}^{d})^{*} italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT : caligraphic_D ‚Üí ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT , which converts the input data ùíü \\mathcal{D} caligraphic_D into a series of tokens which are embedded in D D italic_D -dimensional space. ‚Äì An encoder backbone f Œ∏ bb : ( ‚Ñù d ) ‚àó ‚Üí ( ‚Ñù d"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S1.I1.i2.I1.i1.p1", "title": "An embedding f Œ∏ emb : ùíü ‚Üí ( ‚Ñù d ) ‚àó f_{\\theta}^{\\mathrm{emb}}\\colon\\mathcal{D}\\to(\\mathbb{R}^{d})^{*} italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_emb end_POSTS", "snippet": "An embedding f Œ∏ emb : ùíü ‚Üí ( ‚Ñù d ) ‚àó f_{\\theta}^{\\mathrm{emb}}\\colon\\mathcal{D}\\to(\\mathbb{R}^{d})^{*} italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT : caligraphic_D ‚Üí ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT , which converts the input data ùíü \\mathcal{D} caligraphic_D into a series of tokens which are embedded in D D italic_D -dimensional space."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S1.I1.i2.I1.i2.p1", "title": "An encoder backbone f Œ∏ bb : ( ‚Ñù d ) ‚àó ‚Üí ( ‚Ñù d ) ‚àó f_{\\theta}^{\\mathrm{bb}}\\colon(\\mathbb{R}^{d})^{*}\\to(\\mathbb{R}^{d})^{*} italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRI", "snippet": "An encoder backbone f Œ∏ bb : ( ‚Ñù d ) ‚àó ‚Üí ( ‚Ñù d ) ‚àó f_{\\theta}^{\\mathrm{bb}}\\colon(\\mathbb{R}^{d})^{*}\\to(\\mathbb{R}^{d})^{*} italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_bb end_POSTSUPERSCRIPT : ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT ‚Üí ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT , which processes the series of embeddings using a sequence-to-sequence operation."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S1.I1.i2.I1.i3.p1", "title": "A decoder backbone g Œ∑ bb : ( ‚Ñù d ) ‚àó ‚Üí ( ‚Ñù d ) ‚àó g_{\\eta}^{\\mathrm{bb}}\\colon(\\mathbb{R}^{d})^{*}\\to(\\mathbb{R}^{d})^{*} italic_g start_POSTSUBSCRIPT italic_Œ∑ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ", "snippet": "A decoder backbone g Œ∑ bb : ( ‚Ñù d ) ‚àó ‚Üí ( ‚Ñù d ) ‚àó g_{\\eta}^{\\mathrm{bb}}\\colon(\\mathbb{R}^{d})^{*}\\to(\\mathbb{R}^{d})^{*} italic_g start_POSTSUBSCRIPT italic_Œ∑ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_bb end_POSTSUPERSCRIPT : ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT ‚Üí ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT , which conceptually undoes the operation of the encoder backbone."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S1.I1.i2.I1.i4.p1", "title": "An unembedding g Œ∑ unemb : ( ‚Ñù d ) ‚àó ‚Üí ùíü g_{\\eta}^{\\mathrm{unemb}}\\colon(\\mathbb{R}^{d})^{*}\\to\\mathcal{D} italic_g start_POSTSUBSCRIPT italic_Œ∑ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_unemb end", "snippet": "An unembedding g Œ∑ unemb : ( ‚Ñù d ) ‚àó ‚Üí ùíü g_{\\eta}^{\\mathrm{unemb}}\\colon(\\mathbb{R}^{d})^{*}\\to\\mathcal{D} italic_g start_POSTSUBSCRIPT italic_Œ∑ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_unemb end_POSTSUPERSCRIPT : ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT ‚Üí caligraphic_D , which acts as an inverse of the embedding."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S1.p3", "title": "We will repeatedly use this notation many times in this chapter, so please feel free to refer back to it if something doesn‚Äôt make sense. This decomposition of our networks also closely mirrors most c", "snippet": "We will repeatedly use this notation many times in this chapter, so please feel free to refer back to it if something doesn‚Äôt make sense. This decomposition of our networks also closely mirrors most code implementations, and you can start your coding projects by defining these networks."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S1.p4", "title": "In this chapter, we will discuss applications of the book‚Äôs principles to contrastive learning in Section 7.2 . This will serve as both an introduction to imagery data, data augmentation techniques, a", "snippet": "In this chapter, we will discuss applications of the book‚Äôs principles to contrastive learning in Section 7.2 . This will serve as both an introduction to imagery data, data augmentation techniques, and the common architecture known as the transformer, as well as a first demonstration of the drastic kinds of simplifications we can make using the demonstrated principles. We will continue with modifications to the network architecture in Sections 7.3 and 7.4 , which demonstrate the capabilities of simplified architectures for encoding within the image and text domains. We then demonstrate simpli"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.p1", "title": "Learning high-quality and faithful representations of data is a fundamental problem in deep learning, known as self-supervised learning . There have been many approaches proposed for this task, many o", "snippet": "Learning high-quality and faithful representations of data is a fundamental problem in deep learning, known as self-supervised learning . There have been many approaches proposed for this task, many of which do not evidently use the techniques and principles outlined in this manuscript. One such approach is called contrastive learning , so named because the learning objective is (roughly speaking) about ensuring that features of ‚Äúsimilar‚Äù data are similar, and features of ‚Äúdis-similar‚Äù data are far apart. Contrastive learning solutions are often highly-engineered, empirically designed approach"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS1.p1", "title": "The data that we will use to explore and simplify the DINO methodology is all 2-dimensional image data. For training , we will use the ImageNet-1K and ImageNet-21K datasets. Each sample in the dataset", "snippet": "The data that we will use to explore and simplify the DINO methodology is all 2-dimensional image data. For training , we will use the ImageNet-1K and ImageNet-21K datasets. Each sample in the dataset is an RGB image, of varying resolution, and a label indicating the object or scene that the image contains (i.e., the class of the image). The ImageNet-1K dataset contains 1.28M training images and 50K validation images partitioned into 1K classes. The ImageNet-21K dataset contains 14.2M training images and 21.8K classes, but the classes are not disjoint (i.e., some classes are subsets of others)"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS1.p2", "title": "On a slightly more formal level, our data ùëø \\bm{X} bold_italic_X will be images; we let ‚Ñê \\mathcal{I} caligraphic_I be the set of all images. Since an image is a rectangular array of pixels, and each ", "snippet": "On a slightly more formal level, our data ùëø \\bm{X} bold_italic_X will be images; we let ‚Ñê \\mathcal{I} caligraphic_I be the set of all images. Since an image is a rectangular array of pixels, and each pixel has a color given by RGB, CMYK, or another color format, we say that an image is an element of ‚Ñù c √ó h √ó w \\mathbb{R}^{c\\times h\\times w} blackboard_R start_POSTSUPERSCRIPT italic_c √ó italic_h √ó italic_w end_POSTSUPERSCRIPT ‚Äî here c c italic_c is the number of channels (i.e., 3 3 3 for RGB and 4 4 4 for CMYK), h h italic_h is the image height, and w w italic_w is the image width. Consequentl"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS2.p1", "title": "Our task is to learn a good representation of the data. Contrastive learning, by and large, does this by defining what properties of the input image we wish the features to reflect, construct images w", "snippet": "Our task is to learn a good representation of the data. Contrastive learning, by and large, does this by defining what properties of the input image we wish the features to reflect, construct images which share these properties but vary others, and set up a loss which promotes that the features of images with shared properties are close and images with different properties are different. The naturally optimal solution to this learning problem is that the learned features preserve the desired properties of the input. However, there are many practical and empirical complications that arise in th"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS2.p2", "title": "In the case of DINO, the authors propose to use a methodology which produces a single feature vector for the whole image and desires the feature vector to contain ‚Äúglobal‚Äù (i.e., image-level) informat", "snippet": "In the case of DINO, the authors propose to use a methodology which produces a single feature vector for the whole image and desires the feature vector to contain ‚Äúglobal‚Äù (i.e., image-level) information. Accordingly, the loss will promote that images with similar global information have similar features and images with different global information have different features."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS2.p3", "title": "This seems intuitive, but as previously mentioned, there are several empirical considerations, even while setting up the loss. First and foremost, how should we promote similarities and differences? T", "snippet": "This seems intuitive, but as previously mentioned, there are several empirical considerations, even while setting up the loss. First and foremost, how should we promote similarities and differences? The answer of DINO [ CTM+21 ] is 1 1 1 In the author‚Äôs view, inexplicably‚Ä¶ to convert the output features into ‚Äúlogits‚Äù corresponding to some probability distribution and take their cross-entropy. More specifically, let Œî m ‚âê { ùíô ‚àà ‚Ñù m : x i ‚â• 0 ‚Äã ‚àÄ i ‚àà [ m ] , ‚àë i = 1 m x i = 1 } \\Delta_{m}\\doteq\\{\\bm{x}\\in\\mathbb{R}^{m}\\colon x_{i}\\geq 0\\ \\forall i\\in[m],\\sum_{i=1}^{m}x_{i}=1\\} roman_Œî start_POST"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS2.p4", "title": "The next question is, how should we obtain samples with similar global information? The answer of DINO (as well as nearly all contrastive learning) is data augmentation ‚Äî from each sample, make severa", "snippet": "The next question is, how should we obtain samples with similar global information? The answer of DINO (as well as nearly all contrastive learning) is data augmentation ‚Äî from each sample, make several correlated samples which share the desired properties. In the DINO case, we use different crops or views of the input image. Recall that we model an image as an element of the set ‚Ñê \\mathcal{I} caligraphic_I . In this notation, a view is a function v : ‚Ñê ‚Üí ‚Ñê v\\colon\\mathcal{I}\\to\\mathcal{I} italic_v : caligraphic_I ‚Üí caligraphic_I . In the DINO case, the view is a random resized crop : it takes "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS2.p5", "title": "There are two types of views we want to use, depicted in Figure 7.4 : ‚Ä¢ global views , which are random resized crops with area percentage parameter p glo ‚àà [ 0 , 1 ] p_{\\mathrm{glo}}\\in[0,1] italic_p", "snippet": "There are two types of views we want to use, depicted in Figure 7.4 : ‚Ä¢ global views , which are random resized crops with area percentage parameter p glo ‚àà [ 0 , 1 ] p_{\\mathrm{glo}}\\in[0,1] italic_p start_POSTSUBSCRIPT roman_glo end_POSTSUBSCRIPT ‚àà [ 0 , 1 ] and output shape ( C , S glo , S glo ) (C,S_{\\mathrm{glo}},S_{\\mathrm{glo}}) ( italic_C , italic_S start_POSTSUBSCRIPT roman_glo end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT roman_glo end_POSTSUBSCRIPT ) ; ‚Ä¢ and local views , which are random resized crops with area percentage parameter p loc ‚àà [ 0 , 1 ] p_{\\mathrm{loc}}\\in[0,1] ital"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.I1.i1.p1", "title": "global views , which are random resized crops with area percentage parameter p glo ‚àà [ 0 , 1 ] p_{\\mathrm{glo}}\\in[0,1] italic_p start_POSTSUBSCRIPT roman_glo end_POSTSUBSCRIPT ‚àà [ 0 , 1 ] and output ", "snippet": "global views , which are random resized crops with area percentage parameter p glo ‚àà [ 0 , 1 ] p_{\\mathrm{glo}}\\in[0,1] italic_p start_POSTSUBSCRIPT roman_glo end_POSTSUBSCRIPT ‚àà [ 0 , 1 ] and output shape ( C , S glo , S glo ) (C,S_{\\mathrm{glo}},S_{\\mathrm{glo}}) ( italic_C , italic_S start_POSTSUBSCRIPT roman_glo end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT roman_glo end_POSTSUBSCRIPT ) ;"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.I1.i2.p1", "title": "and local views , which are random resized crops with area percentage parameter p loc ‚àà [ 0 , 1 ] p_{\\mathrm{loc}}\\in[0,1] italic_p start_POSTSUBSCRIPT roman_loc end_POSTSUBSCRIPT ‚àà [ 0 , 1 ] and outp", "snippet": "and local views , which are random resized crops with area percentage parameter p loc ‚àà [ 0 , 1 ] p_{\\mathrm{loc}}\\in[0,1] italic_p start_POSTSUBSCRIPT roman_loc end_POSTSUBSCRIPT ‚àà [ 0 , 1 ] and output shape ( C , S loc , S loc ) (C,S_{\\mathrm{loc}},S_{\\mathrm{loc}}) ( italic_C , italic_S start_POSTSUBSCRIPT roman_loc end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT roman_loc end_POSTSUBSCRIPT ) . Here p loc < p glo p_{\\mathrm{loc}}<p_{\\mathrm{glo}} italic_p start_POSTSUBSCRIPT roman_loc end_POSTSUBSCRIPT < italic_p start_POSTSUBSCRIPT roman_glo end_POSTSUBSCRIPT and S loc < S glo S_{\\mathrm{"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS2.p6", "title": "DINO desires that the aggregate features ùíõ Œ∏ ‚Äã ( ùëø v ) ‚âê ( f Œ∏ ext ‚àò f Œ∏ ) ‚Äã ( ùëø v ) \\bm{z}_{\\theta}(\\bm{X}_{v})\\doteq(f_{\\theta}^{\\mathrm{ext}}\\circ f_{\\theta})(\\bm{X}_{v}) bold_italic_z start_POSTSU", "snippet": "DINO desires that the aggregate features ùíõ Œ∏ ‚Äã ( ùëø v ) ‚âê ( f Œ∏ ext ‚àò f Œ∏ ) ‚Äã ( ùëø v ) \\bm{z}_{\\theta}(\\bm{X}_{v})\\doteq(f_{\\theta}^{\\mathrm{ext}}\\circ f_{\\theta})(\\bm{X}_{v}) bold_italic_z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ) ‚âê ( italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT ‚àò italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ) ( bold_italic_X start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT ) of all views ùëø v ‚âê v ‚Äã ( ùëø ) \\bm{X}_{v}\\doteq v(\\bm{X}) bold_it"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS2.p7", "title": "In particular, DINO minimizes the difference between the probability vector ùíë Œ∏ , ùëæ , ùùÅ ‚Äã ( ùëø g ) ‚âê h ùëæ , ùùÅ ‚Äã ( ùíõ Œ∏ ‚Äã ( ùëø g ) ) \\bm{p}_{\\theta,\\bm{W},\\bm{\\mu}}(\\bm{X}_{g})\\doteq h_{\\bm{W},\\bm{\\mu}}(\\b", "snippet": "In particular, DINO minimizes the difference between the probability vector ùíë Œ∏ , ùëæ , ùùÅ ‚Äã ( ùëø g ) ‚âê h ùëæ , ùùÅ ‚Äã ( ùíõ Œ∏ ‚Äã ( ùëø g ) ) \\bm{p}_{\\theta,\\bm{W},\\bm{\\mu}}(\\bm{X}_{g})\\doteq h_{\\bm{W},\\bm{\\mu}}(\\bm{z}_{\\theta}(\\bm{X}_{g})) bold_italic_p start_POSTSUBSCRIPT italic_Œ∏ , bold_italic_W , bold_italic_Œº end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ) ‚âê italic_h start_POSTSUBSCRIPT bold_italic_W , bold_italic_Œº end_POSTSUBSCRIPT ( bold_italic_z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ) ) for ea"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS2.p8", "title": "In this specific case, however, if you try to implement ( 7.2.7 ) and optimize it on a real network, it is very likely that you run into a problem: after running a few iterations of the learning algor", "snippet": "In this specific case, however, if you try to implement ( 7.2.7 ) and optimize it on a real network, it is very likely that you run into a problem: after running a few iterations of the learning algorithm, the feature mapping f Œ∏ ext ‚àò f Œ∏ f_{\\theta}^{\\mathrm{ext}}\\circ f_{\\theta} italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT ‚àò italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT will become the constant function ! This certainly optimizes the above loss since it minimizes the distance between features of different views of the s"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS2.p9", "title": "Actually avoiding collapse is a very common consideration in contrastive learning. So how to do it in this case? The solution of DINO, again, is empirically designed, and carefully tunes the optimizat", "snippet": "Actually avoiding collapse is a very common consideration in contrastive learning. So how to do it in this case? The solution of DINO, again, is empirically designed, and carefully tunes the optimization of the parameter ùùÅ \\bm{\\mu} bold_italic_Œº (which is updated using all samples in the batch) and a ‚Äútemperature‚Äù hyperparameter œÑ \\tau italic_œÑ which is part of the implementation of h ùëæ , ùùÅ h_{\\bm{W},\\bm{\\mu}} italic_h start_POSTSUBSCRIPT bold_italic_W , bold_italic_Œº end_POSTSUBSCRIPT and discussed in Section 7.2.3 . Given a certain special set of hyperparameters that work well, this is indee"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS2.p10", "title": "Towards amending this state of affairs, let us discuss simplifications to the formulation. First, instead of computing a probability vector using a learned transformation h ùëæ , ùùÅ h_{\\bm{W},\\bm{\\mu}} i", "snippet": "Towards amending this state of affairs, let us discuss simplifications to the formulation. First, instead of computing a probability vector using a learned transformation h ùëæ , ùùÅ h_{\\bm{W},\\bm{\\mu}} italic_h start_POSTSUBSCRIPT bold_italic_W , bold_italic_Œº end_POSTSUBSCRIPT of the aggregate features ùíõ Œ∏ \\bm{z}_{\\theta} bold_italic_z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT , we can directly use the aggregate representation , ignoring the task-specific head (or equivalently, setting it to the identity mapping). But now we need a way to compare the vectors directly. Using our hypothesis f"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS2.p11", "title": "Before, collapse was avoided by using tricks to update ùùÅ \\bm{\\mu} bold_italic_Œº and œÑ \\tau italic_œÑ . In our simplification, if we compare the features within the representation space instead of conve", "snippet": "Before, collapse was avoided by using tricks to update ùùÅ \\bm{\\mu} bold_italic_Œº and œÑ \\tau italic_œÑ . In our simplification, if we compare the features within the representation space instead of converting them to probabilities, we do not have either of these parameters and so must consider a different way to avoid collapse. To do this, we return to the fundamentals. The basic idea of avoiding collapse is that in order to make sure that all samples do not return the same exact same features, we need different samples to have different features. In other words, we would like the covariance of t"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS3.p1", "title": "For the architecture, we use a standard vision transformer. Here is how such an architecture works, formally, in the context of image data. Recall from Section 7.1 that there are four components to an", "snippet": "For the architecture, we use a standard vision transformer. Here is how such an architecture works, formally, in the context of image data. Recall from Section 7.1 that there are four components to an encoder architecture, namely an embedding, a backbone, a feature extractor, and a task-specific head. We discuss these three parts presently."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS3.SSS0.Px1.p1", "title": "Given imagery data ùëø ‚àà ‚Ñê \\bm{X}\\in\\mathcal{I} bold_italic_X ‚àà caligraphic_I , we embed it as a sequence of tokens in ‚Ñù d \\mathbb{R}^{d} blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ", "snippet": "Given imagery data ùëø ‚àà ‚Ñê \\bm{X}\\in\\mathcal{I} bold_italic_X ‚àà caligraphic_I , we embed it as a sequence of tokens in ‚Ñù d \\mathbb{R}^{d} blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT using the map f Œ∏ emb f_{\\theta}^{\\mathrm{emb}} italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT , as follows. The first two steps are depicted in Figure 7.5 , and the latter two are depicted in Figure 7.6 . 1. First, we turn the image data ùëø \\bm{X} bold_italic_X into a sequence of patches of shape ( C , P H , P W ) (C,P_{H},P_{W}) ( itali"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.I2.i1.p1", "title": "First, we turn the image data ùëø \\bm{X} bold_italic_X into a sequence of patches of shape ( C , P H , P W ) (C,P_{H},P_{W}) ( italic_C , italic_P start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT , italic", "snippet": "First, we turn the image data ùëø \\bm{X} bold_italic_X into a sequence of patches of shape ( C , P H , P W ) (C,P_{H},P_{W}) ( italic_C , italic_P start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT , italic_P start_POSTSUBSCRIPT italic_W end_POSTSUBSCRIPT ) where P H P_{H} italic_P start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT and P W P_{W} italic_P start_POSTSUBSCRIPT italic_W end_POSTSUBSCRIPT are the patch dimensions. We assume that P H P_{H} italic_P start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT and P W P_{W} italic_P start_POSTSUBSCRIPT italic_W end_POSTSUBSCRIPT evenly divide the height and "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.I2.i2.p1", "title": "We unroll each patch into a vector of length D ‚âê C ‚Äã P H ‚Äã P W D\\doteq CP_{H}P_{W} italic_D ‚âê italic_C italic_P start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT italic_P start_POSTSUBSCRIPT italic_W end", "snippet": "We unroll each patch into a vector of length D ‚âê C ‚Äã P H ‚Äã P W D\\doteq CP_{H}P_{W} italic_D ‚âê italic_C italic_P start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT italic_P start_POSTSUBSCRIPT italic_W end_POSTSUBSCRIPT . There are N ‚âê N H ‚Äã N W N\\doteq N_{H}N_{W} italic_N ‚âê italic_N start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT italic_N start_POSTSUBSCRIPT italic_W end_POSTSUBSCRIPT patch vectors, which we place in ‚Äúraster order‚Äù (top left ‚Üí \\to ‚Üí top right ‚Üí \\to ‚Üí bottom left ‚Üí \\to ‚Üí bottom right) into a matrix ùëø patch ‚àà ‚Ñù D √ó N \\bm{X}^{\\mathrm{patch}}\\in\\mathbb{R}^{D\\times N} bold_italic_X star"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.I2.i3.p1", "title": "We then perform the following operation on ùëø patch ‚àà ‚Ñù D √ó N \\bm{X}^{\\mathrm{patch}}\\in\\mathbb{R}^{D\\times N} bold_italic_X start_POSTSUPERSCRIPT roman_patch end_POSTSUPERSCRIPT ‚àà blackboard_R start_P", "snippet": "We then perform the following operation on ùëø patch ‚àà ‚Ñù D √ó N \\bm{X}^{\\mathrm{patch}}\\in\\mathbb{R}^{D\\times N} bold_italic_X start_POSTSUPERSCRIPT roman_patch end_POSTSUPERSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT italic_D √ó italic_N end_POSTSUPERSCRIPT to project it to ‚Ñù d √ó n \\mathbb{R}^{d\\times n} blackboard_R start_POSTSUPERSCRIPT italic_d √ó italic_n end_POSTSUPERSCRIPT where n ‚âê N + 1 n\\doteq N+1 italic_n ‚âê italic_N + 1 : ùëø patch ‚Ü¶ [ ùíõ cls 1 , ùëæ emb ‚Äã ùëø ] + ùë¨ pos . \\bm{X}^{\\mathrm{patch}}\\mapsto[\\bm{z}_{\\mathrm{cls}}^{1},\\bm{W}^{\\mathrm{emb}}\\bm{X}]+\\bm{E}^{\\mathrm{pos}}. bold_italic_X s"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.I2.i3.I0.i1.p1", "title": "ùëæ emb ‚àà ‚Ñù d √ó D \\bm{W}^{\\mathrm{emb}}\\in\\mathbb{R}^{d\\times D} bold_italic_W start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d √ó italic_D end_POSTSUPERS", "snippet": "ùëæ emb ‚àà ‚Ñù d √ó D \\bm{W}^{\\mathrm{emb}}\\in\\mathbb{R}^{d\\times D} bold_italic_W start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d √ó italic_D end_POSTSUPERSCRIPT is a matrix which projects each patch vector to a token feature."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.I2.i3.I0.i2.p1", "title": "ùíõ cls 1 ‚àà ‚Ñù d \\bm{z}_{\\mathrm{cls}}^{1}\\in\\mathbb{R}^{d} bold_italic_z start_POSTSUBSCRIPT roman_cls end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT ", "snippet": "ùíõ cls 1 ‚àà ‚Ñù d \\bm{z}_{\\mathrm{cls}}^{1}\\in\\mathbb{R}^{d} bold_italic_z start_POSTSUBSCRIPT roman_cls end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT is a so-called class token or register token . The class token heuristically holds global information of the whole data and is used for downstream tasks. In the framework of compressive deep networks from Chapter 4 , we expect that the class token is projected onto the same subspaces as the salient or semantically relevant tokens during the progression of the forward p"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.I2.i3.I0.i3.p1", "title": "ùë¨ pos ‚àà ‚Ñù d √ó N \\bm{E}^{\\mathrm{pos}}\\in\\mathbb{R}^{d\\times N} bold_italic_E start_POSTSUPERSCRIPT roman_pos end_POSTSUPERSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d √ó italic_N end_POSTSUPERS", "snippet": "ùë¨ pos ‚àà ‚Ñù d √ó N \\bm{E}^{\\mathrm{pos}}\\in\\mathbb{R}^{d\\times N} bold_italic_E start_POSTSUPERSCRIPT roman_pos end_POSTSUPERSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d √ó italic_N end_POSTSUPERSCRIPT is a so-called positional encoding which distinguishes tokens of different patches from each other. That is, token features should have positional information, so that the overall map f pre f^{\\mathrm{pre}} italic_f start_POSTSUPERSCRIPT roman_pre end_POSTSUPERSCRIPT is not invariant to permutations of the patches, and ùë¨ pos \\bm{E}^{\\mathrm{pos}} bold_italic_E start_POSTSUPERSCRIPT roman_pos"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.I2.i3.I0.i3.I1.i1.p1", "title": "In this DINO case, where the transformer receives differently-sized images, we learn a positional encoding for the largest size received during training, and interpolate to get the positional encoding", "snippet": "In this DINO case, where the transformer receives differently-sized images, we learn a positional encoding for the largest size received during training, and interpolate to get the positional encodings for smaller-sized inputs."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS3.SSS0.Px2.p1", "title": "Given a sequence of embeddings ùíÅ Œ∏ 1 ‚Äã ( ùëø ) ‚âê f Œ∏ emb ‚Äã ( ùëø ) ‚àà ( ‚Ñù d ) ‚àó \\bm{Z}_{\\theta}^{1}(\\bm{X})\\doteq f_{\\theta}^{\\mathrm{emb}}(\\bm{X})\\in(\\mathbb{R}^{d})^{*} bold_italic_Z start_POSTSUBSCRIPT ", "snippet": "Given a sequence of embeddings ùíÅ Œ∏ 1 ‚Äã ( ùëø ) ‚âê f Œ∏ emb ‚Äã ( ùëø ) ‚àà ( ‚Ñù d ) ‚àó \\bm{Z}_{\\theta}^{1}(\\bm{X})\\doteq f_{\\theta}^{\\mathrm{emb}}(\\bm{X})\\in(\\mathbb{R}^{d})^{*} bold_italic_Z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ( bold_italic_X ) ‚âê italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT ( bold_italic_X ) ‚àà ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT , we process it using the backbone map f Œ∏ bb f_{\\theta}^{\\mathrm{bb}"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.I3.i1.p1", "title": "The MHSA Œ∏ ‚Ñì \\operatorname{MHSA}_{\\theta}^{\\ell} roman_MHSA start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT operator is multi-head-self-attention, the ", "snippet": "The MHSA Œ∏ ‚Ñì \\operatorname{MHSA}_{\\theta}^{\\ell} roman_MHSA start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT operator is multi-head-self-attention, the predecessor of the multi-head subspace self-attention (cf Chapter 4 ). The formulation is as follows: MHSA Œ∏ ‚Ñì ‚Å° ( ùíÅ ) \\displaystyle\\operatorname{MHSA}_{\\theta}^{\\ell}(\\bm{Z}) roman_MHSA start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ( bold_italic_Z ) ‚âê ùëº out ‚Ñì ‚Äã [ SA ‚Å° ( [ ùëº qry 1 , ‚Ñì ] ‚ä§ ‚Äã ùíÅ , [ ùëº key 1 , ‚Ñì ] ‚ä§ ‚Äã ùíÅ , [ ùëº val 1 , ‚Ñì ] ‚ä§ ‚Äã ùíÅ ) ‚ãÆ SA "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.I3.i2.p1", "title": "The MLP Œ∏ ‚Ñì \\operatorname{MLP}_{\\theta}^{\\ell} roman_MLP start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT is a two-layer perceptron, a regular nonlinear", "snippet": "The MLP Œ∏ ‚Ñì \\operatorname{MLP}_{\\theta}^{\\ell} roman_MLP start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT is a two-layer perceptron, a regular nonlinearity used in deep networks, and has the form MLP Œ∏ ‚Ñì ‚Å° ( ùíÅ ) ‚âê ùëæ down ‚Ñì ‚Äã ReLU ‚Å° ( ùëæ up ‚Ñì ‚Äã ùíÅ + ùíÉ up ‚Ñì ‚Äã ùüè n ‚ä§ ) + ùíÉ down ‚Ñì ‚Äã ùüè n ‚ä§ \\operatorname{MLP}_{\\theta}^{\\ell}(\\bm{Z})\\doteq\\bm{W}_{\\mathrm{down}}^{\\ell}\\operatorname{ReLU}(\\bm{W}_{\\mathrm{up}}^{\\ell}\\bm{Z}+\\bm{b}_{\\mathrm{up}}^{\\ell}\\bm{1}_{n}^{\\top})+\\bm{b}_{\\mathrm{down}}^{\\ell}\\bm{1}_{n}^{\\top} roman_MLP start_POSTSUBSCRIPT italic_Œ∏ end_PO"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.I3.i3.p1", "title": "Each layer-norm LN Œ∏ i , ‚Ñì \\operatorname{LN}_{\\theta}^{i,\\ell} roman_LN start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i , roman_‚Ñì end_POSTSUPERSCRIPT for i ‚àà { 1 , 2 } i\\", "snippet": "Each layer-norm LN Œ∏ i , ‚Ñì \\operatorname{LN}_{\\theta}^{i,\\ell} roman_LN start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i , roman_‚Ñì end_POSTSUPERSCRIPT for i ‚àà { 1 , 2 } i\\in\\{1,2\\} italic_i ‚àà { 1 , 2 } is a standard normalization, which applies column-wise to each token feature independently: LN Œ∏ i , ‚Ñì ‚Å° ( ùíÅ ) = LN Œ∏ i , ‚Ñì ‚Å° ( [ ùíõ 1 , ‚Ä¶ , ùíõ n ] ) = [ LN Œ∏ i , ‚Ñì ‚Å° ( ùíõ 1 ) , ‚Ä¶ , LN Œ∏ i , ‚Ñì ‚Å° ( ùíõ n ) ] \\operatorname{LN}_{\\theta}^{i,\\ell}(\\bm{Z})=\\operatorname{LN}_{\\theta}^{i,\\ell}(\\begin{bmatrix}\\bm{z}_{1},\\dots,\\bm{z}_{n}\\end{bmatrix})=\\begin{bmatrix}\\operatorname{L"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS3.SSS0.Px2.p2", "title": "The transformer is one of the most popular neural network architectures in history, powering applications in almost all fields of deep learning.", "snippet": "The transformer is one of the most popular neural network architectures in history, powering applications in almost all fields of deep learning."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS3.SSS0.Px3.p1", "title": "We use a post-processing step f Œ∏ ext f_{\\theta}^{\\mathrm{ext}} italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT which extracts the class tok", "snippet": "We use a post-processing step f Œ∏ ext f_{\\theta}^{\\mathrm{ext}} italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT which extracts the class token feature , which (recall) is the feature meant to contain aggregate information about the input image, and applies an MLP and normalization to it. Namely, we have ùíõ Œ∏ ‚Äã ( ùëø ) ‚âê f Œ∏ ext ‚Äã ( ùíÅ Œ∏ ‚Äã ( ùëø ) ) = f Œ∏ ext ‚Äã ( [ ùíõ Œ∏ 1 ‚Äã ( ùëø ) , ‚Ä¶ , ùíõ Œ∏ n ‚Äã ( ùëø ) ] ) ‚âê MLP Œ∏ ext ‚Å° ( ùíõ Œ∏ 1 ‚Äã ( ùëø ) ) ‚Äñ MLP Œ∏ ext ‚Å° ( ùíõ Œ∏ 1 ‚Äã ( ùëø ) ) ‚Äñ 2 . \\bm{z}_{\\theta}(\\bm{X})\\doteq f_{\\theta}^{\\mathrm{ext}}(\\bm{Z}_{\\theta}"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS3.SSS0.Px4.p1", "title": "For DINO, we use the task-specific DINO head h ùëæ , ùùÅ h_{\\bm{W},\\bm{\\mu}} italic_h start_POSTSUBSCRIPT bold_italic_W , bold_italic_Œº end_POSTSUBSCRIPT . For SimDINO, we use no task-specific head at all", "snippet": "For DINO, we use the task-specific DINO head h ùëæ , ùùÅ h_{\\bm{W},\\bm{\\mu}} italic_h start_POSTSUBSCRIPT bold_italic_W , bold_italic_Œº end_POSTSUBSCRIPT . For SimDINO, we use no task-specific head at all , as previously described."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS4.SSS0.Px1.p1", "title": "We have a loss function and an architecture, so we now discuss the optimization strategy. The optimization strategy for DINO uses two sets of weights for the same architecture : student weights Œ∏ s \\t", "snippet": "We have a loss function and an architecture, so we now discuss the optimization strategy. The optimization strategy for DINO uses two sets of weights for the same architecture : student weights Œ∏ s \\theta_{\\mathrm{s}} italic_Œ∏ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT and teacher weights Œ∏ t \\theta_{\\mathrm{t}} italic_Œ∏ start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT . These correspond to two different neural networks, called the teacher network and student network, with the same architecture. The teacher network encodes all global views, while the student network encodes all ‚Äúother‚Äù views. "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS4.SSS0.Px1.p2", "title": "While it is easy to reason about ( 7.2.24 ), it is impossible in practice to implement optimization algorithms such as gradient descent with a loss given by ‚Ñí DINO ‚àí st \\mathcal{L}_{\\mathrm{DINO}{}-\\m", "snippet": "While it is easy to reason about ( 7.2.24 ), it is impossible in practice to implement optimization algorithms such as gradient descent with a loss given by ‚Ñí DINO ‚àí st \\mathcal{L}_{\\mathrm{DINO}{}-\\mathrm{s}\\mathrm{t}} caligraphic_L start_POSTSUBSCRIPT roman_DINO - roman_st end_POSTSUBSCRIPT . This is because the expectations in the loss are impossible to evaluate, much less take the gradient of. In this extremely frequent case, we approximate the expectation via finite samples. That is, at each timestep k k italic_k we: ‚Ä¢ Subsample B B italic_B data points from our dataset { ùëø 1 ( k ) , ‚Ä¶ , "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.I4.i1.p1", "title": "Subsample B B italic_B data points from our dataset { ùëø 1 ( k ) , ‚Ä¶ , ùëø B ( k ) } ‚äÇ ‚Ñê \\{\\bm{X}_{1}^{(k)},\\dots,\\bm{X}_{B}^{(k)}\\}\\subset\\mathcal{I} { bold_italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCR", "snippet": "Subsample B B italic_B data points from our dataset { ùëø 1 ( k ) , ‚Ä¶ , ùëø B ( k ) } ‚äÇ ‚Ñê \\{\\bm{X}_{1}^{(k)},\\dots,\\bm{X}_{B}^{(k)}\\}\\subset\\mathcal{I} { bold_italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT , ‚Ä¶ , bold_italic_X start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT } ‚äÇ caligraphic_I ."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.I4.i2.p1", "title": "For each data point ùëø b ( k ) \\bm{X}_{b}^{(k)} bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT , sample M glo M_{\\mathrm{glo}} itali", "snippet": "For each data point ùëø b ( k ) \\bm{X}_{b}^{(k)} bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT , sample M glo M_{\\mathrm{glo}} italic_M start_POSTSUBSCRIPT roman_glo end_POSTSUBSCRIPT global views v b , g ( k ) , i v_{b,g}^{(k),i} italic_v start_POSTSUBSCRIPT italic_b , italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT and M loc M_{\\mathrm{loc}} italic_M start_POSTSUBSCRIPT roman_loc end_POSTSUBSCRIPT local views v b , ‚Ñì ( k ) , i v_{b,\\ell}^{(k),i} italic_v start_POSTSUBSCRIPT italic"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.I4.i3.p1", "title": "For each local view ùëø b , ‚Ñì ( k ) , i \\bm{X}_{b,\\ell}^{(k),i} bold_italic_X start_POSTSUBSCRIPT italic_b , roman_‚Ñì end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT c", "snippet": "For each local view ùëø b , ‚Ñì ( k ) , i \\bm{X}_{b,\\ell}^{(k),i} bold_italic_X start_POSTSUBSCRIPT italic_b , roman_‚Ñì end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT compute the following quantities: ùíõ Œ∏ s ‚Äã ( ùëø b , ‚Ñì ( k ) , i ) ‚âê ( f Œ∏ s ext ‚àò f Œ∏ s ) ‚Äã ( ùëø b , ‚Ñì ( k ) , i ) , ùíë Œ∏ s , ùëæ s ‚Äã ( ùëø b , ‚Ñì ( k ) , i ) ‚âê h ùëæ s , ùüé m ‚Äã ( ùíõ Œ∏ s ‚Äã ( ùëø b , ‚Ñì ( k ) , i ‚Äã ( Œ∏ ) ) ) \\bm{z}_{\\theta_{\\mathrm{s}}}(\\bm{X}_{b,\\ell}^{(k),i})\\doteq(f_{\\theta_{\\mathrm{s}}}^{\\mathrm{ext}}\\circ f_{\\theta_{\\mathrm{s}}})(\\bm{X}_{b,\\ell}^{(k),i}),\\qquad\\bm{p}_{\\theta_{\\mathrm{s}},\\bm{W"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.I4.i4.p1", "title": "Compute the surrogate, approximate loss ‚Ñí ^ DINO ‚àí st ( k ) \\hat{\\mathcal{L}}_{\\mathrm{DINO}-\\mathrm{s}\\mathrm{t}}^{(k)} over^ start_ARG caligraphic_L end_ARG start_POSTSUBSCRIPT roman_DINO - roman_st", "snippet": "Compute the surrogate, approximate loss ‚Ñí ^ DINO ‚àí st ( k ) \\hat{\\mathcal{L}}_{\\mathrm{DINO}-\\mathrm{s}\\mathrm{t}}^{(k)} over^ start_ARG caligraphic_L end_ARG start_POSTSUBSCRIPT roman_DINO - roman_st end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT , defined as follows: ‚Ñí ^ DINO ‚àí st ( k ) ‚Äã ( Œ∏ s , Œ∏ t , ùëæ s , ùëæ t , ùùÅ ) ‚âê 1 B ‚Äã M glo ‚Äã ( M glo + M loc ‚àí 1 ) ‚Äã ‚àë b = 1 B ‚àë i = 1 M glo \\displaystyle\\hat{\\mathcal{L}}_{\\mathrm{DINO}{}-\\mathrm{s}\\mathrm{t}}^{(k)}(\\theta_{\\mathrm{s}},\\theta_{\\mathrm{t}},\\bm{W}_{\\mathrm{s}},\\bm{W}_{\\mathrm{t}},\\bm{\\mu})\\doteq\\frac{1}{BM_{\\mat"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.I4.i5.p1", "title": "Update the student parameters Œ∏ s \\theta_{\\mathrm{s}} italic_Œ∏ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT and ùëæ s \\bm{W}_{\\mathrm{s}} bold_italic_W start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT via", "snippet": "Update the student parameters Œ∏ s \\theta_{\\mathrm{s}} italic_Œ∏ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT and ùëæ s \\bm{W}_{\\mathrm{s}} bold_italic_W start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT via an iterative gradient-based optimization algorithm, and update Œ∏ t \\theta_{\\mathrm{t}} italic_Œ∏ start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT , ùëæ t \\bm{W}_{\\mathrm{t}} bold_italic_W start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT , and ùùÅ \\bm{\\mu} bold_italic_Œº via exponential moving averages with decay parameters ŒΩ ( k ) \\nu^{(k)} italic_ŒΩ start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT , ŒΩ "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS4.SSS0.Px1.p3", "title": "The way that ŒΩ \\nu italic_ŒΩ and œÅ \\rho italic_œÅ change over the optimization trajectory (i.e., the functions k ‚Ü¶ ŒΩ ( k ) k\\mapsto\\nu^{(k)} italic_k ‚Ü¶ italic_ŒΩ start_POSTSUPERSCRIPT ( italic_k ) end_PO", "snippet": "The way that ŒΩ \\nu italic_ŒΩ and œÅ \\rho italic_œÅ change over the optimization trajectory (i.e., the functions k ‚Ü¶ ŒΩ ( k ) k\\mapsto\\nu^{(k)} italic_k ‚Ü¶ italic_ŒΩ start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT and k ‚Ü¶ œÅ ( k ) k\\mapsto\\rho^{(k)} italic_k ‚Ü¶ italic_œÅ start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ) are hyperparameters or design decisions, with ŒΩ ( 1 ) < 1 \\nu^{(1)}<1 italic_ŒΩ start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT < 1 and lim k ‚Üí ‚àû ŒΩ ( k ) = 1 \\lim_{k\\to\\infty}\\nu^{(k)}=1 roman_lim start_POSTSUBSCRIPT italic_k ‚Üí ‚àû end_POSTSUBSCRIPT italic_ŒΩ start_POSTSUPERSCR"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS4.SSS0.Px1.p4", "title": "Using the surrogate (‚Äúempirical‚Äù) loss transforms our intractable optimization problem, as in optimizing the loss in ( 7.2.24 ), into a tractable stochastic optimization problem which is run to train ", "snippet": "Using the surrogate (‚Äúempirical‚Äù) loss transforms our intractable optimization problem, as in optimizing the loss in ( 7.2.24 ), into a tractable stochastic optimization problem which is run to train essentially every deep learning model in the world. This conversion is extremely natural once you have seen some examples, and we will hopefully give these examples throughout the chapter."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS4.SSS0.Px2.p1", "title": "The simplified DINO population-level objective is very similar in spirit but much simpler in execution, i.e., ‚Ñí SimDINO ‚àí st ( Œ∏ s , Œ∏ t ) ‚âê ùîº [ d ‚Ñì 2 ( ùíõ Œ∏ t ( ùëø g ) , ùíõ Œ∏ s ( ùëø c ) ) ] ‚àí Œ≥ 2 log det", "snippet": "The simplified DINO population-level objective is very similar in spirit but much simpler in execution, i.e., ‚Ñí SimDINO ‚àí st ( Œ∏ s , Œ∏ t ) ‚âê ùîº [ d ‚Ñì 2 ( ùíõ Œ∏ t ( ùëø g ) , ùíõ Œ∏ s ( ùëø c ) ) ] ‚àí Œ≥ 2 log det ( ùë∞ + d Œµ 2 Cov ( ùíõ Œ∏ s ( ùëø g ) ) ) ) . \\mathcal{L}_{\\mathrm{SimDINO}-\\mathrm{s}\\mathrm{t}}(\\theta_{\\mathrm{s}},\\theta_{\\mathrm{t}})\\doteq\\operatorname{\\mathbb{E}}\\left[d_{\\ell^{2}}(\\bm{z}_{\\theta_{\\mathrm{t}}}(\\bm{X}_{g}),\\bm{z}_{\\theta_{\\mathrm{s}}}(\\bm{X}_{c}))\\right]-\\frac{\\gamma}{2}\\log\\det\\left(\\bm{I}+\\frac{d}{\\varepsilon^{2}}\\operatorname{Cov}(\\bm{z}_{\\theta_{\\mathrm{s}}}(\\bm{X}_{g})))\\rig"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.I5.i1.p1", "title": "Subsample B B italic_B data points from our dataset { ùëø 1 ( k ) , ‚Ä¶ , ùëø B ( k ) } ‚äÇ ‚Ñê \\{\\bm{X}_{1}^{(k)},\\dots,\\bm{X}_{B}^{(k)}\\}\\subset\\mathcal{I} { bold_italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCR", "snippet": "Subsample B B italic_B data points from our dataset { ùëø 1 ( k ) , ‚Ä¶ , ùëø B ( k ) } ‚äÇ ‚Ñê \\{\\bm{X}_{1}^{(k)},\\dots,\\bm{X}_{B}^{(k)}\\}\\subset\\mathcal{I} { bold_italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT , ‚Ä¶ , bold_italic_X start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT } ‚äÇ caligraphic_I ."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.I5.i2.p1", "title": "For each data point ùëø b ( k ) \\bm{X}_{b}^{(k)} bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT , sample M glo M_{\\mathrm{glo}} itali", "snippet": "For each data point ùëø b ( k ) \\bm{X}_{b}^{(k)} bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT , sample M glo M_{\\mathrm{glo}} italic_M start_POSTSUBSCRIPT roman_glo end_POSTSUBSCRIPT global views v b , g ( k ) , i v_{b,g}^{(k),i} italic_v start_POSTSUBSCRIPT italic_b , italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT and M loc M_{\\mathrm{loc}} italic_M start_POSTSUBSCRIPT roman_loc end_POSTSUBSCRIPT local views v b , ‚Ñì ( k ) , i v_{b,\\ell}^{(k),i} italic_v start_POSTSUBSCRIPT italic"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.I5.i3.p1", "title": "For each local view ùëø b , ‚Ñì ( k ) , i \\bm{X}_{b,\\ell}^{(k),i} bold_italic_X start_POSTSUBSCRIPT italic_b , roman_‚Ñì end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT c", "snippet": "For each local view ùëø b , ‚Ñì ( k ) , i \\bm{X}_{b,\\ell}^{(k),i} bold_italic_X start_POSTSUBSCRIPT italic_b , roman_‚Ñì end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) , italic_i end_POSTSUPERSCRIPT compute ùíõ Œ∏ s ‚Äã ( ùëø b , ‚Ñì ( k ) , i ) ‚âê ( f Œ∏ s ext ‚àò f Œ∏ s ) ‚Äã ( ùëø b , ‚Ñì ( k ) , i ) \\bm{z}_{\\theta_{\\mathrm{s}}}(\\bm{X}_{b,\\ell}^{(k),i})\\doteq(f_{\\theta_{\\mathrm{s}}}^{\\mathrm{ext}}\\circ f_{\\theta_{\\mathrm{s}}})(\\bm{X}_{b,\\ell}^{(k),i}) bold_italic_z start_POSTSUBSCRIPT italic_Œ∏ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b , roman_‚Ñì"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.I5.i4.p1", "title": "Compute the surrogate, approximate loss ‚Ñí ^ SimDINO ‚àí st ( k ) \\hat{\\mathcal{L}}_{\\mathrm{SimDINO}-\\mathrm{s}\\mathrm{t}}^{(k)} over^ start_ARG caligraphic_L end_ARG start_POSTSUBSCRIPT roman_SimDINO -", "snippet": "Compute the surrogate, approximate loss ‚Ñí ^ SimDINO ‚àí st ( k ) \\hat{\\mathcal{L}}_{\\mathrm{SimDINO}-\\mathrm{s}\\mathrm{t}}^{(k)} over^ start_ARG caligraphic_L end_ARG start_POSTSUBSCRIPT roman_SimDINO - roman_st end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT , defined as follows: ‚Ñí ^ SimDINO ‚àí st ( k ) ( Œ∏ s , Œ∏ t ) ‚âê 1 B ‚Äã M glo ‚Äã ( M glo + M loc ‚àí 1 ) ‚àë b = 1 B ‚àë i = 1 M glo [ ‚àë j = 1 M loc d ‚Ñì 2 ( ùíõ Œ∏ t ( ùëø b , g ( k ) , i ) , ùíõ Œ∏ s ( ùëø b , ‚Ñì ( k ) , j ) ) \\displaystyle\\hat{\\mathcal{L}}_{\\mathrm{SimDINO}{}-\\mathrm{s}\\mathrm{t}}^{(k)}(\\theta_{\\mathrm{s}},\\theta_{\\math"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.I5.i5.p1", "title": "Update the student parameters Œ∏ s \\theta_{\\mathrm{s}} italic_Œ∏ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT via an iterative gradient-based optimization algorithm, and update Œ∏ t \\theta_{\\mathrm{t}} ", "snippet": "Update the student parameters Œ∏ s \\theta_{\\mathrm{s}} italic_Œ∏ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT via an iterative gradient-based optimization algorithm, and update Œ∏ t \\theta_{\\mathrm{t}} italic_Œ∏ start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT via an exponential moving average with decay parameter ŒΩ ( k ) \\nu^{(k)} italic_ŒΩ start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT , i.e., Œ∏ s ( k + 1 ) \\displaystyle\\theta_{\\mathrm{s}}^{(k+1)} italic_Œ∏ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k + 1 ) end_POSTSUPERSCRIPT = OptUpdate ( k ) ‚Äã ( Œ∏ s ( k )"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS5.p1", "title": "There are several ways to evaluate a trained transformer model. We highlight two in this section. Let us define the center crop view v cc : ‚Ñê ‚Üí ‚Ñê v_{\\mathrm{cc}}\\colon\\mathcal{I}\\to\\mathcal{I} italic_", "snippet": "There are several ways to evaluate a trained transformer model. We highlight two in this section. Let us define the center crop view v cc : ‚Ñê ‚Üí ‚Ñê v_{\\mathrm{cc}}\\colon\\mathcal{I}\\to\\mathcal{I} italic_v start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT : caligraphic_I ‚Üí caligraphic_I which is a a deterministic resized crop : ‚Ä¢ it resizes the image so that the shortest edge is of size S rsz S_{\\mathrm{rsz}} italic_S start_POSTSUBSCRIPT roman_rsz end_POSTSUBSCRIPT (similar to random resized crops with area percentage parameter 1 1 1 ); ‚Ä¢ then it takes the center S cc √ó S cc S_{\\mathrm{cc}}\\times S_{"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.I6.i1.p1", "title": "it resizes the image so that the shortest edge is of size S rsz S_{\\mathrm{rsz}} italic_S start_POSTSUBSCRIPT roman_rsz end_POSTSUBSCRIPT (similar to random resized crops with area percentage paramete", "snippet": "it resizes the image so that the shortest edge is of size S rsz S_{\\mathrm{rsz}} italic_S start_POSTSUBSCRIPT roman_rsz end_POSTSUBSCRIPT (similar to random resized crops with area percentage parameter 1 1 1 );"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.I6.i2.p1", "title": "then it takes the center S cc √ó S cc S_{\\mathrm{cc}}\\times S_{\\mathrm{cc}} italic_S start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT √ó italic_S start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT crop;", "snippet": "then it takes the center S cc √ó S cc S_{\\mathrm{cc}}\\times S_{\\mathrm{cc}} italic_S start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT √ó italic_S start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT crop;"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS5.SSS0.Px1.p1", "title": "The first, and most architecture-agnostic, way to evaluate an encoder model ùëø ‚Ü¶ ùíõ Œ∏ ‚Äã ( ùëø ) \\bm{X}\\mapsto\\bm{z}_{\\theta}(\\bm{X}) bold_italic_X ‚Ü¶ bold_italic_z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBS", "snippet": "The first, and most architecture-agnostic, way to evaluate an encoder model ùëø ‚Ü¶ ùíõ Œ∏ ‚Äã ( ùëø ) \\bm{X}\\mapsto\\bm{z}_{\\theta}(\\bm{X}) bold_italic_X ‚Ü¶ bold_italic_z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( bold_italic_X ) is to employ linear probing . Linear probing is, in a sentence, running logistic regression on the aggregate features computed by the encoder. This tells us how much semantic information exists in the representations, as well as how easily extractable this information is. (That is: to what extent do the features of images with different semantics live on different subspaces"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS5.SSS0.Px1.p2", "title": "More formally, let us suppose that we want to evaluate the quality and faithfulness of the features of the encoder on image-label data ( ùëø , ùíö ) (\\bm{X},\\bm{y}) ( bold_italic_X , bold_italic_y ) , whe", "snippet": "More formally, let us suppose that we want to evaluate the quality and faithfulness of the features of the encoder on image-label data ( ùëø , ùíö ) (\\bm{X},\\bm{y}) ( bold_italic_X , bold_italic_y ) , where there are N cls N_{\\mathrm{cls}} italic_N start_POSTSUBSCRIPT roman_cls end_POSTSUBSCRIPT classes and ùíö ‚àà { 0 , 1 } N cls \\bm{y}\\in\\{0,1\\}^{N_{\\mathrm{cls}}} bold_italic_y ‚àà { 0 , 1 } start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT roman_cls end_POSTSUBSCRIPT end_POSTSUPERSCRIPT is a ‚Äúone-hot encoding‚Äù (namely, zeros in all positions except a 1 1 1 in the i th i^{\\textnormal{th}} italic_i st"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS5.SSS0.Px2.p1", "title": "We can also evaluate the performance of the features on classification tasks without needing to explicitly train a classifier by using the k k italic_k -nearest neighbor algorithm to get an average pr", "snippet": "We can also evaluate the performance of the features on classification tasks without needing to explicitly train a classifier by using the k k italic_k -nearest neighbor algorithm to get an average predicted label. Namely, given a dataset { ùíõ b } b = 1 B ‚äÜ ‚Ñù d \\{\\bm{z}_{b}\\}_{b=1}^{B}\\subseteq\\mathbb{R}^{d} { bold_italic_z start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_b = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT ‚äÜ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , define the k k italic_k -nearest neighbors of another po"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS5.SSS0.Px3.p1", "title": "Another way to check the performance of the representations, for a transformer-based encoder, is to examine the fidelity of the attention maps ùë® L , k ‚àà ‚Ñù n √ó n \\bm{A}^{L,k}\\in\\mathbb{R}^{n\\times n} b", "snippet": "Another way to check the performance of the representations, for a transformer-based encoder, is to examine the fidelity of the attention maps ùë® L , k ‚àà ‚Ñù n √ó n \\bm{A}^{L,k}\\in\\mathbb{R}^{n\\times n} bold_italic_A start_POSTSUPERSCRIPT italic_L , italic_k end_POSTSUPERSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT italic_n √ó italic_n end_POSTSUPERSCRIPT as defined in Equation 7.2.19 , at the last layer L L italic_L , and given by the following pipeline: ùëø ‚Ü¶ ‚ãØ ‚Ü¶ ùíÅ L ‚àí 1 = [ ùíõ 1 L ‚àí 1 ‚èü class token , ùíõ 2 L ‚àí 1 ‚Äã ‚Ä¶ , ùíõ n L ‚àí 1 ‚èü patch tokens ] ‚Ü¶ ùë® k , L = [ ùë® 1 , 1 k , L ùë® 1 , 2 : k , L ùë® 2 ‚Å£ : , 1 k"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS5.SSS0.Px4.p1", "title": "We can evaluate how the representations capture the fine-grained (i.e., smaller or more detailed) properties of the input by using them for semantic segmentation . Roughly, this means that we use the ", "snippet": "We can evaluate how the representations capture the fine-grained (i.e., smaller or more detailed) properties of the input by using them for semantic segmentation . Roughly, this means that we use the features to construct bounding boxes for all objects in the input. There are several ways to do this, and several ways to score the resulting bounding boxes compared to a ground-truth. Each combination of methods corresponds to a particular segmentation metric. We do not formally describe them here as they are not particularly insightful, but the DINO paper [ CTM+21 ] and DINOv2 paper [ ODM+23 ] c"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS6.p1", "title": "Since SimDINO is directly built upon DINO, we compare the optimal settings for DINO as given by their original paper [ CTM+21 ] with the same settings applied to SimDINO for fair comparison.", "snippet": "Since SimDINO is directly built upon DINO, we compare the optimal settings for DINO as given by their original paper [ CTM+21 ] with the same settings applied to SimDINO for fair comparison."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS6.SSS0.Px1.p1", "title": "We use 10 10 10 local views (i.e., M loc = 10 M_{\\mathrm{loc}}=10 italic_M start_POSTSUBSCRIPT roman_loc end_POSTSUBSCRIPT = 10 ) of resolution 96 √ó 96 96\\times 96 96 √ó 96 (i.e., S loc = 96 S_{\\mathrm", "snippet": "We use 10 10 10 local views (i.e., M loc = 10 M_{\\mathrm{loc}}=10 italic_M start_POSTSUBSCRIPT roman_loc end_POSTSUBSCRIPT = 10 ) of resolution 96 √ó 96 96\\times 96 96 √ó 96 (i.e., S loc = 96 S_{\\mathrm{loc}}=96 italic_S start_POSTSUBSCRIPT roman_loc end_POSTSUBSCRIPT = 96 ) and 2 2 2 global views (i.e., M glo = 2 M_{\\mathrm{glo}}=2 italic_M start_POSTSUBSCRIPT roman_glo end_POSTSUBSCRIPT = 2 ) of resolution 224 √ó 224 224\\times 224 224 √ó 224 (i.e., S glo = 224 S_{\\mathrm{glo}}=224 italic_S start_POSTSUBSCRIPT roman_glo end_POSTSUBSCRIPT = 224 ) for all experiments. The corresponding portions of "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS6.SSS0.Px2.p1", "title": "For all inputs we set the patch size to be 16 √ó 16 16\\times 16 16 √ó 16 (namely, P H = P W = 16 P_{H}=P_{W}=16 italic_P start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT = italic_P start_POSTSUBSCRIPT ita", "snippet": "For all inputs we set the patch size to be 16 √ó 16 16\\times 16 16 √ó 16 (namely, P H = P W = 16 P_{H}=P_{W}=16 italic_P start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT = italic_P start_POSTSUBSCRIPT italic_W end_POSTSUBSCRIPT = 16 ). We use the small, base, and large models of the ViT [ DBK+21 ] architecture as the embedding and backbone. The feature extractor is a three-layer MLP with a hidden size of 2048 2048 2048 and an output dimension of 256 256 256 , followed by an ‚Ñì 2 \\ell^{2} roman_‚Ñì start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT -normalization, as specified in Section 7.2.3 . For DINO arc"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS6.SSS0.Px3.p1", "title": "For pre-training, both our DINO reproduction and SimDINO use the ImageNet-1K dataset across all methods. We use AdamW [ LH17 ] as the optimizer, which is a very standard choice. We follow the followin", "snippet": "For pre-training, both our DINO reproduction and SimDINO use the ImageNet-1K dataset across all methods. We use AdamW [ LH17 ] as the optimizer, which is a very standard choice. We follow the following hyperparameter recommendations: ‚Ä¢ The batch size is B = 1024 B=1024 italic_B = 1024 . ‚Ä¢ The learning rate (for AdamW and the student model) has ‚Äúbase‚Äù value 2 √ó 10 ‚àí 3 2\\times 10^{-3} 2 √ó 10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT . In the first 10 10 10 epochs the learning rate linearly increases from 0 to the base value (i.e., at the i th i^{\\mathrm{th}} italic_i start_POSTSUPERSCRIPT ro"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.I7.i1.p1", "title": "The batch size is B = 1024 B=1024 italic_B = 1024 .", "snippet": "The batch size is B = 1024 B=1024 italic_B = 1024 ."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.I7.i2.p1", "title": "The learning rate (for AdamW and the student model) has ‚Äúbase‚Äù value 2 √ó 10 ‚àí 3 2\\times 10^{-3} 2 √ó 10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT . In the first 10 10 10 epochs the learning rate li", "snippet": "The learning rate (for AdamW and the student model) has ‚Äúbase‚Äù value 2 √ó 10 ‚àí 3 2\\times 10^{-3} 2 √ó 10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT . In the first 10 10 10 epochs the learning rate linearly increases from 0 to the base value (i.e., at the i th i^{\\mathrm{th}} italic_i start_POSTSUPERSCRIPT roman_th end_POSTSUPERSCRIPT epoch the learning rate is ( i / 10 ) ‚ãÖ 2 √ó 10 ‚àí 3 (i/10)\\cdot 2\\times 10^{-3} ( italic_i / 10 ) ‚ãÖ 2 √ó 10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT , for 1 ‚â§ i ‚â§ 10 1\\leq i\\leq 10 1 ‚â§ italic_i ‚â§ 10 ). Then over the next 90 90 90 epochs the learning rate decay"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.I7.i3.p1", "title": "The weight decay (the W in AdamW) follows a cosine schedule from 0.04 to 0.4 0.4 0.4 over training.", "snippet": "The weight decay (the W in AdamW) follows a cosine schedule from 0.04 to 0.4 0.4 0.4 over training."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.I7.i4.p1", "title": "The EMA rate ŒΩ \\nu italic_ŒΩ follows a cosine schedule from 0.996 0.996 0.996 to 1.0 1.0 1.0 over training. Specifically for DINO, the centering EMA rate œÅ \\rho italic_œÅ is fixed at 0.9 0.9 0.9 .", "snippet": "The EMA rate ŒΩ \\nu italic_ŒΩ follows a cosine schedule from 0.996 0.996 0.996 to 1.0 1.0 1.0 over training. Specifically for DINO, the centering EMA rate œÅ \\rho italic_œÅ is fixed at 0.9 0.9 0.9 ."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.I7.i5.p1", "title": "Specifically for DINO, the teacher temperature œÑ t \\tau_{\\mathrm{t}} italic_œÑ start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT is fixed at 0.1 0.1 0.1 , while the student temperature œÑ s \\tau_{\\mathrm{s}", "snippet": "Specifically for DINO, the teacher temperature œÑ t \\tau_{\\mathrm{t}} italic_œÑ start_POSTSUBSCRIPT roman_t end_POSTSUBSCRIPT is fixed at 0.1 0.1 0.1 , while the student temperature œÑ s \\tau_{\\mathrm{s}} italic_œÑ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT linearly increases from 0.04 0.04 0.04 to 0.07 0.07 0.07 during the first 30 30 30 epochs and is fixed at 0.07 0.07 0.07 thereafter."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS6.SSS0.Px3.p2", "title": "For linear probing, the linear probe is usually trained using the AdamW optimizer with learning rate 2 √ó 10 ‚àí 4 2\\times 10^{-4} 2 √ó 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT , weight decay 0.01", "snippet": "For linear probing, the linear probe is usually trained using the AdamW optimizer with learning rate 2 √ó 10 ‚àí 4 2\\times 10^{-4} 2 √ó 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT , weight decay 0.01 0.01 0.01 , and batch size 512 512 512 , but these are often modified on a case-by-case basis to minimize the loss."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.SS6.SSS0.Px4.p1", "title": "In terms of downsream classification performance, we obtain the performance in Table 7.1 . We observe that the performance of SimDINO is much higher than in DINO under fair comparison. Also, it is muc", "snippet": "In terms of downsream classification performance, we obtain the performance in Table 7.1 . We observe that the performance of SimDINO is much higher than in DINO under fair comparison. Also, it is much more stable: the prescribed settings of DINO cannot train a ViT-L(arge) model. On the other hand, Figure 7.10 shows visualizations of the average saliency maps in DINO and our simplified DINO, observing that the saliency maps look quite similar across models, indicating that the models learn features which are at least as good at capturing fine-grained details. The segmentation and object detect"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.p1", "title": "In the previous section, we simplified an overly complex learning objective using our intuition of representation learning through the lens of compression. However, many of the most popular learning p", "snippet": "In the previous section, we simplified an overly complex learning objective using our intuition of representation learning through the lens of compression. However, many of the most popular learning procedures are incredibly simple. In these cases, it is difficult to further simplify the objective. Thus, in this and future sections, we will focus on principled ways to modify the deep network architectures for a variety of tasks."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.p2", "title": "Let us first start with arguably the most classical task in machine learning: image classification , which is often used as a standard task to evaluate pattern recognition algorithms or deep network a", "snippet": "Let us first start with arguably the most classical task in machine learning: image classification , which is often used as a standard task to evaluate pattern recognition algorithms or deep network architectures. From our discussion of white-box architectures in Chapter 4 , we only need a semantically meaningful task to learn good representations with white-box architectures. We will validate this idea in this section."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.p3", "title": "First, the dataset stays largely the same as Section 7.2.1 . Both the training and test data consist of labeled images, i.e., image-label pairs ( ùëø , ùíö ) ‚àà ‚Ñù C √ó H √ó W √ó { 0 , 1 } N cls (\\bm{X},\\bm{y}", "snippet": "First, the dataset stays largely the same as Section 7.2.1 . Both the training and test data consist of labeled images, i.e., image-label pairs ( ùëø , ùíö ) ‚àà ‚Ñù C √ó H √ó W √ó { 0 , 1 } N cls (\\bm{X},\\bm{y})\\in\\mathbb{R}^{C\\times H\\times W}\\times\\{0,1\\}^{N_{\\mathrm{cls}}} ( bold_italic_X , bold_italic_y ) ‚àà blackboard_R start_POSTSUPERSCRIPT italic_C √ó italic_H √ó italic_W end_POSTSUPERSCRIPT √ó { 0 , 1 } start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT roman_cls end_POSTSUBSCRIPT end_POSTSUPERSCRIPT . We still apply various data augmentations (e.g., flips, Gaussian blurring, solarization, etc.) to "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.SS1.p1", "title": "Unlike before, our task is not just to learn a good representation of the data, but also simultaneously build a classifier. Formally, we have labeled data pairs ( ùëø , ùíö ) (\\bm{X},\\bm{y}) ( bold_italic", "snippet": "Unlike before, our task is not just to learn a good representation of the data, but also simultaneously build a classifier. Formally, we have labeled data pairs ( ùëø , ùíö ) (\\bm{X},\\bm{y}) ( bold_italic_X , bold_italic_y ) , where ùíö ‚àà { 0 , 1 } N cls \\bm{y}\\in\\{0,1\\}^{N_{\\mathrm{cls}}} bold_italic_y ‚àà { 0 , 1 } start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT roman_cls end_POSTSUBSCRIPT end_POSTSUPERSCRIPT is a one-hot vector denoting the class membership of ùëø \\bm{X} bold_italic_X . We consider a deterministic center crop view v cc v_{\\mathrm{cc}} italic_v start_POSTSUBSCRIPT roman_cc end_POST"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.SS2.p1", "title": "The architecture that we use is the CRATE architecture, described in some level of detail in Chapter 4 . The overall setup is similar to that of the regular transformer in Section 7.2.3 , with a few c", "snippet": "The architecture that we use is the CRATE architecture, described in some level of detail in Chapter 4 . The overall setup is similar to that of the regular transformer in Section 7.2.3 , with a few changes. While the embedding step is the same as both DINO and SimDINO in Section 7.2.3 , the feature extraction step is the same as SimDINO in Section 7.2.3 as it just extracts the feature corresponding to the class token, and the classification head is described in Section 7.3.1 , the backbone architecture is different. Each layer takes the form ùíÅ Œ∏ ‚Ñì + 1 / 2 ‚Äã ( ùëø ) \\displaystyle\\bm{Z}_{\\theta}^"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.I1.i1.p1", "title": "The MSSA \\operatorname{MSSA} roman_MSSA operator is multi-head-subspace-self-attention, defined as follows: MSSA Œ∏ ‚Ñì ‚Å° ( ùíÅ ) ‚âê ùëº out ‚Ñì ‚Äã [ SA ‚Å° ( [ ùëº 1 , ‚Ñì ] ‚ä§ ‚Äã ùíÅ , [ ùëº 1 , ‚Ñì ] ‚ä§ ‚Äã ùíÅ , [ ùëº 1 , ‚Ñì ] ‚ä§ ", "snippet": "The MSSA \\operatorname{MSSA} roman_MSSA operator is multi-head-subspace-self-attention, defined as follows: MSSA Œ∏ ‚Ñì ‚Å° ( ùíÅ ) ‚âê ùëº out ‚Ñì ‚Äã [ SA ‚Å° ( [ ùëº 1 , ‚Ñì ] ‚ä§ ‚Äã ùíÅ , [ ùëº 1 , ‚Ñì ] ‚ä§ ‚Äã ùíÅ , [ ùëº 1 , ‚Ñì ] ‚ä§ ‚Äã ùíÅ ) ‚ãÆ SA ‚Å° ( [ ùëº K , ‚Ñì ] ‚ä§ ‚Äã ùíÅ , [ ùëº K , ‚Ñì ] ‚ä§ ‚Äã ùíÅ , [ ùëº 1 , ‚Ñì ] ‚ä§ ‚Äã ùíÅ ) ] + ùíÉ out ‚Ñì ‚Äã ùüè n ‚ä§ \\operatorname{MSSA}_{\\theta}^{\\ell}(\\bm{Z})\\doteq\\bm{U}_{\\mathrm{out}}^{\\ell}\\begin{bmatrix}\\operatorname{SA}([\\bm{U}^{1,\\ell}]^{\\top}\\bm{Z},[\\bm{U}^{1,\\ell}]^{\\top}\\bm{Z},[\\bm{U}^{1,\\ell}]^{\\top}\\bm{Z})\\\\ \\vdots\\\\ \\operatorname{SA}([\\bm{U}^{K,\\ell}]^{\\top}\\bm{Z},[\\bm{U}^{K,\\ell}]^{\\top}\\bm{Z},[\\bm{U}^{1"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.I1.i2.p1", "title": "The ISTA \\operatorname{ISTA} roman_ISTA operator is the iterative-shrinkage-thresholding-algorithm operator, defined as follows: ISTA Œ∏ ‚Ñì ‚Å° ( ùíÅ ) ‚âê ReLU ‚Å° ( ùíÅ ‚àí Œ≤ ‚Äã ( ùë´ ‚Ñì ) ‚ä§ ‚Äã ( ùë´ ‚Ñì ‚Äã ùíÅ ‚àí ùíÅ ) + Œ≤ ‚Äã Œª", "snippet": "The ISTA \\operatorname{ISTA} roman_ISTA operator is the iterative-shrinkage-thresholding-algorithm operator, defined as follows: ISTA Œ∏ ‚Ñì ‚Å° ( ùíÅ ) ‚âê ReLU ‚Å° ( ùíÅ ‚àí Œ≤ ‚Äã ( ùë´ ‚Ñì ) ‚ä§ ‚Äã ( ùë´ ‚Ñì ‚Äã ùíÅ ‚àí ùíÅ ) + Œ≤ ‚Äã Œª ‚Äã ùüè d ‚Äã ùüè n ‚ä§ ) , \\operatorname{ISTA}_{\\theta}^{\\ell}(\\bm{Z})\\doteq\\operatorname{ReLU}(\\bm{Z}-\\beta(\\bm{D}^{\\ell})^{\\top}(\\bm{D}^{\\ell}\\bm{Z}-\\bm{Z})+\\beta\\lambda\\bm{1}_{d}\\bm{1}_{n}^{\\top}), roman_ISTA start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ( bold_italic_Z ) ‚âê roman_ReLU ( bold_italic_Z - italic_Œ≤ ( bold_italic_D start_POSTSUPERSCRIPT rom"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.SS2.p2", "title": "We call this architecture CRATE, and a layer of the backbone is depicted in Figure 4.13 . CRATE models, on top of being interpretable, are generally also highly performant as well as parameter-efficie", "snippet": "We call this architecture CRATE, and a layer of the backbone is depicted in Figure 4.13 . CRATE models, on top of being interpretable, are generally also highly performant as well as parameter-efficient."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.SS3.p1", "title": "We train our classifier using a simple end-to-end stochastic optimization procedure, where we sub-sample data and views, compute the average loss and its gradient over these samples, and use an optimi", "snippet": "We train our classifier using a simple end-to-end stochastic optimization procedure, where we sub-sample data and views, compute the average loss and its gradient over these samples, and use an optimization algorithm to change the parameters. At each timestep k k italic_k , we: ‚Ä¢ Subsample B B italic_B different labeled samples { ( ùëø b ( k ) , ùíö b ( k ) ) } b = 1 B ‚äÜ ‚Ñê √ó { 0 , 1 } N cls \\{(\\bm{X}_{b}^{(k)},\\bm{y}_{b}^{(k)})\\}_{b=1}^{B}\\subseteq\\mathcal{I}\\times\\{0,1\\}^{N_{\\mathrm{cls}}} { ( bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POST"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.I2.i1.p1", "title": "Subsample B B italic_B different labeled samples { ( ùëø b ( k ) , ùíö b ( k ) ) } b = 1 B ‚äÜ ‚Ñê √ó { 0 , 1 } N cls \\{(\\bm{X}_{b}^{(k)},\\bm{y}_{b}^{(k)})\\}_{b=1}^{B}\\subseteq\\mathcal{I}\\times\\{0,1\\}^{N_{\\mat", "snippet": "Subsample B B italic_B different labeled samples { ( ùëø b ( k ) , ùíö b ( k ) ) } b = 1 B ‚äÜ ‚Ñê √ó { 0 , 1 } N cls \\{(\\bm{X}_{b}^{(k)},\\bm{y}_{b}^{(k)})\\}_{b=1}^{B}\\subseteq\\mathcal{I}\\times\\{0,1\\}^{N_{\\mathrm{cls}}} { ( bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT , bold_italic_y start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ) } start_POSTSUBSCRIPT italic_b = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT ‚äÜ caligraphic_I √ó { 0 , 1 } start_POSTSUPERSC"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.I2.i2.p1", "title": "For each labeled sample ( ùëø b ( k ) , ùíö b ( k ) ) (\\bm{X}_{b}^{(k)},\\bm{y}_{b}^{(k)}) ( bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCR", "snippet": "For each labeled sample ( ùëø b ( k ) , ùíö b ( k ) ) (\\bm{X}_{b}^{(k)},\\bm{y}_{b}^{(k)}) ( bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT , bold_italic_y start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ) , compute the central crop view v b , cc ( k ) v_{b,\\mathrm{cc}}^{(k)} italic_v start_POSTSUBSCRIPT italic_b , roman_cc end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT and apply it to ùëø b ( k ) \\bm{X}_{b}^{(k)} bold_italic_X start_POSTSUBSCRIPT italic"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.I2.i3.p1", "title": "Compute the predictions ùíë Œ∏ ‚Äã ( ùëø b , cc ( k ) ) ‚âê ( h Œ∏ ‚àò f Œ∏ ext ‚àò f Œ∏ ) ‚Äã ( ùëø b , cc ( k ) ) \\bm{p}_{\\theta}(\\bm{X}_{b,\\mathrm{cc}}^{(k)})\\doteq(h_{\\theta}\\circ f_{\\theta}^{\\mathrm{ext}}\\circ f_{\\t", "snippet": "Compute the predictions ùíë Œ∏ ‚Äã ( ùëø b , cc ( k ) ) ‚âê ( h Œ∏ ‚àò f Œ∏ ext ‚àò f Œ∏ ) ‚Äã ( ùëø b , cc ( k ) ) \\bm{p}_{\\theta}(\\bm{X}_{b,\\mathrm{cc}}^{(k)})\\doteq(h_{\\theta}\\circ f_{\\theta}^{\\mathrm{ext}}\\circ f_{\\theta})(\\bm{X}_{b,\\mathrm{cc}}^{(k)}) bold_italic_p start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b , roman_cc end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ) ‚âê ( italic_h start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ‚àò italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT ‚àò"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.I2.i4.p1", "title": "Form the surrogate stochastic loss ‚Ñí ^ CE ( k ) ‚Äã ( Œ∏ ) ‚âê 1 B ‚Äã ‚àë b = 1 B CE ‚Å° ( ùíö b ( k ) , ùíë Œ∏ ‚Äã ( ùëø b , cc ( k ) ) ) . \\hat{\\mathcal{L}}_{\\operatorname{CE}}^{(k)}(\\theta)\\doteq\\frac{1}{B}\\sum_{b=1}", "snippet": "Form the surrogate stochastic loss ‚Ñí ^ CE ( k ) ‚Äã ( Œ∏ ) ‚âê 1 B ‚Äã ‚àë b = 1 B CE ‚Å° ( ùíö b ( k ) , ùíë Œ∏ ‚Äã ( ùëø b , cc ( k ) ) ) . \\hat{\\mathcal{L}}_{\\operatorname{CE}}^{(k)}(\\theta)\\doteq\\frac{1}{B}\\sum_{b=1}^{B}\\operatorname{CE}(\\bm{y}_{b}^{(k)},\\bm{p}_{\\theta}(\\bm{X}_{b,\\mathrm{cc}}^{(k)})). over^ start_ARG caligraphic_L end_ARG start_POSTSUBSCRIPT roman_CE end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_Œ∏ ) ‚âê divide start_ARG 1 end_ARG start_ARG italic_B end_ARG ‚àë start_POSTSUBSCRIPT italic_b = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.I2.i5.p1", "title": "Compute one step of an optimization algorithm on Œ∏ \\theta italic_Œ∏ , giving the following iteration: Œ∏ ( k + 1 ) ‚âê OptUpdate ( k ) ‚Äã ( Œ∏ ( k ) ; ‚àá Œ∏ ‚Ñí ^ CE ( k ) ) . \\theta^{(k+1)}\\doteq\\textsc{OptUpd", "snippet": "Compute one step of an optimization algorithm on Œ∏ \\theta italic_Œ∏ , giving the following iteration: Œ∏ ( k + 1 ) ‚âê OptUpdate ( k ) ‚Äã ( Œ∏ ( k ) ; ‚àá Œ∏ ‚Ñí ^ CE ( k ) ) . \\theta^{(k+1)}\\doteq\\textsc{OptUpdate}^{(k)}(\\theta^{(k)};\\nabla_{\\theta}\\hat{\\mathcal{L}}_{\\operatorname{CE}}^{(k)}). italic_Œ∏ start_POSTSUPERSCRIPT ( italic_k + 1 ) end_POSTSUPERSCRIPT ‚âê OptUpdate start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_Œ∏ start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ; ‚àá start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT over^ start_ARG caligraphic_L end_ARG start_POSTSUBSCRIPT roman"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.SS4.p1", "title": "We use the same evaluation procedure as Section 7.2.5 . To summarize, for all evaluations (as well as training) we use a center crop view v cc v_{\\mathrm{cc}} italic_v start_POSTSUBSCRIPT roman_cc end", "snippet": "We use the same evaluation procedure as Section 7.2.5 . To summarize, for all evaluations (as well as training) we use a center crop view v cc v_{\\mathrm{cc}} italic_v start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT which reshapes the input image and takes a large central crop of size ( C , S cc , S cc ) (C,S_{\\mathrm{cc}},S_{\\mathrm{cc}}) ( italic_C , italic_S start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT ) where C C italic_C is the number of channels in the input image. We can then do linear probing, attention map visualization, and d"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.SS5.p1", "title": "Since CRATE is directly based on the transformer, we compare the optimal settings for ViT as given by [ DBK+21 , TCD+20 ] with the same settings applied to CRATE for fair comparison.", "snippet": "Since CRATE is directly based on the transformer, we compare the optimal settings for ViT as given by [ DBK+21 , TCD+20 ] with the same settings applied to CRATE for fair comparison."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.SS5.SSS0.Px1.p1", "title": "The center crop resizes the whole image so that the shorter edge is of size 256 256 256 (i.e., S rsz = 256 S_{\\mathrm{rsz}}=256 italic_S start_POSTSUBSCRIPT roman_rsz end_POSTSUBSCRIPT = 256 ) before ", "snippet": "The center crop resizes the whole image so that the shorter edge is of size 256 256 256 (i.e., S rsz = 256 S_{\\mathrm{rsz}}=256 italic_S start_POSTSUBSCRIPT roman_rsz end_POSTSUBSCRIPT = 256 ) before taking a center crop of size 224 √ó 224 224\\times 224 224 √ó 224 (i.e., S cc = 224 S_{\\mathrm{cc}}=224 italic_S start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT = 224 ), both in evaluation and training. We take patch size 16 16 16 (i.e., P H = P W = 16 P_{H}=P_{W}=16 italic_P start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT = italic_P start_POSTSUBSCRIPT italic_W end_POSTSUBSCRIPT = 16 ). We use the tin"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.SS5.SSS0.Px2.p1", "title": "For pre-training, we use the ImageNet-1K dataset. We use the LION optimizer [ CLH+24 ] to pre-train both our ViT replication as well as CRATE. We set the base learning rate as 2.4 √ó 10 ‚àí 4 2.4\\times 1", "snippet": "For pre-training, we use the ImageNet-1K dataset. We use the LION optimizer [ CLH+24 ] to pre-train both our ViT replication as well as CRATE. We set the base learning rate as 2.4 √ó 10 ‚àí 4 2.4\\times 10^{-4} 2.4 √ó 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT , the weight decay as 0.5 0.5 0.5 , and batch size as B = 2048 B=2048 italic_B = 2048 . Our learning rate schedule increases the learning rate linearly to the base learning rate over the first 5 5 5 epochs, and decreases to 0 using a cosine schedule over the next 145 145 145 epochs (training all models for 150 150 150 epochs each). For "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.SS5.SSS0.Px2.p2", "title": "For linear probing, we use several evaluation datasets such as CIFAR10, Oxford-Flowers, and Oxford-IIT-Pets. We use the AdamW optimizer to train the linear probe, using learning rate 5 √ó 10 ‚àí 5 5\\time", "snippet": "For linear probing, we use several evaluation datasets such as CIFAR10, Oxford-Flowers, and Oxford-IIT-Pets. We use the AdamW optimizer to train the linear probe, using learning rate 5 √ó 10 ‚àí 5 5\\times 10^{-5} 5 √ó 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT , weight decay 0.01 0.01 0.01 , and batch size B = 256 B=256 italic_B = 256 . We also apply the aforementioned data augmentations to the image data."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.SS5.SSS0.Px3.p1", "title": "Table 7.3 demonstrates that CRATE models achieve parity or improvement compared to the popular Vision Transformer (ViT) architecture at similar parameter counts, at least in terms of the linear separa", "snippet": "Table 7.3 demonstrates that CRATE models achieve parity or improvement compared to the popular Vision Transformer (ViT) architecture at similar parameter counts, at least in terms of the linear separability of their features w.r.t. different classes. In terms of attention map fidelity, Figure 7.11 demonstrates a truly extraordinary result: without needing to train on any segmentation or object detection data, not only do the saliency maps effectively capture all relevant parts of the input image, the saliency maps self-organize to each correspond to a discrete set of concepts, even across samp"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.p1", "title": "We now study causal language modeling , a method for training large language models (LLMs). This is the same setup used to train, among many others, GPT-2 and many more language models.", "snippet": "We now study causal language modeling , a method for training large language models (LLMs). This is the same setup used to train, among many others, GPT-2 and many more language models."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS1.p1", "title": "The data we will use to investigate the performance of CRATE for language tasks will be OpenWebText (OWT) [ GC19 ] , an open-source reproduction of the unreleased WebText dataset used by OpenAI to tra", "snippet": "The data we will use to investigate the performance of CRATE for language tasks will be OpenWebText (OWT) [ GC19 ] , an open-source reproduction of the unreleased WebText dataset used by OpenAI to train GPT2. Each sample in OWT is a web document, typically sourced from high-quality web pages, blogs, articles, or online discussions, that is written in well-formed natural language. The OpenWebText dataset contains around 8.01M documents of varying lengths, totaling around 41.70GB of text. For evaluation, we will use several datasets, such as WikiText [ MXB+16 ] 3 3 3 For WikiText2 and WikiText10"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS1.p2", "title": "On a more formal level, our data ùëø \\bm{X} bold_italic_X will be text, or strings of characters; we let ùíØ \\mathcal{T} caligraphic_T be the set of all strings.", "snippet": "On a more formal level, our data ùëø \\bm{X} bold_italic_X will be text, or strings of characters; we let ùíØ \\mathcal{T} caligraphic_T be the set of all strings."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS2.p1", "title": "For causal language modeling pre-training, the idea is that we want to train the model to output human-like text . The most popular way to do this by far is to use a two-stage training process: 5 5 5 ", "snippet": "For causal language modeling pre-training, the idea is that we want to train the model to output human-like text . The most popular way to do this by far is to use a two-stage training process: 5 5 5 Modern language model training has several additional training steps which demand different data distributions and algorithm approaches. However, training a model to merely mimic human writing only requires these few presented steps. ‚Ä¢ First , we wish to learn a way to optimally encode documents as a sequence of basic (‚Äúbuilding block‚Äù) strings, called tokens . This is called tokenization , and we"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.I1.i1.p1", "title": "First , we wish to learn a way to optimally encode documents as a sequence of basic (‚Äúbuilding block‚Äù) strings, called tokens . This is called tokenization , and we build a tokenizer .", "snippet": "First , we wish to learn a way to optimally encode documents as a sequence of basic (‚Äúbuilding block‚Äù) strings, called tokens . This is called tokenization , and we build a tokenizer ."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.I1.i2.p1", "title": "Second , we wish to learn a way to predict the distribution of a token given all previous tokens . This is called next-token prediction , and we build a language model .", "snippet": "Second , we wish to learn a way to predict the distribution of a token given all previous tokens . This is called next-token prediction , and we build a language model ."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS2.SSSx1.p1", "title": "To build a tokenizer, it amounts to building a vocabulary ùí± \\mathcal{V} caligraphic_V , which is a set of tokens and has some pre-specified size V V italic_V . There are several methods to do this. On", "snippet": "To build a tokenizer, it amounts to building a vocabulary ùí± \\mathcal{V} caligraphic_V , which is a set of tokens and has some pre-specified size V V italic_V . There are several methods to do this. One popular algorithm is known as Byte Pair Encoding (BPE), which can be described as: ‚Ä¢ Start with a list of all unique characters in your training data, and their frequencies. Ensure that there are fewer than V V italic_V such characters, and add each character as a separate string (‚Äútoken‚Äù) to the vocabulary along with its frequency. ‚Ä¢ Until there are V V italic_V tokens in the vocabulary: ‚Äì Cons"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.I2.i1.p1", "title": "Start with a list of all unique characters in your training data, and their frequencies. Ensure that there are fewer than V V italic_V such characters, and add each character as a separate string (‚Äúto", "snippet": "Start with a list of all unique characters in your training data, and their frequencies. Ensure that there are fewer than V V italic_V such characters, and add each character as a separate string (‚Äútoken‚Äù) to the vocabulary along with its frequency."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.I2.i2.p1", "title": "Until there are V V italic_V tokens in the vocabulary: ‚Äì Construct a token by taking the two most frequent existing tokens and merging them. ‚Äì Compute this token‚Äôs frequency in the dataset. ‚Äì Add it t", "snippet": "Until there are V V italic_V tokens in the vocabulary: ‚Äì Construct a token by taking the two most frequent existing tokens and merging them. ‚Äì Compute this token‚Äôs frequency in the dataset. ‚Äì Add it to the vocabulary (along with its frequency)."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.I2.i2.I1.i1.p1", "title": "Construct a token by taking the two most frequent existing tokens and merging them.", "snippet": "Construct a token by taking the two most frequent existing tokens and merging them."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.I2.i2.I1.i2.p1", "title": "Compute this token‚Äôs frequency in the dataset.", "snippet": "Compute this token‚Äôs frequency in the dataset."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.I2.i2.I1.i3.p1", "title": "Add it to the vocabulary (along with its frequency).", "snippet": "Add it to the vocabulary (along with its frequency)."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.I2.i3.p1", "title": "At this point, the frequency information is no longer needed and can be discarded.", "snippet": "At this point, the frequency information is no longer needed and can be discarded."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS2.SSSx1.p2", "title": "After such a vocabulary is built, a tokenizer can break down a document into tokens (i.e., ‚Äútokenize‚Äù it). BPE uses a similar procedure to tokenize data as in training: ‚Ä¢ Separate the document into a ", "snippet": "After such a vocabulary is built, a tokenizer can break down a document into tokens (i.e., ‚Äútokenize‚Äù it). BPE uses a similar procedure to tokenize data as in training: ‚Ä¢ Separate the document into a long list of one-character-long tokens. That is, if the document is ‚ÄúHello‚Äù then the initial list is ‚ÄòH‚Äô, ‚Äòe‚Äô, ‚Äòl‚Äô, ‚Äòl‚Äô, ‚Äòo‚Äô. ‚Ä¢ While any two adjacent tokens can be concatenated and their concatenation is another token, we do it, i.e., we replace this pair of tokens with the merged token. Namely, if ‚ÄòHe‚Äô is a token in the vocabulary, ‚ÄòH‚Äô, ‚Äòe‚Äô, ‚Äòl‚Äô, ‚Äòl‚Äô, ‚Äòo‚Äô would become ‚ÄòHe‚Äô, ‚Äòl‚Äô, ‚Äòl‚Äô, ‚Äòo‚Äô. ‚Ä¢ Repe"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.I3.i1.p1", "title": "Separate the document into a long list of one-character-long tokens. That is, if the document is ‚ÄúHello‚Äù then the initial list is ‚ÄòH‚Äô, ‚Äòe‚Äô, ‚Äòl‚Äô, ‚Äòl‚Äô, ‚Äòo‚Äô.", "snippet": "Separate the document into a long list of one-character-long tokens. That is, if the document is ‚ÄúHello‚Äù then the initial list is ‚ÄòH‚Äô, ‚Äòe‚Äô, ‚Äòl‚Äô, ‚Äòl‚Äô, ‚Äòo‚Äô."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.I3.i2.p1", "title": "While any two adjacent tokens can be concatenated and their concatenation is another token, we do it, i.e., we replace this pair of tokens with the merged token. Namely, if ‚ÄòHe‚Äô is a token in the voca", "snippet": "While any two adjacent tokens can be concatenated and their concatenation is another token, we do it, i.e., we replace this pair of tokens with the merged token. Namely, if ‚ÄòHe‚Äô is a token in the vocabulary, ‚ÄòH‚Äô, ‚Äòe‚Äô, ‚Äòl‚Äô, ‚Äòl‚Äô, ‚Äòo‚Äô would become ‚ÄòHe‚Äô, ‚Äòl‚Äô, ‚Äòl‚Äô, ‚Äòo‚Äô."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.I3.i3.p1", "title": "Repeat the above process until no more merges can be done. At this point, the document is partitioned into the final list (sequence) of tokens.", "snippet": "Repeat the above process until no more merges can be done. At this point, the document is partitioned into the final list (sequence) of tokens."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS2.SSSx1.p3", "title": "There are many practical and efficiency-based considerations to take into account during tokenization. The above algorithm, as presented, is very far from optimal if naively implemented, for instance.", "snippet": "There are many practical and efficiency-based considerations to take into account during tokenization. The above algorithm, as presented, is very far from optimal if naively implemented, for instance. We do not cover this topic in great detail; there are many resources online to learn more, such as HuggingFace tutorials ."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS2.SSSx1.p4", "title": "For instance, each token has a corresponding index which is just its index in the vocabulary (which after all is just a list of length V V italic_V ). Thus, the output of most tokenizers is a list of ", "snippet": "For instance, each token has a corresponding index which is just its index in the vocabulary (which after all is just a list of length V V italic_V ). Thus, the output of most tokenizers is a list of indices, say an element of [ V ] ‚àó [V]^{*} [ italic_V ] start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT . Keep in mind that they correspond to substrings of the original document, as shown above."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS2.SSSx1.p5", "title": "Once a tokenizer is learned, it can be used as a black box by any language model. For instance, many models have the same (OpenAI-based) tokenizer based off of the tiktoken library. In the remainder o", "snippet": "Once a tokenizer is learned, it can be used as a black box by any language model. For instance, many models have the same (OpenAI-based) tokenizer based off of the tiktoken library. In the remainder of this section, we will use such a fixed and pre-built tokenizer for everything, and thus identify each text document ùëø ‚àà ùíØ \\bm{X}\\in\\mathcal{T} bold_italic_X ‚àà caligraphic_T with its tokenized version in [ V ] ‚àó [V]^{*} [ italic_V ] start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT . Therefore, we may as well consider the text space ùíØ \\mathcal{T} caligraphic_T as equal to the space of token sequences ["}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS2.SSSx2.p1", "title": "Once we have each document as a sequence of tokens ùëø ‚àà [ V ] N ‚äÜ [ V ] ‚àó = ùíØ \\bm{X}\\in[V]^{N}\\subseteq[V]^{*}=\\mathcal{T} bold_italic_X ‚àà [ italic_V ] start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIP", "snippet": "Once we have each document as a sequence of tokens ùëø ‚àà [ V ] N ‚äÜ [ V ] ‚àó = ùíØ \\bm{X}\\in[V]^{N}\\subseteq[V]^{*}=\\mathcal{T} bold_italic_X ‚àà [ italic_V ] start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ‚äÜ [ italic_V ] start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT = caligraphic_T , we wish to perform next-token prediction. That is, given a context ùëø : n ‚àà [ V ] n \\bm{X}_{:n}\\in[V]^{n} bold_italic_X start_POSTSUBSCRIPT : italic_n end_POSTSUBSCRIPT ‚àà [ italic_V ] start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT (i.e., the first n n italic_n tokens ùíô 1 , ‚Ä¶ , ùíô n ‚àà [ V ] \\bm{x}_{1},\\dots,\\bm{x}_{n}\\"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS2.SSSx2.p2", "title": "Optimizing this loss is usually called ‚Äúpre-training‚Äù in the language model community (contrast with ‚Äúpost-training‚Äù and, more recently, ‚Äúmid-training‚Äù, which are methodologies to modify a next-token-", "snippet": "Optimizing this loss is usually called ‚Äúpre-training‚Äù in the language model community (contrast with ‚Äúpost-training‚Äù and, more recently, ‚Äúmid-training‚Äù, which are methodologies to modify a next-token-predictor for useful tasks)."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS2.SSSx2.p3", "title": "Side note: Why does the first term of ( 7.4.1 ) predict ùüè ‚Äã ( ùíô 2 ) \\bm{1}(\\bm{x}_{2}) bold_1 ( bold_italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) , and there is no term which measures the loss t", "snippet": "Side note: Why does the first term of ( 7.4.1 ) predict ùüè ‚Äã ( ùíô 2 ) \\bm{1}(\\bm{x}_{2}) bold_1 ( bold_italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) , and there is no term which measures the loss to predict the first token? It‚Äôs because if we wanted to predict the first token, we would have the empty sequence as context, and therefore make this first token prediction using a qualitatively different mechanism than that which applies to the other tokens. So actually this model is not trained to predict the very first token of any document. The reason this is OK is due to an implementation det"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS3.p1", "title": "For the architecture, we use a standard GPT-2 style transformer, substituting CRATE layers in for the transformer layers. 9 9 9 In direct contravention of the conventions in this book and those of man", "snippet": "For the architecture, we use a standard GPT-2 style transformer, substituting CRATE layers in for the transformer layers. 9 9 9 In direct contravention of the conventions in this book and those of many other communities, the NLP community calls such GPT-2 style transformers (encompassing nearly all current LLMs) as ‚Äúdecoder-only‚Äù transformers. ‚ÄúEncoder-only‚Äù transformers have a different architecture, and ‚Äúencoder-decoder‚Äù transformers concatenate an ‚Äúencoder-only‚Äù transformer with a ‚Äúdecoder-only‚Äù transformer. This despite the fact that ‚Äúdecoder-only‚Äù transformers also compute an encoding of "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS3.SSS0.Px1.p1", "title": "We first embed the token sequence ùëø ‚àà [ V ] N \\bm{X}\\in[V]^{N} bold_italic_X ‚àà [ italic_V ] start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT to Euclidean space. This is often done by associating eac", "snippet": "We first embed the token sequence ùëø ‚àà [ V ] N \\bm{X}\\in[V]^{N} bold_italic_X ‚àà [ italic_V ] start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT to Euclidean space. This is often done by associating each index in [ V ] [V] [ italic_V ] with a vector in ‚Ñù d \\mathbb{R}^{d} blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT using a massive 10 10 10 By ‚Äúmassive‚Äù we mean that such a structure is often a large fraction of the language model‚Äôs total size. array ùë¨ ‚àà ‚Ñù V √ó d \\bm{E}\\in\\mathbb{R}^{V\\times d} bold_italic_E ‚àà blackboard_R start_POSTSUPERSCRIPT italic_V √ó italic_d end_POSTSUPERSC"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS3.SSS0.Px2.p1", "title": "We process the embeddings using a CRATE-like backbone which uses causal masking. To motivate causal masking, consider the causal language modeling loss ‚Ñí CLM \\mathcal{L}_{\\mathrm{CLM}} caligraphic_L s", "snippet": "We process the embeddings using a CRATE-like backbone which uses causal masking. To motivate causal masking, consider the causal language modeling loss ‚Ñí CLM \\mathcal{L}_{\\mathrm{CLM}} caligraphic_L start_POSTSUBSCRIPT roman_CLM end_POSTSUBSCRIPT defined in ( 7.4.1 ). The most naive implementation would require us to compute hte forward pass N N italic_N times in order to backpropagate once. Obviously this is extremely inefficient, since N N italic_N can often be in the thousands. In order to scale training with this loss efficiently, we impose a causal constraint, i.e., ùíÅ Œ∏ ‚Äã ( ùëø : n ) = ùíÅ Œ∏ "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS3.SSS0.Px2.p2", "title": "So now that we want a causal architecture for the backbone, how can we get it? Since the MLP and layer normalizations inside each transformer layer affect each token individually, the only thing that ", "snippet": "So now that we want a causal architecture for the backbone, how can we get it? Since the MLP and layer normalizations inside each transformer layer affect each token individually, the only thing that matters for causality is the attention block (or MSSA \\operatorname{MSSA} roman_MSSA in terms of CRATE). In order to make MSSA \\operatorname{MSSA} roman_MSSA causal, we define the CausalMSSA \\mathrm{CausalMSSA} roman_CausalMSSA block as CausalMSSA Œ∏ ‚Ñì ‚Å° ( ùíÅ ) ‚âê ùëº out ‚Ñì ‚Äã [ CausalSA ‚Å° ( [ ùëº 1 , ‚Ñì ] ‚ä§ ‚Äã ùíÅ , [ ùëº 1 , ‚Ñì ] ‚ä§ ‚Äã ùíÅ , [ ùëº 1 , ‚Ñì ] ‚ä§ ‚Äã ùíÅ ) ‚ãÆ CausalSA ‚Å° ( [ ùëº K , ‚Ñì ] ‚ä§ ‚Äã ùíÅ , [ ùëº K , ‚Ñì ] ‚ä§ ‚Äã ùíÅ "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS3.SSS0.Px3.p1", "title": "We use a post-processing step f Œ∏ ext f_{\\theta}^{\\mathrm{ext}} italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT which extracts the feature v", "snippet": "We use a post-processing step f Œ∏ ext f_{\\theta}^{\\mathrm{ext}} italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT which extracts the feature vector of the last known token so as to predict the next token. In theory this means that each token ùíÅ Œ∏ ‚Äã ( ùëø ) n \\bm{Z}_{\\theta}(\\bm{X})_{n} bold_italic_Z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( bold_italic_X ) start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT should contain rich information about all tokens that come before or on index n n italic_n , i.e., ùíô 1 , ‚Ä¶ , ùíô n \\bm{x}_{1},\\dots,\\b"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS3.SSS0.Px4.p1", "title": "For our classification head h Œ∏ h_{\\theta} italic_h start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT , the GPT-2 architecture uses a simple linear layer and a softmax to get the desired probability vect", "snippet": "For our classification head h Œ∏ h_{\\theta} italic_h start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT , the GPT-2 architecture uses a simple linear layer and a softmax to get the desired probability vectors: h Œ∏ ‚Äã ( ùíõ ) ‚âê softmax ‚Å° ( ùëæ out ‚Äã ùíõ + ùíÉ out ) , h_{\\theta}(\\bm{z})\\doteq\\operatorname{\\mathrm{softmax}}(\\bm{W}^{\\mathrm{out}}\\bm{z}+\\bm{b}^{\\mathrm{out}}), italic_h start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( bold_italic_z ) ‚âê roman_softmax ( bold_italic_W start_POSTSUPERSCRIPT roman_out end_POSTSUPERSCRIPT bold_italic_z + bold_italic_b start_POSTSUPERSCRIPT roman_out end_POSTSUPERSCRIPT"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS3.SSS0.Px4.p2", "title": "All these architectural choices mean that causal training is extremely efficient relative to non-causal training: ‚Ä¢ We only need one forward pass through the backbone to compute the loss for the whole", "snippet": "All these architectural choices mean that causal training is extremely efficient relative to non-causal training: ‚Ä¢ We only need one forward pass through the backbone to compute the loss for the whole sequence. ‚Ä¢ The feature extraction is basically free . ‚Ä¢ All tokens can be pushed through the task-specific head in parallel ."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.I4.i1.p1", "title": "We only need one forward pass through the backbone to compute the loss for the whole sequence.", "snippet": "We only need one forward pass through the backbone to compute the loss for the whole sequence."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.I4.i2.p1", "title": "The feature extraction is basically free .", "snippet": "The feature extraction is basically free ."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.I4.i3.p1", "title": "All tokens can be pushed through the task-specific head in parallel .", "snippet": "All tokens can be pushed through the task-specific head in parallel ."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS4.p1", "title": "We train our language model using end-to-end stochastic optimization. One remaining issue is that, in practice, different documents in a batch have different lengths (in terms of the number of tokens ", "snippet": "We train our language model using end-to-end stochastic optimization. One remaining issue is that, in practice, different documents in a batch have different lengths (in terms of the number of tokens required for each sequence), but as of the time of writing this book, the main deep learning frameworks by-and-large allow only ‚Äúrectangular‚Äù tensors, which do not accommodate this behavior. To try to resolve this issue, we just insert a special padding token <|pad|> for all shorter samples in the batch, so that we can batch process everything using rectangular tensors. At each timestep k k italic"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.I5.i1.p1", "title": "Subsample B B italic_B different tokenized documents { ùëø b ( k ) } b = 1 B ‚äÜ ùíØ = [ V ] ‚àó \\{\\bm{X}_{b}^{(k)}\\}_{b=1}^{B}\\subseteq\\mathcal{T}=[V]^{*} { bold_italic_X start_POSTSUBSCRIPT italic_b end_POS", "snippet": "Subsample B B italic_B different tokenized documents { ùëø b ( k ) } b = 1 B ‚äÜ ùíØ = [ V ] ‚àó \\{\\bm{X}_{b}^{(k)}\\}_{b=1}^{B}\\subseteq\\mathcal{T}=[V]^{*} { bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_b = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT ‚äÜ caligraphic_T = [ italic_V ] start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT , each with length N b ( k ) N_{b}^{(k)} italic_N start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.I5.i2.p1", "title": "Compute N max ( k ) ‚âê max b ‚àà [ B ] ‚Å° N b ( k ) N_{\\max}^{(k)}\\doteq\\max_{b\\in[B]}N_{b}^{(k)} italic_N start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERS", "snippet": "Compute N max ( k ) ‚âê max b ‚àà [ B ] ‚Å° N b ( k ) N_{\\max}^{(k)}\\doteq\\max_{b\\in[B]}N_{b}^{(k)} italic_N start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ‚âê roman_max start_POSTSUBSCRIPT italic_b ‚àà [ italic_B ] end_POSTSUBSCRIPT italic_N start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT and pad each ùëø b ( k ) \\bm{X}_{b}^{(k)} bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT to length N max ( k ) N_{\\max}^{(k)} italic_N start_P"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.I5.i3.p1", "title": "Compute the features ùíÅ Œ∏ ‚Äã ( ùëø b ( k ) ) \\bm{Z}_{\\theta}(\\bm{X}_{b}^{(k)}) bold_italic_Z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT s", "snippet": "Compute the features ùíÅ Œ∏ ‚Äã ( ùëø b ( k ) ) \\bm{Z}_{\\theta}(\\bm{X}_{b}^{(k)}) bold_italic_Z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ) ."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.I5.i4.p1", "title": "Compute the predicted distributions ùíë Œ∏ ‚Äã ( ùëø b , : n ( k ) ) ‚âê ( h Œ∏ ‚àò f Œ∏ ext ) ‚Äã ( ùíÅ Œ∏ ‚Äã ( ùëø b ( k ) ) : n ) \\bm{p}_{\\theta}(\\bm{X}_{b,:n}^{(k)})\\doteq(h_{\\theta}\\circ f_{\\theta}^{\\mathrm{ext}})(\\b", "snippet": "Compute the predicted distributions ùíë Œ∏ ‚Äã ( ùëø b , : n ( k ) ) ‚âê ( h Œ∏ ‚àò f Œ∏ ext ) ‚Äã ( ùíÅ Œ∏ ‚Äã ( ùëø b ( k ) ) : n ) \\bm{p}_{\\theta}(\\bm{X}_{b,:n}^{(k)})\\doteq(h_{\\theta}\\circ f_{\\theta}^{\\mathrm{ext}})(\\bm{Z}_{\\theta}(\\bm{X}_{b}^{(k)})_{:n}) bold_italic_p start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b , : italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ) ‚âê ( italic_h start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ‚àò italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIP"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.I5.i5.p1", "title": "Form the surrogate stochastic loss ‚Ñí ^ CLM ( k ) ( Œ∏ ) ‚âê 1 B ‚Äã ( N max ( k ) ‚àí 1 ) ‚àë b = 1 B ‚àë n = 1 N max ( k ) ‚àí 1 CE ( ùüè ( ùíô b , n + 1 ( k ) ) , ùíë Œ∏ ( ùëø b , : n ( k ) ) ) ) . \\hat{\\mathcal{L}}_{\\ma", "snippet": "Form the surrogate stochastic loss ‚Ñí ^ CLM ( k ) ( Œ∏ ) ‚âê 1 B ‚Äã ( N max ( k ) ‚àí 1 ) ‚àë b = 1 B ‚àë n = 1 N max ( k ) ‚àí 1 CE ( ùüè ( ùíô b , n + 1 ( k ) ) , ùíë Œ∏ ( ùëø b , : n ( k ) ) ) ) . \\hat{\\mathcal{L}}_{\\mathrm{CLM}}^{(k)}(\\theta)\\doteq\\frac{1}{B(N_{\\max}^{(k)}-1)}\\sum_{b=1}^{B}\\sum_{n=1}^{N_{\\max}^{(k)}-1}\\operatorname{CE}(\\bm{1}(\\bm{x}_{b,n+1}^{(k)}),\\bm{p}_{\\theta}(\\bm{X}_{b,:n}^{(k)}))). over^ start_ARG caligraphic_L end_ARG start_POSTSUBSCRIPT roman_CLM end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_Œ∏ ) ‚âê divide start_ARG 1 end_ARG start_ARG italic_B ( italic_"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.I5.i6.p1", "title": "Compute one step of an optimization algorithm on Œ∏ \\theta italic_Œ∏ , giving the following iteration: Œ∏ ( k + 1 ) ‚âê OptUpdate ( k ) ‚Äã ( Œ∏ ( k ) ; ‚àá Œ∏ ‚Ñí ^ CLM ( k ) ) . \\theta^{(k+1)}\\doteq\\textsc{OptUp", "snippet": "Compute one step of an optimization algorithm on Œ∏ \\theta italic_Œ∏ , giving the following iteration: Œ∏ ( k + 1 ) ‚âê OptUpdate ( k ) ‚Äã ( Œ∏ ( k ) ; ‚àá Œ∏ ‚Ñí ^ CLM ( k ) ) . \\theta^{(k+1)}\\doteq\\textsc{OptUpdate}^{(k)}(\\theta^{(k)};\\nabla_{\\theta}\\hat{\\mathcal{L}}_{\\mathrm{CLM}}^{(k)}). italic_Œ∏ start_POSTSUPERSCRIPT ( italic_k + 1 ) end_POSTSUPERSCRIPT ‚âê OptUpdate start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_Œ∏ start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ; ‚àá start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT over^ start_ARG caligraphic_L end_ARG start_POSTSUBSCRIPT roman_CLM"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS5.p1", "title": "There are several ways to evaluate a trained transformer language model. ‚Ä¢ On a holdout dataset of arbitrary text, we can evaluate ‚Ñí CLM \\mathcal{L}_{\\mathrm{CLM}} caligraphic_L start_POSTSUBSCRIPT ro", "snippet": "There are several ways to evaluate a trained transformer language model. ‚Ä¢ On a holdout dataset of arbitrary text, we can evaluate ‚Ñí CLM \\mathcal{L}_{\\mathrm{CLM}} caligraphic_L start_POSTSUBSCRIPT roman_CLM end_POSTSUBSCRIPT on it; lower losses are better since they mean the model‚Äôs sampling yields better performance. ‚Ä¢ On a multiple choice question dataset, for each question we can put it as the context and check the estimated probability of the correct answer being generated. ‚Ä¢ We can also test the text generation capabilities. Namely, we can repeatedly sample from the model‚Äôs probability d"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.I6.i1.p1", "title": "On a holdout dataset of arbitrary text, we can evaluate ‚Ñí CLM \\mathcal{L}_{\\mathrm{CLM}} caligraphic_L start_POSTSUBSCRIPT roman_CLM end_POSTSUBSCRIPT on it; lower losses are better since they mean th", "snippet": "On a holdout dataset of arbitrary text, we can evaluate ‚Ñí CLM \\mathcal{L}_{\\mathrm{CLM}} caligraphic_L start_POSTSUBSCRIPT roman_CLM end_POSTSUBSCRIPT on it; lower losses are better since they mean the model‚Äôs sampling yields better performance."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.I6.i2.p1", "title": "On a multiple choice question dataset, for each question we can put it as the context and check the estimated probability of the correct answer being generated.", "snippet": "On a multiple choice question dataset, for each question we can put it as the context and check the estimated probability of the correct answer being generated."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.I6.i3.p1", "title": "We can also test the text generation capabilities. Namely, we can repeatedly sample from the model‚Äôs probability distribution over the next token given the context. Each time we sample we generate a n", "snippet": "We can also test the text generation capabilities. Namely, we can repeatedly sample from the model‚Äôs probability distribution over the next token given the context. Each time we sample we generate a new token, which we print and add to the context. This allows us to sample from the LLM, and judge the generated samples however we please. 12 12 12 Having to re-run the model on each token every time can become prohibitively expensive. Clever storages of different internal features of the language model (such as the so-called K K italic_K - V V italic_V cache ), along with the causality of the arc"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS6.p1", "title": "Since our causal CRATE architecture is directly built upon GPT-2, we compare the optimal settings for GPT-2 as given by the NanoGPT repository [ Kar22 ] with the same settings applied to CRATE for fai", "snippet": "Since our causal CRATE architecture is directly built upon GPT-2, we compare the optimal settings for GPT-2 as given by the NanoGPT repository [ Kar22 ] with the same settings applied to CRATE for fair comparison."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS6.SSS0.Px1.p1", "title": "We use the GPT-2 tokenizer, which has vocabulary size V = 50257 V=50257 italic_V = 50257 , including a special token for <|pad|> . 13 13 13 The <|bos|> token is not included in this setup, although it", "snippet": "We use the GPT-2 tokenizer, which has vocabulary size V = 50257 V=50257 italic_V = 50257 , including a special token for <|pad|> . 13 13 13 The <|bos|> token is not included in this setup, although it is very common in modern language models. The context length is N max = 1024 N_{\\max}=1024 italic_N start_POSTSUBSCRIPT roman_max end_POSTSUBSCRIPT = 1024 . The backbone model follows the GPT2-Base architecture [ RWC+19 ] with the appropriate alterations to have causal CRATE layers, and we compare against GPT2-Small and GPT2-Base."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS6.SSS0.Px2.p1", "title": "For training causal CRATE, we follow the implementations in the NanoGPT repository [ Kar22 ] . Specifically, we use a batch size of 384 and train for 600,000 steps with the Adam optimizer [ KB14 ] . F", "snippet": "For training causal CRATE, we follow the implementations in the NanoGPT repository [ Kar22 ] . Specifically, we use a batch size of 384 and train for 600,000 steps with the Adam optimizer [ KB14 ] . For the Adam optimizer, we use ( Œ≤ 1 , Œ≤ 2 ) = ( 0.9 , 0.95 ) (\\beta_{1},\\beta_{2})=(0.9,0.95) ( italic_Œ≤ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_Œ≤ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) = ( 0.9 , 0.95 ) and weight decay of 0.1 0.1 0.1 . For the learning rate schedule, we apply a linear warm-up and cosine decay, with a peak value of Œ∑ = 6 √ó 10 ‚àí 4 \\eta=6\\times 10^{-4} italic_Œ∑ = 6 √ó 10 "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS6.SSS0.Px3.p1", "title": "Table 7.5 demonstrates that CRATE models achieve reasonable performance on the causal language modeling loss across a variety of datasets compared to GPT-2 models with similar parameter counts and sim", "snippet": "Table 7.5 demonstrates that CRATE models achieve reasonable performance on the causal language modeling loss across a variety of datasets compared to GPT-2 models with similar parameter counts and similar architectures."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S5.p1", "title": "In this section, we will discuss three ways in which various parts of CRATE-type models can be scaled up or made more efficient while still remaining white-box. These developments mix both conceptual ", "snippet": "In this section, we will discuss three ways in which various parts of CRATE-type models can be scaled up or made more efficient while still remaining white-box. These developments mix both conceptual and empirical insights, and can be viewed as case studies about how to use white-box understanding to improve deep learning models in practice. The tasks that we use to evaluate the methods will be image classification and next-token-prediction, the data will be ImageNet and OpenWebText respectively, the optimization procedure will be the same backpropagation, and the only thing that changes is th"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S5.SS1.p1", "title": "One design decision enforced by the CRATE framework is that the width of the nonlinearity in the network. In a regular transformer, the width is usually set to 4 4 4 , 8 8 8 , or 11 3 \\frac{11}{3} div", "snippet": "One design decision enforced by the CRATE framework is that the width of the nonlinearity in the network. In a regular transformer, the width is usually set to 4 4 4 , 8 8 8 , or 11 3 \\frac{11}{3} divide start_ARG 11 end_ARG start_ARG 3 end_ARG times the feature dimension. However, CRATE enforces that the width is exactly equal to the feature dimension, i.e., the dictionaries ùë´ ‚Ñì \\bm{D}^{\\ell} bold_italic_D start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT are square, which could lead to reduced performance. The fundamental reason that the CRATE framework constrains us to this choice is as fol"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S5.I1.i1.p1", "title": "The ISTA block takes a single step of optimization for dictionary learning.", "snippet": "The ISTA block takes a single step of optimization for dictionary learning."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S5.I1.i2.p1", "title": "Usually one step of any iterative optimization algorithm cannot effectively optimize the objective. So then why does this work?", "snippet": "Usually one step of any iterative optimization algorithm cannot effectively optimize the objective. So then why does this work?"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S5.I1.i3.p1", "title": "Optimization algorithms usually converge very quickly if and only if they have good initializations, or warm starts . The ISTA block has a warm start ‚Äî it treats the input features as an initializatio", "snippet": "Optimization algorithms usually converge very quickly if and only if they have good initializations, or warm starts . The ISTA block has a warm start ‚Äî it treats the input features as an initialization to the resulting sparse codes."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S5.I1.i4.p1", "title": "This enforces that the input features and sparse codes are have the same dimension. Namely, ISTA learns a complete sparsifying dictionary (cf Chapter 2 ).", "snippet": "This enforces that the input features and sparse codes are have the same dimension. Namely, ISTA learns a complete sparsifying dictionary (cf Chapter 2 )."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S5.SS1.p2", "title": "However, this presents an empirical problem. Using the above configuration, if ùíÅ ‚Ñì + 1 / 2 ‚àà ‚Ñù d √ó n \\bm{Z}^{\\ell+1/2}\\in\\mathbb{R}^{d\\times n} bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì + 1 / 2 end_", "snippet": "However, this presents an empirical problem. Using the above configuration, if ùíÅ ‚Ñì + 1 / 2 ‚àà ‚Ñù d √ó n \\bm{Z}^{\\ell+1/2}\\in\\mathbb{R}^{d\\times n} bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì + 1 / 2 end_POSTSUPERSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d √ó italic_n end_POSTSUPERSCRIPT then ùíÅ ‚Ñì + 1 ‚àà ‚Ñù s √ó n \\bm{Z}^{\\ell+1}\\in\\mathbb{R}^{s\\times n} bold_italic_Z start_POSTSUPERSCRIPT roman_‚Ñì + 1 end_POSTSUPERSCRIPT ‚àà blackboard_R start_POSTSUPERSCRIPT italic_s √ó italic_n end_POSTSUPERSCRIPT , which can have arbitrarily large feature dimension. In practice, we want the feature dimension a"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S5.I2.i1.p1", "title": "The feature dimension at each layer is the same.", "snippet": "The feature dimension at each layer is the same."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S5.I2.i2.p1", "title": "The dictionary is wide, i.e., overcomplete.", "snippet": "The dictionary is wide, i.e., overcomplete."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S5.I2.i3.p1", "title": "The output of the nonlinearity is the sparse codes of the input w.r.t. the dictionary.", "snippet": "The output of the nonlinearity is the sparse codes of the input w.r.t. the dictionary."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S5.SS1.p3", "title": "The CRATE- Œ± \\alpha italic_Œ± layer is shown in Figure 7.14 . In practice this modification of CRATE performs very well at larger scales. For example, when we pre-train CRATE- Œ± \\alpha italic_Œ± models ", "snippet": "The CRATE- Œ± \\alpha italic_Œ± layer is shown in Figure 7.14 . In practice this modification of CRATE performs very well at larger scales. For example, when we pre-train CRATE- Œ± \\alpha italic_Œ± models on ImageNet-21K, unsupervised tasks like segmentation (see Figure 7.15 and Table 7.6 ) generally have significantly improved performance compared to CRATE. Similar trends are present in language model training using causal self-attention (see Table 7.7 ). Overall, it is a promising avenue to scaling up the performance to match black-box models such as transformers. 14 14 14 Note that the experimen"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S5.SS2.p1", "title": "In practice, deep learning models suffer bottlenecks to space and time complexity, representing problem sizes which they cannot scale beyond given fixed resources. One such bottleneck, particularly me", "snippet": "In practice, deep learning models suffer bottlenecks to space and time complexity, representing problem sizes which they cannot scale beyond given fixed resources. One such bottleneck, particularly meaningful when dealing with data where each sample is itself high-dimensional and rich (such as long streams of text or videos), is the time complexity of processing long sequences of data. In order to alleviate the time-complexity of processing data using transformers, in Section 4.3.2 we proposed a token statistics self-attention operator TSSA Œ∏ ‚Ñì \\operatorname{TSSA}_{\\theta}^{\\ell} roman_TSSA st"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S5.SS2.p2", "title": "Moreover, the proposed architecture, named ToST (for ‚ÄúToken Statistics Transformer‚Äù) performs well at vision tasks (i.e., Table 7.8 ) and language tasks (i.e., Table 7.9 ). This is especially true for", "snippet": "Moreover, the proposed architecture, named ToST (for ‚ÄúToken Statistics Transformer‚Äù) performs well at vision tasks (i.e., Table 7.8 ) and language tasks (i.e., Table 7.9 ). This is especially true for long-sequence-length tasks (cf Table 7.10 ), where it is both more performant and much more efficient than conventional transformers and all other transformer-like architectures."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S5.SS3.p1", "title": "Another bottleneck to remove from deep learning models, specifically transformer-like architectures, is the memory bottleneck which comes from massive matrix multiplications in MLPs, where the interna", "snippet": "Another bottleneck to remove from deep learning models, specifically transformer-like architectures, is the memory bottleneck which comes from massive matrix multiplications in MLPs, where the internal dimension is far greater than the feature dimension d d italic_d . It thus is an interesting and important question to ask: do we really need the MLP inside a transformer, and how good can the performance get without it? To explore this question, we use the attention-only-transformer (AoT) architecture (see Section 4.3.1 ), depicted in Figure 7.17 . Namely, each layer is simply of the form ùíÅ Œ∏ ‚Ñì"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S5.SS3.p2", "title": "We conduct the experiments using the proposed AoT architecture and demonstrate its potential. We pre-train the AoT-MSSA and AoT-MHSA models of different sizes, along with GPT-2, on OpenWebText [ GC19 ", "snippet": "We conduct the experiments using the proposed AoT architecture and demonstrate its potential. We pre-train the AoT-MSSA and AoT-MHSA models of different sizes, along with GPT-2, on OpenWebText [ GC19 ] . We plot the training loss and validation loss against the number of training iterations in Figure 7.18 (a) and (b), respectively. It is observed that medium- and large-sized AoT-based models achieve training and validation losses comparable to those of the GPT-2 base model. In addition, compared to the GPT-2 base model, the AoT-MHSA model is identical to the GPT-2 base model, except for the ab"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.p1", "title": "The second application we discuss is nonlinear image completion , also known as masked autoencoding (MAE), which is a direct generalization of the low-rank matrix completion problem discussed in Chapt", "snippet": "The second application we discuss is nonlinear image completion , also known as masked autoencoding (MAE), which is a direct generalization of the low-rank matrix completion problem discussed in Chapter 2 . Masked autoencoding, since its introduction in the deep learning context by [ HCX+22 ] has been a staple and simple self-supervised representation learning method, which aims to endow each patch feature within ùíÅ Œ∏ \\bm{Z}_{\\theta} bold_italic_Z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT with aggregate information as well as information about its neighbors, such that both the patch featur"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.p2", "title": "The dataset is kept to be the same imagery datasets as discussed in Section 7.2.1 . As usual, we still apply data augmentations to each sample in each new batch.", "snippet": "The dataset is kept to be the same imagery datasets as discussed in Section 7.2.1 . As usual, we still apply data augmentations to each sample in each new batch."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.SS1.p1", "title": "As the name suggests, masked autoencoding involves a view v m v_{m} italic_v start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT which, given an input, performs a random resized crop (cf Section 7.2.2 ) to", "snippet": "As the name suggests, masked autoencoding involves a view v m v_{m} italic_v start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT which, given an input, performs a random resized crop (cf Section 7.2.2 ) to turn the input image into a square image of size ( C , S mask , S mask ) (C,S_{\\mathrm{mask}},S_{\\mathrm{mask}}) ( italic_C , italic_S start_POSTSUBSCRIPT roman_mask end_POSTSUBSCRIPT , italic_S start_POSTSUBSCRIPT roman_mask end_POSTSUBSCRIPT ) , then masks (i.e., sets to zero) a fixed percentage p mask ‚àà [ 0 , 1 ] p_{\\mathrm{mask}}\\in[0,1] italic_p start_POSTSUBSCRIPT roman_mask end_POSTSUBSCRI"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.SS2.p1", "title": "We use a CRATE encoder and decoder, depicted in Figure 7.7 , though of course it is possible to use a regular transformer encoder and decoder. Details follow now.", "snippet": "We use a CRATE encoder and decoder, depicted in Figure 7.7 , though of course it is possible to use a regular transformer encoder and decoder. Details follow now."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.SS2.SSS0.Px1.p1", "title": "The encoder is the same as the CRATE encoder in Section 7.3.2 , with the caveat that there is no feature extractor f Œ∏ ext f_{\\theta}^{\\mathrm{ext}} italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSC", "snippet": "The encoder is the same as the CRATE encoder in Section 7.3.2 , with the caveat that there is no feature extractor f Œ∏ ext f_{\\theta}^{\\mathrm{ext}} italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT . However, both the embedding f Œ∏ emb f_{\\theta}^{\\mathrm{emb}} italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT and the backbone f Œ∏ bb f_{\\theta}^{\\mathrm{bb}} italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_bb end_POSTSUPERSCRIPT are the same."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.SS2.SSS0.Px2.p1", "title": "The decoder backbone is the CRATE decoder described in Chapter 5 . For completeness‚Äô sake, we describe it now. Given a feature sequence ùíÅ Œ∏ ‚Äã ( ùëø ) ‚âê f Œ∏ ‚Äã ( ùëø ) ‚àà ( ‚Ñù d ) ‚àó \\bm{Z}_{\\theta}(\\bm{X})\\do", "snippet": "The decoder backbone is the CRATE decoder described in Chapter 5 . For completeness‚Äô sake, we describe it now. Given a feature sequence ùíÅ Œ∏ ‚Äã ( ùëø ) ‚âê f Œ∏ ‚Äã ( ùëø ) ‚àà ( ‚Ñù d ) ‚àó \\bm{Z}_{\\theta}(\\bm{X})\\doteq f_{\\theta}(\\bm{X})\\in(\\mathbb{R}^{d})^{*} bold_italic_Z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( bold_italic_X ) ‚âê italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( bold_italic_X ) ‚àà ( blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT , we can process it using the decoder backbone g Œ∑ bb g_{\\eta}^{\\mathrm{bb}} italic_g"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.SS2.SSS0.Px3.p1", "title": "To transform ùíÅ ~ Œ∏ , Œ∑ ‚Äã ( ùëø ) \\tilde{\\bm{Z}}_{\\theta,\\eta}(\\bm{X}) over~ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_Œ∏ , italic_Œ∑ end_POSTSUBSCRIPT ( bold_italic_X ) back into an estim", "snippet": "To transform ùíÅ ~ Œ∏ , Œ∑ ‚Äã ( ùëø ) \\tilde{\\bm{Z}}_{\\theta,\\eta}(\\bm{X}) over~ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_Œ∏ , italic_Œ∑ end_POSTSUBSCRIPT ( bold_italic_X ) back into an estimate for ùëø \\bm{X} bold_italic_X , we need to undo the effect of the embedding module f Œ∏ emb f_{\\theta}^{\\mathrm{emb}} italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT using the unembedding module g Œ∑ unemb g_{\\eta}^{\\mathrm{unemb}} italic_g start_POSTSUBSCRIPT italic_Œ∑ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_unemb end_POSTSUPERSCRIPT"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.SS2.SSS0.Px3.p2", "title": "This architecture is a white-box autoencoder ( f Œ∏ , g Œ∑ ) (f_{\\theta},g_{\\eta}) ( italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT , italic_g start_POSTSUBSCRIPT italic_Œ∑ end_POSTSUBSCRIPT ) w", "snippet": "This architecture is a white-box autoencoder ( f Œ∏ , g Œ∑ ) (f_{\\theta},g_{\\eta}) ( italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT , italic_g start_POSTSUBSCRIPT italic_Œ∑ end_POSTSUBSCRIPT ) where (recall) f Œ∏ = f Œ∏ bb ‚àò f Œ∏ emb f_{\\theta}=f_{\\theta}^{\\mathrm{bb}}\\circ f_{\\theta}^{\\mathrm{emb}} italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT = italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_bb end_POSTSUPERSCRIPT ‚àò italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT and g Œ∑ = g Œ∑ unemb ‚àò"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.SS3.p1", "title": "As in Section 7.3.3 , we use a simple optimization setup: we sample images and masks, compute the loss on those samples and the gradients of this loss, and update the parameters using a generic optimi", "snippet": "As in Section 7.3.3 , we use a simple optimization setup: we sample images and masks, compute the loss on those samples and the gradients of this loss, and update the parameters using a generic optimization algorithm and the aforementioned gradients. For each timestep k k italic_k , we: ‚Ä¢ Subsample B B italic_B different samples { ùëø b ( k ) } b = 1 B ‚äÜ ‚Ñê \\{\\bm{X}_{b}^{(k)}\\}_{b=1}^{B}\\subseteq\\mathcal{I} { bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_b = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.I1.i1.p1", "title": "Subsample B B italic_B different samples { ùëø b ( k ) } b = 1 B ‚äÜ ‚Ñê \\{\\bm{X}_{b}^{(k)}\\}_{b=1}^{B}\\subseteq\\mathcal{I} { bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRI", "snippet": "Subsample B B italic_B different samples { ùëø b ( k ) } b = 1 B ‚äÜ ‚Ñê \\{\\bm{X}_{b}^{(k)}\\}_{b=1}^{B}\\subseteq\\mathcal{I} { bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_b = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT ‚äÜ caligraphic_I ."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.I1.i2.p1", "title": "For each sample ùëø b ( k ) \\bm{X}_{b}^{(k)} bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT , compute a different randomized resized ", "snippet": "For each sample ùëø b ( k ) \\bm{X}_{b}^{(k)} bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT , compute a different randomized resized crop and mask v b , m ( k ) v_{b,m}^{(k)} italic_v start_POSTSUBSCRIPT italic_b , italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT and apply it to ùëø b ( k ) \\bm{X}_{b}^{(k)} bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT to get ùëø b , m ( k ) ‚âê v b , m t ‚Äã ( ùëø b ( k ) ) \\bm{X}_{b,m}^{(k)}\\doteq v_{"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.I1.i3.p1", "title": "Compute the estimated autoencoding ùëø ^ Œ∏ , Œ∑ ‚Äã ( ùëø b , r ( k ) ) ‚âê ( g Œ∑ ‚àò f Œ∏ ) ‚Äã ( ùëø b , r ( k ) ) \\hat{\\bm{X}}_{\\theta,\\eta}(\\bm{X}_{b,r}^{(k)})\\doteq(g_{\\eta}\\circ f_{\\theta})(\\bm{X}_{b,r}^{(k)}) ", "snippet": "Compute the estimated autoencoding ùëø ^ Œ∏ , Œ∑ ‚Äã ( ùëø b , r ( k ) ) ‚âê ( g Œ∑ ‚àò f Œ∏ ) ‚Äã ( ùëø b , r ( k ) ) \\hat{\\bm{X}}_{\\theta,\\eta}(\\bm{X}_{b,r}^{(k)})\\doteq(g_{\\eta}\\circ f_{\\theta})(\\bm{X}_{b,r}^{(k)}) over^ start_ARG bold_italic_X end_ARG start_POSTSUBSCRIPT italic_Œ∏ , italic_Œ∑ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_b , italic_r end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ) ‚âê ( italic_g start_POSTSUBSCRIPT italic_Œ∑ end_POSTSUBSCRIPT ‚àò italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ) ( bold_italic_X start_POSTSUBSCRIPT italic_b , italic"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.I1.i4.p1", "title": "Form the surrogate stochastic loss ‚Ñí ^ MAE ( k ) ‚Äã ( Œ∏ , Œ∑ ) ‚âê 1 B ‚Äã ‚àë b = 1 B ‚Äñ ùëø ^ Œ∏ , Œ∑ ‚Äã ( ùëø b , r ( k ) ) ‚àí ùëø b ( k ) ‚Äñ F 2 . \\hat{\\mathcal{L}}_{\\mathrm{MAE}}^{(k)}(\\theta,\\eta)\\doteq\\frac{1}{B}\\", "snippet": "Form the surrogate stochastic loss ‚Ñí ^ MAE ( k ) ‚Äã ( Œ∏ , Œ∑ ) ‚âê 1 B ‚Äã ‚àë b = 1 B ‚Äñ ùëø ^ Œ∏ , Œ∑ ‚Äã ( ùëø b , r ( k ) ) ‚àí ùëø b ( k ) ‚Äñ F 2 . \\hat{\\mathcal{L}}_{\\mathrm{MAE}}^{(k)}(\\theta,\\eta)\\doteq\\frac{1}{B}\\sum_{b=1}^{B}\\|\\hat{\\bm{X}}_{\\theta,\\eta}(\\bm{X}_{b,r}^{(k)})-\\bm{X}_{b}^{(k)}\\|_{F}^{2}. over^ start_ARG caligraphic_L end_ARG start_POSTSUBSCRIPT roman_MAE end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_Œ∏ , italic_Œ∑ ) ‚âê divide start_ARG 1 end_ARG start_ARG italic_B end_ARG ‚àë start_POSTSUBSCRIPT italic_b = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_B end_P"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.I1.i5.p1", "title": "Compute one step of an optimization algorithm on ( Œ∏ , Œ∑ ) (\\theta,\\eta) ( italic_Œ∏ , italic_Œ∑ ) , giving the following iteration: ( Œ∏ ( k + 1 ) , Œ∑ ( k + 1 ) ) ‚âê OptUpdate ( k ) ‚Äã ( Œ∏ ( k ) , Œ∑ ( k )", "snippet": "Compute one step of an optimization algorithm on ( Œ∏ , Œ∑ ) (\\theta,\\eta) ( italic_Œ∏ , italic_Œ∑ ) , giving the following iteration: ( Œ∏ ( k + 1 ) , Œ∑ ( k + 1 ) ) ‚âê OptUpdate ( k ) ‚Äã ( Œ∏ ( k ) , Œ∑ ( k ) ; ‚àá ( Œ∏ , Œ∑ ) ‚Ñí ^ MAE ( k ) ) . (\\theta^{(k+1)},\\eta^{(k+1)})\\doteq\\textsc{OptUpdate}^{(k)}(\\theta^{(k)},\\eta^{(k)};\\nabla_{(\\theta,\\eta)}\\hat{\\mathcal{L}}_{\\mathrm{MAE}}^{(k)}). ( italic_Œ∏ start_POSTSUPERSCRIPT ( italic_k + 1 ) end_POSTSUPERSCRIPT , italic_Œ∑ start_POSTSUPERSCRIPT ( italic_k + 1 ) end_POSTSUPERSCRIPT ) ‚âê OptUpdate start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_Œ∏ "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.SS4.p1", "title": "This is the first autoencoder network we discuss in this chapter. We use the same center crop view v cc v_{\\mathrm{cc}} italic_v start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT as in Sections 7.2.5 and", "snippet": "This is the first autoencoder network we discuss in this chapter. We use the same center crop view v cc v_{\\mathrm{cc}} italic_v start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT as in Sections 7.2.5 and 7.3.4 , resizing the final image to a square with side length S cc = S mask S_{\\mathrm{cc}}=S_{\\mathrm{mask}} italic_S start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT = italic_S start_POSTSUBSCRIPT roman_mask end_POSTSUBSCRIPT pixels so as to match the shapes of the input images seen during training."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.SS4.p2", "title": "On top of evaluating the masked autoencoding loss itself, it is also possible to evaluate the features ùíÅ Œ∏ ‚Äã ( ùëø cc ) \\bm{Z}_{\\theta}(\\bm{X}_{\\mathrm{cc}}) bold_italic_Z start_POSTSUBSCRIPT italic_Œ∏ e", "snippet": "On top of evaluating the masked autoencoding loss itself, it is also possible to evaluate the features ùíÅ Œ∏ ‚Äã ( ùëø cc ) \\bm{Z}_{\\theta}(\\bm{X}_{\\mathrm{cc}}) bold_italic_Z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT ) of the view ùëø cc ‚âê v cc ‚Äã ( ùëø ) \\bm{X}_{\\mathrm{cc}}\\doteq v_{\\mathrm{cc}}(\\bm{X}) bold_italic_X start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT ‚âê italic_v start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT ( bold_italic_X ) of the data ùëø \\bm{X} bold_italic_X directly. For the sake of attention map fidelity evaluation, ju"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.SS5.p1", "title": "Since CRATE-MAE is directly based on the ViT-MAE, we compare the optimal settings for ViT-MAE as given by [ HCX+22 ] with the same settings applied to CRATE-MAE for fair comparison.", "snippet": "Since CRATE-MAE is directly based on the ViT-MAE, we compare the optimal settings for ViT-MAE as given by [ HCX+22 ] with the same settings applied to CRATE-MAE for fair comparison."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.SS5.SSS0.Px1.p1", "title": "During training, the masked crop v m v_{m} italic_v start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT resizes the whole image so that the shorter edge is of size 256 256 256 (i.e., S rsz = 256 S_{\\mathrm", "snippet": "During training, the masked crop v m v_{m} italic_v start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT resizes the whole image so that the shorter edge is of size 256 256 256 (i.e., S rsz = 256 S_{\\mathrm{rsz}}=256 italic_S start_POSTSUBSCRIPT roman_rsz end_POSTSUBSCRIPT = 256 ) before taking a random crop of size 224 √ó 224 224\\times 224 224 √ó 224 (i.e., S mask = 224 S_{\\mathrm{mask}}=224 italic_S start_POSTSUBSCRIPT roman_mask end_POSTSUBSCRIPT = 224 ), and masking p mask = 3 4 p_{\\mathrm{mask}}=\\frac{3}{4} italic_p start_POSTSUBSCRIPT roman_mask end_POSTSUBSCRIPT = divide start_ARG 3 end_ARG sta"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.SS5.SSS0.Px2.p1", "title": "For pre-training, we use the ImageNet-1K dataset. We use the AdamW optimizer to pre-train both our ViT-MAE replication as well as CRATE-MAE. We set the base learning rate as 3 √ó 10 ‚àí 5 3\\times 10^{-5}", "snippet": "For pre-training, we use the ImageNet-1K dataset. We use the AdamW optimizer to pre-train both our ViT-MAE replication as well as CRATE-MAE. We set the base learning rate as 3 √ó 10 ‚àí 5 3\\times 10^{-5} 3 √ó 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT , the weight decay as 0.1 0.1 0.1 , and batch size as B = 4096 B=4096 italic_B = 4096 . Our learning rate schedule increases the learning rate linearly to the base learning rate over the first 40 40 40 epochs, and decreases to 0 using a cosine schedule over the next 760 760 760 epochs (training all models for 800 800 800 epochs each). For pre-t"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.SS5.SSS0.Px2.p2", "title": "For linear probing, we use several evaluation datasets such as CIFAR10, CIFAR100, Oxford-Flowers, and Oxford-IIT-Pets. For linear probing, we precompute the feature of all samples in the target datase", "snippet": "For linear probing, we use several evaluation datasets such as CIFAR10, CIFAR100, Oxford-Flowers, and Oxford-IIT-Pets. For linear probing, we precompute the feature of all samples in the target dataset and apply a fast linear regression solver, e.g., from a standard package such as Scikit-Learn."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.SS5.SSS0.Px3.p1", "title": "Table 7.12 demonstrates that CRATE-MAE models achieve, roughly speaking, parity compared to the popular ViT-MAE architecture at similar parameter counts, and also that the feature learning performance", "snippet": "Table 7.12 demonstrates that CRATE-MAE models achieve, roughly speaking, parity compared to the popular ViT-MAE architecture at similar parameter counts, and also that the feature learning performance (as measured by performance on downstream classification tasks) increases with scale. Meanwhile, Figure 7.20 demonstrates that the encoder saliency maps (and therefore the fine-grained features learned by the encoder) indeed isolate and highlight the key parts of the input image."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S7.p1", "title": "All work in this chapter is downstream of the Transformer architecture, which was introduced by [ VSP+17 ] . The Transformer architecture is formally described in Section 7.2 . A main empirical innova", "snippet": "All work in this chapter is downstream of the Transformer architecture, which was introduced by [ VSP+17 ] . The Transformer architecture is formally described in Section 7.2 . A main empirical innovation in recent years, spurred by the prevalence and performance of the transformer architecture, is to formulate a given learning problem as a sequence-to-sequence problem and apply the transformer architecture. This has enabled the transformer architecture to be essentially ubiquitous in (almost) all deep learning applications. As such, direct improvements to the transformer can propagate to beco"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S7.p2", "title": "There is also much more theory specifically about the practice of scaling neural networks, which is enormously practically viable, and we at least remark on it here. This line of work was popularized ", "snippet": "There is also much more theory specifically about the practice of scaling neural networks, which is enormously practically viable, and we at least remark on it here. This line of work was popularized by the ‚ÄúTensor Programs‚Äù line of work [ YHB+22 ] . The basic prescription is that we want the initial gradient updates in a transformer to be constant size, and by working through the backpropagation equations ( Appendix A ) carefully, we can determine the scale of the initialization and learning rates (chosen layer-wise) that are required to achieve this. In practice, such prescriptions greatly i"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#Thmexercise1.p1", "title": "Read the DINO paper [ CTM+21 ] .", "snippet": "Read the DINO paper [ CTM+21 ] ."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#Thmexercise2.p1", "title": "DINO v2 [ ODM+23 ] uses everything from DINO v1 but also, during the data augmentation phase, randomly masks out patches within each view. This kind of augmentation should enforce that the features of", "snippet": "DINO v2 [ ODM+23 ] uses everything from DINO v1 but also, during the data augmentation phase, randomly masks out patches within each view. This kind of augmentation should enforce that the features of images with similar local information are similar. Formulate an optimization problem which promotes this in the encoder, and implement it."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#Thmexercise3.p1", "title": "This exercise considers the implementation of stochastic optimization algorithms to minimize losses involving expectations. (a) Propose an alternative to the term involving R Œµ R_{\\varepsilon} italic_", "snippet": "This exercise considers the implementation of stochastic optimization algorithms to minimize losses involving expectations. (a) Propose an alternative to the term involving R Œµ R_{\\varepsilon} italic_R start_POSTSUBSCRIPT italic_Œµ end_POSTSUBSCRIPT in ( 7.2.34 ) for approximating the covariance regularization term in ( 7.2.33 ). Evaluate the time complexity required to compute your proposed term and its gradient. Include analysis for computing it on a single compute node vs. multiple nodes. (b) Evaluate the time complexity required to compute the existing term in ( 7.2.34 ) and its gradient."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S8.I1.i1.p1", "title": "Propose an alternative to the term involving R Œµ R_{\\varepsilon} italic_R start_POSTSUBSCRIPT italic_Œµ end_POSTSUBSCRIPT in ( 7.2.34 ) for approximating the covariance regularization term in ( 7.2.33 ", "snippet": "Propose an alternative to the term involving R Œµ R_{\\varepsilon} italic_R start_POSTSUBSCRIPT italic_Œµ end_POSTSUBSCRIPT in ( 7.2.34 ) for approximating the covariance regularization term in ( 7.2.33 ). Evaluate the time complexity required to compute your proposed term and its gradient. Include analysis for computing it on a single compute node vs. multiple nodes."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S8.I1.i2.p1", "title": "Evaluate the time complexity required to compute the existing term in ( 7.2.34 ) and its gradient.", "snippet": "Evaluate the time complexity required to compute the existing term in ( 7.2.34 ) and its gradient."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#Thmexercise4.p1", "title": "Prove that ( 7.2.37 ) and ( 7.2.38 ) are convex optimization problems.", "snippet": "Prove that ( 7.2.37 ) and ( 7.2.38 ) are convex optimization problems."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#Thmexercise5.p1", "title": "(a) Implement the CRATE and CRATE- Œ± \\alpha italic_Œ± models. (b) Compare their performance and efficiency on the CIFAR-10 dataset. (c) Compare their interpretability in two ways: ‚Ä¢ The sparsity ‚Äñ ùíÅ ‚Äñ ", "snippet": "(a) Implement the CRATE and CRATE- Œ± \\alpha italic_Œ± models. (b) Compare their performance and efficiency on the CIFAR-10 dataset. (c) Compare their interpretability in two ways: ‚Ä¢ The sparsity ‚Äñ ùíÅ ‚Äñ 0 \\|\\bm{Z}\\|_{0} ‚à• bold_italic_Z ‚à• start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT of the representation ùíÅ \\bm{Z} bold_italic_Z . ‚Ä¢ The attention maps ùíÇ Œ∏ k , ‚Ñì \\bm{a}_{\\theta}^{k,\\ell} bold_italic_a start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k , roman_‚Ñì end_POSTSUPERSCRIPT ."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S8.I2.i1.p1", "title": "Implement the CRATE and CRATE- Œ± \\alpha italic_Œ± models.", "snippet": "Implement the CRATE and CRATE- Œ± \\alpha italic_Œ± models."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S8.I2.i2.p1", "title": "Compare their performance and efficiency on the CIFAR-10 dataset.", "snippet": "Compare their performance and efficiency on the CIFAR-10 dataset."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S8.I2.i3.p1", "title": "Compare their interpretability in two ways: ‚Ä¢ The sparsity ‚Äñ ùíÅ ‚Äñ 0 \\|\\bm{Z}\\|_{0} ‚à• bold_italic_Z ‚à• start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT of the representation ùíÅ \\bm{Z} bold_italic_Z . ‚Ä¢ The attenti", "snippet": "Compare their interpretability in two ways: ‚Ä¢ The sparsity ‚Äñ ùíÅ ‚Äñ 0 \\|\\bm{Z}\\|_{0} ‚à• bold_italic_Z ‚à• start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT of the representation ùíÅ \\bm{Z} bold_italic_Z . ‚Ä¢ The attention maps ùíÇ Œ∏ k , ‚Ñì \\bm{a}_{\\theta}^{k,\\ell} bold_italic_a start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k , roman_‚Ñì end_POSTSUPERSCRIPT ."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S8.I2.i3.I0.i1.p1", "title": "The sparsity ‚Äñ ùíÅ ‚Äñ 0 \\|\\bm{Z}\\|_{0} ‚à• bold_italic_Z ‚à• start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT of the representation ùíÅ \\bm{Z} bold_italic_Z .", "snippet": "The sparsity ‚Äñ ùíÅ ‚Äñ 0 \\|\\bm{Z}\\|_{0} ‚à• bold_italic_Z ‚à• start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT of the representation ùíÅ \\bm{Z} bold_italic_Z ."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S8.I2.i3.I0.i2.p1", "title": "The attention maps ùíÇ Œ∏ k , ‚Ñì \\bm{a}_{\\theta}^{k,\\ell} bold_italic_a start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k , roman_‚Ñì end_POSTSUPERSCRIPT .", "snippet": "The attention maps ùíÇ Œ∏ k , ‚Ñì \\bm{a}_{\\theta}^{k,\\ell} bold_italic_a start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k , roman_‚Ñì end_POSTSUPERSCRIPT ."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#Thmexercise1", "title": "Exercise 7.1 .", "snippet": "Exercise 7.1 . Read the DINO paper [ CTM+21 ] ."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#Thmexercise2", "title": "Exercise 7.2 .", "snippet": "Exercise 7.2 . DINO v2 [ ODM+23 ] uses everything from DINO v1 but also, during the data augmentation phase, randomly masks out patches within each view. This kind of augmentation should enforce that the features of images with similar local information are similar. Formulate an optimization problem which promotes this in the encoder, and implement it."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#Thmexercise3", "title": "Exercise 7.3 .", "snippet": "Exercise 7.3 . This exercise considers the implementation of stochastic optimization algorithms to minimize losses involving expectations. (a) Propose an alternative to the term involving R Œµ R_{\\varepsilon} italic_R start_POSTSUBSCRIPT italic_Œµ end_POSTSUBSCRIPT in ( 7.2.34 ) for approximating the covariance regularization term in ( 7.2.33 ). Evaluate the time complexity required to compute your proposed term and its gradient. Include analysis for computing it on a single compute node vs. multiple nodes. (b) Evaluate the time complexity required to compute the existing term in ( 7.2.34 ) and "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#Thmexercise4", "title": "Exercise 7.4 .", "snippet": "Exercise 7.4 . Prove that ( 7.2.37 ) and ( 7.2.38 ) are convex optimization problems."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#Thmexercise5", "title": "Exercise 7.5 .", "snippet": "Exercise 7.5 . (a) Implement the CRATE and CRATE- Œ± \\alpha italic_Œ± models. (b) Compare their performance and efficiency on the CIFAR-10 dataset. (c) Compare their interpretability in two ways: ‚Ä¢ The sparsity ‚Äñ ùíÅ ‚Äñ 0 \\|\\bm{Z}\\|_{0} ‚à• bold_italic_Z ‚à• start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT of the representation ùíÅ \\bm{Z} bold_italic_Z . ‚Ä¢ The attention maps ùíÇ Œ∏ k , ‚Ñì \\bm{a}_{\\theta}^{k,\\ell} bold_italic_a start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k , roman_‚Ñì end_POSTSUPERSCRIPT ."}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E1", "title": "d CE ‚Äã ( ùíë , ùíí ) ‚âê CE ‚Å° ( ùíë , ùíí ) , ‚àÄ ùíë , ùíí ‚àà Œî m d_{\\operatorname{CE}}(\\bm{p},\\bm{q})\\doteq\\operatorname{CE}(\\bm{p},\\bm{q}),\\quad\\forall\\bm{p},\\bm{q}\\in\\Delta_{m} italic_d start_POSTSUBSCRIPT roman_C", "snippet": "d CE ‚Äã ( ùíë , ùíí ) ‚âê CE ‚Å° ( ùíë , ùíí ) , ‚àÄ ùíë , ùíí ‚àà Œî m d_{\\operatorname{CE}}(\\bm{p},\\bm{q})\\doteq\\operatorname{CE}(\\bm{p},\\bm{q}),\\quad\\forall\\bm{p},\\bm{q}\\in\\Delta_{m} italic_d start_POSTSUBSCRIPT roman_CE end_POSTSUBSCRIPT ( bold_italic_p , bold_italic_q ) ‚âê roman_CE ( bold_italic_p , bold_italic_q ) , ‚àÄ bold_italic_p , bold_italic_q ‚àà roman_Œî start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT (7.2.1)"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E2", "title": "CE ‚Å° ( ùíë , ùíí ) ‚âê ‚àí ‚àë i = 1 m p i ‚Äã log ‚Å° q i , ‚àÄ ùíë = ( p 1 , ‚Ä¶ , p m ) , ùíí = ( q 1 , ‚Ä¶ , q m ) ‚àà Œî m . \\operatorname{CE}(\\bm{p},\\bm{q})\\doteq-\\sum_{i=1}^{m}p_{i}\\log q_{i},\\quad\\forall\\bm{p}=(p_{1},\\d", "snippet": "CE ‚Å° ( ùíë , ùíí ) ‚âê ‚àí ‚àë i = 1 m p i ‚Äã log ‚Å° q i , ‚àÄ ùíë = ( p 1 , ‚Ä¶ , p m ) , ùíí = ( q 1 , ‚Ä¶ , q m ) ‚àà Œî m . \\operatorname{CE}(\\bm{p},\\bm{q})\\doteq-\\sum_{i=1}^{m}p_{i}\\log q_{i},\\quad\\forall\\bm{p}=(p_{1},\\dots,p_{m}),\\bm{q}=(q_{1},\\dots,q_{m})\\in\\Delta_{m}. roman_CE ( bold_italic_p , bold_italic_q ) ‚âê - ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT roman_log italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , ‚àÄ bold_italic_p = ( italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ‚Ä¶ "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E4", "title": "ùñ™ùñ´ ‚Å° ( ùíë ‚à• ùíí ) ‚âê ‚àë i = 1 m p i ‚Äã log ‚Å° ( p i / q i ) , \\operatorname{\\mathsf{KL}}(\\bm{p}\\;\\|\\;\\bm{q})\\doteq\\sum_{i=1}^{m}p_{i}\\log(p_{i}/q_{i}), sansserif_KL ( bold_italic_p ‚à• bold_italic_q ) ‚âê ‚àë star", "snippet": "ùñ™ùñ´ ‚Å° ( ùíë ‚à• ùíí ) ‚âê ‚àë i = 1 m p i ‚Äã log ‚Å° ( p i / q i ) , \\operatorname{\\mathsf{KL}}(\\bm{p}\\;\\|\\;\\bm{q})\\doteq\\sum_{i=1}^{m}p_{i}\\log(p_{i}/q_{i}), sansserif_KL ( bold_italic_p ‚à• bold_italic_q ) ‚âê ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT roman_log ( italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT / italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) , (7.2.4)"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E5", "title": "h ùëæ , ùùÅ ‚Äã ( ùíõ ) ‚âê softmax ‚Å° ( [ ùëæ ‚Äã ùíõ ‚àí ùùÅ ] / œÑ ) , ‚àÄ ùíõ ‚àà ‚Ñù d , h_{\\bm{W},\\bm{\\mu}}(\\bm{z})\\doteq\\operatorname{\\mathrm{softmax}}([\\bm{W}\\bm{z}-\\bm{\\mu}]/\\tau),\\qquad\\forall\\bm{z}\\in\\mathbb{R}^{d}, ita", "snippet": "h ùëæ , ùùÅ ‚Äã ( ùíõ ) ‚âê softmax ‚Å° ( [ ùëæ ‚Äã ùíõ ‚àí ùùÅ ] / œÑ ) , ‚àÄ ùíõ ‚àà ‚Ñù d , h_{\\bm{W},\\bm{\\mu}}(\\bm{z})\\doteq\\operatorname{\\mathrm{softmax}}([\\bm{W}\\bm{z}-\\bm{\\mu}]/\\tau),\\qquad\\forall\\bm{z}\\in\\mathbb{R}^{d}, italic_h start_POSTSUBSCRIPT bold_italic_W , bold_italic_Œº end_POSTSUBSCRIPT ( bold_italic_z ) ‚âê roman_softmax ( [ bold_italic_W bold_italic_z - bold_italic_Œº ] / italic_œÑ ) , ‚àÄ bold_italic_z ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , (7.2.5)"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E6", "title": "softmax ‚Å° ( [ x 1 ‚ãÆ x s ] ) ‚âê 1 ‚àë i = 1 s e x i ‚Äã [ e x 1 ‚ãÆ e x s ] \\operatorname{\\mathrm{softmax}}\\left(\\begin{bmatrix}x_{1}\\\\ \\vdots\\\\ x_{s}\\end{bmatrix}\\right)\\doteq\\frac{1}{\\sum_{i=1}^{s}e^{x_{i}}", "snippet": "softmax ‚Å° ( [ x 1 ‚ãÆ x s ] ) ‚âê 1 ‚àë i = 1 s e x i ‚Äã [ e x 1 ‚ãÆ e x s ] \\operatorname{\\mathrm{softmax}}\\left(\\begin{bmatrix}x_{1}\\\\ \\vdots\\\\ x_{s}\\end{bmatrix}\\right)\\doteq\\frac{1}{\\sum_{i=1}^{s}e^{x_{i}}}\\begin{bmatrix}e^{x_{1}}\\\\ \\vdots\\\\ e^{x_{s}}\\end{bmatrix} roman_softmax ( [ start_ARG start_ROW start_CELL italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_CELL end_ROW start_ROW start_CELL ‚ãÆ end_CELL end_ROW start_ROW start_CELL italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_CELL end_ROW end_ARG ] ) ‚âê divide start_ARG 1 end_ARG start_ARG ‚àë start_POSTSUBSCRIPT italic_i = 1 end_P"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E7", "title": "min Œ∏ , ùëæ , ùùÅ ‚Å° ‚Ñí DINO ‚Äã ( Œ∏ , ùëæ , ùùÅ ) where ‚Ñí DINO ‚Äã ( Œ∏ , ùëæ , ùùÅ ) ‚âê ùîº ‚Å° [ d CE ‚Äã ( ùíë Œ∏ , ùëæ , ùùÅ ‚Äã ( ùëø g ) , ùíë Œ∏ , ùëæ ‚Äã ( ùëø c ) ) ] , \\min_{\\theta,\\bm{W},\\bm{\\mu}}\\mathcal{L}_{\\mathrm{DINO}}(\\theta,\\bm", "snippet": "min Œ∏ , ùëæ , ùùÅ ‚Å° ‚Ñí DINO ‚Äã ( Œ∏ , ùëæ , ùùÅ ) where ‚Ñí DINO ‚Äã ( Œ∏ , ùëæ , ùùÅ ) ‚âê ùîº ‚Å° [ d CE ‚Äã ( ùíë Œ∏ , ùëæ , ùùÅ ‚Äã ( ùëø g ) , ùíë Œ∏ , ùëæ ‚Äã ( ùëø c ) ) ] , \\min_{\\theta,\\bm{W},\\bm{\\mu}}\\mathcal{L}_{\\mathrm{DINO}}(\\theta,\\bm{W},\\bm{\\mu})\\qquad\\text{where}\\qquad\\mathcal{L}_{\\mathrm{DINO}}(\\theta,\\bm{W},\\bm{\\mu})\\doteq\\operatorname{\\mathbb{E}}[d_{\\operatorname{CE}}(\\bm{p}_{\\theta,\\bm{W},\\bm{\\mu}}(\\bm{X}_{g}),\\bm{p}_{\\theta,\\bm{W}}(\\bm{X}_{c}))], roman_min start_POSTSUBSCRIPT italic_Œ∏ , bold_italic_W , bold_italic_Œº end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT roman_DINO end_POSTSUBSCRIPT ( italic_Œ∏ , bold_italic"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E8", "title": "d ‚Ñì 2 ‚Äã ( ùíô , ùíö ) ‚âê 1 2 ‚Äã ‚Äñ ùíô ‚àí ùíö ‚Äñ 2 2 , ‚àÄ ùíô , ùíö ‚àà ‚Ñù d . d_{\\ell^{2}}(\\bm{x},\\bm{y})\\doteq\\frac{1}{2}\\|\\bm{x}-\\bm{y}\\|_{2}^{2},\\qquad\\forall\\bm{x},\\bm{y}\\in\\mathbb{R}^{d}. italic_d start_POSTSUBSCRIP", "snippet": "d ‚Ñì 2 ‚Äã ( ùíô , ùíö ) ‚âê 1 2 ‚Äã ‚Äñ ùíô ‚àí ùíö ‚Äñ 2 2 , ‚àÄ ùíô , ùíö ‚àà ‚Ñù d . d_{\\ell^{2}}(\\bm{x},\\bm{y})\\doteq\\frac{1}{2}\\|\\bm{x}-\\bm{y}\\|_{2}^{2},\\qquad\\forall\\bm{x},\\bm{y}\\in\\mathbb{R}^{d}. italic_d start_POSTSUBSCRIPT roman_‚Ñì start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_x , bold_italic_y ) ‚âê divide start_ARG 1 end_ARG start_ARG 2 end_ARG ‚à• bold_italic_x - bold_italic_y ‚à• start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , ‚àÄ bold_italic_x , bold_italic_y ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT . (7.2.8)"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E9", "title": "‚Ñí SimDINO ‚Äã ( Œ∏ ) ‚âê ùîº ‚Å° [ d ‚Ñì 2 ‚Äã ( ùíõ Œ∏ ‚Äã ( ùëø g ) , ùíõ Œ∏ ‚Äã ( ùëø c ) ) ] ‚àí Œ≥ 2 ‚Äã log ‚Äã det ( ùë∞ + d Œµ 2 ‚Äã Cov ‚Å° ( ùíõ Œ∏ ‚Äã ( ùëø g ) ) ) , \\mathcal{L}_{\\mathrm{SimDINO}}(\\theta)\\doteq\\operatorname{\\mathbb{E}}[", "snippet": "‚Ñí SimDINO ‚Äã ( Œ∏ ) ‚âê ùîº ‚Å° [ d ‚Ñì 2 ‚Äã ( ùíõ Œ∏ ‚Äã ( ùëø g ) , ùíõ Œ∏ ‚Äã ( ùëø c ) ) ] ‚àí Œ≥ 2 ‚Äã log ‚Äã det ( ùë∞ + d Œµ 2 ‚Äã Cov ‚Å° ( ùíõ Œ∏ ‚Äã ( ùëø g ) ) ) , \\mathcal{L}_{\\mathrm{SimDINO}}(\\theta)\\doteq\\operatorname{\\mathbb{E}}[d_{\\ell^{2}}(\\bm{z}_{\\theta}(\\bm{X}_{g}),\\bm{z}_{\\theta}(\\bm{X}_{c}))]-\\frac{\\gamma}{2}\\log\\det\\left(\\bm{I}+\\frac{d}{\\varepsilon^{2}}\\operatorname{Cov}(\\bm{z}_{\\theta}(\\bm{X}_{g}))\\right), caligraphic_L start_POSTSUBSCRIPT roman_SimDINO end_POSTSUBSCRIPT ( italic_Œ∏ ) ‚âê blackboard_E [ italic_d start_POSTSUBSCRIPT roman_‚Ñì start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ( bold_italic_z "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E10", "title": "ùëø patch ‚Ü¶ [ ùíõ cls 1 , ùëæ emb ‚Äã ùëø ] + ùë¨ pos . \\bm{X}^{\\mathrm{patch}}\\mapsto[\\bm{z}_{\\mathrm{cls}}^{1},\\bm{W}^{\\mathrm{emb}}\\bm{X}]+\\bm{E}^{\\mathrm{pos}}. bold_italic_X start_POSTSUPERSCRIPT roman_patch", "snippet": "ùëø patch ‚Ü¶ [ ùíõ cls 1 , ùëæ emb ‚Äã ùëø ] + ùë¨ pos . \\bm{X}^{\\mathrm{patch}}\\mapsto[\\bm{z}_{\\mathrm{cls}}^{1},\\bm{W}^{\\mathrm{emb}}\\bm{X}]+\\bm{E}^{\\mathrm{pos}}. bold_italic_X start_POSTSUPERSCRIPT roman_patch end_POSTSUPERSCRIPT ‚Ü¶ [ bold_italic_z start_POSTSUBSCRIPT roman_cls end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , bold_italic_W start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT bold_italic_X ] + bold_italic_E start_POSTSUPERSCRIPT roman_pos end_POSTSUPERSCRIPT . (7.2.10)"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E11", "title": "f Œ∏ emb ‚Äã ( ùëø ) ‚âê [ ùíõ cls 1 , ùëæ emb ‚Äã f patch ‚Äã ( ùëø ) + ùë¨ pos ] . f_{\\theta}^{\\mathrm{emb}}(\\bm{X})\\doteq\\begin{bmatrix}\\bm{z}_{\\mathrm{cls}}^{1},\\bm{W}^{\\mathrm{emb}}f^{\\mathrm{patch}}(\\bm{X})+\\bm{E}", "snippet": "f Œ∏ emb ‚Äã ( ùëø ) ‚âê [ ùíõ cls 1 , ùëæ emb ‚Äã f patch ‚Äã ( ùëø ) + ùë¨ pos ] . f_{\\theta}^{\\mathrm{emb}}(\\bm{X})\\doteq\\begin{bmatrix}\\bm{z}_{\\mathrm{cls}}^{1},\\bm{W}^{\\mathrm{emb}}f^{\\mathrm{patch}}(\\bm{X})+\\bm{E}^{\\mathrm{pos}}\\end{bmatrix}. italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT ( bold_italic_X ) ‚âê [ start_ARG start_ROW start_CELL bold_italic_z start_POSTSUBSCRIPT roman_cls end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , bold_italic_W start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT italic_f start_POSTSUPERSCRIPT roma"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E12", "title": "f Œ∏ bb = f Œ∏ L ‚àò ‚ãØ ‚àò f Œ∏ 1 . f_{\\theta}^{\\mathrm{bb}}=f_{\\theta}^{L}\\circ\\cdots\\circ f_{\\theta}^{1}. italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_bb end_POSTSUPE", "snippet": "f Œ∏ bb = f Œ∏ L ‚àò ‚ãØ ‚àò f Œ∏ 1 . f_{\\theta}^{\\mathrm{bb}}=f_{\\theta}^{L}\\circ\\cdots\\circ f_{\\theta}^{1}. italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_bb end_POSTSUPERSCRIPT = italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT ‚àò ‚ãØ ‚àò italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT . (7.2.12)"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E19", "title": "ùë® Œ∏ k , ‚Ñì ‚Äã ( ùíÅ ) ‚âê ùë® ‚Äã ( [ ùëº qry k , ‚Ñì ] ‚ä§ ‚Äã ùíÅ , [ ùëº key k , ‚Ñì ] ‚ä§ ‚Äã ùíÅ ) , SA Œ∏ k , ‚Ñì ‚Å° ( ùíÅ ) ‚âê SA ‚Å° ( [ ùëº qry k , ‚Ñì ] ‚ä§ ‚Äã ùíÅ , [ ùëº key k , ‚Ñì ] ‚ä§ ‚Äã ùíÅ , [ ùëº val k , ‚Ñì ] ‚ä§ ‚Äã ùíÅ ) \\bm{A}_{\\theta}^{k,\\ell}", "snippet": "ùë® Œ∏ k , ‚Ñì ‚Äã ( ùíÅ ) ‚âê ùë® ‚Äã ( [ ùëº qry k , ‚Ñì ] ‚ä§ ‚Äã ùíÅ , [ ùëº key k , ‚Ñì ] ‚ä§ ‚Äã ùíÅ ) , SA Œ∏ k , ‚Ñì ‚Å° ( ùíÅ ) ‚âê SA ‚Å° ( [ ùëº qry k , ‚Ñì ] ‚ä§ ‚Äã ùíÅ , [ ùëº key k , ‚Ñì ] ‚ä§ ‚Äã ùíÅ , [ ùëº val k , ‚Ñì ] ‚ä§ ‚Äã ùíÅ ) \\bm{A}_{\\theta}^{k,\\ell}(\\bm{Z})\\doteq\\bm{A}([\\bm{U}_{\\mathrm{qry}}^{k,\\ell}]^{\\top}\\bm{Z},[\\bm{U}_{\\mathrm{key}}^{k,\\ell}]^{\\top}\\bm{Z}),\\qquad\\operatorname{SA}_{\\theta}^{k,\\ell}(\\bm{Z})\\doteq\\operatorname{SA}([\\bm{U}_{\\mathrm{qry}}^{k,\\ell}]^{\\top}\\bm{Z},[\\bm{U}_{\\mathrm{key}}^{k,\\ell}]^{\\top}\\bm{Z},[\\bm{U}_{\\mathrm{val}}^{k,\\ell}]^{\\top}\\bm{Z}) bold_italic_A start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUP"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E20", "title": "MLP Œ∏ ‚Ñì ‚Å° ( ùíÅ ) ‚âê ùëæ down ‚Ñì ‚Äã ReLU ‚Å° ( ùëæ up ‚Ñì ‚Äã ùíÅ + ùíÉ up ‚Ñì ‚Äã ùüè n ‚ä§ ) + ùíÉ down ‚Ñì ‚Äã ùüè n ‚ä§ \\operatorname{MLP}_{\\theta}^{\\ell}(\\bm{Z})\\doteq\\bm{W}_{\\mathrm{down}}^{\\ell}\\operatorname{ReLU}(\\bm{W}_{\\mathrm{", "snippet": "MLP Œ∏ ‚Ñì ‚Å° ( ùíÅ ) ‚âê ùëæ down ‚Ñì ‚Äã ReLU ‚Å° ( ùëæ up ‚Ñì ‚Äã ùíÅ + ùíÉ up ‚Ñì ‚Äã ùüè n ‚ä§ ) + ùíÉ down ‚Ñì ‚Äã ùüè n ‚ä§ \\operatorname{MLP}_{\\theta}^{\\ell}(\\bm{Z})\\doteq\\bm{W}_{\\mathrm{down}}^{\\ell}\\operatorname{ReLU}(\\bm{W}_{\\mathrm{up}}^{\\ell}\\bm{Z}+\\bm{b}_{\\mathrm{up}}^{\\ell}\\bm{1}_{n}^{\\top})+\\bm{b}_{\\mathrm{down}}^{\\ell}\\bm{1}_{n}^{\\top} roman_MLP start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ( bold_italic_Z ) ‚âê bold_italic_W start_POSTSUBSCRIPT roman_down end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT roman_ReLU ( bold_italic_W start_POSTSUBSCRIPT ro"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E21", "title": "LN Œ∏ i , ‚Ñì ‚Å° ( ùíÅ ) = LN Œ∏ i , ‚Ñì ‚Å° ( [ ùíõ 1 , ‚Ä¶ , ùíõ n ] ) = [ LN Œ∏ i , ‚Ñì ‚Å° ( ùíõ 1 ) , ‚Ä¶ , LN Œ∏ i , ‚Ñì ‚Å° ( ùíõ n ) ] \\operatorname{LN}_{\\theta}^{i,\\ell}(\\bm{Z})=\\operatorname{LN}_{\\theta}^{i,\\ell}(\\begin{bma", "snippet": "LN Œ∏ i , ‚Ñì ‚Å° ( ùíÅ ) = LN Œ∏ i , ‚Ñì ‚Å° ( [ ùíõ 1 , ‚Ä¶ , ùíõ n ] ) = [ LN Œ∏ i , ‚Ñì ‚Å° ( ùíõ 1 ) , ‚Ä¶ , LN Œ∏ i , ‚Ñì ‚Å° ( ùíõ n ) ] \\operatorname{LN}_{\\theta}^{i,\\ell}(\\bm{Z})=\\operatorname{LN}_{\\theta}^{i,\\ell}(\\begin{bmatrix}\\bm{z}_{1},\\dots,\\bm{z}_{n}\\end{bmatrix})=\\begin{bmatrix}\\operatorname{LN}_{\\theta}^{i,\\ell}(\\bm{z}_{1}),\\dots,\\operatorname{LN}_{\\theta}^{i,\\ell}(\\bm{z}_{n})\\end{bmatrix} roman_LN start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i , roman_‚Ñì end_POSTSUPERSCRIPT ( bold_italic_Z ) = roman_LN start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E22", "title": "LN Œ∏ i , ‚Ñì ‚Å° ( ùíõ ) = ùíõ ‚àí mean ‚Å° ( ùíõ ) ‚Äã ùüè d ‚Äñ ùíõ ‚àí mean ‚Å° ( ùíõ ) ‚Äã ùüè d ‚Äñ 2 ‚äô ùú∂ i , ‚Ñì + ùú∑ i , ‚Ñì where mean ‚Å° ( ùíõ ) = 1 d ‚Äã ùüè d ‚ä§ ‚Äã ùíõ \\operatorname{LN}_{\\theta}^{i,\\ell}(\\bm{z})=\\frac{\\bm{z}-\\operatorname", "snippet": "LN Œ∏ i , ‚Ñì ‚Å° ( ùíõ ) = ùíõ ‚àí mean ‚Å° ( ùíõ ) ‚Äã ùüè d ‚Äñ ùíõ ‚àí mean ‚Å° ( ùíõ ) ‚Äã ùüè d ‚Äñ 2 ‚äô ùú∂ i , ‚Ñì + ùú∑ i , ‚Ñì where mean ‚Å° ( ùíõ ) = 1 d ‚Äã ùüè d ‚ä§ ‚Äã ùíõ \\operatorname{LN}_{\\theta}^{i,\\ell}(\\bm{z})=\\frac{\\bm{z}-\\operatorname{mean}(\\bm{z})\\bm{1}_{d}}{\\|\\bm{z}-\\operatorname{mean}(\\bm{z})\\bm{1}_{d}\\|_{2}}\\mathbin{\\mathchoice{\\raisebox{1.3pt}{$\\displaystyle\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}}{\\scalebox{0.8}{$\\scriptstyle\\odot$}}{\\scalebox{0.8}{$\\scriptscriptstyle\\odot$}}$}}{\\raisebox{1.3pt}{$\\mathchoice{\\scalebox{0.8}{$\\displaystyle\\odot$}}{\\scalebox{0.8}{$\\textstyle\\odot$}"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E23", "title": "ùíõ Œ∏ ‚Äã ( ùëø ) ‚âê f Œ∏ ext ‚Äã ( ùíÅ Œ∏ ‚Äã ( ùëø ) ) = f Œ∏ ext ‚Äã ( [ ùíõ Œ∏ 1 ‚Äã ( ùëø ) , ‚Ä¶ , ùíõ Œ∏ n ‚Äã ( ùëø ) ] ) ‚âê MLP Œ∏ ext ‚Å° ( ùíõ Œ∏ 1 ‚Äã ( ùëø ) ) ‚Äñ MLP Œ∏ ext ‚Å° ( ùíõ Œ∏ 1 ‚Äã ( ùëø ) ) ‚Äñ 2 . \\bm{z}_{\\theta}(\\bm{X})\\doteq f_{\\th", "snippet": "ùíõ Œ∏ ‚Äã ( ùëø ) ‚âê f Œ∏ ext ‚Äã ( ùíÅ Œ∏ ‚Äã ( ùëø ) ) = f Œ∏ ext ‚Äã ( [ ùíõ Œ∏ 1 ‚Äã ( ùëø ) , ‚Ä¶ , ùíõ Œ∏ n ‚Äã ( ùëø ) ] ) ‚âê MLP Œ∏ ext ‚Å° ( ùíõ Œ∏ 1 ‚Äã ( ùëø ) ) ‚Äñ MLP Œ∏ ext ‚Å° ( ùíõ Œ∏ 1 ‚Äã ( ùëø ) ) ‚Äñ 2 . \\bm{z}_{\\theta}(\\bm{X})\\doteq f_{\\theta}^{\\mathrm{ext}}(\\bm{Z}_{\\theta}(\\bm{X}))=f_{\\theta}^{\\mathrm{ext}}([\\bm{z}_{\\theta}^{1}(\\bm{X}),\\dots,\\bm{z}_{\\theta}^{n}(\\bm{X})])\\doteq\\frac{\\operatorname{MLP}_{\\theta}^{\\mathrm{ext}}(\\bm{z}_{\\theta}^{1}(\\bm{X}))}{\\|\\operatorname{MLP}_{\\theta}^{\\mathrm{ext}}(\\bm{z}_{\\theta}^{1}(\\bm{X}))\\|_{2}}. bold_italic_z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( bold_italic_X ) ‚âê italic_f start_PO"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E24", "title": "‚Ñí DINO ‚àí st ‚Äã ( Œ∏ s , Œ∏ t , ùëæ s , ùëæ t , ùùÅ ) ‚âê ùîº ‚Å° [ d CE ‚Äã ( ùíë Œ∏ t , ùëæ t , ùùÅ ‚Äã ( ùëø g ) , ùíë Œ∏ s , ùëæ s ‚Äã ( ùëø c ) ) ] . \\mathcal{L}_{\\mathrm{DINO}{}-\\mathrm{s}\\mathrm{t}}(\\theta_{\\mathrm{s}},\\theta_{\\mat", "snippet": "‚Ñí DINO ‚àí st ‚Äã ( Œ∏ s , Œ∏ t , ùëæ s , ùëæ t , ùùÅ ) ‚âê ùîº ‚Å° [ d CE ‚Äã ( ùíë Œ∏ t , ùëæ t , ùùÅ ‚Äã ( ùëø g ) , ùíë Œ∏ s , ùëæ s ‚Äã ( ùëø c ) ) ] . \\mathcal{L}_{\\mathrm{DINO}{}-\\mathrm{s}\\mathrm{t}}(\\theta_{\\mathrm{s}},\\theta_{\\mathrm{t}},\\bm{W}_{\\mathrm{s}},\\bm{W}_{\\mathrm{t}},\\bm{\\mu})\\doteq\\operatorname{\\mathbb{E}}[d_{\\operatorname{CE}}(\\bm{p}_{\\theta_{\\mathrm{t}},\\bm{W}_{\\mathrm{t}},\\bm{\\mu}}(\\bm{X}_{g}),\\bm{p}_{\\theta_{\\mathrm{s}},\\bm{W}_{\\mathrm{s}}}(\\bm{X}_{c}))]. caligraphic_L start_POSTSUBSCRIPT roman_DINO - roman_st end_POSTSUBSCRIPT ( italic_Œ∏ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT , italic_Œ∏ start_POSTSUB"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E25", "title": "ùíõ Œ∏ s ‚Äã ( ùëø b , ‚Ñì ( k ) , i ) ‚âê ( f Œ∏ s ext ‚àò f Œ∏ s ) ‚Äã ( ùëø b , ‚Ñì ( k ) , i ) , ùíë Œ∏ s , ùëæ s ‚Äã ( ùëø b , ‚Ñì ( k ) , i ) ‚âê h ùëæ s , ùüé m ‚Äã ( ùíõ Œ∏ s ‚Äã ( ùëø b , ‚Ñì ( k ) , i ‚Äã ( Œ∏ ) ) ) \\bm{z}_{\\theta_{\\mathrm{s}", "snippet": "ùíõ Œ∏ s ‚Äã ( ùëø b , ‚Ñì ( k ) , i ) ‚âê ( f Œ∏ s ext ‚àò f Œ∏ s ) ‚Äã ( ùëø b , ‚Ñì ( k ) , i ) , ùíë Œ∏ s , ùëæ s ‚Äã ( ùëø b , ‚Ñì ( k ) , i ) ‚âê h ùëæ s , ùüé m ‚Äã ( ùíõ Œ∏ s ‚Äã ( ùëø b , ‚Ñì ( k ) , i ‚Äã ( Œ∏ ) ) ) \\bm{z}_{\\theta_{\\mathrm{s}}}(\\bm{X}_{b,\\ell}^{(k),i})\\doteq(f_{\\theta_{\\mathrm{s}}}^{\\mathrm{ext}}\\circ f_{\\theta_{\\mathrm{s}}})(\\bm{X}_{b,\\ell}^{(k),i}),\\qquad\\bm{p}_{\\theta_{\\mathrm{s}},\\bm{W}_{\\mathrm{s}}}(\\bm{X}_{b,\\ell}^{(k),i})\\doteq h_{\\bm{W}_{\\mathrm{s}},\\bm{0}_{m}}(\\bm{z}_{\\theta_{\\mathrm{s}}}(\\bm{X}_{b,\\ell}^{(k),i}(\\theta))) bold_italic_z start_POSTSUBSCRIPT italic_Œ∏ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E33", "title": "‚Ñí SimDINO ‚àí st ( Œ∏ s , Œ∏ t ) ‚âê ùîº [ d ‚Ñì 2 ( ùíõ Œ∏ t ( ùëø g ) , ùíõ Œ∏ s ( ùëø c ) ) ] ‚àí Œ≥ 2 log det ( ùë∞ + d Œµ 2 Cov ( ùíõ Œ∏ s ( ùëø g ) ) ) ) . \\mathcal{L}_{\\mathrm{SimDINO}-\\mathrm{s}\\mathrm{t}}(\\theta_{\\mathrm{s", "snippet": "‚Ñí SimDINO ‚àí st ( Œ∏ s , Œ∏ t ) ‚âê ùîº [ d ‚Ñì 2 ( ùíõ Œ∏ t ( ùëø g ) , ùíõ Œ∏ s ( ùëø c ) ) ] ‚àí Œ≥ 2 log det ( ùë∞ + d Œµ 2 Cov ( ùíõ Œ∏ s ( ùëø g ) ) ) ) . \\mathcal{L}_{\\mathrm{SimDINO}-\\mathrm{s}\\mathrm{t}}(\\theta_{\\mathrm{s}},\\theta_{\\mathrm{t}})\\doteq\\operatorname{\\mathbb{E}}\\left[d_{\\ell^{2}}(\\bm{z}_{\\theta_{\\mathrm{t}}}(\\bm{X}_{g}),\\bm{z}_{\\theta_{\\mathrm{s}}}(\\bm{X}_{c}))\\right]-\\frac{\\gamma}{2}\\log\\det\\left(\\bm{I}+\\frac{d}{\\varepsilon^{2}}\\operatorname{Cov}(\\bm{z}_{\\theta_{\\mathrm{s}}}(\\bm{X}_{g})))\\right). caligraphic_L start_POSTSUBSCRIPT roman_SimDINO - roman_st end_POSTSUBSCRIPT ( italic_Œ∏ start_POSTSUBSCRI"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E37", "title": "min ùëæ ‚àà ‚Ñù N cls √ó d ‚Å° ùîº ‚Å° [ CE ‚Å° ( ùíö , ùëæ ‚Äã ùíõ Œ∏ ‚Äã ( ùëø cc ) ) ] . \\min_{\\bm{W}\\in\\mathbb{R}^{N_{\\mathrm{cls}}\\times d}}\\operatorname{\\mathbb{E}}[\\operatorname{CE}(\\bm{y},\\bm{W}\\bm{z}_{\\theta}(\\bm{X}_{\\m", "snippet": "min ùëæ ‚àà ‚Ñù N cls √ó d ‚Å° ùîº ‚Å° [ CE ‚Å° ( ùíö , ùëæ ‚Äã ùíõ Œ∏ ‚Äã ( ùëø cc ) ) ] . \\min_{\\bm{W}\\in\\mathbb{R}^{N_{\\mathrm{cls}}\\times d}}\\operatorname{\\mathbb{E}}[\\operatorname{CE}(\\bm{y},\\bm{W}\\bm{z}_{\\theta}(\\bm{X}_{\\mathrm{cc}}))]. roman_min start_POSTSUBSCRIPT bold_italic_W ‚àà blackboard_R start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT roman_cls end_POSTSUBSCRIPT √ó italic_d end_POSTSUPERSCRIPT end_POSTSUBSCRIPT blackboard_E [ roman_CE ( bold_italic_y , bold_italic_W bold_italic_z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT ) ) ] . (7.2.37)"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E38", "title": "min ùëæ ‚àà ‚Ñù N cls √ó d ‚Å° 1 B ‚Äã ‚àë b = 1 B CE ‚Å° ( ùíö b , ùëæ ‚Äã ùíõ Œ∏ ‚Äã ( ùëø b , cc ) ) . \\min_{\\bm{W}\\in\\mathbb{R}^{N_{\\mathrm{cls}}\\times d}}\\frac{1}{B}\\sum_{b=1}^{B}\\operatorname{CE}(\\bm{y}_{b},\\bm{W}\\bm{z}_{\\", "snippet": "min ùëæ ‚àà ‚Ñù N cls √ó d ‚Å° 1 B ‚Äã ‚àë b = 1 B CE ‚Å° ( ùíö b , ùëæ ‚Äã ùíõ Œ∏ ‚Äã ( ùëø b , cc ) ) . \\min_{\\bm{W}\\in\\mathbb{R}^{N_{\\mathrm{cls}}\\times d}}\\frac{1}{B}\\sum_{b=1}^{B}\\operatorname{CE}(\\bm{y}_{b},\\bm{W}\\bm{z}_{\\theta}(\\bm{X}_{b,\\mathrm{cc}})). roman_min start_POSTSUBSCRIPT bold_italic_W ‚àà blackboard_R start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT roman_cls end_POSTSUBSCRIPT √ó italic_d end_POSTSUPERSCRIPT end_POSTSUBSCRIPT divide start_ARG 1 end_ARG start_ARG italic_B end_ARG ‚àë start_POSTSUBSCRIPT italic_b = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT roman_CE ( bold_italic"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E39", "title": "ùíö ^ Œ∏ ‚Äã ( ùëø ‚à£ { ( ùëø b , ùíö b ) } b = 1 B ) = ùüè ‚Äã ( i ‚ãÜ ) where i ‚ãÜ ‚âê arg ‚Äã max i ‚àà [ Q ] ‚Äã ‚àë b = 1 B ùíö b ‚Äã ùüè ‚Äã [ ùíõ Œ∏ ‚Äã ( ùëø cc , b ) ‚àà NN k ‚Å° ( ùíõ Œ∏ ‚Äã ( ùëø cc ) ) ] . \\hat{\\bm{y}}_{\\theta}(\\bm{X}\\mid\\{(\\b", "snippet": "ùíö ^ Œ∏ ‚Äã ( ùëø ‚à£ { ( ùëø b , ùíö b ) } b = 1 B ) = ùüè ‚Äã ( i ‚ãÜ ) where i ‚ãÜ ‚âê arg ‚Äã max i ‚àà [ Q ] ‚Äã ‚àë b = 1 B ùíö b ‚Äã ùüè ‚Äã [ ùíõ Œ∏ ‚Äã ( ùëø cc , b ) ‚àà NN k ‚Å° ( ùíõ Œ∏ ‚Äã ( ùëø cc ) ) ] . \\hat{\\bm{y}}_{\\theta}(\\bm{X}\\mid\\{(\\bm{X}_{b},\\bm{y}_{b})\\}_{b=1}^{B})=\\bm{1}(i^{\\star})\\quad\\text{where}\\quad i^{\\star}\\doteq\\operatorname*{arg\\ max}_{i\\in[Q]}\\sum_{b=1}^{B}\\bm{y}_{b}\\mathbf{1}[\\bm{z}_{\\theta}(\\bm{X}_{\\mathrm{cc},b})\\in\\operatorname{NN}_{k}(\\bm{z}_{\\theta}(\\bm{X}_{\\mathrm{cc}}))]. over^ start_ARG bold_italic_y end_ARG start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( bold_italic_X ‚à£ { ( bold_italic_X start_POSTSUBSCR"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S2.E40", "title": "ùîº ùëø , ùíö ‚Å° [ ùüè ‚Äã ( ùíö ^ Œ∏ ‚Äã ( ùëø ‚à£ { ( ùëø b , ùíö b ) } b = 1 B ) = ùíö ) ] \\operatorname{\\mathbb{E}}_{\\bm{X},\\bm{y}}[\\mathbf{1}(\\hat{\\bm{y}}_{\\theta}(\\bm{X}\\mid\\{(\\bm{X}_{b},\\bm{y}_{b})\\}_{b=1}^{B})=\\bm{y})]", "snippet": "ùîº ùëø , ùíö ‚Å° [ ùüè ‚Äã ( ùíö ^ Œ∏ ‚Äã ( ùëø ‚à£ { ( ùëø b , ùíö b ) } b = 1 B ) = ùíö ) ] \\operatorname{\\mathbb{E}}_{\\bm{X},\\bm{y}}[\\mathbf{1}(\\hat{\\bm{y}}_{\\theta}(\\bm{X}\\mid\\{(\\bm{X}_{b},\\bm{y}_{b})\\}_{b=1}^{B})=\\bm{y})] blackboard_E start_POSTSUBSCRIPT bold_italic_X , bold_italic_y end_POSTSUBSCRIPT [ bold_1 ( over^ start_ARG bold_italic_y end_ARG start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( bold_italic_X ‚à£ { ( bold_italic_X start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT , bold_italic_y start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_b = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.E1", "title": "h Œ∏ ‚Äã ( ùíõ ) ‚âê softmax ‚Å° ( ùëæ head ‚Äã ùíõ + ùíÉ head ) , ‚àÄ ùíõ ‚àà ‚Ñù d h_{\\theta}(\\bm{z})\\doteq\\operatorname{\\mathrm{softmax}}(\\bm{W}^{\\mathrm{head}}\\bm{z}+\\bm{b}^{\\mathrm{head}}),\\qquad\\forall\\bm{z}\\in\\mathbb{R", "snippet": "h Œ∏ ‚Äã ( ùíõ ) ‚âê softmax ‚Å° ( ùëæ head ‚Äã ùíõ + ùíÉ head ) , ‚àÄ ùíõ ‚àà ‚Ñù d h_{\\theta}(\\bm{z})\\doteq\\operatorname{\\mathrm{softmax}}(\\bm{W}^{\\mathrm{head}}\\bm{z}+\\bm{b}^{\\mathrm{head}}),\\qquad\\forall\\bm{z}\\in\\mathbb{R}^{d} italic_h start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( bold_italic_z ) ‚âê roman_softmax ( bold_italic_W start_POSTSUPERSCRIPT roman_head end_POSTSUPERSCRIPT bold_italic_z + bold_italic_b start_POSTSUPERSCRIPT roman_head end_POSTSUPERSCRIPT ) , ‚àÄ bold_italic_z ‚àà blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT (7.3.1)"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.E2", "title": "min Œ∏ ‚Å° { ‚Ñí CE ‚Äã ( Œ∏ ) ‚âê ùîº ‚Å° [ CE ‚Å° ( ùíö , ùíë Œ∏ ‚Äã ( ùëø cc ) ) ] } . \\min_{\\theta}\\left\\{\\mathcal{L}_{\\operatorname{CE}}(\\theta)\\doteq\\operatorname{\\mathbb{E}}[\\operatorname{CE}(\\bm{y},\\bm{p}_{\\theta}(\\bm", "snippet": "min Œ∏ ‚Å° { ‚Ñí CE ‚Äã ( Œ∏ ) ‚âê ùîº ‚Å° [ CE ‚Å° ( ùíö , ùíë Œ∏ ‚Äã ( ùëø cc ) ) ] } . \\min_{\\theta}\\left\\{\\mathcal{L}_{\\operatorname{CE}}(\\theta)\\doteq\\operatorname{\\mathbb{E}}[\\operatorname{CE}(\\bm{y},\\bm{p}_{\\theta}(\\bm{X}_{\\mathrm{cc}}))]\\right\\}. roman_min start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT { caligraphic_L start_POSTSUBSCRIPT roman_CE end_POSTSUBSCRIPT ( italic_Œ∏ ) ‚âê blackboard_E [ roman_CE ( bold_italic_y , bold_italic_p start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT roman_cc end_POSTSUBSCRIPT ) ) ] } . (7.3.2)"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.E5", "title": "MSSA Œ∏ ‚Ñì ‚Å° ( ùíÅ ) ‚âê ùëº out ‚Ñì ‚Äã [ SA ‚Å° ( [ ùëº 1 , ‚Ñì ] ‚ä§ ‚Äã ùíÅ , [ ùëº 1 , ‚Ñì ] ‚ä§ ‚Äã ùíÅ , [ ùëº 1 , ‚Ñì ] ‚ä§ ‚Äã ùíÅ ) ‚ãÆ SA ‚Å° ( [ ùëº K , ‚Ñì ] ‚ä§ ‚Äã ùíÅ , [ ùëº K , ‚Ñì ] ‚ä§ ‚Äã ùíÅ , [ ùëº 1 , ‚Ñì ] ‚ä§ ‚Äã ùíÅ ) ] + ùíÉ out ‚Ñì ‚Äã ùüè n ‚ä§ \\operatorname", "snippet": "MSSA Œ∏ ‚Ñì ‚Å° ( ùíÅ ) ‚âê ùëº out ‚Ñì ‚Äã [ SA ‚Å° ( [ ùëº 1 , ‚Ñì ] ‚ä§ ‚Äã ùíÅ , [ ùëº 1 , ‚Ñì ] ‚ä§ ‚Äã ùíÅ , [ ùëº 1 , ‚Ñì ] ‚ä§ ‚Äã ùíÅ ) ‚ãÆ SA ‚Å° ( [ ùëº K , ‚Ñì ] ‚ä§ ‚Äã ùíÅ , [ ùëº K , ‚Ñì ] ‚ä§ ‚Äã ùíÅ , [ ùëº 1 , ‚Ñì ] ‚ä§ ‚Äã ùíÅ ) ] + ùíÉ out ‚Ñì ‚Äã ùüè n ‚ä§ \\operatorname{MSSA}_{\\theta}^{\\ell}(\\bm{Z})\\doteq\\bm{U}_{\\mathrm{out}}^{\\ell}\\begin{bmatrix}\\operatorname{SA}([\\bm{U}^{1,\\ell}]^{\\top}\\bm{Z},[\\bm{U}^{1,\\ell}]^{\\top}\\bm{Z},[\\bm{U}^{1,\\ell}]^{\\top}\\bm{Z})\\\\ \\vdots\\\\ \\operatorname{SA}([\\bm{U}^{K,\\ell}]^{\\top}\\bm{Z},[\\bm{U}^{K,\\ell}]^{\\top}\\bm{Z},[\\bm{U}^{1,\\ell}]^{\\top}\\bm{Z})\\end{bmatrix}+\\bm{b}_{\\mathrm{out}}^{\\ell}\\bm{1}_{n}^{\\top} roman_MSSA start_POSTSUBSCR"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.E6", "title": "ISTA Œ∏ ‚Ñì ‚Å° ( ùíÅ ) ‚âê ReLU ‚Å° ( ùíÅ ‚àí Œ≤ ‚Äã ( ùë´ ‚Ñì ) ‚ä§ ‚Äã ( ùë´ ‚Ñì ‚Äã ùíÅ ‚àí ùíÅ ) + Œ≤ ‚Äã Œª ‚Äã ùüè d ‚Äã ùüè n ‚ä§ ) , \\operatorname{ISTA}_{\\theta}^{\\ell}(\\bm{Z})\\doteq\\operatorname{ReLU}(\\bm{Z}-\\beta(\\bm{D}^{\\ell})^{\\top}(\\bm{D}", "snippet": "ISTA Œ∏ ‚Ñì ‚Å° ( ùíÅ ) ‚âê ReLU ‚Å° ( ùíÅ ‚àí Œ≤ ‚Äã ( ùë´ ‚Ñì ) ‚ä§ ‚Äã ( ùë´ ‚Ñì ‚Äã ùíÅ ‚àí ùíÅ ) + Œ≤ ‚Äã Œª ‚Äã ùüè d ‚Äã ùüè n ‚ä§ ) , \\operatorname{ISTA}_{\\theta}^{\\ell}(\\bm{Z})\\doteq\\operatorname{ReLU}(\\bm{Z}-\\beta(\\bm{D}^{\\ell})^{\\top}(\\bm{D}^{\\ell}\\bm{Z}-\\bm{Z})+\\beta\\lambda\\bm{1}_{d}\\bm{1}_{n}^{\\top}), roman_ISTA start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ( bold_italic_Z ) ‚âê roman_ReLU ( bold_italic_Z - italic_Œ≤ ( bold_italic_D start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ( bold_italic_D start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPE"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.E7", "title": "‚Ñí ^ CE ( k ) ‚Äã ( Œ∏ ) ‚âê 1 B ‚Äã ‚àë b = 1 B CE ‚Å° ( ùíö b ( k ) , ùíë Œ∏ ‚Äã ( ùëø b , cc ( k ) ) ) . \\hat{\\mathcal{L}}_{\\operatorname{CE}}^{(k)}(\\theta)\\doteq\\frac{1}{B}\\sum_{b=1}^{B}\\operatorname{CE}(\\bm{y}_{b}^{(", "snippet": "‚Ñí ^ CE ( k ) ‚Äã ( Œ∏ ) ‚âê 1 B ‚Äã ‚àë b = 1 B CE ‚Å° ( ùíö b ( k ) , ùíë Œ∏ ‚Äã ( ùëø b , cc ( k ) ) ) . \\hat{\\mathcal{L}}_{\\operatorname{CE}}^{(k)}(\\theta)\\doteq\\frac{1}{B}\\sum_{b=1}^{B}\\operatorname{CE}(\\bm{y}_{b}^{(k)},\\bm{p}_{\\theta}(\\bm{X}_{b,\\mathrm{cc}}^{(k)})). over^ start_ARG caligraphic_L end_ARG start_POSTSUBSCRIPT roman_CE end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_Œ∏ ) ‚âê divide start_ARG 1 end_ARG start_ARG italic_B end_ARG ‚àë start_POSTSUBSCRIPT italic_b = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT roman_CE ( bold_italic_y start_POST"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S3.E8", "title": "Œ∏ ( k + 1 ) ‚âê OptUpdate ( k ) ‚Äã ( Œ∏ ( k ) ; ‚àá Œ∏ ‚Ñí ^ CE ( k ) ) . \\theta^{(k+1)}\\doteq\\textsc{OptUpdate}^{(k)}(\\theta^{(k)};\\nabla_{\\theta}\\hat{\\mathcal{L}}_{\\operatorname{CE}}^{(k)}). italic_Œ∏ start_P", "snippet": "Œ∏ ( k + 1 ) ‚âê OptUpdate ( k ) ‚Äã ( Œ∏ ( k ) ; ‚àá Œ∏ ‚Ñí ^ CE ( k ) ) . \\theta^{(k+1)}\\doteq\\textsc{OptUpdate}^{(k)}(\\theta^{(k)};\\nabla_{\\theta}\\hat{\\mathcal{L}}_{\\operatorname{CE}}^{(k)}). italic_Œ∏ start_POSTSUPERSCRIPT ( italic_k + 1 ) end_POSTSUPERSCRIPT ‚âê OptUpdate start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_Œ∏ start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ; ‚àá start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT over^ start_ARG caligraphic_L end_ARG start_POSTSUBSCRIPT roman_CE end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ) . (7.3.8)"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.E1", "title": "min Œ∏ ‚Å° { ‚Ñí CLM ‚Äã ( Œ∏ ) ‚âê ùîº ùëø ‚Å° [ 1 N ‚àí 1 ‚Äã ‚àë n = 1 N ‚àí 1 CE ‚Å° ( ùüè ‚Äã ( ùíô n + 1 ) , ùíë Œ∏ ‚Äã ( ùëø : n ) ) ] } \\min_{\\theta}\\left\\{\\mathcal{L}_{\\mathrm{CLM}}(\\theta)\\doteq\\operatorname{\\mathbb{E}}_{\\bm{X}}\\", "snippet": "min Œ∏ ‚Å° { ‚Ñí CLM ‚Äã ( Œ∏ ) ‚âê ùîº ùëø ‚Å° [ 1 N ‚àí 1 ‚Äã ‚àë n = 1 N ‚àí 1 CE ‚Å° ( ùüè ‚Äã ( ùíô n + 1 ) , ùíë Œ∏ ‚Äã ( ùëø : n ) ) ] } \\min_{\\theta}\\left\\{\\mathcal{L}_{\\mathrm{CLM}}(\\theta)\\doteq\\operatorname{\\mathbb{E}}_{\\bm{X}}\\left[\\frac{1}{N-1}\\sum_{n=1}^{N-1}\\operatorname{CE}(\\bm{1}(\\bm{x}_{n+1}),\\bm{p}_{\\theta}(\\bm{X}_{:n}))\\right]\\right\\} roman_min start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT { caligraphic_L start_POSTSUBSCRIPT roman_CLM end_POSTSUBSCRIPT ( italic_Œ∏ ) ‚âê blackboard_E start_POSTSUBSCRIPT bold_italic_X end_POSTSUBSCRIPT [ divide start_ARG 1 end_ARG start_ARG italic_N - 1 end_ARG ‚àë start_POSTSUBSCRIPT"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.E2", "title": "f Œ∏ emb ‚Äã ( ùëø ) ‚âê [ ùë¨ ùíô 1 , ‚Ä¶ , ùë¨ ùíô N ] + ùë¨ : N pos f_{\\theta}^{\\mathrm{emb}}(\\bm{X})\\doteq[\\bm{E}_{\\bm{x}_{1}},\\dots,\\bm{E}_{\\bm{x}_{N}}]+\\bm{E}_{:N}^{\\mathrm{pos}} italic_f start_POSTSUBSCRIPT itali", "snippet": "f Œ∏ emb ‚Äã ( ùëø ) ‚âê [ ùë¨ ùíô 1 , ‚Ä¶ , ùë¨ ùíô N ] + ùë¨ : N pos f_{\\theta}^{\\mathrm{emb}}(\\bm{X})\\doteq[\\bm{E}_{\\bm{x}_{1}},\\dots,\\bm{E}_{\\bm{x}_{N}}]+\\bm{E}_{:N}^{\\mathrm{pos}} italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT ( bold_italic_X ) ‚âê [ bold_italic_E start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT , ‚Ä¶ , bold_italic_E start_POSTSUBSCRIPT bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT end_POSTSUBSCRIPT ] + bold_italic_E start_POSTSUBSCRIPT : italic_N end_POSTSUBSCRIPT start_POSTSU"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.E3", "title": "ùíÅ Œ∏ ‚Äã ( ùëø : n ) = ùíÅ Œ∏ ‚Äã ( ùëø ) : n \\bm{Z}_{\\theta}(\\bm{X}_{:n})=\\bm{Z}_{\\theta}(\\bm{X})_{:n} bold_italic_Z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT : italic_n ", "snippet": "ùíÅ Œ∏ ‚Äã ( ùëø : n ) = ùíÅ Œ∏ ‚Äã ( ùëø ) : n \\bm{Z}_{\\theta}(\\bm{X}_{:n})=\\bm{Z}_{\\theta}(\\bm{X})_{:n} bold_italic_Z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT : italic_n end_POSTSUBSCRIPT ) = bold_italic_Z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( bold_italic_X ) start_POSTSUBSCRIPT : italic_n end_POSTSUBSCRIPT (7.4.3)"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.E7", "title": "CausalSA ( ùë∏ , ùë≤ , ùëΩ ) t = ‚àë i = 1 t ùëΩ i softmax ( [ ùë≤ : t ] ‚ä§ ùë∏ t ) i \\operatorname{CausalSA}(\\bm{Q},\\bm{K},\\bm{V})_{t}=\\sum_{i=1}^{t}\\bm{V}_{i}\\operatorname{\\mathrm{softmax}}\\left([\\bm{K}_{:t}]^{\\to", "snippet": "CausalSA ( ùë∏ , ùë≤ , ùëΩ ) t = ‚àë i = 1 t ùëΩ i softmax ( [ ùë≤ : t ] ‚ä§ ùë∏ t ) i \\operatorname{CausalSA}(\\bm{Q},\\bm{K},\\bm{V})_{t}=\\sum_{i=1}^{t}\\bm{V}_{i}\\operatorname{\\mathrm{softmax}}\\left([\\bm{K}_{:t}]^{\\top}\\bm{Q}_{t}\\right)_{i} roman_CausalSA ( bold_italic_Q , bold_italic_K , bold_italic_V ) start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT bold_italic_V start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT roman_softmax ( [ bold_italic_K start_POSTSUBSCRIPT : italic_t end_POSTSUBSCRIPT ] start_POSTSUPE"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.E8", "title": "f Œ∏ ext ‚Äã ( ùíÅ Œ∏ ‚Äã ( ùëø : n ) ) ‚âê ( ùíÅ Œ∏ ‚Äã ( ùëø ) ) n f_{\\theta}^{\\mathrm{ext}}(\\bm{Z}_{\\theta}(\\bm{X}_{:n}))\\doteq(\\bm{Z}_{\\theta}(\\bm{X}))_{n} italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT sta", "snippet": "f Œ∏ ext ‚Äã ( ùíÅ Œ∏ ‚Äã ( ùëø : n ) ) ‚âê ( ùíÅ Œ∏ ‚Äã ( ùëø ) ) n f_{\\theta}^{\\mathrm{ext}}(\\bm{Z}_{\\theta}(\\bm{X}_{:n}))\\doteq(\\bm{Z}_{\\theta}(\\bm{X}))_{n} italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT : italic_n end_POSTSUBSCRIPT ) ) ‚âê ( bold_italic_Z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( bold_italic_X ) ) start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT (7.4.8)"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.E9", "title": "h Œ∏ ‚Äã ( ùíõ ) ‚âê softmax ‚Å° ( ùëæ out ‚Äã ùíõ + ùíÉ out ) , h_{\\theta}(\\bm{z})\\doteq\\operatorname{\\mathrm{softmax}}(\\bm{W}^{\\mathrm{out}}\\bm{z}+\\bm{b}^{\\mathrm{out}}), italic_h start_POSTSUBSCRIPT italic_Œ∏ end_PO", "snippet": "h Œ∏ ‚Äã ( ùíõ ) ‚âê softmax ‚Å° ( ùëæ out ‚Äã ùíõ + ùíÉ out ) , h_{\\theta}(\\bm{z})\\doteq\\operatorname{\\mathrm{softmax}}(\\bm{W}^{\\mathrm{out}}\\bm{z}+\\bm{b}^{\\mathrm{out}}), italic_h start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT ( bold_italic_z ) ‚âê roman_softmax ( bold_italic_W start_POSTSUPERSCRIPT roman_out end_POSTSUPERSCRIPT bold_italic_z + bold_italic_b start_POSTSUPERSCRIPT roman_out end_POSTSUPERSCRIPT ) , (7.4.9)"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.E10", "title": "‚Ñí ^ CLM ( k ) ( Œ∏ ) ‚âê 1 B ‚Äã ( N max ( k ) ‚àí 1 ) ‚àë b = 1 B ‚àë n = 1 N max ( k ) ‚àí 1 CE ( ùüè ( ùíô b , n + 1 ( k ) ) , ùíë Œ∏ ( ùëø b , : n ( k ) ) ) ) . \\hat{\\mathcal{L}}_{\\mathrm{CLM}}^{(k)}(\\theta)\\doteq\\frac", "snippet": "‚Ñí ^ CLM ( k ) ( Œ∏ ) ‚âê 1 B ‚Äã ( N max ( k ) ‚àí 1 ) ‚àë b = 1 B ‚àë n = 1 N max ( k ) ‚àí 1 CE ( ùüè ( ùíô b , n + 1 ( k ) ) , ùíë Œ∏ ( ùëø b , : n ( k ) ) ) ) . \\hat{\\mathcal{L}}_{\\mathrm{CLM}}^{(k)}(\\theta)\\doteq\\frac{1}{B(N_{\\max}^{(k)}-1)}\\sum_{b=1}^{B}\\sum_{n=1}^{N_{\\max}^{(k)}-1}\\operatorname{CE}(\\bm{1}(\\bm{x}_{b,n+1}^{(k)}),\\bm{p}_{\\theta}(\\bm{X}_{b,:n}^{(k)}))). over^ start_ARG caligraphic_L end_ARG start_POSTSUBSCRIPT roman_CLM end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_Œ∏ ) ‚âê divide start_ARG 1 end_ARG start_ARG italic_B ( italic_N start_POSTSUBSCRIPT roman_max end"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.E11", "title": "Œ∏ ( k + 1 ) ‚âê OptUpdate ( k ) ‚Äã ( Œ∏ ( k ) ; ‚àá Œ∏ ‚Ñí ^ CLM ( k ) ) . \\theta^{(k+1)}\\doteq\\textsc{OptUpdate}^{(k)}(\\theta^{(k)};\\nabla_{\\theta}\\hat{\\mathcal{L}}_{\\mathrm{CLM}}^{(k)}). italic_Œ∏ start_POSTS", "snippet": "Œ∏ ( k + 1 ) ‚âê OptUpdate ( k ) ‚Äã ( Œ∏ ( k ) ; ‚àá Œ∏ ‚Ñí ^ CLM ( k ) ) . \\theta^{(k+1)}\\doteq\\textsc{OptUpdate}^{(k)}(\\theta^{(k)};\\nabla_{\\theta}\\hat{\\mathcal{L}}_{\\mathrm{CLM}}^{(k)}). italic_Œ∏ start_POSTSUPERSCRIPT ( italic_k + 1 ) end_POSTSUPERSCRIPT ‚âê OptUpdate start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_Œ∏ start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ; ‚àá start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT over^ start_ARG caligraphic_L end_ARG start_POSTSUBSCRIPT roman_CLM end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ) . (7.4.11)"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S5.E1", "title": "ùíÅ Œ∏ ‚Ñì + 1 = ISTA Œ∏ ‚Ñì ‚Å° ( ùíÅ Œ∏ ‚Ñì + 1 / 2 ‚à£ ùíÅ Œ∏ ‚Ñì + 1 / 2 ) \\bm{Z}_{\\theta}^{\\ell+1}=\\operatorname{ISTA}_{\\theta}^{\\ell}(\\bm{Z}_{\\theta}^{\\ell+1/2}\\mid\\bm{Z}_{\\theta}^{\\ell+1/2}) bold_italic_Z start_POST", "snippet": "ùíÅ Œ∏ ‚Ñì + 1 = ISTA Œ∏ ‚Ñì ‚Å° ( ùíÅ Œ∏ ‚Ñì + 1 / 2 ‚à£ ùíÅ Œ∏ ‚Ñì + 1 / 2 ) \\bm{Z}_{\\theta}^{\\ell+1}=\\operatorname{ISTA}_{\\theta}^{\\ell}(\\bm{Z}_{\\theta}^{\\ell+1/2}\\mid\\bm{Z}_{\\theta}^{\\ell+1/2}) bold_italic_Z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì + 1 end_POSTSUPERSCRIPT = roman_ISTA start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì + 1 / 2 end_POSTSUPERSCRIPT ‚à£ bold_italic_Z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPER"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S5.E2", "title": "ISTA Œ∏ ‚Ñì ‚Å° ( ùíÅ ‚à£ ùíÄ ) ‚âê ReLU ‚Å° ( ùíÅ ‚àí Œ≤ ‚Äã ( ùë´ ‚Ñì ) ‚ä§ ‚Äã ( ùë´ ‚Ñì ‚Äã ùíÅ ‚àí ùíÄ ) + Œ≤ ‚Äã Œª ‚Äã ùüè s ‚Äã ùüè n ‚ä§ ) \\operatorname{ISTA}_{\\theta}^{\\ell}(\\bm{Z}\\mid\\bm{Y})\\doteq\\operatorname{ReLU}(\\bm{Z}-\\beta(\\bm{D}^{\\ell})^{", "snippet": "ISTA Œ∏ ‚Ñì ‚Å° ( ùíÅ ‚à£ ùíÄ ) ‚âê ReLU ‚Å° ( ùíÅ ‚àí Œ≤ ‚Äã ( ùë´ ‚Ñì ) ‚ä§ ‚Äã ( ùë´ ‚Ñì ‚Äã ùíÅ ‚àí ùíÄ ) + Œ≤ ‚Äã Œª ‚Äã ùüè s ‚Äã ùüè n ‚ä§ ) \\operatorname{ISTA}_{\\theta}^{\\ell}(\\bm{Z}\\mid\\bm{Y})\\doteq\\operatorname{ReLU}(\\bm{Z}-\\beta(\\bm{D}^{\\ell})^{\\top}(\\bm{D}^{\\ell}\\bm{Z}-\\bm{Y})+\\beta\\lambda\\bm{1}_{s}\\bm{1}_{n}^{\\top}) roman_ISTA start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ( bold_italic_Z ‚à£ bold_italic_Y ) ‚âê roman_ReLU ( bold_italic_Z - italic_Œ≤ ( bold_italic_D start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT ( bold_italic_D start_POSTSUPER"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S5.E3", "title": "ùíÅ Œ∏ ‚Ñì + 1 = ùë® Œ∏ ‚Ñì , T ; ùë® Œ∏ ‚Ñì , t + 1 = ISTA Œ∏ ‚Ñì ‚Å° ( ùë® Œ∏ ‚Ñì , t ‚à£ ùíÅ Œ∏ ‚Ñì + 1 / 2 ) ‚àÄ 0 ‚â§ t < T ; ùë® Œ∏ ‚Ñì , 0 = ùüé s √ó n , \\bm{Z}_{\\theta}^{\\ell+1}=\\bm{A}_{\\theta}^{\\ell,T};\\qquad\\bm{A}_{\\theta}^{\\ell,t+1}=", "snippet": "ùíÅ Œ∏ ‚Ñì + 1 = ùë® Œ∏ ‚Ñì , T ; ùë® Œ∏ ‚Ñì , t + 1 = ISTA Œ∏ ‚Ñì ‚Å° ( ùë® Œ∏ ‚Ñì , t ‚à£ ùíÅ Œ∏ ‚Ñì + 1 / 2 ) ‚àÄ 0 ‚â§ t < T ; ùë® Œ∏ ‚Ñì , 0 = ùüé s √ó n , \\bm{Z}_{\\theta}^{\\ell+1}=\\bm{A}_{\\theta}^{\\ell,T};\\qquad\\bm{A}_{\\theta}^{\\ell,t+1}=\\operatorname{ISTA}_{\\theta}^{\\ell}(\\bm{A}_{\\theta}^{\\ell,t}\\mid\\bm{Z}_{\\theta}^{\\ell+1/2})\\quad\\forall 0\\leq t<T;\\qquad\\bm{A}_{\\theta}^{\\ell,0}=\\bm{0}_{s\\times n}, bold_italic_Z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì + 1 end_POSTSUPERSCRIPT = bold_italic_A start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì , italic_T end_POSTSUPERSCR"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S5.E4", "title": "ùíÅ Œ∏ ‚Ñì + 1 = ùë´ ‚Ñì ‚Äã ùë® Œ∏ ‚Ñì , T ; ùë® Œ∏ ‚Ñì , t + 1 = ISTA Œ∏ ‚Ñì ‚Å° ( ùë® Œ∏ ‚Ñì , t ‚à£ ùíÅ Œ∏ ‚Ñì + 1 / 2 ) ; ùë® Œ∏ ‚Ñì , 0 = ùüé , \\bm{Z}_{\\theta}^{\\ell+1}=\\bm{D}^{\\ell}\\bm{A}_{\\theta}^{\\ell,T};\\qquad\\bm{A}_{\\theta}^{\\ell,t+1}", "snippet": "ùíÅ Œ∏ ‚Ñì + 1 = ùë´ ‚Ñì ‚Äã ùë® Œ∏ ‚Ñì , T ; ùë® Œ∏ ‚Ñì , t + 1 = ISTA Œ∏ ‚Ñì ‚Å° ( ùë® Œ∏ ‚Ñì , t ‚à£ ùíÅ Œ∏ ‚Ñì + 1 / 2 ) ; ùë® Œ∏ ‚Ñì , 0 = ùüé , \\bm{Z}_{\\theta}^{\\ell+1}=\\bm{D}^{\\ell}\\bm{A}_{\\theta}^{\\ell,T};\\qquad\\bm{A}_{\\theta}^{\\ell,t+1}=\\operatorname{ISTA}_{\\theta}^{\\ell}(\\bm{A}_{\\theta}^{\\ell,t}\\mid\\bm{Z}_{\\theta}^{\\ell+1/2});\\qquad\\bm{A}_{\\theta}^{\\ell,0}=\\bm{0}, bold_italic_Z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì + 1 end_POSTSUPERSCRIPT = bold_italic_D start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT bold_italic_A start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roma"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S5.E5", "title": "ùíÅ Œ∏ ‚Ñì + 1 ‚Äã ( ùëø ) ‚âê ODL Œ∏ ‚Ñì ‚Å° ( ùíÅ Œ∏ ‚Ñì + 1 / 2 ‚Äã ( ùëø ) ) . \\bm{Z}_{\\theta}^{\\ell+1}(\\bm{X})\\doteq\\operatorname{ODL}_{\\theta}^{\\ell}(\\bm{Z}_{\\theta}^{\\ell+1/2}(\\bm{X})). bold_italic_Z start_POSTSUBSCRIP", "snippet": "ùíÅ Œ∏ ‚Ñì + 1 ‚Äã ( ùëø ) ‚âê ODL Œ∏ ‚Ñì ‚Å° ( ùíÅ Œ∏ ‚Ñì + 1 / 2 ‚Äã ( ùëø ) ) . \\bm{Z}_{\\theta}^{\\ell+1}(\\bm{X})\\doteq\\operatorname{ODL}_{\\theta}^{\\ell}(\\bm{Z}_{\\theta}^{\\ell+1/2}(\\bm{X})). bold_italic_Z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì + 1 end_POSTSUPERSCRIPT ( bold_italic_X ) ‚âê roman_ODL start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ( bold_italic_Z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì + 1 / 2 end_POSTSUPERSCRIPT ( bold_italic_X ) ) . (7.5.5)"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S5.E8", "title": "ùíÅ Œ∏ ‚Ñì + 1 ‚Äã ( ùëø ) = ùíÅ Œ∏ ‚Ñì ‚Äã ( ùëø ) + MSSA Œ∏ ‚Ñì ‚Å° ( LN Œ∏ ‚Ñì ‚Å° ( ùíÅ Œ∏ ‚Ñì ‚Äã ( ùëø ) ) ) . \\bm{Z}_{\\theta}^{\\ell+1}(\\bm{X})=\\bm{Z}_{\\theta}^{\\ell}(\\bm{X})+\\operatorname{MSSA}_{\\theta}^{\\ell}(\\operatorname{LN}_{\\", "snippet": "ùíÅ Œ∏ ‚Ñì + 1 ‚Äã ( ùëø ) = ùíÅ Œ∏ ‚Ñì ‚Äã ( ùëø ) + MSSA Œ∏ ‚Ñì ‚Å° ( LN Œ∏ ‚Ñì ‚Å° ( ùíÅ Œ∏ ‚Ñì ‚Äã ( ùëø ) ) ) . \\bm{Z}_{\\theta}^{\\ell+1}(\\bm{X})=\\bm{Z}_{\\theta}^{\\ell}(\\bm{X})+\\operatorname{MSSA}_{\\theta}^{\\ell}(\\operatorname{LN}_{\\theta}^{\\ell}(\\bm{Z}_{\\theta}^{\\ell}(\\bm{X}))). bold_italic_Z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì + 1 end_POSTSUPERSCRIPT ( bold_italic_X ) = bold_italic_Z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ( bold_italic_X ) + roman_MSSA start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.E1", "title": "min Œ∏ , Œ∑ ‚Å° { ‚Ñí MAE ‚Äã ( Œ∏ , Œ∑ ) ‚âê ùîº ‚Å° ‚Äñ ùëø ^ Œ∏ , Œ∑ ‚Äã ( ùëø m ) ‚àí ùëø ‚Äñ F 2 } \\min_{\\theta,\\eta}\\left\\{\\mathcal{L}_{\\mathrm{MAE}}(\\theta,\\eta)\\doteq\\operatorname{\\mathbb{E}}\\|\\hat{\\bm{X}}_{\\theta,\\eta}(\\bm{", "snippet": "min Œ∏ , Œ∑ ‚Å° { ‚Ñí MAE ‚Äã ( Œ∏ , Œ∑ ) ‚âê ùîº ‚Å° ‚Äñ ùëø ^ Œ∏ , Œ∑ ‚Äã ( ùëø m ) ‚àí ùëø ‚Äñ F 2 } \\min_{\\theta,\\eta}\\left\\{\\mathcal{L}_{\\mathrm{MAE}}(\\theta,\\eta)\\doteq\\operatorname{\\mathbb{E}}\\|\\hat{\\bm{X}}_{\\theta,\\eta}(\\bm{X}_{m})-\\bm{X}\\|_{F}^{2}\\right\\} roman_min start_POSTSUBSCRIPT italic_Œ∏ , italic_Œ∑ end_POSTSUBSCRIPT { caligraphic_L start_POSTSUBSCRIPT roman_MAE end_POSTSUBSCRIPT ( italic_Œ∏ , italic_Œ∑ ) ‚âê blackboard_E ‚à• over^ start_ARG bold_italic_X end_ARG start_POSTSUBSCRIPT italic_Œ∏ , italic_Œ∑ end_POSTSUBSCRIPT ( bold_italic_X start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ) - bold_italic_X ‚à• start_POSTSUBSC"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.E2", "title": "g Œ∑ bb = g Œ∑ L ‚àò ‚ãØ ‚àò g Œ∑ 1 . g_{\\eta}^{\\mathrm{bb}}=g_{\\eta}^{L}\\circ\\cdots\\circ g_{\\eta}^{1}. italic_g start_POSTSUBSCRIPT italic_Œ∑ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_bb end_POSTSUPERSCRIP", "snippet": "g Œ∑ bb = g Œ∑ L ‚àò ‚ãØ ‚àò g Œ∑ 1 . g_{\\eta}^{\\mathrm{bb}}=g_{\\eta}^{L}\\circ\\cdots\\circ g_{\\eta}^{1}. italic_g start_POSTSUBSCRIPT italic_Œ∑ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_bb end_POSTSUPERSCRIPT = italic_g start_POSTSUBSCRIPT italic_Œ∑ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT ‚àò ‚ãØ ‚àò italic_g start_POSTSUBSCRIPT italic_Œ∑ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT . (7.6.2)"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.E5", "title": "f Œ∏ emb ‚Äã ( ùëø ) ‚âê [ ùíõ cls 1 , ùëæ emb ‚Äã f patch ‚Äã ( ùëø ) + ùë¨ pos ] f_{\\theta}^{\\mathrm{emb}}(\\bm{X})\\doteq\\begin{bmatrix}\\bm{z}_{\\mathrm{cls}}^{1},\\bm{W}^{\\mathrm{emb}}f^{\\mathrm{patch}}(\\bm{X})+\\bm{E}^{", "snippet": "f Œ∏ emb ‚Äã ( ùëø ) ‚âê [ ùíõ cls 1 , ùëæ emb ‚Äã f patch ‚Äã ( ùëø ) + ùë¨ pos ] f_{\\theta}^{\\mathrm{emb}}(\\bm{X})\\doteq\\begin{bmatrix}\\bm{z}_{\\mathrm{cls}}^{1},\\bm{W}^{\\mathrm{emb}}f^{\\mathrm{patch}}(\\bm{X})+\\bm{E}^{\\mathrm{pos}}\\end{bmatrix} italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT ( bold_italic_X ) ‚âê [ start_ARG start_ROW start_CELL bold_italic_z start_POSTSUBSCRIPT roman_cls end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , bold_italic_W start_POSTSUPERSCRIPT roman_emb end_POSTSUPERSCRIPT italic_f start_POSTSUPERSCRIPT roman_p"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.E6", "title": "g Œ∑ unemb ‚Äã ( ùíÅ ~ ) ‚âê g Œ∑ unemb ‚Äã ( [ ùíõ ~ 1 , ‚Ä¶ , ùíõ ~ n ] ) = g unpatch ‚Äã ( ùëæ unemb ‚Äã ( [ ùíõ ~ 2 , ‚Ä¶ , ùíõ ~ n ] ‚àí ùë¨ ~ pos ) ) , g_{\\eta}^{\\mathrm{unemb}}(\\tilde{\\bm{Z}})\\doteq g_{\\eta}^{\\mathrm{unemb}}(", "snippet": "g Œ∑ unemb ‚Äã ( ùíÅ ~ ) ‚âê g Œ∑ unemb ‚Äã ( [ ùíõ ~ 1 , ‚Ä¶ , ùíõ ~ n ] ) = g unpatch ‚Äã ( ùëæ unemb ‚Äã ( [ ùíõ ~ 2 , ‚Ä¶ , ùíõ ~ n ] ‚àí ùë¨ ~ pos ) ) , g_{\\eta}^{\\mathrm{unemb}}(\\tilde{\\bm{Z}})\\doteq g_{\\eta}^{\\mathrm{unemb}}(\\begin{bmatrix}\\tilde{\\bm{z}}^{1},\\dots,\\tilde{\\bm{z}}^{n}\\end{bmatrix})=g^{\\mathrm{unpatch}}(\\bm{W}^{\\mathrm{unemb}}([\\tilde{\\bm{z}}^{2},\\dots,\\tilde{\\bm{z}}^{n}]-\\tilde{\\bm{E}}^{\\mathrm{pos}})), italic_g start_POSTSUBSCRIPT italic_Œ∑ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_unemb end_POSTSUPERSCRIPT ( over~ start_ARG bold_italic_Z end_ARG ) ‚âê italic_g start_POSTSUBSCRIPT italic_Œ∑ end_POSTSUB"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.E7", "title": "‚Ñí ^ MAE ( k ) ‚Äã ( Œ∏ , Œ∑ ) ‚âê 1 B ‚Äã ‚àë b = 1 B ‚Äñ ùëø ^ Œ∏ , Œ∑ ‚Äã ( ùëø b , r ( k ) ) ‚àí ùëø b ( k ) ‚Äñ F 2 . \\hat{\\mathcal{L}}_{\\mathrm{MAE}}^{(k)}(\\theta,\\eta)\\doteq\\frac{1}{B}\\sum_{b=1}^{B}\\|\\hat{\\bm{X}}_{\\theta", "snippet": "‚Ñí ^ MAE ( k ) ‚Äã ( Œ∏ , Œ∑ ) ‚âê 1 B ‚Äã ‚àë b = 1 B ‚Äñ ùëø ^ Œ∏ , Œ∑ ‚Äã ( ùëø b , r ( k ) ) ‚àí ùëø b ( k ) ‚Äñ F 2 . \\hat{\\mathcal{L}}_{\\mathrm{MAE}}^{(k)}(\\theta,\\eta)\\doteq\\frac{1}{B}\\sum_{b=1}^{B}\\|\\hat{\\bm{X}}_{\\theta,\\eta}(\\bm{X}_{b,r}^{(k)})-\\bm{X}_{b}^{(k)}\\|_{F}^{2}. over^ start_ARG caligraphic_L end_ARG start_POSTSUBSCRIPT roman_MAE end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_Œ∏ , italic_Œ∑ ) ‚âê divide start_ARG 1 end_ARG start_ARG italic_B end_ARG ‚àë start_POSTSUBSCRIPT italic_b = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT ‚à• over^ start_ARG bo"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.E8", "title": "( Œ∏ ( k + 1 ) , Œ∑ ( k + 1 ) ) ‚âê OptUpdate ( k ) ‚Äã ( Œ∏ ( k ) , Œ∑ ( k ) ; ‚àá ( Œ∏ , Œ∑ ) ‚Ñí ^ MAE ( k ) ) . (\\theta^{(k+1)},\\eta^{(k+1)})\\doteq\\textsc{OptUpdate}^{(k)}(\\theta^{(k)},\\eta^{(k)};\\nabla_{(\\thet", "snippet": "( Œ∏ ( k + 1 ) , Œ∑ ( k + 1 ) ) ‚âê OptUpdate ( k ) ‚Äã ( Œ∏ ( k ) , Œ∑ ( k ) ; ‚àá ( Œ∏ , Œ∑ ) ‚Ñí ^ MAE ( k ) ) . (\\theta^{(k+1)},\\eta^{(k+1)})\\doteq\\textsc{OptUpdate}^{(k)}(\\theta^{(k)},\\eta^{(k)};\\nabla_{(\\theta,\\eta)}\\hat{\\mathcal{L}}_{\\mathrm{MAE}}^{(k)}). ( italic_Œ∏ start_POSTSUPERSCRIPT ( italic_k + 1 ) end_POSTSUPERSCRIPT , italic_Œ∑ start_POSTSUPERSCRIPT ( italic_k + 1 ) end_POSTSUPERSCRIPT ) ‚âê OptUpdate start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_Œ∏ start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT , italic_Œ∑ start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ; ‚àá start_"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S6.E9", "title": "f Œ∏ ext ‚Äã ( ùíÅ ) ‚âê f Œ∏ ext ‚Äã ( [ ùíõ 1 , ‚Ä¶ , ùíõ n ] ) = ùíõ 1 , f_{\\theta}^{\\mathrm{ext}}(\\bm{Z})\\doteq f_{\\theta}^{\\mathrm{ext}}([\\bm{z}^{1},\\dots,\\bm{z}^{n}])=\\bm{z}^{1}, italic_f start_POSTSUBSCRIPT ital", "snippet": "f Œ∏ ext ‚Äã ( ùíÅ ) ‚âê f Œ∏ ext ‚Äã ( [ ùíõ 1 , ‚Ä¶ , ùíõ n ] ) = ùíõ 1 , f_{\\theta}^{\\mathrm{ext}}(\\bm{Z})\\doteq f_{\\theta}^{\\mathrm{ext}}([\\bm{z}^{1},\\dots,\\bm{z}^{n}])=\\bm{z}^{1}, italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT ( bold_italic_Z ) ‚âê italic_f start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_ext end_POSTSUPERSCRIPT ( [ bold_italic_z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , ‚Ä¶ , bold_italic_z start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ] ) = bold_italic_z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT "}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#A2.S3.EGx86", "title": "CE ‚Å° ( ùíë , ùíí ) \\displaystyle\\operatorname{CE}(\\bm{p},\\bm{q}) roman_CE ( bold_italic_p , bold_italic_q ) = ‚àí ‚àë i = 1 m p i ‚Äã log ‚Å° q i = ‚àë i = 1 m p i ‚Äã log ‚Å° ( p i / q i ) ‚àí ‚àë i = 1 m p i ‚Äã log ‚Å° p i ", "snippet": "CE ‚Å° ( ùíë , ùíí ) \\displaystyle\\operatorname{CE}(\\bm{p},\\bm{q}) roman_CE ( bold_italic_p , bold_italic_q ) = ‚àí ‚àë i = 1 m p i ‚Äã log ‚Å° q i = ‚àë i = 1 m p i ‚Äã log ‚Å° ( p i / q i ) ‚àí ‚àë i = 1 m p i ‚Äã log ‚Å° p i = ùñ™ùñ´ ‚Å° ( ùíë ‚à• ùíí ) + H ‚Äã ( ùíë ) \\displaystyle=-\\sum_{i=1}^{m}p_{i}\\log q_{i}=\\sum_{i=1}^{m}p_{i}\\log(p_{i}/q_{i})-\\sum_{i=1}^{m}p_{i}\\log p_{i}=\\operatorname{\\mathsf{KL}}(\\bm{p}\\;\\|\\;\\bm{q})+H(\\bm{p}) = - ‚àë start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT roman_log italic_q start_POSTSUBSCRIPT"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#A2.S3.EGx87", "title": "ùíÅ Œ∏ ‚Ñì + 1 / 2 ‚Äã ( ùëø ) \\displaystyle\\bm{Z}_{\\theta}^{\\ell+1/2}(\\bm{X}) bold_italic_Z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì + 1 / 2 end_POSTSUPERSCRIPT ( bold_ital", "snippet": "ùíÅ Œ∏ ‚Ñì + 1 / 2 ‚Äã ( ùëø ) \\displaystyle\\bm{Z}_{\\theta}^{\\ell+1/2}(\\bm{X}) bold_italic_Z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì + 1 / 2 end_POSTSUPERSCRIPT ( bold_italic_X ) = ùíÅ Œ∏ ‚Ñì ‚Äã ( ùëø ) + MHSA Œ∏ ‚Ñì ‚Å° ( LN Œ∏ 1 , ‚Ñì ‚Å° ( ùíÅ Œ∏ ‚Ñì ‚Äã ( ùëø ) ) ) \\displaystyle=\\bm{Z}_{\\theta}^{\\ell}(\\bm{X})+\\operatorname{MHSA}_{\\theta}^{\\ell}(\\operatorname{LN}_{\\theta}^{1,\\ell}(\\bm{Z}_{\\theta}^{\\ell}(\\bm{X}))) = bold_italic_Z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ( bold_italic_X ) + roman_MHSA start_POSTSUBSCRIPT italic_Œ∏ end_PO"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#A2.S3.EGx88", "title": "MHSA Œ∏ ‚Ñì ‚Å° ( ùíÅ ) \\displaystyle\\operatorname{MHSA}_{\\theta}^{\\ell}(\\bm{Z}) roman_MHSA start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ( bold_italic_Z ) ", "snippet": "MHSA Œ∏ ‚Ñì ‚Å° ( ùíÅ ) \\displaystyle\\operatorname{MHSA}_{\\theta}^{\\ell}(\\bm{Z}) roman_MHSA start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ( bold_italic_Z ) ‚âê ùëº out ‚Ñì ‚Äã [ SA ‚Å° ( [ ùëº qry 1 , ‚Ñì ] ‚ä§ ‚Äã ùíÅ , [ ùëº key 1 , ‚Ñì ] ‚ä§ ‚Äã ùíÅ , [ ùëº val 1 , ‚Ñì ] ‚ä§ ‚Äã ùíÅ ) ‚ãÆ SA ‚Å° ( [ ùëº qry K , ‚Ñì ] ‚ä§ ‚Äã ùíÅ , [ ùëº key K , ‚Ñì ] ‚ä§ ‚Äã ùíÅ , [ ùëº val K , ‚Ñì ] ‚ä§ ‚Äã ùíÅ ) ] + ùíÉ out ‚Ñì ‚Äã ùüè n ‚ä§ , \\displaystyle\\doteq\\bm{U}_{\\mathrm{out}}^{\\ell}\\begin{bmatrix}\\operatorname{SA}([\\bm{U}_{\\mathrm{qry}}^{1,\\ell}]^{\\top}\\bm{Z},[\\bm{U}_{\\mathrm{key}}^{1,\\ell}]^{\\top}\\bm{Z},[\\bm{U}_{\\mathrm{val}}^{1,\\ell}]"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#A2.S3.EGx89", "title": "softmax ‚Å° ( ùë¥ ) \\displaystyle\\operatorname{\\mathrm{softmax}}(\\bm{M}) roman_softmax ( bold_italic_M ) ‚âê [ softmax ‚Å° ( ùíé 1 ) ‚ãØ softmax ‚Å° ( ùíé p ) ] , \\displaystyle\\doteq\\begin{bmatrix}\\operatorname{\\math", "snippet": "softmax ‚Å° ( ùë¥ ) \\displaystyle\\operatorname{\\mathrm{softmax}}(\\bm{M}) roman_softmax ( bold_italic_M ) ‚âê [ softmax ‚Å° ( ùíé 1 ) ‚ãØ softmax ‚Å° ( ùíé p ) ] , \\displaystyle\\doteq\\begin{bmatrix}\\operatorname{\\mathrm{softmax}}(\\bm{m}_{1})&\\cdots&\\operatorname{\\mathrm{softmax}}(\\bm{m}_{p})\\end{bmatrix}, ‚âê [ start_ARG start_ROW start_CELL roman_softmax ( bold_italic_m start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) end_CELL start_CELL ‚ãØ end_CELL start_CELL roman_softmax ( bold_italic_m start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) end_CELL end_ROW end_ARG ] , (7.2.17) ‚àÄ ùë¥ \\displaystyle\\forall\\bm{M} ‚àÄ bold_italic"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#A2.S3.EGx90", "title": "ùíõ Œ∏ s ‚Äã ( ùëø b , g ( k ) , i ) ‚âê ( f Œ∏ s ext ‚àò f Œ∏ s ) ‚Äã ( ùëø b , g ( k ) , i ) , ùíë Œ∏ s , ùëæ s ‚Äã ( ùëø b , g ( k ) , i ) ‚âê h ùëæ s , ùüé m ‚Äã ( ùíõ Œ∏ s ‚Äã ( ùëø b , g ( k ) , i ) ) , \\displaystyle\\bm{z}_{\\theta_{\\ma", "snippet": "ùíõ Œ∏ s ‚Äã ( ùëø b , g ( k ) , i ) ‚âê ( f Œ∏ s ext ‚àò f Œ∏ s ) ‚Äã ( ùëø b , g ( k ) , i ) , ùíë Œ∏ s , ùëæ s ‚Äã ( ùëø b , g ( k ) , i ) ‚âê h ùëæ s , ùüé m ‚Äã ( ùíõ Œ∏ s ‚Äã ( ùëø b , g ( k ) , i ) ) , \\displaystyle\\bm{z}_{\\theta_{\\mathrm{s}}}(\\bm{X}_{b,g}^{(k),i})\\doteq(f_{\\theta_{\\mathrm{s}}}^{\\mathrm{ext}}\\circ f_{\\theta_{\\mathrm{s}}})(\\bm{X}_{b,g}^{(k),i}),\\qquad\\bm{p}_{\\theta_{\\mathrm{s}},\\bm{W}_{\\mathrm{s}}}(\\bm{X}_{b,g}^{(k),i})\\doteq h_{\\bm{W}_{\\mathrm{s}},\\bm{0}_{m}}(\\bm{z}_{\\theta_{\\mathrm{s}}}(\\bm{X}_{b,g}^{(k),i})), bold_italic_z start_POSTSUBSCRIPT italic_Œ∏ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT end_POSTSUB"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#A2.S3.EGx91", "title": "‚Ñí ^ DINO ‚àí st ( k ) ‚Äã ( Œ∏ s , Œ∏ t , ùëæ s , ùëæ t , ùùÅ ) ‚âê 1 B ‚Äã M glo ‚Äã ( M glo + M loc ‚àí 1 ) ‚Äã ‚àë b = 1 B ‚àë i = 1 M glo \\displaystyle\\hat{\\mathcal{L}}_{\\mathrm{DINO}{}-\\mathrm{s}\\mathrm{t}}^{(k)}(\\theta_{", "snippet": "‚Ñí ^ DINO ‚àí st ( k ) ‚Äã ( Œ∏ s , Œ∏ t , ùëæ s , ùëæ t , ùùÅ ) ‚âê 1 B ‚Äã M glo ‚Äã ( M glo + M loc ‚àí 1 ) ‚Äã ‚àë b = 1 B ‚àë i = 1 M glo \\displaystyle\\hat{\\mathcal{L}}_{\\mathrm{DINO}{}-\\mathrm{s}\\mathrm{t}}^{(k)}(\\theta_{\\mathrm{s}},\\theta_{\\mathrm{t}},\\bm{W}_{\\mathrm{s}},\\bm{W}_{\\mathrm{t}},\\bm{\\mu})\\doteq\\frac{1}{BM_{\\mathrm{glo}}(M_{\\mathrm{glo}}+M_{\\mathrm{loc}}-1)}\\sum_{b=1}^{B}\\sum_{i=1}^{M_{\\mathrm{glo}}} over^ start_ARG caligraphic_L end_ARG start_POSTSUBSCRIPT roman_DINO - roman_st end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_Œ∏ start_POSTSUBSCRIPT roman_s end_POSTSUBSC"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#A2.S3.EGx92", "title": "( Œ∏ s ( k + 1 ) , ùëæ s ( k + 1 ) ) \\displaystyle(\\theta_{\\mathrm{s}}^{(k+1)},\\bm{W}_{\\mathrm{s}}^{(k+1)}) ( italic_Œ∏ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k + 1 )", "snippet": "( Œ∏ s ( k + 1 ) , ùëæ s ( k + 1 ) ) \\displaystyle(\\theta_{\\mathrm{s}}^{(k+1)},\\bm{W}_{\\mathrm{s}}^{(k+1)}) ( italic_Œ∏ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k + 1 ) end_POSTSUPERSCRIPT , bold_italic_W start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k + 1 ) end_POSTSUPERSCRIPT ) = OptUpdate ( k ) ‚Äã ( Œ∏ s ( k ) , ùëæ s ( k ) ; ‚àá ( Œ∏ s , ùëæ s ) ‚Ñí ^ DINO ‚àí st ( k ) ) \\displaystyle=\\textsc{OptUpdate}^{(k)}(\\theta_{\\mathrm{s}}^{(k)},\\bm{W}_{\\mathrm{s}}^{(k)};\\nabla_{(\\theta_{\\mathrm{s}},\\bm{W}_{\\mathrm{s}})}\\hat{\\mathcal{L}}_{\\mathrm{DINO"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#A2.S3.EGx93", "title": "‚Ñí ^ SimDINO ‚àí st ( k ) ( Œ∏ s , Œ∏ t ) ‚âê 1 B ‚Äã M glo ‚Äã ( M glo + M loc ‚àí 1 ) ‚àë b = 1 B ‚àë i = 1 M glo [ ‚àë j = 1 M loc d ‚Ñì 2 ( ùíõ Œ∏ t ( ùëø b , g ( k ) , i ) , ùíõ Œ∏ s ( ùëø b , ‚Ñì ( k ) , j ) ) \\displaystyle\\hat", "snippet": "‚Ñí ^ SimDINO ‚àí st ( k ) ( Œ∏ s , Œ∏ t ) ‚âê 1 B ‚Äã M glo ‚Äã ( M glo + M loc ‚àí 1 ) ‚àë b = 1 B ‚àë i = 1 M glo [ ‚àë j = 1 M loc d ‚Ñì 2 ( ùíõ Œ∏ t ( ùëø b , g ( k ) , i ) , ùíõ Œ∏ s ( ùëø b , ‚Ñì ( k ) , j ) ) \\displaystyle\\hat{\\mathcal{L}}_{\\mathrm{SimDINO}{}-\\mathrm{s}\\mathrm{t}}^{(k)}(\\theta_{\\mathrm{s}},\\theta_{\\mathrm{t}})\\doteq\\frac{1}{BM_{\\mathrm{glo}}(M_{\\mathrm{glo}}+M_{\\mathrm{loc}}-1)}\\sum_{b=1}^{B}\\sum_{i=1}^{M_{\\mathrm{glo}}}\\Bigg{[}\\sum_{j=1}^{M_{\\mathrm{loc}}}d_{\\ell^{2}}(\\bm{z}_{\\theta_{\\mathrm{t}}}(\\bm{X}_{b,g}^{(k),i}),\\bm{z}_{\\theta_{\\mathrm{s}}}(\\bm{X}_{b,\\ell}^{(k),j})) over^ start_ARG caligraphic_L"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#A2.S3.EGx94", "title": "Œ∏ s ( k + 1 ) \\displaystyle\\theta_{\\mathrm{s}}^{(k+1)} italic_Œ∏ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k + 1 ) end_POSTSUPERSCRIPT = OptUpdate ( k ) ‚Äã ( Œ∏ s ( k )", "snippet": "Œ∏ s ( k + 1 ) \\displaystyle\\theta_{\\mathrm{s}}^{(k+1)} italic_Œ∏ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k + 1 ) end_POSTSUPERSCRIPT = OptUpdate ( k ) ‚Äã ( Œ∏ s ( k ) ; ‚àá Œ∏ s ‚Ñí ^ SimDINO ‚àí st ( k ) ) \\displaystyle=\\textsc{OptUpdate}^{(k)}(\\theta_{\\mathrm{s}}^{(k)};\\nabla_{\\theta_{\\mathrm{s}}}\\hat{\\mathcal{L}}_{\\mathrm{SimDINO}-\\mathrm{s}\\mathrm{t}}^{(k)}) = OptUpdate start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ( italic_Œ∏ start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_k ) end_POSTSUPERSCRIPT ; ‚àá start_POSTSUBSCRIPT itali"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#A2.S3.EGx95", "title": "ùëø ‚Ü¶ ‚ãØ ‚Ü¶ ùíÅ L ‚àí 1 = [ ùíõ 1 L ‚àí 1 ‚èü class token , ùíõ 2 L ‚àí 1 ‚Äã ‚Ä¶ , ùíõ n L ‚àí 1 ‚èü patch tokens ] ‚Ü¶ ùë® k , L = [ ùë® 1 , 1 k , L ùë® 1 , 2 : k , L ùë® 2 ‚Å£ : , 1 k , L ùë® 2 ‚Å£ : , 2 : k , L ] . \\displaystyle\\bm{X}\\mapst", "snippet": "ùëø ‚Ü¶ ‚ãØ ‚Ü¶ ùíÅ L ‚àí 1 = [ ùíõ 1 L ‚àí 1 ‚èü class token , ùíõ 2 L ‚àí 1 ‚Äã ‚Ä¶ , ùíõ n L ‚àí 1 ‚èü patch tokens ] ‚Ü¶ ùë® k , L = [ ùë® 1 , 1 k , L ùë® 1 , 2 : k , L ùë® 2 ‚Å£ : , 1 k , L ùë® 2 ‚Å£ : , 2 : k , L ] . \\displaystyle\\bm{X}\\mapsto\\cdots\\mapsto\\bm{Z}^{L-1}=[\\underbrace{\\bm{z}_{1}^{L-1}}_{\\text{class token}},\\underbrace{\\bm{z}_{2}^{L-1}\\dots,\\bm{z}_{n}^{L-1}}_{\\text{patch tokens}}]\\mapsto\\bm{A}^{k,L}=\\begin{bmatrix}\\bm{A}_{1,1}^{k,L}&\\bm{A}_{1,2:}^{k,L}\\\\ \\bm{A}_{2:,1}^{k,L}&\\bm{A}_{2:,2:}^{k,L}\\end{bmatrix}. bold_italic_X ‚Ü¶ ‚ãØ ‚Ü¶ bold_italic_Z start_POSTSUPERSCRIPT italic_L - 1 end_POSTSUPERSCRIPT = [ under‚èü start_ARG bold_i"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#A2.S3.EGx96", "title": "ùíÅ Œ∏ ‚Ñì + 1 / 2 ‚Äã ( ùëø ) \\displaystyle\\bm{Z}_{\\theta}^{\\ell+1/2}(\\bm{X}) bold_italic_Z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì + 1 / 2 end_POSTSUPERSCRIPT ( bold_ital", "snippet": "ùíÅ Œ∏ ‚Ñì + 1 / 2 ‚Äã ( ùëø ) \\displaystyle\\bm{Z}_{\\theta}^{\\ell+1/2}(\\bm{X}) bold_italic_Z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì + 1 / 2 end_POSTSUPERSCRIPT ( bold_italic_X ) = ùíÅ Œ∏ ‚Ñì ‚Äã ( ùëø ) + MSSA Œ∏ ‚Ñì ‚Å° ( LN Œ∏ 1 , ‚Ñì ‚Å° ( ùíÅ Œ∏ ‚Ñì ‚Äã ( ùëø ) ) ) , \\displaystyle=\\bm{Z}_{\\theta}^{\\ell}(\\bm{X})+\\operatorname{MSSA}_{\\theta}^{\\ell}(\\operatorname{LN}_{\\theta}^{1,\\ell}(\\bm{Z}_{\\theta}^{\\ell}(\\bm{X}))), = bold_italic_Z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ( bold_italic_X ) + roman_MSSA start_POSTSUBSCRIPT italic_Œ∏ end"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#A2.S3.EGx97", "title": "CausalMSSA Œ∏ ‚Ñì ‚Å° ( ùíÅ ) ‚âê ùëº out ‚Ñì ‚Äã [ CausalSA ‚Å° ( [ ùëº 1 , ‚Ñì ] ‚ä§ ‚Äã ùíÅ , [ ùëº 1 , ‚Ñì ] ‚ä§ ‚Äã ùíÅ , [ ùëº 1 , ‚Ñì ] ‚ä§ ‚Äã ùíÅ ) ‚ãÆ CausalSA ‚Å° ( [ ùëº K , ‚Ñì ] ‚ä§ ‚Äã ùíÅ , [ ùëº K , ‚Ñì ] ‚ä§ ‚Äã ùíÅ , [ ùëº 1 , ‚Ñì ] ‚ä§ ‚Äã ùíÅ ) ] + ùíÉ out ‚Ñì ‚Äã ùüè", "snippet": "CausalMSSA Œ∏ ‚Ñì ‚Å° ( ùíÅ ) ‚âê ùëº out ‚Ñì ‚Äã [ CausalSA ‚Å° ( [ ùëº 1 , ‚Ñì ] ‚ä§ ‚Äã ùíÅ , [ ùëº 1 , ‚Ñì ] ‚ä§ ‚Äã ùíÅ , [ ùëº 1 , ‚Ñì ] ‚ä§ ‚Äã ùíÅ ) ‚ãÆ CausalSA ‚Å° ( [ ùëº K , ‚Ñì ] ‚ä§ ‚Äã ùíÅ , [ ùëº K , ‚Ñì ] ‚ä§ ‚Äã ùíÅ , [ ùëº 1 , ‚Ñì ] ‚ä§ ‚Äã ùíÅ ) ] + ùíÉ out ‚Ñì ‚Äã ùüè N ‚ä§ \\displaystyle\\operatorname{CausalMSSA}_{\\theta}^{\\ell}(\\bm{Z})\\doteq\\bm{U}_{\\mathrm{out}}^{\\ell}\\begin{bmatrix}\\operatorname{CausalSA}([\\bm{U}^{1,\\ell}]^{\\top}\\bm{Z},[\\bm{U}^{1,\\ell}]^{\\top}\\bm{Z},[\\bm{U}^{1,\\ell}]^{\\top}\\bm{Z})\\\\ \\vdots\\\\ \\operatorname{CausalSA}([\\bm{U}^{K,\\ell}]^{\\top}\\bm{Z},[\\bm{U}^{K,\\ell}]^{\\top}\\bm{Z},[\\bm{U}^{1,\\ell}]^{\\top}\\bm{Z})\\end{bmatrix}+\\bm{b}_{\\mathrm{out}}^{\\"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#A2.S3.EGx98", "title": "ùíÅ Œ∏ ‚Ñì + 1 / 2 ‚Äã ( ùëø ) \\displaystyle\\bm{Z}_{\\theta}^{\\ell+1/2}(\\bm{X}) bold_italic_Z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì + 1 / 2 end_POSTSUPERSCRIPT ( bold_ital", "snippet": "ùíÅ Œ∏ ‚Ñì + 1 / 2 ‚Äã ( ùëø ) \\displaystyle\\bm{Z}_{\\theta}^{\\ell+1/2}(\\bm{X}) bold_italic_Z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì + 1 / 2 end_POSTSUPERSCRIPT ( bold_italic_X ) = ùíÅ Œ∏ ‚Ñì ‚Äã ( ùëø ) + TSSA Œ∏ ‚Ñì ‚Å° ( LN Œ∏ 1 , ‚Ñì ‚Å° ( ùíÅ Œ∏ ‚Ñì ‚Äã ( ùëø ) ) ) \\displaystyle=\\bm{Z}_{\\theta}^{\\ell}(\\bm{X})+\\operatorname{TSSA}_{\\theta}^{\\ell}(\\operatorname{LN}_{\\theta}^{1,\\ell}(\\bm{Z}_{\\theta}^{\\ell}(\\bm{X}))) = bold_italic_Z start_POSTSUBSCRIPT italic_Œ∏ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ( bold_italic_X ) + roman_TSSA start_POSTSUBSCRIPT italic_Œ∏ end_PO"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#A2.S3.EGx99", "title": "ùíÅ ~ Œ∏ , Œ∑ ‚Ñì + 1 / 2 ‚Äã ( ùëø ) \\displaystyle\\tilde{\\bm{Z}}_{\\theta,\\eta}^{\\ell+1/2}(\\bm{X}) over~ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_Œ∏ , italic_Œ∑ end_POSTSUBSCRIPT start_POSTSUPER", "snippet": "ùíÅ ~ Œ∏ , Œ∑ ‚Ñì + 1 / 2 ‚Äã ( ùëø ) \\displaystyle\\tilde{\\bm{Z}}_{\\theta,\\eta}^{\\ell+1/2}(\\bm{X}) over~ start_ARG bold_italic_Z end_ARG start_POSTSUBSCRIPT italic_Œ∏ , italic_Œ∑ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT roman_‚Ñì + 1 / 2 end_POSTSUPERSCRIPT ( bold_italic_X ) = [ ùë´ ~ ‚Ñì ] ‚ä§ ‚Äã LN Œ∑ 1 , ‚Ñì ‚Å° ( ùíÅ ~ Œ∏ , Œ∑ ‚Ñì ‚Äã ( ùëø ) ) \\displaystyle=[\\tilde{\\bm{D}}^{\\ell}]^{\\top}\\operatorname{LN}_{\\eta}^{1,\\ell}(\\tilde{\\bm{Z}}_{\\theta,\\eta}^{\\ell}(\\bm{X})) = [ over~ start_ARG bold_italic_D end_ARG start_POSTSUPERSCRIPT roman_‚Ñì end_POSTSUPERSCRIPT ] start_POSTSUPERSCRIPT ‚ä§ end_POSTSUPERSCRIPT roman_LN start_POSTSUBSCR"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS2.SSSx1", "title": "Training a Tokenizer", "snippet": "Training a Tokenizer To build a tokenizer, it amounts to building a vocabulary ùí± \\mathcal{V} caligraphic_V , which is a set of tokens and has some pre-specified size V V italic_V . There are several methods to do this. One popular algorithm is known as Byte Pair Encoding (BPE), which can be described as: ‚Ä¢ Start with a list of all unique characters in your training data, and their frequencies. Ensure that there are fewer than V V italic_V such characters, and add each character as a separate string (‚Äútoken‚Äù) to the vocabulary along with its frequency. ‚Ä¢ Until there are V V italic_V tokens in t"}, {"page": "Chapter 7 Learning Representations for Real-World Data", "href": "Ch7.html#S4.SS2.SSSx2", "title": "Training a Language Model", "snippet": "Training a Language Model Once we have each document as a sequence of tokens ùëø ‚àà [ V ] N ‚äÜ [ V ] ‚àó = ùíØ \\bm{X}\\in[V]^{N}\\subseteq[V]^{*}=\\mathcal{T} bold_italic_X ‚àà [ italic_V ] start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT ‚äÜ [ italic_V ] start_POSTSUPERSCRIPT ‚àó end_POSTSUPERSCRIPT = caligraphic_T , we wish to perform next-token prediction. That is, given a context ùëø : n ‚àà [ V ] n \\bm{X}_{:n}\\in[V]^{n} bold_italic_X start_POSTSUBSCRIPT : italic_n end_POSTSUBSCRIPT ‚àà [ italic_V ] start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT (i.e., the first n n italic_n tokens ùíô 1 , ‚Ä¶ , ùíô n ‚àà [ V ] \\b"}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#top", "title": "Chapter 8 Future Study of Intelligence", "snippet": ""}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S1", "title": "8.1 Towards Autonomous Intelligence: Close the Loop?", "snippet": "8.1 Towards Autonomous Intelligence: Close the Loop? From the practice of machine intelligence in the past decade, it has become clear that, if there were sufficient data and computational resources, one could build a large enough model and pre-train it to learn the a priori distribution of all the data, say p ‚Äã ( ùíô ) p(\\bm{x}) italic_p ( bold_italic_x ) . Theoretically, such a large model can memorize almost all existing knowledge about the world that has been encoded in massive languages and texts. As we have discussed at the beginning of the book, in a way, such a large model plays a simila"}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S2", "title": "8.2 Towards Intelligence of Nature: Beyond Back Propagation?", "snippet": "8.2 Towards Intelligence of Nature: Beyond Back Propagation? The practice of machine intelligence in the past few years has led many to believe that one needs to build a single large model to learn the distribution of all data and memorize all knowledge. Even if this might be technologically possible, it is likely that such a solution is far from necessary and efficient. As we have known from the practice of training deep networks, the only known scalable method to train such networks at scale is through back propagation (BP) [ RHW86a ] . Although BP has offered a way to correct errors via gra"}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S3", "title": "8.3 Towards Artificial Intelligence of Human: Beyond the Turing Test?", "snippet": "8.3 Towards Artificial Intelligence of Human: Beyond the Turing Test? As we have discussed at the beginning of this book, Chapter 1 , intelligence in nature has evolved through multiple phases and manifested in four different forms: phylogentic ‚üπ ontogenetic ‚üπ societal ‚üπ artificial intelligence . \\mbox{{phylogentic}}\\;\\Longrightarrow\\;\\mbox{{ontogenetic}}\\;\\Longrightarrow\\;\\mbox{{societal}}\\;\\Longrightarrow\\;\\mbox{{artificial intelligence}}. phylogentic ‚üπ ontogenetic ‚üπ societal ‚üπ artificial intelligence . (8.3.1) All forms of intelligence share the common objective of learning useful knowledge"}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#p1", "title": "‚Äú The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to sim", "snippet": "‚Äú The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it. An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problem now reserved for humans, and improve themselves. ‚Äù ‚Äì Proposal for the Dartmouth AI program, 1956"}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#p2", "title": "Generally speaking, this manuscript is meant to systematically introduce mathematical principles and computational mechanisms for how memory or knowledge can be developed from empirical observations. ", "snippet": "Generally speaking, this manuscript is meant to systematically introduce mathematical principles and computational mechanisms for how memory or knowledge can be developed from empirical observations. The capability to seek parsimony in a seemingly random world is a fundamental characteristic of any intelligence, natural or man-made. We believe that the principles and mechanisms presented in this book are rather unifying and universal and are applicable to both animals and machines."}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#p3", "title": "We hope that this book can help the readers fully clarify the mystery around modern practices of artificial deep neural networks by developing a rigorous understanding of their functions and roles in ", "snippet": "We hope that this book can help the readers fully clarify the mystery around modern practices of artificial deep neural networks by developing a rigorous understanding of their functions and roles in achieving the objective of learning low-dimensional distributions from high-dimensional data. With such a understanding, we should have become rather clear both capabilities and limitations of existing AI models and systems: 1. Existing models and systems are short of being complete in terms of a memory system that is capable of self-learning and self-improving. 2. Existing realizations of these f"}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S0.I1.i1.p1", "title": "Existing models and systems are short of being complete in terms of a memory system that is capable of self-learning and self-improving.", "snippet": "Existing models and systems are short of being complete in terms of a memory system that is capable of self-learning and self-improving."}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S0.I1.i2.p1", "title": "Existing realizations of these functions are still rather primitive and brute force and certainly far from optimal in terms of optimization strategies hence network architectures.", "snippet": "Existing realizations of these functions are still rather primitive and brute force and certainly far from optimal in terms of optimization strategies hence network architectures."}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S0.I1.i3.p1", "title": "Existing AI models only learn the data distribution and conduct inductive (Bayesian) inference, which is different from the high-level human intelligence.", "snippet": "Existing AI models only learn the data distribution and conduct inductive (Bayesian) inference, which is different from the high-level human intelligence."}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#p4", "title": "One of the goals of this book is for people to establish an objective and systematic understanding of current machine intelligence technologies and to realize what open problems and challenges remain ", "snippet": "One of the goals of this book is for people to establish an objective and systematic understanding of current machine intelligence technologies and to realize what open problems and challenges remain ahead for further advancement of machine intelligence. In the last chapter of the book, we provide some of our views and projections for the future."}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S1.p1", "title": "From the practice of machine intelligence in the past decade, it has become clear that, if there were sufficient data and computational resources, one could build a large enough model and pre-train it", "snippet": "From the practice of machine intelligence in the past decade, it has become clear that, if there were sufficient data and computational resources, one could build a large enough model and pre-train it to learn the a priori distribution of all the data, say p ‚Äã ( ùíô ) p(\\bm{x}) italic_p ( bold_italic_x ) . Theoretically, such a large model can memorize almost all existing knowledge about the world that has been encoded in massive languages and texts. As we have discussed at the beginning of the book, in a way, such a large model plays a similar role as DNAs with which life uses to record and pas"}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S1.p2", "title": "The so learned model and distribution can then be used to regenerate new data samples based on the same distribution. One can also use the model to conduct inference (e.g., estimation, prediction) wit", "snippet": "The so learned model and distribution can then be used to regenerate new data samples based on the same distribution. One can also use the model to conduct inference (e.g., estimation, prediction) with the memorized knowledge under various conditions, say by sampling the a posteriori distribution p ‚Äã ( ùíô ‚à£ ùíö ) p(\\bm{x}\\mid\\bm{y}) italic_p ( bold_italic_x ‚à£ bold_italic_y ) under a new observation ùíö \\bm{y} bold_italic_y . Strictly speaking, such inference is statistical."}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S1.p3", "title": "Any pre-trained model, however large, cannot guarantee that the distribution that it has learned so far is entirely correct or complete. In case our samples ùíô ^ t \\hat{\\bm{x}}_{t} over^ start_ARG bold", "snippet": "Any pre-trained model, however large, cannot guarantee that the distribution that it has learned so far is entirely correct or complete. In case our samples ùíô ^ t \\hat{\\bm{x}}_{t} over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT from the current a priori p t ‚Äã ( ùíô ) p_{t}(\\bm{x}) italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_x ) or estimates ùíô ^ t ‚Äã ( ùíö ) \\hat{\\bm{x}}_{t}(\\bm{y}) over^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_y ) based on the a posteriori p t ‚Äã ( ùíô ‚à£ ùíö ) p_{t}(\\bm{x}"}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S1.p4", "title": "As we have studied earlier in this book (Chapter 5 in particular), closed-loop systems allows to align an internal representation with the (sensed) observations of the external world. It can continue ", "snippet": "As we have studied earlier in this book (Chapter 5 in particular), closed-loop systems allows to align an internal representation with the (sensed) observations of the external world. It can continue to improve the internally learned distribution and its representation to achieve consistency or self-consistency. An immediate step forward for the future is to develop and build truly closed-loop memory systems, as shown in Figure 8.1 , that are capable of learning and improving more general data distributions autonomously and continuously based on error feedback."}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S1.p5", "title": "Therefore, transition from the current popular end-to-end trained open-loop models to continuously-learning closed-loop systems: open-ended models ‚üπ closed-loop systems \\mbox{{open-ended} models}\\;\\Lo", "snippet": "Therefore, transition from the current popular end-to-end trained open-loop models to continuously-learning closed-loop systems: open-ended models ‚üπ closed-loop systems \\mbox{{open-ended} models}\\;\\Longrightarrow\\;\\mbox{{closed-loop} systems} bold_open-ended models ‚üπ bold_closed-loop systems (8.1.2) is the key for machines to truly emulate how the (animal) brain learns and applies knowledge in an open world. We believe that open-ended models are for a closed world, however large; closed-loop systems are for an open world, however small. In fact, ‚Äúgeneral intelligence‚Äù could never be achieved b"}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S2.p1", "title": "The practice of machine intelligence in the past few years has led many to believe that one needs to build a single large model to learn the distribution of all data and memorize all knowledge. Even i", "snippet": "The practice of machine intelligence in the past few years has led many to believe that one needs to build a single large model to learn the distribution of all data and memorize all knowledge. Even if this might be technologically possible, it is likely that such a solution is far from necessary and efficient. As we have known from the practice of training deep networks, the only known scalable method to train such networks at scale is through back propagation (BP) [ RHW86a ] . Although BP has offered a way to correct errors via gradient signals propagated back through the whole model, it is "}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S2.p2", "title": "More generally, we cannot truly understand intelligence unless we also understand how it can be efficiently implemented. That is, one needs to address the computational complexity of realizing mechani", "snippet": "More generally, we cannot truly understand intelligence unless we also understand how it can be efficiently implemented. That is, one needs to address the computational complexity of realizing mechanisms associated with achieving the objectives of intelligence. Note that, historically, our understanding of (machine) intelligence has precisely evolved through several phases, from the incomputable Kolmogorov complexity to Shannon‚Äôs entropy, from Turing‚Äôs computability to later understanding of tractability, 1 1 1 We say a problem is tractable if it allows an algorithm whose complexity is polynom"}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S2.p3", "title": "There remains a huge room for improvement of the efficiency of machine intelligence so that it can emulate the level of efficiency of natural intelligence, which should be of magnitudes more efficient", "snippet": "There remains a huge room for improvement of the efficiency of machine intelligence so that it can emulate the level of efficiency of natural intelligence, which should be of magnitudes more efficient than the current brute-force implementations. To this end, we need to discover new learning architectures and optimization mechanisms that enable learning data distributions under natural physical conditions and resource constraints, similar to those for intelligent beings in nature, say, without accessing all data at once or updating all model parameters at once (by BP)."}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S2.p4", "title": "The principled framework and approach laid out in this book can guide us to discover such new architectures and mechanisms. These new architectures and mechanisms should enable online continuous learn", "snippet": "The principled framework and approach laid out in this book can guide us to discover such new architectures and mechanisms. These new architectures and mechanisms should enable online continuous learning and can be updated through highly localized and sparse forward or backward optimization. So far, for learning a distribution, the only case for which we know such a solution exists is the simplest case of PCA, with the online PCA method introduced in Chapter 5 ."}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S2.p5", "title": "As we have learned from neuroscience, the cortex of our brain consists of tens of thousands of cortical columns. All cortical columns have similar physical structures and functions. They are highly pa", "snippet": "As we have learned from neuroscience, the cortex of our brain consists of tens of thousands of cortical columns. All cortical columns have similar physical structures and functions. They are highly parallel and distributed, though sparsely interconnected. Hence, we believe that in order to develop a more scalable and structured memory system, we need to consider architectures that emulate that of the cortex. Figure 8.2 shows such a hypothesized architecture, a massively distributed and hierarchical system that consists of many largely parallel closed-loop auto-encoding modules. These modules l"}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S2.p6", "title": "The distributed, hierarchical, and closed-loop system architecture illustrated in Figure 8.2 shares many characteristics of cortex of the brain. Such a system architecture may open up many more possib", "snippet": "The distributed, hierarchical, and closed-loop system architecture illustrated in Figure 8.2 shares many characteristics of cortex of the brain. Such a system architecture may open up many more possibilities than the current single large-model architecture. It makes exploring much more efficient learning and optimization mechanisms possible, and resulting more structured modular organization of the learned data distribution and knowledge. This would allow us to bring the implementation of machine intelligence to the next level of evolution: incomputable ‚üπ computable ‚üπ tractable ‚üπ scalable ‚üπ na"}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S3.p1", "title": "As we have discussed at the beginning of this book, Chapter 1 , intelligence in nature has evolved through multiple phases and manifested in four different forms: phylogentic ‚üπ ontogenetic ‚üπ societal ", "snippet": "As we have discussed at the beginning of this book, Chapter 1 , intelligence in nature has evolved through multiple phases and manifested in four different forms: phylogentic ‚üπ ontogenetic ‚üπ societal ‚üπ artificial intelligence . \\mbox{{phylogentic}}\\;\\Longrightarrow\\;\\mbox{{ontogenetic}}\\;\\Longrightarrow\\;\\mbox{{societal}}\\;\\Longrightarrow\\;\\mbox{{artificial intelligence}}. phylogentic ‚üπ ontogenetic ‚üπ societal ‚üπ artificial intelligence . (8.3.1) All forms of intelligence share the common objective of learning useful knowledge as certain low-dimensional distributions of sensed high-dimensional d"}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S3.I1.i1.p1", "title": "The codebook that one uses to learn and encode the intended information or knowledge.", "snippet": "The codebook that one uses to learn and encode the intended information or knowledge."}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S3.I1.i2.p1", "title": "The information or knowledge that are encoded and represented using the codebook.", "snippet": "The information or knowledge that are encoded and represented using the codebook."}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S3.I1.i3.p1", "title": "The optimization mechanisms used to improve the information or knowledge encoded.", "snippet": "The optimization mechanisms used to improve the information or knowledge encoded."}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S3.p2", "title": "As we now know, humans have achieved two quantum leaps in intelligence in history. The first is the development of spoken and written languages about five to ten thousand years ago. That has enabled h", "snippet": "As we now know, humans have achieved two quantum leaps in intelligence in history. The first is the development of spoken and written languages about five to ten thousand years ago. That has enabled human to share and pass on learned knowledge for generations, similar to the role of DNAs in nature. The second is the development of mathematics and logic about three thousand years ago, which have become a precise language for modern science. This new language has freed us from summarizing knowledge from observations in empirical forms and allowed us to formalize knowledge as verifiable or falsif"}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S3.p3", "title": "As we have discussed in the Introduction (Chapter 1 ), the 1956 ‚Äúartificial intelligence‚Äù (AI) program precisely aimed to study high-level functions such as mathematical abstractions, logical inferenc", "snippet": "As we have discussed in the Introduction (Chapter 1 ), the 1956 ‚Äúartificial intelligence‚Äù (AI) program precisely aimed to study high-level functions such as mathematical abstractions, logical inference, and problem solving that are believed to differentiate humans from animals: low-level (animal) intelligence ‚üπ high-level (human) intelligence. \\mbox{{low-level} (animal) intelligence}\\;\\Longrightarrow\\;\\mbox{{high-level} (human) intelligence.} bold_low-level (animal) intelligence ‚üπ bold_high-level (human) intelligence. (8.3.2) As we have clarified repeatedly in this book, much of the technologi"}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S3.p4", "title": "In fact, we know little about how to rigorously verify whether a system is truly capable of certain high-level intelligence, despite the fact that the Turing test has been proposed since 1950 [ Tur50 ", "snippet": "In fact, we know little about how to rigorously verify whether a system is truly capable of certain high-level intelligence, despite the fact that the Turing test has been proposed since 1950 [ Tur50 ] . 3 3 3 In Turning‚Äôs proposal, the evaluator is a human. However, most human evaluators‚Äô scientific training and knowledge can be limited and their conclusions can be subjective. For long such a test was not deemed necessary since the capabilities of machines were far below that of a human (or even animal). However, given recent technological advances, many models and systems have claimed to rea"}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S3.I2.i1.p1", "title": "simply having memorized the distribution of some knowledge-carrying data and regenerating them;", "snippet": "simply having memorized the distribution of some knowledge-carrying data and regenerating them;"}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S3.I2.i2.p1", "title": "being able to autonomously and continuously develop new knowledge from new observations;", "snippet": "being able to autonomously and continuously develop new knowledge from new observations;"}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S3.I2.i3.p1", "title": "truly having understood certain abstract knowledge and knowing how to apply it correctly;", "snippet": "truly having understood certain abstract knowledge and knowing how to apply it correctly;"}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S3.I2.i4.p1", "title": "being able to generate new scientific hypotheses or mathematical conjectures and verify them.", "snippet": "being able to generate new scientific hypotheses or mathematical conjectures and verify them."}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S3.p5", "title": "Figure 8.3 illustrate that probably there should be at least three different types of tests to evaluate and differentiate different types of intelligence capabilities: 1. The Norbert Wiener Test: The ", "snippet": "Figure 8.3 illustrate that probably there should be at least three different types of tests to evaluate and differentiate different types of intelligence capabilities: 1. The Norbert Wiener Test: The evaluate whether a system is capable of improving and developing new knowledge of its own or simply receives information through reinforced or supervised learning; 2. The Alan Turning Test: The evaluate whether a system can understand abstract knowledge or simply learns its statistics and uses it for Bayesian inference. 3. The Karl Popper Test: To evaluate whether a system is capable of exploring "}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S3.I3.i1.p1", "title": "The Norbert Wiener Test: The evaluate whether a system is capable of improving and developing new knowledge of its own or simply receives information through reinforced or supervised learning;", "snippet": "The Norbert Wiener Test: The evaluate whether a system is capable of improving and developing new knowledge of its own or simply receives information through reinforced or supervised learning;"}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S3.I3.i2.p1", "title": "The Alan Turning Test: The evaluate whether a system can understand abstract knowledge or simply learns its statistics and uses it for Bayesian inference.", "snippet": "The Alan Turning Test: The evaluate whether a system can understand abstract knowledge or simply learns its statistics and uses it for Bayesian inference."}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S3.I3.i3.p1", "title": "The Karl Popper Test: To evaluate whether a system is capable of exploring new knowledge through forming and verifying new theories based on self-consistency.", "snippet": "The Karl Popper Test: To evaluate whether a system is capable of exploring new knowledge through forming and verifying new theories based on self-consistency."}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S3.p6", "title": "As we have seen throughout this entire book, compression has played a most fundamental role in learning. It is the governing principle and a unified mechanism for identifying an (empirical) data distr", "snippet": "As we have seen throughout this entire book, compression has played a most fundamental role in learning. It is the governing principle and a unified mechanism for identifying an (empirical) data distribution and organizing information encoded therein. To a large extent, it explains most of the practice of ‚Äúartificial intelligence‚Äù in the past decade or so. Here the word ‚Äúartificial‚Äù largely means ‚Äúman-made.‚Äù An outstanding question for future study is whether compression alone is sufficient to achieve all higher-level capabilities of intelligence listed above? Is compression all there is? Whet"}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S1.E1", "title": "p t ‚Äã ( ùíô ) ‚Üí p t + 1 ‚Äã ( ùíô ) or p t ‚Äã ( ùíô ‚à£ ùíö ) ‚Üí p t + 1 ‚Äã ( ùíô ‚à£ ùíö ) , p_{t}(\\bm{x})\\rightarrow p_{t+1}(\\bm{x})\\quad\\mbox{or}\\quad p_{t}(\\bm{x}\\mid\\bm{y})\\rightarrow p_{t+1}(\\bm{x}\\mid\\bm{y}), itali", "snippet": "p t ‚Äã ( ùíô ) ‚Üí p t + 1 ‚Äã ( ùíô ) or p t ‚Äã ( ùíô ‚à£ ùíö ) ‚Üí p t + 1 ‚Äã ( ùíô ‚à£ ùíö ) , p_{t}(\\bm{x})\\rightarrow p_{t+1}(\\bm{x})\\quad\\mbox{or}\\quad p_{t}(\\bm{x}\\mid\\bm{y})\\rightarrow p_{t+1}(\\bm{x}\\mid\\bm{y}), italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_x ) ‚Üí italic_p start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT ( bold_italic_x ) or italic_p start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_italic_x ‚à£ bold_italic_y ) ‚Üí italic_p start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT ( bold_italic_x ‚à£ bold_italic_y ) , (8.1.1)"}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S1.E2", "title": "open-ended models ‚üπ closed-loop systems \\mbox{{open-ended} models}\\;\\Longrightarrow\\;\\mbox{{closed-loop} systems} bold_open-ended models ‚üπ bold_closed-loop systems (8.1.2)", "snippet": "open-ended models ‚üπ closed-loop systems \\mbox{{open-ended} models}\\;\\Longrightarrow\\;\\mbox{{closed-loop} systems} bold_open-ended models ‚üπ bold_closed-loop systems (8.1.2)"}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S2.E1", "title": "incomputable ‚üπ computable ‚üπ tractable ‚üπ scalable . \\mbox{{incomputable}}\\;\\Longrightarrow\\;\\mbox{{computable}}\\;\\Longrightarrow\\;\\mbox{{tractable}}\\;\\Longrightarrow\\;\\mbox{{scalable}}. incomputable ‚üπ ", "snippet": "incomputable ‚üπ computable ‚üπ tractable ‚üπ scalable . \\mbox{{incomputable}}\\;\\Longrightarrow\\;\\mbox{{computable}}\\;\\Longrightarrow\\;\\mbox{{tractable}}\\;\\Longrightarrow\\;\\mbox{{scalable}}. incomputable ‚üπ computable ‚üπ tractable ‚üπ scalable . (8.2.1)"}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S2.E2", "title": "incomputable ‚üπ computable ‚üπ tractable ‚üπ scalable ‚üπ natural . \\mbox{{incomputable}}\\;\\Longrightarrow\\;\\mbox{{computable}}\\;\\Longrightarrow\\;\\mbox{{tractable}}\\;\\Longrightarrow\\;\\mbox{{scalable}}\\;\\Long", "snippet": "incomputable ‚üπ computable ‚üπ tractable ‚üπ scalable ‚üπ natural . \\mbox{{incomputable}}\\;\\Longrightarrow\\;\\mbox{{computable}}\\;\\Longrightarrow\\;\\mbox{{tractable}}\\;\\Longrightarrow\\;\\mbox{{scalable}}\\;\\Longrightarrow\\;\\mbox{{natural}}. incomputable ‚üπ computable ‚üπ tractable ‚üπ scalable ‚üπ natural . (8.2.2)"}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S3.E1", "title": "phylogentic ‚üπ ontogenetic ‚üπ societal ‚üπ artificial intelligence . \\mbox{{phylogentic}}\\;\\Longrightarrow\\;\\mbox{{ontogenetic}}\\;\\Longrightarrow\\;\\mbox{{societal}}\\;\\Longrightarrow\\;\\mbox{{artificial int", "snippet": "phylogentic ‚üπ ontogenetic ‚üπ societal ‚üπ artificial intelligence . \\mbox{{phylogentic}}\\;\\Longrightarrow\\;\\mbox{{ontogenetic}}\\;\\Longrightarrow\\;\\mbox{{societal}}\\;\\Longrightarrow\\;\\mbox{{artificial intelligence}}. phylogentic ‚üπ ontogenetic ‚üπ societal ‚üπ artificial intelligence . (8.3.1)"}, {"page": "Chapter 8 Future Study of Intelligence", "href": "Ch8.html#S3.E2", "title": "low-level (animal) intelligence ‚üπ high-level (human) intelligence. \\mbox{{low-level} (animal) intelligence}\\;\\Longrightarrow\\;\\mbox{{high-level} (human) intelligence.} bold_low-level (animal) intellig", "snippet": "low-level (animal) intelligence ‚üπ high-level (human) intelligence. \\mbox{{low-level} (animal) intelligence}\\;\\Longrightarrow\\;\\mbox{{high-level} (human) intelligence.} bold_low-level (animal) intelligence ‚üπ bold_high-level (human) intelligence. (8.3.2)"}, {"page": "Preface", "href": "Chx1.html#top", "title": "Preface", "snippet": ""}, {"page": "Preface", "href": "Chx1.html#p1", "title": "‚Äú All roads lead to Rome .‚Äù", "snippet": "‚Äú All roads lead to Rome .‚Äù"}, {"page": "Preface", "href": "Chx1.html#p2", "title": "This book reveals and studies a common and fundamental problem behind almost all modern practices of (artificial) intelligence. That is, how to effectively and efficiently learn a low-dimensional dist", "snippet": "This book reveals and studies a common and fundamental problem behind almost all modern practices of (artificial) intelligence. That is, how to effectively and efficiently learn a low-dimensional distribution of data in a high-dimensional space and then transform the distribution to a compact and structured representation? For any intelligent system, natural or man-made, such a representation can be generally regarded as a memory or knowledge learned from data sensed from the external world."}, {"page": "Preface", "href": "Chx1.html#p3", "title": "This textbook aims to provide a systematic introduction to the mathematical and computational principles for learning (deep) representations of such data distributions to senior undergraduate students", "snippet": "This textbook aims to provide a systematic introduction to the mathematical and computational principles for learning (deep) representations of such data distributions to senior undergraduate students and starting graduate students . The main prerequisite for this book is undergraduate linear algebra, probability/statistics, and optimization. Some familiarity with basic concepts from signal processing (sparse representation and compressed sensing in particular), information theory, and feedback control would enhance your appreciation."}, {"page": "Preface", "href": "Chx1.html#p4", "title": "The main motivation for writing this book is because there have been tremendous developments in the past several years, by the authors and many colleagues, that aim to establish a principled and rigor", "snippet": "The main motivation for writing this book is because there have been tremendous developments in the past several years, by the authors and many colleagues, that aim to establish a principled and rigorous approach to understand deep neural networks and, more generally, intelligence itself. The deductive methodology advocated by this new approach is in direct contrast, and highly complementary, to the dominant methodology behind current practices of artificial intelligence, which is largely inductive and trial-and-error. The lack of understanding about so-developed powerful AI models and systems"}, {"page": "Preface", "href": "Chx1.html#p5", "title": "At the technical levels, the theoretical framework presented in this book helps reconcile a long-standing gap between the classical approach to model data structures that are mainly based on analytica", "snippet": "At the technical levels, the theoretical framework presented in this book helps reconcile a long-standing gap between the classical approach to model data structures that are mainly based on analytical geometric, algebraic and probabilistic models (e.g., subspaces, Gaussians, and equations) and the ‚Äúmodern‚Äù approach that are based on empirically designed non-parametric models (e.g., deep networks). As it turns out, a unification of the two seemingly separate methodologies becomes possible and even natural if one realizes that they all try to model and learn low-dimensional structures in the da"}, {"page": "Preface", "href": "Chx1.html#p6", "title": "We believe that the unified conceptual and computational framework presented in this book will be of great value to readers who truly want to clarify mysteries and misunderstandings about deep neural ", "snippet": "We believe that the unified conceptual and computational framework presented in this book will be of great value to readers who truly want to clarify mysteries and misunderstandings about deep neural networks and (artificial) intelligence. Furthermore, the framework is meant to provide the readers guiding principles for developing significantly better and truly intelligent systems in the future. More specifically, beside a general introduction (chapter), the main technical content of the book will be organized as six closely related topics (chapters): 1. We will start with the classical and mo"}, {"page": "Preface", "href": "Chx1.html#S0.I1.i1.p1", "title": "We will start with the classical and most basic models of Principal Component Analysis (PCA), Independent Component Analysis (ICA), and Dictionary Learning (DL), which assume that the low-dimensional ", "snippet": "We will start with the classical and most basic models of Principal Component Analysis (PCA), Independent Component Analysis (ICA), and Dictionary Learning (DL), which assume that the low-dimensional distributions of interest have linear and independent structures. From these simple idealistic models that are well studied and understood in signal processing and compressed sensing, we will introduce the most basic and important ideas for how to learn low-dimensional distributions."}, {"page": "Preface", "href": "Chx1.html#S0.I1.i2.p1", "title": "To generalize these classical models and their solutions to general low-dimensional distributions, we introduce a universal computational principle for learning such distributions: compression . As we", "snippet": "To generalize these classical models and their solutions to general low-dimensional distributions, we introduce a universal computational principle for learning such distributions: compression . As we will see, data compression provides a unifying view on all seemingly different classic and modern approaches to distribution or representation learning, including dimensionality reduction, entropy minimization, score matching for denoising, and lossy compression with rate distortion etc."}, {"page": "Preface", "href": "Chx1.html#S0.I1.i3.p1", "title": "Within this unifying framework, modern Deep Neural Networks (DNNs), such as ResNet, CNN, and Transformer, can all be mathematically interpreted as (unrolled) optimization algorithms to iteratively ach", "snippet": "Within this unifying framework, modern Deep Neural Networks (DNNs), such as ResNet, CNN, and Transformer, can all be mathematically interpreted as (unrolled) optimization algorithms to iteratively achieve better compression and better representations by reducing coding length/rate or gaining information. Not only does this framework help explain empirically designed deep network architectures thus far, it also leads to new architecture designs that can be significantly simpler and more efficient."}, {"page": "Preface", "href": "Chx1.html#S0.I1.i4.p1", "title": "Furthermore, to ensure that the learned representation for a data distribution is correct and consistent, the auto-encoding architectures that consist of both encoding and decoding become necessary. I", "snippet": "Furthermore, to ensure that the learned representation for a data distribution is correct and consistent, the auto-encoding architectures that consist of both encoding and decoding become necessary. In order for a learning system to be fully automatic and continuous, we will introduce a powerful closed-loop transcription framework that enables an auto-encoding system to self-correct and thus self-improve via a minimax game between the encoder and decoder."}, {"page": "Preface", "href": "Chx1.html#S0.I1.i5.p1", "title": "We will also study how the so learned data distribution and representation can be utilized as a powerful prior or constraint to conduct Bayesian inference to facilitate almost all types of tasks and s", "snippet": "We will also study how the so learned data distribution and representation can be utilized as a powerful prior or constraint to conduct Bayesian inference to facilitate almost all types of tasks and settings that are popular in the practice of modern artificial intelligence, including conditional estimation, completion, and generation of real-word high-dimensional data such as images and texts."}, {"page": "Preface", "href": "Chx1.html#S0.I1.i6.p1", "title": "Last but not the least, to connect theory to practice, we will demonstrate step-by-step how to effectively and efficiently learn deep representations of low-dimensional data distributions with large-s", "snippet": "Last but not the least, to connect theory to practice, we will demonstrate step-by-step how to effectively and efficiently learn deep representations of low-dimensional data distributions with large-scale datasets, including both images and texts, and use them in many practical applications such as image classification, image completion, image segmentation, image generation, and similar tasks for text data too."}, {"page": "Preface", "href": "Chx1.html#p7", "title": "To summarize, the technical content presented in this book is to establish strong conceptual and technical connections between the classical analytical approach and the modern computational approach, ", "snippet": "To summarize, the technical content presented in this book is to establish strong conceptual and technical connections between the classical analytical approach and the modern computational approach, between simple parametric models and deep non-parametric models, between diverse inductive practices and a unified deductive framework from the first principle. We will reveal that many seemingly unrelated or even competing approaches, though developed in separate fields at different times, all strive to achieve a common objective: pursuing and exploiting intrinsic low-dimensional distributions of"}]}