<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Preface ‣ Learning Deep Representations of Data Distributions</title>
<!--Generated on Sun Aug 10 23:41:34 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on August 10, 2025.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/gh/arXiv/arxiv-browse@master/arxiv/browse/static/css/ar5iv.0.8.2.min.css" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/gh/arXiv/arxiv-browse@master/arxiv/browse/static/css/ar5iv-fonts.0.8.2.min.css" rel="stylesheet" type="text/css"/>
<link href="https://cdn.jsdelivr.net/gh/arXiv/arxiv-browse@master/arxiv/browse/static/css/latexml_styles.0.8.2.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<link href="book-main.html" rel="up" title="Learning Deep Representations of Data Distributions"/>
<link href="book-main.html" rel="start" title="Learning Deep Representations of Data Distributions"/>


<link href="Ch1.html" rel="chapter" title="Chapter 1 Introduction ‣ Learning Deep Representations of Data Distributions"/>
<link href="Ch2.html" rel="chapter" title="Chapter 2 Learning Linear and Independent Structures ‣ Learning Deep Representations of Data Distributions"/>
<link href="Ch3.html" rel="chapter" title="Chapter 3 Pursuing Low-Dimensional Distributions via Lossy Compression ‣ Learning Deep Representations of Data Distributions"/>
<link href="Ch4.html" rel="chapter" title="Chapter 4 Deep Representations from Unrolled Optimization ‣ Learning Deep Representations of Data Distributions"/>
<link href="Ch5.html" rel="chapter" title="Chapter 5 Consistent and Self-Consistent Representations ‣ Learning Deep Representations of Data Distributions"/>
<link href="Ch6.html" rel="chapter" title="Chapter 6 Inference with Low-Dimensional Distributions ‣ Learning Deep Representations of Data Distributions"/>
<link href="Ch7.html" rel="chapter" title="Chapter 7 Learning Representations for Real-World Data ‣ Learning Deep Representations of Data Distributions"/>
<link href="Ch8.html" rel="chapter" title="Chapter 8 Future Study of Intelligence ‣ Learning Deep Representations of Data Distributions"/>
<link href="Ptx1.html" rel="part" title="Appendices ‣ Learning Deep Representations of Data Distributions"/>
<link href="chapter.css" rel="stylesheet" type="text/css"/><script defer="defer" src="chapter.js"></script><style id="stacked-topbars">:root{--navbar-h:0px;--book-topbar-h:64px;}
html, body{margin:0!important;padding:0!important;}
body>*:first-child{margin-top:0!important;}
.ltx_page_main{padding-top:calc(var(--book-topbar-h) + var(--navbar-h) + var(--header-h, 56px))!important;}
.ltx_page_navbar{display:none!important;}
.ltx_page_header{position:fixed!important;top:var(--book-topbar-h)!important;left:0;right:0;z-index:30;background:#fff;margin:0!important;border-top:none!important;}
.ltx_page_navbar, .ltx_page_header{margin-top:0!important;}
h1[id], h2[id], h3[id], h4[id], section[id], .ltx_theorem[id], .ltx_equation[id], .ltx_equationgroup[id], .ltx_figure[id], .ltx_table[id], .ltx_tabular[id], .ltx_float[id]{scroll-margin-top:calc(var(--book-topbar-h) + var(--navbar-h) + 20px);}</style><link href="Ch1.html" rel="next"/></head>
<body id="top"><div class="book-topbar" id="book-topbar"><a class="brand brand-link" href="index.html"><span class="logo"><svg fill="none" height="18" viewBox="0 0 24 24" width="18"><path d="M3 12c0-1.1.9-2 2-2h6V4c0-1.1.9-2 2-2h0c1.1 0 2 .9 2 2v6h6c1.1 0 2 .9 2 2h0c0 1.1-.9 2-2 2h-6v6c0 1.1-.9 2-2 2h0c-1.1 0-2-.9-2-2v-6H5c-1.1 0-2-.9-2-2Z" fill="url(#g1)"></path><defs><linearGradient id="g1" x1="0" x2="24" y1="0" y2="24"><stop offset="0%" stopColor="#7aa2ff"></stop><stop offset="100%" stopColor="#8b78ff"></stop></linearGradient></defs></svg></span><div class="title">Learning Deep Representations of Data Distributions</div></a><div class="topbar-right"><div class="search"><input aria-label="Search" class="search-input" placeholder="Search pages…" type="search"/><div class="search-results"></div></div><button class="btn-cn" title="Chinese version (coming soon)">CN</button><a class="gh-link" href="https://github.com/Ma-Lab-Berkeley/ldrdd-book" rel="noopener noreferrer" target="_blank"><svg aria-label="GitHub" fill="currentColor" height="16" role="img" viewBox="0 0 16 16" width="16" xmlns="http://www.w3.org/2000/svg"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.2 1.87.86 2.33.66.07-.52.28-.86.51-1.06-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path></svg><span>GitHub</span></a></div></div>
<nav class="ltx_page_navbar"><a class="ltx_ref" href="book-main.html" rel="start" title=""><span class="ltx_text ltx_ref_title">Learning Deep Representations of Data Distributions</span></a>

</nav>
<div class="ltx_page_main">
<header class="ltx_page_header">
<div class="ltx_align_center"><a class="ltx_ref" href="#top"><span class="ltx_text ltx_ref_title">Top of Page</span></a><a class="ltx_ref" href="Ch1.html"><span class="ltx_text ltx_ref_title">Next Chapter</span></a></div></header>
<div class="ltx_page_content">
<section class="ltx_chapter ltx_authors_1line">
<h1 class="ltx_title ltx_title_chapter">Preface</h1>
<div class="ltx_para" id="p1">
<p class="ltx_p ltx_align_center">“<span class="ltx_text ltx_font_italic">All roads lead to Rome</span>.”</p>
</div>
<div class="ltx_para" id="p2">
<p class="ltx_p">This book reveals and studies a common and fundamental problem behind almost all modern practices of (artificial) intelligence. That is, <span class="ltx_text ltx_font_italic">how to effectively and efficiently learn a low-dimensional distribution of data in a high-dimensional space and then transform the distribution to a compact and structured representation?</span> For an intelligent system, natural or artificial, such a representation can be generally regarded as a <span class="ltx_text ltx_font_italic">memory</span> or <span class="ltx_text ltx_font_italic">knowledge</span> learned from data sensed from the external world.</p>
</div>
<div class="ltx_para" id="p3">
<p class="ltx_p">This textbook aims to provide a systematic introduction to the mathematical and computational principles for learning (deep) representations of such data distributions to <span class="ltx_text ltx_font_italic">senior undergraduate students and starting graduate students</span>. The main prerequisite for this book is undergraduate linear algebra, probability/statistics, and optimization. Some familiarity with basic concepts from signal processing, information theory, and feedback control would enhance your appreciation.</p>
</div>
<div class="ltx_para" id="p4">
<p class="ltx_p">The main motivation for writing this book is because there have been tremendous developments in the past several years, by the authors and many colleagues, that aim to establish a principled and rigorous approach to understand deep neural networks and, more generally, intelligence itself. The deductive methodology advocated by this new approach is in direct contrast, and highly complementary, to the dominant methodology behind current practices of artificial intelligence, which is largely inductive and trial-and-error. The lack of understanding about so-developed powerful AI models and systems has led to increasing hypes and fears in the society. We believe that a serious attempt to establish a principled approach to understand intelligence is more needed than ever. An overarching goal of this book is to provide solid theoretical evidence and experimental evidence showing that it is now possible to study intelligence as a scientific and mathematical subject.</p>
</div>
<div class="ltx_para" id="p5">
<p class="ltx_p">At a more technical level, the framework presented in this book helps reconcile a long-standing gap between the classical approach to model data structures that are mainly based on analytical geometric, algebraic and probabilistic models (e.g., subspaces and Gaussians) and the “modern” approach that are based on empirically designed non-parametric models (e.g., deep networks). As it turns out, a unification of the two seemingly separate methodologies becomes possible and even natural if one realizes that they all try to model and learn <span class="ltx_text ltx_font_italic">low-dimensional</span> structures in the data distribution of interest. They are merely different ways to pursue, represent, and exploit the low-dimensional structures. From this perspective, even many seemingly unrelated computational techniques, developed independently in separate fields at different times, can now be better understood under a common computational framework and probably could be studied together from now on. As we will see in this book, these techniques include but are not limited to lossy compressive encoding-decoding developed in information theory and coding theory, diffusion and denoising in signal processing and machine learning, and continuation techniques such as augmented Lagrangian methods in optimization.</p>
</div>
<div class="ltx_para" id="p6">
<p class="ltx_p">We believe that the unified conceptual and computational framework presented in this book will be of great value to readers who truly want to clarify mysteries and misunderstandings about deep neural networks and (artificial) intelligence. Furthermore, the framework is meant to provide the readers guiding principles for developing significantly better and truly intelligent systems in the future. More specifically, the main technical content of the book is organized as follows:</p>
<ul class="ltx_itemize" id="S0.I1">
<li class="ltx_item" id="S0.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S0.I1.i1.p1">
<p class="ltx_p">We will start with the classical and most basic models of Principal Component Analysis (PCA), Independent Component Analysis (ICA), and Dictionary Learning (DL), which assume that the low-dimensional distributions of interest have linear and independent structures. From these simple idealistic models, we will introduce the most basic and important ideas for how to learn low-dimensional distributions.</p>
</div>
</li>
<li class="ltx_item" id="S0.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S0.I1.i2.p1">
<p class="ltx_p">To generalize these classical models and their solutions to general low-dimensional distributions, we introduce a universal computational principle for learning such distributions: <span class="ltx_text ltx_font_italic">compression</span>. As we will see, data compression provides a unifying view on all seemingly different classic and modern approaches to distribution or representation learning, including dimensionality reduction, score matching for denoising, and entropy or coding rate reduction for lossy compression.</p>
</div>
</li>
<li class="ltx_item" id="S0.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S0.I1.i3.p1">
<p class="ltx_p">Within this unifying framework, modern Deep Neural Networks (DNNs), such as ResNet, CNN, and Transformer, can all be mathematically interpreted as (unrolled) optimization algorithms to iteratively achieve better compression and better representations by reducing coding length/rate or gaining information. Not only does this framework help explain empirically designed deep network architectures thus far, it also leads to new architecture designs that can be significantly simpler and more efficient.</p>
</div>
</li>
<li class="ltx_item" id="S0.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S0.I1.i4.p1">
<p class="ltx_p">Furthermore, to ensure that the learned representation for a data distribution is correct and consistent, the <span class="ltx_text ltx_font_italic">auto-encoding</span> architectures that consist of both encoding and decoding (say via denoising and diffusion) become necessary. In order for a learning system to be fully automatic and continuous, we will introduce a powerful <span class="ltx_text ltx_font_italic">closed-loop transcription framework</span> that enables the system to self-correct and thus self-improve via a minimax game between the encoder and decoder.</p>
</div>
</li>
<li class="ltx_item" id="S0.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S0.I1.i5.p1">
<p class="ltx_p">We will also study how the so learned data distribution and representation can be utilized as a powerful prior to conduct Bayesian inference and to facilitate many types of practical tasks that rely on conditional estimation, completion, and generation with real-word high-dimensional data such as images.</p>
</div>
</li>
<li class="ltx_item" id="S0.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S0.I1.i6.p1">
<p class="ltx_p">Last but not the least, to connect theory to practice, we will demonstrate step-by-step how to effectively and efficiently learn (deep) representations of data distributions with large-scale datasets, including both images and texts, and use them in many practical applications such as image classification, image completion, image segmentation, image generation, and similar tasks for text data.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="p7">
<p class="ltx_p">To summarize, the technical content presented in this book is to establish strong conceptual and technical connections between the classical analytical approach and the modern computational approach, between simple parametric models and deep non-parametric models, between diverse inductive practices and a unified deductive framework from the first principle. Although developed in separate fields at different times, many seeming unrelated or even competing approaches all strive to achieve a common objective:</p>
<blockquote class="ltx_quote">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">pursuing and exploiting intrinsic low-dimensional distributions of high-dimensional data.</span></p>
<div class="ltx_pagination ltx_role_newpage"></div>
</blockquote>
<p class="ltx_p">To this end, the book will take us through a complete journey from theoretical formulation, to mathematical verification, to computational realization, and to practical applications.</p>
</div>

<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</div>
<footer class="ltx_page_footer">

<div class="ltx_page_logo">Generated  on Sun Aug 10 23:41:34 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
