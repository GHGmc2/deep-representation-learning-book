\documentclass{article}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{druv-macros}
\usepackage{druv-thm}

\title{Principles of Deep Representation Learning}
\author{Sam Buchanan, Yi Ma,  Druv Pai, and Yaodong Yu }
\date{}

\begin{document}

\maketitle


\section*{Description}

This book aims to provide a rigorous and systematic introduction to the mathematical and computational principles of deep learning. We achieve this by centering the course around a common and fundamental problem behind almost all modern practices of artificial intelligence and machine learning such as image recognition and generation. The problem is how to effectively and efficiently learn a low-dimensional distribution of data in a high-dimensional space and then transform the distribution to a compact and structure representation. Such a representation can be generally referred to as a {\em memory} learned from the sensed data. 

We will start with the most basic and classical cases of PCA, ICA, and Dictionary Learning that assume the distribution has linear and independent structures. To generalize these classical models and solutions to general data distributions, we introduce a universal computational principle for learning low-dimensional distributions: {\em compression}. As we will see, data compression provides a unifying view on popular approaches for distribution or representation learning such as Score Matching (for denoising) and coding Rate Reduction. Within this framework, modern deep neural networks, such as ResNet and Transformers, can all be mathematically fully interpreted as (unrolled) optimization algorithms to achieve better compression and representation. 

Furthermore, to ensure the learned representation to be correct and consistent, we will study the effective Auto-Encoding architecture that consists of both encoding and decoding (say for denoising and diffusion). In order for a learning system to be fully automatic and continuous, we will also study a powerful framework of Closed-Loop Transcription that enables the encoding and decoding networks to self-correct hence self-improve via the ubiquitous mechanism of {\em closed-loop feedback}. 

To connect theory to practice, we will demonstrate many applications with large-scale real-world data such as image classification, image completion, image segmentation, image generation, as well as similar tasks for text data. 

\section*{Plan}
\begin{itemize}
    \item Before ICASSP, draft of Chapters 4, 5, and 6 for ``Transparent Deep Representation Learning by Design''.
    \item Before CVPR, add draft of Chapter 2 and 3. 
    \item Before July Summer School, add a draft of introduction Chapter 1.
    \item Fall 2024, teach a course on this topic. Add teaching material and rewrite the chapters.
    \item Have a complete draft by the end of this year.
\end{itemize}


{\color{red} Yi: As a starter, I made a more detailed tentative, possibly idealistic, outline below. Let us start to flesh out all chapters with raw material already. First cut is to get all notation and terminology consistent.}
\newpage
\section{Introduction (Yi)}
\begin{itemize}
\item Straighten out the history and clarify the contributions and credits. 
\item Clarify concepts,  misunderstandings, and  confusions about notions such as intelligence, knowledge, memory, and learning.
\item Goals: Rigorous formulation of a correct framework with clear assumptions and objectives. 
\item Goals: Bridge theory and practice with computational tractable and scalable algorithms. 
\end{itemize}

\subsection{The History of Intelligence}
The 40's and the 50's. and two winters for neural networks.

\subsection{The Past Decade}
The deep learning revolution in 2010's and its essence of success.

\subsection{The Future}
From the perspective of a new and unifying framework to be introduced here... to formulate a new mathematical program for learning, with the principle of parsimony and self-consistency, which requires to integrate ideas from signal processing, information theory, optimization, feedback control and game theory. 

\section{Low-dimensional Linear and Independent Structures (Druv and Sam)}
This chapter connects to the classic roots of data analysis with low-dimensional structures: geometrically linear, statistically independent.

\subsection{Principal Component Analysis (PCA)} 
Use the classic case of learning a single linear subspace to exemplify the general approaches to learn a low-dim model via denoising, completion and error correction. Might worth mentioning variatnts such as Matrix Completion, Robust PCA for learning low-dimensional linear models under broader settings. But normally know the number of subsapces or their dimensions.
\subsection{Independent Component Analysis (ICA)} Non-Gaussian... independence and incoherence... Kurtosis, 

\subsection{Sparse Coding and Dictionary Learning} Mixture of low-dim linear independent structures.... L1, L4, ... Main sources for this section/chapter: 
\begin{itemize}
\item Introduce basic algorithm for sparse coding such as ISTA.
\item our work on the complete case: \href{https://arxiv.org/abs/1906.02435}{L4-based dictionary learning} and
\item \href{https://openreview.net/pdf?id=SJeY-1BKDS}{the subsequent ICLR paper} provides a unifying view on PCA, ICA, and Dicionary Learning and their algorithms (power iteration type); 
\item  Mention the work from John Wright and Qing Qu etc. on the over-complete case.
\end{itemize}

\section{Learning Low-dimensional Distributions via Compression (Yi)}
From Section 2, a unifying view on how we extend to learning general low-dim distributions that are not necessarily (piecewise) linear or independent... The unifying idea of learning via compression and denoising (or completion).

When we do not know the number of subspaces and their dimensions. Compression is the only way to learn. Rate distortion and that of Gaussians, subspaces, information gain and rate reduction for Gaussians and subspaces. This chapter connects to the roots of Information Theory. 

Main sources for this chapter: 
\begin{itemize}
\item 
Empirical Bayesian denoising, Tweedie's formula, score function, Lagevine dynamics, MCMC etc... (Can parameterize score function, but don't need to... denoising is well-defined in general)
\item In the important/practical case when the distributions can be approximated by mixture of degenerate Gaussians or subspaces: \href{http://people.eecs.berkeley.edu/~yima/psfile/Ma-PAMI07.pdf}{Unsupervised clustering} + MAE paper theorem connects rate distortion to Gaussian score
\item Good applications in \href{http://people.eecs.berkeley.edu/~yima/psfile/MobahiH2011-IJCV.pdf}{image segmentation} etc. 
\item \href{http://people.eecs.berkeley.edu/~yima/psfile/MICL_SJIS.pdf}{Supervised classification} from the perspective of (lossy) compression
\end{itemize}

\subsection{General Principle of Compression}
Denoising and score function etc... 

\subsection{Mixture of Gaussians}
Minimum coding length and rate reduction... for clustering and classification. Optimization properties of the rate reduction objective.

\subsection{Applications}
Application in image segmentation and classification... 

\section{Representation Learning via Deep Networks (Yaodong)}
Representation learning: Identify the low-dimensional distribution and transform it to a compact and structured form (e.g. linear and independent). 

From nonlinear to linear and from dependent to independent, unrolled optimization that maximizes rate reduction or information gain to promote linearality and independence, via iterative compression and transformation. This chapter connects to roots of Optimization. 

Main sources for this chapter: 
\begin{itemize}
\item Sparse Dictionary Learning and LISTA, SparseLand type network from unrolling... 
\item \href{https://jmlr.org/papers/v23/21-0631.html}{the white-box ReduNet} (and connections to ResNet and CNNs).
\item \href{https://arxiv.org/abs/2306.01129}{the white-box transformer CRATE} and any new follow-up work.
\end{itemize}

\subsection{White-box Deep Network: ReduNet}

\subsection{White-box Transformer: CRATE}

\section{Bi-directional Autoencoding (Sam)}
Autoencoding to ensure correctness and consistency: from PCA, dictionary learning to more general but structured distributions, say mixture of low-dim subspaces and  manifolds. Also from classical autoencoding with two separate, mutually inverse, mappings, to incremental loops such as diffusion/denoising and sparse coding/decoding. 

\begin{itemize}
    \item Generalize PCA to Nonlinear PCA or \href{https://arxiv.org/abs/2305.01777}{nonlinear manifold flattening and reconstruction} 
    \item  Generalize Matrix completion to MAE.
\item A more general autoencoding for structured representations via diffusion and denoising. (The CSS autoencoding or a better version with a CRATE backbone.)
\end{itemize}

\section{Closed-loop Transcription (Druv)} Closed-Loop Transcription for autonomous, incremental learning and to ensure correctness and self-consistency. This chapter connects to roots of Feedback Control and Game Theory. 

Main sources for this chapter:
\begin{itemize}
\item \href{https://www.mdpi.com/1099-4300/24/4/456/htm}{The CTRL work} and \href{https://arxiv.org/abs/2206.09120}{The case with multiple linear subspaces}
\item incremental learning and unsupervised closed-loop learning

\end{itemize}

\section{Real-world Applications (Yaodong and Yi)}

\begin{itemize}
    \item ViT for classification
    \item MAE for completion
    \item DINO for segmentation
    \item BERT an GPT for texts.
    \item CRATE denoising and diffusion.
    \item CRATE closed-loop transcription: incremental continuous learning.
\end{itemize}

\end{document}
