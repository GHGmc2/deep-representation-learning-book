from typing import Tuple
import jax
import jax.numpy as jnp
from jax import Array, vmap

RCOND_OPTIMAL = 1e-8


def logdeth(M: Array) -> Array:
    """
    Compute the logarithm of the determinant for a batch of Hermitian matrices.

    This function calculates log(det(M)) for each Hermitian matrix in the input array.
    The matrices are assumed to be symmetric positive definite.

    Args:
            M (Array): Input array of shape (*, D, D) containing Hermitian matrices.
                                    The last two dimensions represent each matrix.

    Returns:
            Array: An array of shape (*) containing the log-determinant of each input matrix.

    Note:
            The function uses eigenvalue decomposition to compute the log-determinant,
            which is numerically stable for positive definite matrices.
    """
    Lmbda = jnp.linalg.eigvalsh(M)
    return jnp.sum(jnp.log(Lmbda), axis=-1)


def sqrtm(M: Array) -> Array:
    """
    Compute the matrix square root of a batch of Hermitian matrices.

    This function calculates the square root of each Hermitian matrix in the input array.
    The matrix square root A of a matrix M satisfies A @ A = M.

    Args:
            M (Array): Input array of shape (..., D, D) containing Hermitian matrices.
                                    The last two dimensions represent each matrix.

    Returns:
            Array: An array of the same shape as the input, containing the matrix square root of each input matrix.

    Note:
            - This function uses eigenvalue decomposition to compute the matrix square root.
            - Negative eigenvalues are replaced with zeros to ensure a real-valued result.
            - The result is guaranteed to be Hermitian if the input is Hermitian.

    Example:
            >>> M = jnp.array([[4, 1], [1, 4]])
            >>> sqrt_M = sqrtm(M)
            >>> jnp.allclose(sqrt_M @ sqrt_M, M)
            True
    """
    Lmbda, U = jnp.linalg.eigh(M)
    return jnp.einsum(
        "...ij, ...j, ...kj -> ...ik",
        U,
        jnp.sqrt(jnp.where(Lmbda >= 0, Lmbda, jnp.zeros_like(Lmbda))),
        U,
    )


def gmm_sample(
    key: jax.random.PRNGKey,
    N: int,
    D: int,
    K: int,
    pi: Array,
    mus: Array,
    Sigmas: Array,
) -> Tuple[Array, Array]:
    """
    Sample from a Gaussian Mixture Model.

    This function generates samples from a Gaussian Mixture Model (GMM) with specified
    parameters. It returns both the generated samples and their corresponding labels.

    Args:
            key (jax.random.PRNGKey): Random key for JAX's PRNG.
            N (int): Number of samples to generate.
            D (int): Dimensionality of the data.
            K (int): Number of mixture components in the GMM.
            pi (Array): Mixture weights of shape (K,), representing the probability of each component.
            mus (Array): Mean vectors of shape (K, D), where each row is the mean of a component.
            Sigmas (Array): Covariance matrices of shape (K, D, D), where each matrix is the
                                             covariance of a component.

    Returns:
            Tuple[Array, Array]:
                    - X (Array): Generated samples of shape (N, D).
                    - y (Array): Labels for each sample of shape (N,), indicating which
                                              component generated each sample.

    Note:
            - The function uses the `sqrtm` function to compute the square root of covariance matrices.
            - Samples are generated by first selecting a component based on mixture weights,
              then sampling from the corresponding multivariate normal distribution.
    """
    # Generate component assignments
    key_cat, key_normal = jax.random.split(key)
    y = jax.random.categorical(key_cat, jnp.log(pi), shape=(N,))  # (N,)

    # Compute square roots of covariance matrices
    sqrt_Sigmas = sqrtm(Sigmas)  # (K, D, D)

    # Generate standard normal samples
    normal_samples = jax.random.normal(key_normal, shape=(N, D))  # (N, D)

    # Function to compute sample for a specific component
    def compute_sample_for_component(k, X):
        mask = y == k
        # Transform standard normal samples using component parameters
        component_samples = mus[k] + normal_samples @ sqrt_Sigmas[k]
        # Update X only where mask is True
        return jnp.where(mask[:, None], component_samples, X)

    # Initialize X with zeros
    X = jnp.zeros((N, D))

    # Apply the sample computation for each component
    X = jax.lax.fori_loop(0, K, lambda k, X: compute_sample_for_component(k, X), X)

    return X, y


def gmm_ce(X: Array, alpha: Array, sigma: Array, pi: Array, mus: Array, Sigmas: Array) -> Array:
    N, D = X.shape
    gaussian_ce = vmap(
        lambda x: vmap(
            lambda mu, Sigma: mu
            + alpha
            * Sigma
            @ jnp.linalg.lstsq(
                alpha**2 * Sigma + sigma**2 * jnp.eye(D), x - alpha * mu, rcond=RCOND_OPTIMAL
            )[0]
        )(mus, Sigmas)
    )(X)  # (N, K, D)
    log_weights = vmap(
        lambda x: vmap(
            lambda p, mu, Sigma: jnp.log(p)
            - (1 / 2) * logdeth(alpha**2 * Sigma + sigma**2 * jnp.eye(D))
            - (1 / 2)
            * (x - alpha * mu)
            @ jnp.linalg.lstsq(
                alpha**2 * Sigma + sigma**2 * jnp.eye(D), x - alpha * mu, rcond=RCOND_OPTIMAL
            )[0]
        )(pi, mus, Sigmas)
    )(X)  # (N, K)
    weights = jax.nn.softmax(log_weights, axis=-1)  # (N, K)
    return jnp.sum(weights[:, :, None] * gaussian_ce, axis=1)  # (N, D)
