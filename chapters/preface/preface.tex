\providecommand{\toplevelprefix}{../..}  % necessary for subfile bibliography + figures compilation to work, do not move this after documentclass
\documentclass[../../book-main.tex]{subfiles}

\begin{document}

\chapter*{Preface}

\begin{center}
%\hfill    
``{\em All roads lead to Rome}.''

%$~$ \hfill --- 
\end{center}
\vspace{5mm}

This book reveals and studies a common and fundamental problem behind almost all modern practices of artificial intelligence. That is, {\em how to effectively and efficiently learn a low-dimensional distribution of data in a high-dimensional space and then transform the distribution to a compact and structured representation?} For an intelligent system, such a representation can be generally regarded as a {\em memory} or {\em knowledge} learned from data sensed from the external world. 

This textbook aims to provide a systematic introduction to the mathematical and computational principles for learning (deep) representations of such data distributions to {\em senior undergraduate students and starting graduate students}. The main prerequisite for this book is undergraduate linear algebra, probability/statistics, and optimization. Some familiarity with basic concepts from signal processing, information theory, and feedback control would enhance your appreciation. 

The main motivation for writing this book is because there have been tremendous developments in the past several years, due in no small part to the effort by the authors and their colleagues, that aim to establish a principled and rigorous approach to understand deep neural networks and, more generally and ultimately, intelligence itself. The deductive methodology advocated by this new approach is in direct contrast, and highly complementary, to the dominant methodology behind current practices of artificial intelligence, which is largely inductive and empirical. As the lack of understanding about so-developed seemingly powerful AI models and systems has led to increasing hypes and fears in the society, we believe that a serious attempt to a principled  approach to intelligence is more needed than ever. An overarching goal of this book is to provide solid theoretical and experimental evidence that it is now possible to study intelligence as a scientific and mathematical subject. 

At the more technical levels, the framework presented in this book helps reconcile a long-standing gap between the classical approach to model data structures that were mainly based on analytical geometric, algebraic and probabilistic models (e.g., subspaces and Gaussians) and the ``modern'' approach that are based on empirically designed models (e.g., deep networks) that are amenable to model and learn data structures in a non-parametric manner. As it turns out, unification of these two seemingly separate methodologies becomes possible and even natural only if one realize they all try to model and learn {\em low-dimensional} structures in the data distribution of interest. They are merely different ways to pursue, represent, and exploit the low-dimensional structures. From this perspective, even many seemingly unrelated computational techniques, developed independently in separated fields at different times to explore and exploit low-dimensional structures, can now be better understood under a common computational framework and be unified and studied together from now on. As we will see in this book, these techniques include but are not limited to lossy compressive encoding-decoding in information and coding theory, diffusion and denoising in signal processing and machine learning, and continuation techniques such as augmented Lagrangian methods in optimization. 

We believe that the unified conceptual and computational framework presented in this book will be of great value to readers who truly want to clear the mysteries and misunderstandings about deep neural networks and (artificial) intelligence. Furthermore, the book is meant to provide the readers guiding principles to develop significantly better and truly intelligent systems in the future. More specifically, the main technical content of the book is organized as follows:
\begin{itemize}
\item We will start with the classical and most basic models of Principal Component Analysis (PCA), Independent Component Analysis (ICA), and Dictionary Learning (DL), which assume that the low-dimensional distributions of interest have linear and independent structures. Through these simple idealistic models, we will introduce the most basic concepts and  important ideas for how to learn low-dimensional distributions.

\item To generalize these classical models and their solutions to general low-dimensional distributions, we introduce a universal computational principle for learning such distributions: {\em compression}. As we will see, data compression provides a unifying view on all seemingly different classic and modern approaches to distribution or representation learning such as dimensionality reduction, score matching for denoising, and entropy or coding rate reduction for lossy compression. 

\item Within this unifying framework, modern Deep Neural Networks (DNNs), such as ResNet, CNN, and Transformer, can all be mathematically interpreted as (unrolled) optimization algorithms to iteratively achieve better compression and better representations in terms of reducing coding length/rate or gaining information. Not only does this framework help explain empirically designed deep network architectures thus far, it also leads to new architecture designs based on the principled objective, which can be significantly simpler and more efficient.

\item Furthermore, to ensure that the learned representation for a data distribution is correct and consistent, we will study effective {\em auto-encoding} architectures that consist of both encoding and decoding (say via denoising and diffusion). In order for a learning system to be fully automatic and continuous, we will introduce a powerful {\em closed-loop transcription framework} that enables encoding and decoding networks to self-correct and thus self-improve via a minimax game between the encoder and decoder.  

\item We will also study how so learned data distribution and representation can be utilized as a powerful prior to conduct Bayesian inferences and facilitate many types of practical tasks that rely on conditional estimation, completion, and generation with real-word high-dimensional data such as images. 

\item Last but not the least, to connect theory to practice, we will demonstrate step-by-step how to effectively and efficiently learn (deep) representations of  data distributions with large-scale datasets, including both images and texts, and use them in many practical applications such as image classification, image completion, image segmentation, image generation, and similar tasks for text data. 
\end{itemize}

To summarize, the technical content presented in this book is organized to establish a smooth transition from the classical analytical approach to the modern computational approach, from shallow parametric models to deep non-parametric models, from diverse inductive practices to a unified deductive framework, from theoretical justification, to computational realizations, and to practical applications. Although developed in different fields at different times, all these approaches actually have strived to achieve a common objective: {\em pursuing and exploiting intrinsic low-dimensional structures of a high-dimensional data distribution.} 
\end{document}
