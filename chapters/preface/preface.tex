\providecommand{\toplevelprefix}{../..}  % necessary for subfile bibliography + figures compilation to work, do not move this after documentclass
\documentclass[../../book-main.tex]{subfiles}

\begin{document}

\chapter*{Preface}

This book studies a common and fundamental problem behind almost all modern practices of artificial intelligence. That is, how to effectively and efficiently learn a low-dimensional distribution of data in a high-dimensional space and then transform the distribution to a compact and structured representation? Such a representation is  generally referred to as a {\em memory} or {\em knowledge} learned from the sensed data. 

This textbook aims to provide a systematic introduction to the mathematical and computational principles for learning (deep) representations of such data distributions to {\em senior undergraduate students and starting graduate students}. The main prerequisite for this book is undergraduate linear algebra, probability/statistics, and optimization. Some familiarity with basic concepts from signal processing, information theory, and feedback control would enhance your appreciation. 

The main technical content of the book is organized as follows:
\begin{itemize}
\item We will start with the most basic and classical models of Principal Component Analysis (PCA), Independent Component Analysis (ICA), and Dictionary Learning (DL), which assume that the distributions of interest have low-dimensional linear and independent structures. Through these simple idealistic models, we will introduce the basic concepts and algorithmic ideas of how to learn low-dimensional distributions.

\item To generalize these classical models and their solutions to general data distributions, we introduce a universal computational principle for learning low-dimensional distributions: {\em compression}. As we will see, data compression provides a unifying view on all past and present approaches to distribution or representation learning such as dimensionality/entropy reduction, score matching (for denoising), and coding rate reduction (for lossy compression). 

\item Within this framework, modern Deep Neural Networks (DNNs), such as ResNet, CNN, and Transformer, can all be mathematically interpreted as (unrolled) optimization algorithms to iteratively achieve better compression and better representations in terms of reducing coding length/rate or gaining information. Not only does this framework help explain empricially designed deep network architectures, it also leads to design new architectures based on the compression objective, which can be significantly simpler and more efficient.

\item Furthermore, to ensure that the learned representation is correct and consistent, we will study effective {\em auto-encoding} architectures that consist of both encoding and decoding (say via denoising and diffusion). We will also study how such a learned representation can be utilized as a powerful prior to facilitate conditional estimation and generation. 

\item In order for a learning system to be fully automatic and continuous, we will introduce a powerful {\em closed-loop transcription framework} that enables encoding and decoding networks to self-correct and thus self-improve via a minimax game between the encoder and decoder. 

\item To connect theory to practice, we will demonstrate how the learned data distributions and representations can be used in many practical applications for real-world tasks with large-scale data such as image classification, image completion, image segmentation, image generation, and similar tasks for text data. 
\end{itemize}


\end{document}
