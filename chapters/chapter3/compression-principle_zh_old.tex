\providecommand{\toplevelprefix}{../..}  % necessary for subfile bibliography + figures compilation to work, do not move this after documentclass

\documentclass[../../book-main_zh.tex]{subfiles}

\graphicspath{{\subfix{../..}}}

\begin{document}

\chapter{通过压缩探寻低维分布}
\label{ch:compression}\label{ch:general-distribution}

\begin{quote}
	\hfill    “{\em 我们为学习而压缩，亦为压缩而学习}。”\\
	$~$ \hfill --- 《高维数据分析》，Wright and Ma, 2022
\end{quote}
\vspace{5mm}

在第 \ref{ch:linear-independent} 章中，我们已经展示了如何学习一类简单的分布，其支撑集被假设为单个或混合的低维子空间或低秩高斯分布。为进一步简化，我们假设不同的（隐藏的）线性或高斯模式是正交或独立的\footnote{或者可以很容易地简化为这种理想情况。}，如 图 \ref{fig:subspaces} 所示。正如我们所展示的，对于这类特殊分布，可以推导出相当简单且有效的学习算法，并带有正确性和效率的保证。相关算法中操作的几何和统计解释也非常清晰。

在实践中，线性和独立性都是相当理想化的假设，现实世界中的高维数据分布很少满足这些条件。我们唯一可以假设的是，分布的内在维度远低于数据嵌入的环境空间的维度。因此，在本章中，我们将展示如何学习一个更一般的高维空间中的低维分布类别，该分布不一定是（分段）线性的。

现实数据的分布通常包含多个成分或模式，例如在图像中对应不同类别的物体。这些模式可能在统计上并非独立，甚至可能具有不同的内在维度。同样典型的是，我们只能接触到分布的有限数量的样本。因此，总的来说，我们可以假设我们的数据分布在一个高维空间中的（非线性）低维子流形的混合体上。图 \ref{fig:mixture-manifolds} 展示了这样一个分布的例子。

要在这样的条件下学习这样一个分布，我们需要解决几个基本问题：
\begin{itemize}
	\item 学习高维空间中的一般低维分布并表示所学分布的通用方法是什么？
	\item 我们如何衡量所得表示的复杂度，以便有效利用低维性进行学习？
	\item 我们如何使学习过程在计算上易于处理甚至可扩展，因为环境维度通常很高，样本数量通常很大？
\end{itemize}
正如我们将看到的，{\em 压缩}或{\em 降维}这一基本思想，已被证明对线性/独立情况非常有效，它仍然是为学习一般低维分布开发有效计算模型和方法的普适原则。

由于其在理论和实践上的极端重要性，我们将更深入地研究，当目标分布可以被低维子空间或低秩高斯分布的混合很好地建模或近似时，这个通过压缩学习分布的通用框架是如何具体实现的。
%\yima{I did not touch the above introduction. We can rewrite it when the rest of the chapter is stable. But should state the purpose clearly: a computional framework that strives for achievable and implementable compressive encoding and decoding schemes!}

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{\toplevelprefix/chapters/chapter3/figs/mixed-manifolds.png}
    \caption{数据分布在一个非常高维的环境空间（例如 $\mathbb{R}^D$）中的低维子流形 $\cup_j \mathcal{M}_j$ 的混合体上。}
    \label{fig:mixture-manifolds}
\end{figure}


\section{熵最小化与压缩}

\subsection{熵与编码率}
在第 \ref{ch:intro} 章中，我们提到学习的目标是找到生成给定数据集的最简单方法。从概念上讲，柯尔莫哥洛夫复杂度旨在提供这样一种复杂性度量，但它不可计算，也与任何能够实际再现数据的可实现方案无关。因此，我们需要一个可替代的、可计算的、可实现的复杂性度量。这就引出了由香农在 1948 年引入的{\em 熵}的概念 \cite{Shannon-1948}。

为了说明熵的构造性，让我们从最简单的情况开始。假设我们有一个离散随机变量，它以相等的概率 $1/N$ 取 $N$ 个不同的值或\textit{符号}，$\{\x_1, \ldots, \x_N\}$。那么我们可以用 $i$ 的 $\log_2 N$ 比特二进制表示来编码每个符号 \(\vx_{i}\)。这种编码方案可以推广到编码任意离散分布 \cite{Cover-Thomas}：给定一个分布 \(p\) 使得 $\sum_{i=1}^N p(\x_i) = 1$，可以为每个概率为 $p(\x_i)$ 的符号 $\x_i$ 分配一个大小为 $\log_2 [1/p(\x_i)] = - \log_2 p(\x_i)$ 比特的二进制码。因此，编码来自该分布 $p(\cdot)$ 的任何样本所需的平均比特数，或称{\em 编码率}，由以下表达式给出：\footnote{根据信息论 \cite{Cover-Thomas} 的惯例，这里的 $\log$ 是以 2 为底的对数。因此，熵的单位是（二进制）比特。}
\begin{equation}
	H(\x) \doteq \mathbb{E}[\log 1/p(\x)]  = - \sum_{i=1}^N p(\x_i) \log  p(\x_i).
	\label{eqn:entropy-discrete}
\end{equation}
这被称为（离散）分布 $p(\cdot)$ 的{\em 熵}。请注意，这个熵总是非负的，并且当且仅当对于某个 $i \in [N]$ 有 $p(\x_i) = 1$ 时，它才为零。\footnote{这里我们利用了事实 $\lim_{p\rightarrow 0} p \log p = 0$。}


\subsection{微分熵}

当随机变量 $\x \in \R^{D}$ 是连续的并且具有概率密度 $p$ 时，可以将上述和 \eqref{eqn:entropy-discrete} 的极限看作与一个积分相关：
\begin{equation}
	h(\vx) \doteq \Ex[\log 1/p(\vx)] = - \int_{\R^{D}} p(\vxi) \log p(\vxi) \odif{\vxi}.
	\label{eqn:entropy-differential}
\end{equation}
{更准确地说，给定一个连续变量 $\x$，我们可以用一个量化大小 $\epsilon > 0$ 对其进行量化。将得到的离散变量记为 $\x^\epsilon$。那么可以证明 $H(\vx^\epsilon) + \log(\epsilon) \approx h(\vx)$。因此，当 $\epsilon$ 很小时，微分熵 $h(\x)$ 可以是负的。感兴趣的读者可以参考 \cite{Cover-Thomas} 获取更详细的解释。}

\begin{example}[高斯分布的熵]
	通过直接计算，可以证明高斯分布 $x \sim \mathcal{N}(\mu, \sigma^2)$ 的熵由下式给出：
	\begin{equation}
		h(x) = \frac{1}{2}\log (2\pi \sigma^2) + \frac{1}{2}.
		\label{eqn:entropy-Gaussian}
	\end{equation}
	众所周知，在所有具有相同方差 $\sigma$ 的分布中，高斯分布的熵最大。在 $\mathbb{R}^D$ 中，多元高斯分布 $\x \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})$ 的熵由下式给出：
	\begin{equation}
		h(\x) = \frac{D}{2}(1 + \log(2\pi)) + \frac{1}{2}\log\det(\boldsymbol{\Sigma}).
		\label{eqn:entropy-Gaussian-multi}
	\end{equation}
\end{example}

与离散分布的熵类似，我们希望微分熵与某个可实现编码方案的编码率相关联。例如，我们可以用一个大小为 $\epsilon >0$ 的网格来离散化分布的域。所得离散分布的编码率可以看作是微分熵的一个近似 \cite{Cover-Thomas}。


需要注意的是，微分熵的定义存在一些需要注意的地方。对于高维空间中的分布，当其支撑集退化（变为低维）时，其微分熵会发散到 \(-\infty\)。这一事实在 \Cref{thm:degenerate_entropy_negative_infinity} 中得到证明，但即使在简单明确的高斯分布 \eqref{eqn:entropy-Gaussian-multi} 的情况下，当协方差 \(\vSigma\) 是奇异的，我们可以看到 \(\logdet(\vSigma) = -\infty\)，因此我们有 $h(\x) = -\infty$。在这种情况下，如何适当地量化或编码这样的分布并不明显。然而，退化（高斯）分布恰恰是高维空间中最简单、也 arguably 是最重要的低维分布实例。在本章中，我们将讨论对这种看似由退化引起的困难的完整解决方案。


%\sdb{it seems that it is not so immediate (e.g., the uniform distribution on size-$N$ alphabet has entropy $log_2 N$ bits)... I think the right way to relate them is to consider quantizing the space of values of the continuous r.v.\ $\vx$ into $N$ buckets, calculating discrete entropy for the result, applying a certain correction to the discrete entropy, then taking $N\to\infty$... this correction term seems to be what allows the differential entropy to be negative, whereas the discrete entropy is always nonnegative...}
%\sdb{not sure about this... for example, isn't the Gaussian only ``max entropy'' versus suitably normalized (unit variance) distributions? but my trepidation extends further (by this measure, all perfectly low-dim distributions have $-\infty$ entropy, say)...}




\subsection{最小化编码率}\label{sub:min_entropy}
回想一下，学习问题是从一组从分布 $p(\x)$ 中抽取的样本 $\{\x_1, \ldots, \x_N\}$ 中恢复一个（可能是连续的）分布 $p(\x)$。为便于说明，我们记 $\X = [\x_1, \ldots, \x_N] \in \mathbb{R}^{D\times N}$。鉴于我们在此关注的分布是（近似）低维的，我们应该期望它们的（微分）熵非常小。但与我们在前一章中研究的情况不同，我们通常不知道分布 $p(\x)$ 属于哪个（解析的）低维模型族。因此，检查熵是否小似乎是我们识别和建模分布时唯一可以依赖的准则。

现在，仅凭样本而不知道 $p(\x)$ 是什么，理论上它们可以被解释为来自任何一般分布的样本。特别是，它们可以被解释为以下任何一种情况：
\begin{enumerate}
	\item 作为来自经验分布 $p^{\vX}$ 本身的样本，该分布为 $N$ 个样本 $\x_i, i=1, \ldots, N$ 中的每一个分配 $1/N$ 的概率。
	\item 作为来自标准正态分布 $\x^n \sim p^{n} \doteq \mathcal{N}(\boldsymbol{0}, \sigma^2 \boldsymbol{I})$ 的样本，其方差 $\sigma^2$ 足够大（例如大于样本范数）；
	\item 作为来自正态分布 $\x^e \sim p^{e} \doteq \mathcal{N}(\boldsymbol{0}, \hat{\vSigma})$ 的样本，其协方差 $\hat{\vSigma} = \frac{1}{N} \X \X^T$ 是样本的经验协方差；
	\item 作为来自一个分布 $\hat \x \sim \hat{q}(\vx)$ 的样本，该分布紧密近似于真实分布 $p$。
\end{enumerate}
现在的问题是，哪一个更好，在什么意义上更好？假设你相信这些数据 $\X$ 是从一个特定的分布 $q(\x)$ 中抽取的，这个分布可能是上面考虑的分布之一。那么我们可以用分布 $q(\x)$ 的最优码本来编码这些数据点。所需的平均编码长度（或编码率）由下式给出：
\begin{equation}
	\frac{1}{N}\sum_{i=1}^N -\log q(\x_i) \quad \approx \quad - \int_{\R^{D}} p(\vxi) \log q(\vxi)\odif{\vxi}
\end{equation}
当样本数量 $N$ 变大时。如果我们已经确定了正确的分布 $p(\x)$，编码率就由熵 $- \int p(\vxi) \log p(\vxi) \odif{\vxi}$ 给出。事实证明，上述编码长度 $- \int p(\vxi) \log q(\vxi) \odif{\vxi}$ 总是大于或等于熵，除非 $q(\x) = p(\x)$。它们的差值，记为
\begin{eqnarray}
	\KL(p, q) &\doteq& - \int_{\R^{D}} p(\vxi) \log q(\vxi) \odif{\vxi}  - \Big(- \int_{\R^{D}} p(\vxi) \log p(\vxi) \odif{\vxi} \Big)\\
	&=& \int_{\R^{D}} p(\vxi) \log \frac{p(\vxi)}{q(\vxi)} \odif{\vxi}
\end{eqnarray}
被称为{\em 库尔贝克-莱布勒}（KL）散度，或相对熵。这个量总是非负的。
\begin{theorem}[信息不等式]\label{thm:information-inequality}
	设 $p(\x), q(\x)$ 是两个概率密度函数（具有相同的支撑集）。那么 $\KL(p, q) \ge 0$，其中不等式取等当且仅当 $p = q$。\footnote{技术上讲，这个等式应该被理解为“几乎处处”，即除了可能在一个零测度（体积）的集合上，因为这个集合不会影响任何积分的值。}
\end{theorem}
\begin{proof}
	\begin{eqnarray*}
		- \KL(p, q)
		&=& - \int_{\R^{D}} p(\vxi) \log \frac{p(\vxi)}{q(\vxi)} \odif{\vxi}
		=  \int_{\R^{D}} p(\vxi) \log \frac{q(\vxi)}{p(\vxi)} \odif{\vxi} \\
		&\le& \log \int_{\R^{D}} p(\vxi)  \frac{q(\vxi)}{p(\vxi)} \odif{\vxi} \label{eqn:jensen-KL}
		= \log \int_{\R^{D}} q(\vxi) \odif{\vxi} = \log 1 = 0,
	\end{eqnarray*}
	其中第一个不等式来自于{\em 琴生不等式}以及函数 $\log(\cdot)$ 是严格凹函数的事实。等号成立当且仅当 $p = q$。
\end{proof}

因此，给定一组采样数据 $\X$，要确定在 $p^{n}$、$p^{e}$ 和 $\hat{q}$ 中哪一个更好，我们可以比较它们对 $\X$ 的编码率，看哪一个给出最低的率。我们从上面知道，一个分布的（理论上可达到的）编码率与其熵密切相关。总的来说，我们有：
\begin{equation}
	h(\x^n) > h(\x^e) > h(\hat \x).
\end{equation}
因此，如果数据 $\X$ 是由与这些分布中每一个相关的码本编码的，那么 $\X$ 的编码率通常会以相同的顺序递减：
\begin{equation}
	p(\x^n) \rightarrow p(\x^e) \rightarrow p(\hat \x).
\end{equation}

这一观察为我们如何追求一个具有低维结构的分布 $p(\x)$ 提供了普遍的指导。它提出了两种可能的方法：
\begin{enumerate}
	\item 从一个具有高熵的一般分布（比如正态分布）开始，通过降低熵来逐步将分布转换为数据的（经验）分布。
	\item 在一大类具有明确编码方案（可以编码给定数据）的（参数或非参数）分布中，逐步搜索能给出更低编码率的更好编码方案。
\end{enumerate}
%\sdb{why not present these through the lens of information gain instead? I think I understand the argument here (a kind of ``coarse to fine'' progression for models of the distribution, with models graded by entropy), but it does not tell us ``when we should stop'' going down this hierarchy of grading. intuitively, it seems like we should ``stop'' when the model is explaining our data well, then the question is how to measure this and how to make it computational...}
从概念上讲，这两种方法本质上都在尝试做同样的事情。对于第一种方法，我们需要确保这样的转换路径存在且可计算。对于第二种方法，所选择的族必须足够丰富，能够紧密近似（或包含）真实分布。对于任何一种方法，我们都需要确保具有较低熵或更好编码率的解能够被高效计算，并能快速收敛到期望的分布。\footnote{例如，像图像和文本这样的现实世界数据的分布。} 我们将在本章剩下的两节中探讨这两种方法。%\sdb{tension/left unsaid here (from Yi's slides ``elephant in the room''): we cannot actually `know' or in general even efficiently estimate these entropies/divergences in high-dimensional spaces from finite samples (e.g.\ the KL divergence finite sum $\approx$ on the previous page---doesn't this have the curse of dimensionality)... maybe this can be the `hinge' of transition in the next section? (denoising provides a tractable way to perform approximations here), or restructure...}

\section{通过去噪实现压缩}\label{sub:compression_denoising}

在本节中，我们将描述一种\textit{自然}且\textit{计算上易于处理}的方法来学习一个分布 \(p(\vx)\)。该方法通过学习我们分布的参数化编码，使得表示具有最小的熵或编码率，然后使用这个编码将来自标准高斯分布的高熵样本转换为来自目标分布的低熵样本，如 \Cref{fig:diffusion-chapter3} 所示。这提供了一种利用上述两种方法来学习和从分布中采样的方法论。

\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{\toplevelprefix/chapters/chapter3/figs/diffusion_pipeline}
	\caption{一个迭代去噪过程的示意图，该过程从一个各向同性的高斯分布开始，收敛到任意的数据分布。}
	\label{fig:diffusion-chapter3}
\end{figure}

\subsection{扩散与去噪过程} \label{sub:intro_diffusion_denoising}

我们首先希望找到一个程序，将一个给定的非常嘈杂的样本的熵降低，使其成为一个来自数据分布的低熵样本。在这里，我们描述一个可能的方法——众多方法之一，但也许是解决这个问题最自然的方式。首先，我们找到一种方法来\textit{逐渐增加}来自数据分布的现有样本的熵。然后，我们找到这个过程的\textit{近似逆过程}。但总的来说，增加熵的操作没有逆过程，因为原始分布的信息可能会被破坏。因此，我们将处理一个特殊情况，其中（1）增加熵的操作具有简单、可计算和可逆的形式；（2）我们可以获得数据分布的（参数化）编码，正如上述两种方法中所暗示的。正如我们将看到的，上述两个因素将确保我们的方法是可行的。

我们将以可以说最简单的方式增加熵，即\textit{添加各向同性高斯噪声}。更准确地说，给定随机变量 \(\vx\)，我们可以考虑\textit{随机过程} \((\vx_{t})_{t \in [0, T]}\)，它向其逐渐添加噪声，即
\begin{equation}\label{eq:additive_gaussian_noise_model}
	\vx_{t} \doteq \vx + t\vg, \qquad \forall t \in [0, T],
\end{equation}
其中 \(T \in [0, \infty]\) 是一个时间范围，\(\vg \sim \dNorm(\vzero, \vI)\) 是独立于 \(\vx\) 抽取的。这个过程是\textit{扩散过程}的一个例子，之所以这么命名，是因为它随着时间的推移将概率质量散布到整个 \(\R^{D}\) 上，从而随时间增加熵。这个直觉得到了 \Cref{fig:ve_forward_density} 的图形证实，并通过以下定理得到严格证明。
\begin{theorem}[\Cref{thm:diffusion_entropy_increases} 的简化版]
	假设 \((\vx_{t})_{t \in [0, T]}\) 遵循模型 \eqref{eq:additive_gaussian_noise_model}。对于任何 \(t \in (0, T]\)，随机变量 \(\vx_{t}\) 的微分熵 \(h(\vx_{t}) > -\infty\)。此外，在 \(\vx\) 的某些技术条件下，
	\begin{equation}
		\odv*{h(\vx_{t})}{t} > 0, \qquad \forall t \in (0, T],
	\end{equation}
	表明加噪后的 \(\vx\) 的熵随时间 \(t\) 增加。
\end{theorem}
证明是初等的，但相当长，所以我们将其推迟到 \Cref{sub:diffusion_entropy_increases}。这个结果尚未说明的主要含义是，对于每个 \(t > 0\)，\(h(\vx_{t}) > h(\vx)\)。要看到这一点，请注意，如果 \(h(\vx) = -\infty\)，那么对于所有 \(t > 0\)，\(h(\vx_{t}) > -\infty\)，如果 \(h(\vx) > -\infty\)，那么根据微积分基本定理，\(h(\vx_{t}) = h(\vx) + \int_{0}^{t}[\odv*{h(\vx_{s})}{s}]\odif{s} > h(\vx)\)，所以在两种情况下，对于每个 \(t > 0\)，都有 \(h(\vx_{t}) > h(\vx)\)。

\begin{figure}
	\includegraphics[width=\textwidth]{\toplevelprefix/chapters/chapter3/figs/ve_forward_diffusion_density.png}
	\caption{\small\textbf{扩散一个高斯混合模型。} 从左到右，我们观察到随着 \(t\) 从 \(0\) 增长到 \(10\)，密度的演变，以及一些代表性样本。每个区域根据其密度着色（\(0.0\) 是纯白色，\(> 0.01\) 是深蓝色，其他所有值映射到介于两者之间的某种蓝色调。）我们观察到随着 \(t\) 的增加，概率质量变得不那么集中，这表明熵在增加。}
	\label{fig:ve_forward_density}
\end{figure}

加噪的逆操作被称为\textit{去噪}。这是信号处理和系统理论中一个经典且被广泛研究的课题，例如维纳滤波器和卡尔曼滤波器。\Cref{ch:classic} 中讨论的几个问题，如 PCA、ICA 和字典学习，都是去噪问题的具体实例。对于一个固定的 \(t\) 和加性高斯噪声模型 \eqref{eq:additive_gaussian_noise_model}，去噪问题可以被表述为试图学习一个函数 \(\bar{\vx}^{\ast}(t, \cdot)\)，它在给定 \(t\) 和 \(\vx_{t}\) 的情况下，形成对真实随机变量 \(\vx\) 的最佳可能近似（在期望意义上）：
\begin{equation}\label{eq:denoising_loss}
	\bar{\vx}^{\ast}(t, \cdot) \in \argmin_{\bar{\vx}(t, \cdot)}\Ex_{\vx, \vx_{t}}\norm{\vx - \bar{\vx}(t, \vx_{t})}_{2}^{2}.
\end{equation}
当在所有可能的（平方可积）函数上优化 \(\bar{\vx}(t, \cdot)\) 时，这个问题的解是所谓的\textit{贝叶斯最优去噪器}：
\begin{equation}\label{eq:optimal_denoiser}
	\bar{\vx}^{\ast}(t, \vxi) \doteq \Ex[\vx \mid \vx_{t} = \vxi].
\end{equation}

这个表达式证明了符号 \(\bar{\vx}\) 的合理性，它意在计算一个条件期望（即条件均值或条件平均值）。简而言之，它试图从带噪输入中去除噪声，输出对（去噪后的）原始随机变量的最佳可能猜测（在期望和 \(\ell^{2}\) 距离的意义上）。

\begin{example}[从高斯混合模型中去噪高斯噪声]\label{example:denoising_gaussian_mixture}
	在这个例子中，我们为一个极其重要的分布类别——高斯混合模型——计算贝叶斯最优去噪器。首先，让我们固定分布的参数：混合权重 \(\vpi \in \R^{K}\)，分量均值 \(\{\vmu_{k}\}_{k = 1}^{K} \subseteq \R^{D}\)，以及分量协方差 \(\{\vSigma_{k}\}_{k = 1}^{K} \subseteq \PSD(D)\)，其中 \(\PSD(D)\) 是 \(D \times D\) 对称半正定矩阵的集合。现在，假设 \(\vx\) 是通过以下两步过程生成的：
	\begin{itemize}
		\item 首先，采样一个索引（或\textit{标签}）\(y \in [K]\)，使得 \(y = k\) 的概率为 \(\pi_{k}\)。
		\item 其次，从正态分布 \(\dNorm(\vmu_{y}, \vSigma_{y})\) 中采样 \(\vx\)。
	\end{itemize}
	那么 \(\vx\) 的分布为
	\begin{equation}
		\vx \sim \sum_{k = 1}^{K}\pi_{k}\dNorm(\vmu_{k}, \vSigma_{k}),
	\end{equation}
	因此
	\begin{equation}
		\vx_{t} = \vx + t\vg \sim \sum_{k = 1}^{K}\pi_{k}\dNorm(\vmu_{k}, \vSigma_{k} + t^{2}\vI).
	\end{equation}
	让我们定义 \(\phi(\vx ; \vmu, \vSigma)\) 为在 \(\vx\) 处评估的 \(\dNorm(\vmu, \vSigma)\) 的概率密度。在这个记法中，\(\vx_{t}\) 的密度是
	\begin{equation}
		p_{t}(\vx_{t}) = \sum_{k = 1}^{K}\pi_{k}\phi(\vx_{t} ; \vmu_{k}, \vSigma_{k} + t^{2}\vI).
	\end{equation}

	以 \(y\) 为条件，这些变量是联合高斯的：如果我们说 \(\vx = \vmu_{y} + \vSigma_{y}^{1/2}\vu\)，其中 \((\cdot)^{1/2}\) 是矩阵平方根，\(\vu \sim \dNorm(\vzero, \vI)\) 独立于 \(y\)（和 \(\vg\)），那么我们有
	\begin{equation}
		\mat{\vx \\ \vx_{t}} = \mat{\vmu_{y} \\ \vmu_{y}} + \mat{\vSigma_{y}^{1/2} & \vzero \\ \vSigma_{y}^{1/2} & t\vI}\mat{\vu \\ \vg}.
	\end{equation}
	这表明 \(\vx\) 和 \(\vx_{t}\) 是联合高斯的（以 \(y\) 为条件），正如所声称的。因此我们可以写出
	\begin{equation}
		\mat{\vx \\ \vx_{t}} \sim \dNorm\rp{\mat{\vmu_{y} \\ \vmu_{y}}, \mat{\vSigma_{y} & \vSigma_{y} \\ \vSigma_{y} & \vSigma_{y} + t^{2}\vI}}.
	\end{equation}
	因此，给定 \(\vx_{t}\) 的 \(\vx\) 的条件期望（即以 \(y\) 为条件的贝叶斯最优去噪器）是著名的
	(\Cref{exercise:conditional_gaussian})
	\begin{equation}
		\Ex[\vx \mid \vx_{t}, y] = \vmu_{y} + \vSigma_{y}(\vSigma_{y} + t^{2}\vI)^{-1}(\vx_{t} - \vmu_{y}).
	\end{equation}
	为了找到总的贝叶斯最优去噪器，我们使用迭代期望定律，得到
	\begin{align}
		\bar{\vx}^{\ast}(t, \vx_{t})
		&= \Ex[\vx \mid \vx_{t}] \\ 
		&= \Ex[\Ex[\vx \mid \vx_{t}, y] \mid \vx_{t}] \\ 
		&= \sum_{k = 1}^{K}\Pr[y = k \mid \vx_{t}]\Ex[\vx \mid \vx_{t}, y = k].
	\end{align}
	概率可以如下处理。设 \(p_{t \mid y}\) 是以 \(y\) 的值为条件的 \(\vx_{t}\) 的概率密度。那么
	\begin{align}
		\Pr[y = k \mid \vx_{t}]
		&= \frac{p_{t \mid y}(\vx_{t} \mid k)\pi_{k}}{p_{t}(\vx_{t})} \\ 
		&= \frac{\pi_{k}\phi(\vx_{t} ; \vmu_{k}, \vSigma_{k}
		+ t^{2}\vI)}{\sum_{i = 1}^{K}\pi_{i}\phi(\vx_{t} ; \vmu_{i}, \vSigma_{i} + t^{2}\vI)}.
	\end{align}
	另一方面，条件期望如前所述：
	\begin{equation}
		\Ex[\vx \mid \vx_{t}, y = k] = \vmu_{k} + \vSigma_{k}(\vSigma_{k} + t^{2}\vI)^{-1}(\vx_{t} - \vmu_{k}).
	\end{equation}
	所以把这些放在一起，真正的贝叶斯最优去噪器是
	\begin{equation}\label{eq:gmm_bayes_optimal_denoiser}
		\bar{\vx}^{\ast}(t, \vx_{t}) = \sum_{k = 1}^{K}\frac{\pi_{k}\phi(\vx_{t}
		;\vmu_{k}, \vSigma_{k} + t^{2}\vI)}{\sum_{i = 1}^{K}\pi_{i}\phi(\vx_{t}
		;\vmu_{i}, \vSigma_{i} + t^{2}\vI)}\cdot\bp{\vmu_{k} + \vSigma_{k}(\vSigma_{k} + t^{2}\vI)^{-1}(\vx_{t} - \vmu_{k})}.
	\end{equation}
	这个例子特别重要，几个特殊情况将在后面给我们带来重要的概念性洞见。现在，让我们尝试从最优去噪器 \eqref{eq:gmm_bayes_optimal_denoiser} 的函数形式中提取一些几何直觉。

	为了直观地理解 \eqref{eq:gmm_bayes_optimal_denoiser}，让我们首先设 \(K = 1\)（即一个高斯分布），使得 \(\vx \sim \dNorm(\vmu, \vSigma)\)。然后让我们对角化 \(\vSigma = \vV\vLambda \vV^{\top}\)。那么贝叶斯最优去噪器是
	\begin{equation}
		\bar{\vx}^{\ast}(t, \vx_{t}) = \vmu + \vSigma(\vSigma + t^{2}\vI)^{-1}(\vx_{t} - \vmu) = \vmu + \vV\mat{\lambda_{1}/(\lambda_{1} + t^{2}) & & \\ & \ddots & \\ & & \lambda_{D}/(\lambda_{D} + t^{2})}\vV^{\top}(\vx_{t} - \vmu),
	\end{equation}
	其中 \(\lambda_{1}, \dots, \lambda_{D}\) 是 \(\vSigma\) 的特征值。我们可以观察到这个去噪器有三个步骤：
	\begin{itemize}
		\item 将输入 \(\vx_{t}\) 平移 \(\vmu\)。
		\item 在每个特征向量方向上，将（平移后的）输入 \(\vx_{t} - \vmu\) 收缩一个量 \(\lambda_{i}/(\lambda_{i} + t^{2})\)。如果平移后的输入是低秩的，并且 \(\vSigma\) 的某些特征值为零，这些方向会立即被去噪器收缩到 \(0\)，确保收缩的输出同样是低秩的。
		\item 将输出平移回 \(\vmu\)。
	\end{itemize}
	很容易证明它将当前的 \(\vx_{t}\) 收缩到均值 \(\vmu\)：
	\begin{equation}\label{eq:contraction}
		\norm{\bar{\vx}^{\ast}(t, \vx_{t}) - \vmu}_{2} \leq \norm{\vx_{t} - \vmu}_{2}.
	\end{equation}

	这是\textit{单个}高斯分布去噪器的几何解释。高斯混合模型 \eqref{eq:gmm_bayes_optimal_denoiser} 的整体去噪器使用 \(K\) 个这样的去噪器，通过后验概率 \(\Pr[y = k \mid \vx_{t}]\) 对它们的输出进行加权。如果高斯分布的均值分离得很好，这些后验概率在每个均值或簇附近非常接近 \(0\) 或 \(1\)。在这种情况下，整体去噪器 \eqref{eq:gmm_bayes_optimal_denoiser} 与上述单个高斯去噪器具有相同的几何解释。

    乍一看，这种收缩映射（\eqref{eq:contraction}）可能看起来类似于幂迭代（见 \Cref{subsec:power iterations}）。然而，这两者有根本的不同。幂迭代实现的是朝向一个子空间的收缩映射——即由第一主成分张成的子空间。相比之下，\eqref{eq:contraction} 中的迭代收敛到底层分布的均值 \(\vmu\)，这是一个单点。
\end{example}

\begin{figure}
	\centering 
	\includegraphics[width=\textwidth]{\toplevelprefix/chapters/chapter3/figs/ve_forward_diffusion_denoising.png}\vspace{-0.15in}
	\caption{\small \textbf{高斯混合模型的贝叶斯最优去噪器和分数函数。} 在与 \Cref{fig:ve_forward_density} 相同的设置下，我们通过绘制 \(\vx_{t}\)（红色）和 \(\bar{\vx}^{\ast}(t, \vx_{t})\)（绿色）来展示贝叶斯最优去噪器 \(\bar{\vx}^{\ast}\) 的效果，其中 \(t\) 和 \(\vx_{t}\) 是某个选定的值。根据 Tweedie 公式 \Cref{thm:tweedie}，它们之间的残差与所谓的（Hyv\"arinen）分数 \(\nabla_{\vx_{t}}\log p_{t}(\vx_{t})\) 成正比。我们可以看到，分数指向 \(\vx_{t}\) 分布的众数。}
	\label{fig:ve_forward_denoising}
\end{figure}

直观上，并且从 \Cref{example:denoising_gaussian_mixture} 中我们可以看到，贝叶斯最优去噪器 \(\bar{\vx}^{\ast}(t, \cdot)\) 应该将其输入 \(\vx_{t}\) 移向 \(\vx\) 分布的众数。事实证明，我们实际上可以通过证明贝叶斯最优去噪器在 \(\vx_{t}\) 的（对数）密度上\textit{进行了一步梯度上升}来量化这一点，我们（回顾一下）将其表示为 \(p_{t}\)。也就是说，跟随去噪器意味着从输入迭代移动到这个（扰动后的）分布中概率更高的区域。对于小的 \(t\)，扰动很小，所以我们最初的直觉因此（几乎）完全正确。这个情景在 \Cref{fig:ve_forward_denoising} 中可视化，并被严格地表述为 Tweedie 公式 \cite{Robbins1956AnEB}。
\begin{theorem}[Tweedie 公式]\label{thm:tweedie}
	假设 \((\vx_{t})_{t \in [0, T]}\) 遵循 \eqref{eq:additive_gaussian_noise_model}。设 \(p_{t}\) 是 \(\vx_{t}\) 的密度（如前所述）。那么
	\begin{equation}\label{eq:tweedie}
		\Ex[\vx \mid \vx_{t}] = \vx_{t} + t^{2}\nabla_{\vx_{t}} \log p_{t}(\vx_{t}).
	\end{equation}
\end{theorem}
\begin{proof}
	为了证明，我们假设 \(\vx\) 有一个密度（尽管定理在没有这个假设的情况下也成立），并称这个密度为 \(p\)。设 \(p_{0 \mid t}\) 和 \(p_{t \mid 0}\) 分别是给定 \(\vx_{t}\) 的 \(\vx = \vx_{0}\) 和给定 \(\vx\) 的 \(\vx_{t}\) 的条件密度。设 \(\phi(\vx ; \vmu, \vSigma)\) 是在 \(\vx\) 处评估的 \(\dNorm(\vmu, \vSigma)\) 的密度，因此 \(p_{t \mid 0}(\vx_{t} \mid \vx) = \phi(\vx_{t} ; \vx, t^{2}\vI)\)。那么一个简单的计算给出
	\begin{align}
		\nabla_{\vx_{t}} \log p_{t}(\vx_{t})
		&= \frac{\nabla_{\vx_{t}}p_{t}(\vx_{t})}{p_{t}(\vx_{t})} \\
		&= \frac{1}{p_{t}(\vx_{t})}\nabla_{\vx_{t}}\int_{\R^{D}}p(\vx)p_{t \mid 0}(\vx_{t} \mid \vx)\odif{\vx} \\
		&=
		\frac{1}{p_{t}(\vx_{t})}\nabla_{\vx_{t}}\int_{\R^{D}}p(\vx)\phi(\vx_{t}
		;\vx, t^{2}\vI)\odif{\vx} \\
		&=
		\frac{1}{p_{t}(\vx_{t})}\int_{\R^{D}}p(\vx)[\nabla_{\vx_{t}}\phi(\vx_{t}
		;\vx, t^{2}\vI)]\odif{\vx} \\
		&= \frac{1}{p_{t}(\vx_{t})}\int_{\R^{D}}p(\vx)\phi(\vx_{t} ; \vx, t^{2}\vI)\bs{-\frac{\vx_{t} - \vx}{t^{2}}}\odif{\vx} \\
		&= \frac{1}{t^{2}p_{t}(\vx_{t})}\int_{\R^{D}}p(\vx)\phi(\vx_{t} ; \vx, t^{2}\vI)[\vx - \vx_{t}]\odif{\vx} \\
		&= \frac{1}{t^{2}p_{t}(\vx_{t})}\int_{\R^{D}}p(\vx)\phi(\vx_{t} ; \vx,
		t^{2}\vI)\vx \odif{\vx}
		- \frac{\vx_{t}}{t^{2}p_{t}(\vx_{t})}\int_{\R^{D}}p(\vx)\phi(\vx_{t} ; \vx, t^{2}\vI)\odif{\vx} \\
		&= \frac{1}{t^{2}p_{t}(\vx_{t})}\int_{\R^{D}}p(\vx)p_{t \mid 0}(\vx_{t} \mid \vx)\vx \odif{\vx} - \frac{\vx_{t}}{t^{2}p_{t}(\vx_{t})}p_{t}(\vx_{t}) \\
		&= \frac{1}{t^{2}p_{t}(\vx_{t})}\int_{\R^{D}}p_{t}(\vx_{t})p_{0 \mid t}(\vx \mid \vx_{t})\vx \odif{\vx} - \frac{\vx_{t}}{t^{2}p_{t}(\vx_{t})}p_{t}(\vx_{t}) \\
		&= \frac{1}{t^{2}}\int_{\R^{D}}p_{0 \mid t}(\vx \mid \vx_{t})\vx \odif{\vx} - \frac{\vx_{t}}{t^{2}} \\
		&= \frac{1}{t^{2}}\Ex[\vx \mid \vx_{t}] - \frac{\vx_{t}}{t^{2}} \\
		&= \frac{\Ex[\vx \mid \vx_{t}] - \vx_{t}}{t^{2}}.
	\end{align}
	对上述等式进行简单整理即可证明该定理。
\end{proof}
这个结果建立了去噪和优化之间的联系：贝叶斯最优去噪器在扰动后的数据密度 \(p_{t}\) 上进行单步梯度上升，并且随着对数据分布的扰动变小，步长自适应地变小（即采取更精确的步骤）。量 \(\nabla_{\vx_{t}}\log p_{t}(\vx_{t})\) 被称为\textit{（Hyv\"arinen）分数}，并经常出现在关于去噪等的讨论中；它最早出现在 Aapo Hyv\"arinen 的一篇关于 ICA 的论文中 \cite{hyvarinen05a}。

就像在实践中，当从远离最优解的地方初始化时，一步梯度下降几乎不足以最小化一个目标函数一样，当 \(t\) 很大时，贝叶斯最优去噪器 \(\bar{\vx}^{\ast}(t, \cdot)\) 的输出几乎不会包含在数据分布的高概率区域内，\textit{尤其}是当数据具有低维结构时。我们在下面的例子中明确地说明了这一点。
\begin{example}[去噪一个两点混合分布]\label{example:denoising_twopoints}
	设 \(x\) 在两点集 \(\{-1, +1\}\) 上均匀分布，并设 \((\vx_{t})_{t \in [0, T]}\) 遵循 \eqref{eq:additive_gaussian_noise_model}。这正是一个退化的高斯混合模型，其先验概率等于 \(\frac{1}{2}\)，均值为 \(\{-1, +1\}\)，协方差都等于 \(0\)。对于一个固定的 \(t > 0\)，我们可以使用 \eqref{eq:gmm_bayes_optimal_denoiser} 中贝叶斯最优去噪器的计算来得到（证明作为练习）
	\begin{equation}
		\bar{x}^{\ast}(t, x_{t}) = \frac{\phi(x_{t} ; +1, t^{2}) - \phi(x_{t}
		; -1, t^{2})}{\phi(x_{t} ; 1, t^{2}) + \phi(x_{t} ; -1, t^{2})} = \tanh\rp{-\frac{x_{t}}{t^{2}}}.
	\end{equation}
	对于接近 \(0\) 的 \(t\)，对于几乎所有的输入 \(\bar{x}^{\ast}(t, x_{t})\)，这个量都接近 \(\{-1, +1\}\)。然而，对于大的 \(t\)，这个量不一定甚至近似地在 \(x\) 的原始支撑集 \(\{-1, +1\}\) 中。特别是，对于 \(x_{t} \approx 0\)，有 \(\bar{x}^{\ast}(t, x_{t}) \approx 0\)，它完全位于两个可能的点之间。因此 \(\bar{x}^{\ast}\) \textit{不会输出“真实的” \(x\)}。或者更数学地说，\(\bar{x}(t, x_{t})\) 的分布与 \(x\) 的分布非常不同。
\end{example}

因此，如果我们想对非常嘈杂的样本 \(\vx_{T}\)（其中——回想一下——\(T\) 是最大时间）进行去噪，我们不能只使用去噪器\textit{一次}。相反，我们必须多次使用去噪器，类似于使用\textit{衰减步长}的梯度下降，以收敛到一个平稳点 \(\hat{\vx}\)。也就是说，我们将使用去噪器从 \(\vx_{T}\) 到达近似 \(\vx_{T - \delta}\) 的 \(\hat{\vx}_{T - \delta}\)，然后从 \(\hat{\vx}_{T - \delta}\) 到 \(\hat{\vx}_{T - 2\delta}\)，等等，一直从 \(\hat{\vx}_{\delta}\) 到 \(\hat{\vx} = \hat{\vx}_{0}\)。每次我们进行去噪步骤时，去噪器的作用都更像是在原始（对数）密度上进行梯度步骤。

更正式地，我们将 \([0, T]\) 均匀离散化为 \(L + 1\) 个时间步 \(0 = t_{0} < t_{1} < \cdots < t_{L} = T\)，即
\begin{equation}
	t_{\ell} = \frac{\ell}{L}T, \qquad \ell \in \{0, 1, \dots, L\}.
\end{equation}
然后对于每个 \(\ell \in [L] = \{1, 2, \dots, L\}\)，从 \(\ell = L\) 到 \(\ell = 1\)，我们可以运行迭代
\begin{align}
	\hat{\vx}_{t_{\ell - 1}}
	&= \Ex[\vx_{t_{\ell - 1}} \mid \vx_{t_{\ell}} = \hat{\vx}_{t_{\ell}}] \\
	&= \Ex[\vx + t_{\ell - 1}\vg \mid \vx_{t_{\ell}} = \hat{\vx}_{t_{\ell}}] \\
	&= \Ex\rs{\vx + t_{\ell - 1}\cdot\frac{\vx_{t_{\ell}} - \vx}{t_{\ell}} \given \vx_{t_{\ell}} = \hat{\vx}_{t_{\ell}}} \\
	&= \frac{t_{\ell - 1}}{t_{\ell}}\hat{\vx}_{t_{\ell}} + \bp{1 - \frac{t_{\ell - 1}}{t_{\ell}}}\Ex[\vx \mid \vx_{t_{\ell}} = \hat{\vx}_{t_{\ell}}] \\
	&= \bp{1 - \frac{1}{\ell}}\cdot\hat{\vx}_{t_{\ell}} + \frac{1}{\ell}\cdot\bar{\vx}^{\ast}(t_{\ell}, \hat{\vx}_{t_{\ell}}).
\end{align}
这个迭代的效果如下。在迭代开始时，当 \(\ell\) 很大时，我们几乎不信任去噪器的输出，而主要保留当前的迭代值。这是有道理的，因为去噪器可能有很大的方差（参见 \Cref{example:denoising_twopoints}）。当 \(\ell\) 很小时，去噪器将“锁定”到数据分布的众数，因为去噪步骤基本上是在真实分布的对数密度上进行梯度步骤，我们可以相信它不会产生不合理的样本，所以去噪步骤主要涉及去噪器的输出。在 \(\ell = 1\) 时，我们甚至扔掉当前的迭代值，只保留去噪器的输出。

\begin{figure}[t]
	\centering
	\includegraphics[width=\textwidth]{\toplevelprefix/chapters/chapter3/figs/ve_gmm_denoising.png}
	\caption{\small\textbf{对一个低秩高斯混合模型进行去噪。} 每张图表示来自真实数据分布的样本（灰色、橙色、红色）和正在经历去噪过程 \eqref{eq:denoising-iteration-basic} 的样本（浅蓝色）。左上角，过程刚刚开始，噪声非常大。随着过程的继续，噪声被进一步推向低秩数据分布的支撑集。最后，在右下角，生成的样本与数据的支撑集完美对齐，看起来非常像从低秩高斯混合模型中抽取的样本。}
	\label{fig:ve_gmm_denoising}
\end{figure}

以上是为什么我们期望去噪过程收敛的直观解释。我们在 \(\R^{3}\) 中可视化了收敛过程，见 \Cref{fig:ve_gmm_denoising}。我们稍后将发展一些关于收敛的严格结果。现在，回想一下我们想要建立一个降低熵的过程。虽然我们通过逆转一个增加熵的过程以一种迂回的方式做到了这一点，但现在是时候付出代价，确认我们的迭代去噪过程确实降低了熵。

\begin{theorem}[\Cref{thm:conditioning_reduces_entropy} 的简化版]
	假设 \((\vx_{t})_{t \in [0, T]}\) 遵循 \eqref{eq:additive_gaussian_noise_model}。那么，在 \(\vx\) 的某些技术条件下，对于每个 \(s < t\) 且 \(s, t \in (0, T]\)，
	\begin{equation}
		h(\Ex[\vx_{s} \mid \vx_{t}]) < h(\vx_{t}).
	\end{equation}
\end{theorem}
该定理的完整陈述及其证明需要一些技术性，因此推迟到 \Cref{sub:denoising_entropy_decreases}。

我们在这里讨论的最后一件事是，很多时候，我们将\textit{无法计算}任何 \(t\) 的 \(\bar{\vx}^{\ast}(t, \cdot)\)，因为我们没有分布 \(p_{t}\)。但我们可以尝试\textit{从数据中学习一个}。回想一下，去噪器 \(\bar{\vx}^{\ast}\) 在 \eqref{eq:denoising_loss} 中被定义为最小化均方误差 \(\Ex\norm{\bar{\vx}(t, \vx_{t}) - \vx}_{2}^{2}\)。我们可以使用这个均方误差作为损失或目标函数来学习去噪器。例如，我们可以用神经网络来参数化 \(\bar{\vx}(t, \cdot)\)，将其写为 \(\bar{\vx}_{\theta}(t, \cdot)\)，并在参数空间 \(\bm\Theta\) 上优化损失：
\begin{equation}
	\min_{\bm \theta \in \bm \Theta}\Ex_{\vx, \vx_{t}}\norm{\bar{\vx}_{\bm\theta}(t, \vx_{t}) - \vx}_{2}^{2}.
\end{equation}
通过梯度下降或类似算法实现的这个优化问题的解，将给我们一个 \(\bar{\vx}_{\bm\theta^{\ast}}(t, \cdot)\)，它是 \(\bar{\vx}^{\ast}(t, \cdot)\) 的一个很好的近似（至少如果训练有效的话），我们将用它作为我们的去噪器。

这个神经网络 \(\bar{\vx}_{\bm\theta^{\ast}}(t, \cdot)\) 的一个好架构是什么？为了回答这个问题，我们将研究一个普遍存在的案例，即\textit{高斯混合模型}，我们在 \Cref{example:denoising_gaussian_mixture} 中计算了它的去噪器。这个模型是相关的，因为它可以近似许多类型的分布：特别是，给定一个 \(\vx\) 的分布，存在一个高斯混合模型可以任意好地近似它。因此，在高斯混合模型的去噪器类中进行优化，可以给我们一些接近真实数据分布的最优去噪器的东西。

在我们的案例中，我们假设 \(\vx\) 是低维的，这粗略地转化为要求 \(\vx\) \textit{近似地}根据一个\textit{低秩高斯混合模型}分布。形式上，我们写
\begin{equation}\label{eq:MoG1}
	\vx \sim \frac{1}{K}\sum_{k = 1}^{K}\dNorm(\vzero, \vU_{k}\vU_{k}^{\top})
\end{equation}
其中 \(\vU_{k} \in \O(D, P) \subseteq \R^{D \times P}\) 是一个正交矩阵。那么在 \eqref{eq:additive_gaussian_noise_model} 下的最优去噪器是（来自 \Cref{example:denoising_gaussian_mixture}）
\begin{equation}
	\bar{\vx}^{\ast}(t, \vx_{t}) = \sum_{k = 1}^{K}\frac{\phi(\vx_{t} ; \vzero,
	\vU_{k}\vU_{k}^{\top} + t^{2}\vI)}{\sum_{i = 1}^{K}\phi(\vx_{t} ; \vzero, \vU_{i}\vU_{i}^{\top} + t^{2}\vI)}\cdot\bp{\vU_{k}\vU_{k}^{\top}(\vU_{k}\vU_{k}^{\top} + t^{2}\vI)^{-1}\vx_{t}}.
\end{equation}
注意，在计算 \(\phi\) 内部和外部，我们都计算了逆矩阵 \((\vU_{k}\vU_{k}^{\top} + t^{2}\vI)^{-1}\)。这是一个满秩矩阵 \(t^{2}\vI\) 的低秩扰动，因此很适合通过\textit{Sherman-Morrison-Woodbury 恒等式}进行简化，即对于矩阵 \(\vA, \vC, \vU, \vV\) 使得 \(\vA\) 和 \(\vC\) 可逆，
\begin{equation}\label{eq:sherman_morrison_woodbury}
	(\vA + \vU\vC\vV)^{-1} = \vA^{-1} - \vA^{-1}\vU(\vC^{-1} + \vV\vA^{-1}\vU)^{-1}\vV\vA^{-1}.
\end{equation}
我们在 \Cref{exercise:sherman_morrison_woodbury_identity} 中证明这个恒等式。现在我们应用这个恒等式，令 \(\vA = t^{2}\vI\)，\(\vU = \vU_{k}\)，\(\vV = \vU_{k}^{\top}\)，和 \(\vC = \vI\)，得到
\begin{align}
	(\vU_{k}\vU_{k}^{\top} + t^{2}\vI)^{-1} 
	&= \frac{1}{t^{2}}\vI - \frac{1}{t^{4}}\vU_{k}\bp{\vI + \frac{1}{t^{2}}\vU_{k}^{\top}\vU_{k}}^{-1}\vU_{k}^{\top} \\
	&= \frac{1}{t^{2}}\vI - \frac{1}{t^{4}\bp{1 + \frac{1}{t^{2}}}}\vU_{k}\vU_{k}^{\top} \\
	&= \frac{1}{t^{2}}\bp{\vI - \frac{1}{1 + t^{2}}\vU_{k}\vU_{k}^{\top}}.
\end{align}
然后我们可以如下计算后验概率。注意，由于 \(\vU_{k}\) 都是正交的，\(\det(\vU_{k}\vU_{k}^{\top} + t^{2}\vI)\) 对于每个 \(k\) 都是相同的。所以
\begin{align}
	\frac{\phi(\vx_{t} ; \vzero, \vU_{k}\vU_{k}^{\top} + t^{2}\vI)}{\sum_{i
	= 1}^{K}\phi(\vx_{t} ; \vzero, \vU_{i}\vU_{i}^{\top} + t^{2}\vI)} 
	&= \frac{\exp\rp{-\frac{1}{2}\vx_{t}^{\top}(\vU_{k}\vU_{k}^{\top} + t^{2}\vI)^{-1}\vx_{t}}}{\sum_{i = 1}^{K}\exp\rp{-\frac{1}{2}\vx_{t}^{\top}(\vU_{i}\vU_{i}^{\top} + t^{2}\vI)^{-1}\vx_{t}}} \\
	&= \frac{\exp\rp{-\frac{1}{2t^{2}}\vx_{t}^{\top}\bp{\vI - \frac{1}{1 + t^{2}}\vU_{k}\vU_{k}^{\top}}\vx_{t}}}{\sum_{i = 1}^{K}\exp\rp{-\frac{1}{2t^{2}}\vx_{t}^{\top}\bp{\vI - \frac{1}{1 + t^{2}}\vU_{i}\vU_{i}^{\top}}\vx_{t}}} \\
	&= \frac{\exp\rp{-\frac{1}{2t^{2}}\norm{\vx_{t}}_{2}^{2} + \frac{1}{2t^{2}(1 + t^{2})}\norm{\vU_{k}^{\top}\vx_{t}}_{2}^{2}}}{\sum_{i = 1}^{K}\exp\rp{-\frac{1}{2t^{2}}\norm{\vx_{t}}_{2}^{2} + \frac{1}{2t^{2}(1 + t^{2})}\norm{\vU_{i}^{\top}\vx_{t}}_{2}^{2}}} \\
	&= \frac{\exp\rp{-\frac{1}{2t^{2}}\norm{\vx_{t}}_{2}^{2}}\exp\rp{\frac{1}{2t^{2}(1 + t^{2})}\norm{\vU_{k}^{\top}\vx_{t}}_{2}^{2}}}{\exp\rp{-\frac{1}{2t^{2}}\norm{\vx_{t}}_{2}^{2}}\sum_{i = 1}^{K}\exp\rp{\frac{1}{2t^{2}(1 + t^{2})}\norm{\vU_{i}^{\top}\vx_{t}}_{2}^{2}}} \\
	&= \frac{\exp\rp{\frac{1}{2t^{2}(1 + t^{2})}\norm{\vU_{k}^{\top}\vx_{t}}_{2}^{2}}}{\sum_{i = 1}^{K}\exp\rp{\frac{1}{2t^{2}(1 + t^{2})}\norm{\vU_{i}^{\top}\vx_{t}}_{2}^{2}}}.
\end{align}
这是一个 softmax 操作，其权重由 \(\vx_{t}\) 在每个子空间上的投影（由 \(\norm{\vU_{i}^{\top}\vx_{t}}_{2}\) 衡量）决定（并由温度 \(2t^{2}(1 + t^{2})\) 调节）。同时，分量去噪器可以写成
\begin{align}
	\vU_{k}\vU_{k}^{\top}(\vU_{k}\vU_{k}^{\top} + t^{2}\vI)^{-1}\vx_{t} 
	&= \frac{1}{t^{2}}\vU_{k}\vU_{k}^{\top}\bp{\vI - \frac{1}{1 + t^{2}}\vU_{k}\vU_{k}^{\top}}\vx_{t} \\
	&= \frac{1}{t^{2}}\bp{1 - \frac{1}{1 + t^{2}}}\vU_{k}\vU_{k}^{\top}\vx_{t} \\
	&= \frac{1}{1 + t^{2}}\vU_{k}\vU_{k}^{\top}\vx_{t}.
\end{align}
将这些放在一起，我们有
\begin{equation}\label{eq:gmm_lowrank_denoiser}
	\bar{\vx}^{\ast}(t, \vx_{t}) = \frac{1}{1 + t^{2}}\sum_{k = 1}^{K}\frac{\exp\rp{\frac{1}{2t^{2}(1 + t^{2})}\norm{\vU_{k}^{\top}\vx_{t}}_{2}^{2}}}{\sum_{i = 1}^{K}\exp\rp{\frac{1}{2t^{2}(1 + t^{2})}\norm{\vU_{i}^{\top}\vx_{t}}_{2}^{2}}}\vU_{k}\vU_{k}^{\top}\vx_{t},
\end{equation}
即，\(\vx_{t}\) 在 \(K\) 个子空间上的投影，由 \(\vx_{t}\) 的二次函数的 soft-max 操作加权。这种函数形式类似于 Transformer 架构中的\textit{注意力机制}！正如我们将在 \Cref{ch:representation} 中看到的，这绝非巧合；去噪和有损压缩（将在 \Cref{sec:lossy_compression} 中介绍）之间的深层联系使得 Transformer 去噪器在实践中如此有效。因此，总的来说，我们的高斯混合模型理论为使用类 Transformer 神经网络进行去噪提供了动机。

\begin{remark}
{\bf 去噪分布与概率 PCA 之间的联系。} 在这里，我们希望将去噪一个低维分布与概率 PCA（有关概率 PCA 的更多细节，请参见 \Cref{subsec:probabilistic PCA}）联系起来。假设我们在 \eqref{eq:MoG1} 中考虑 $K=1$，即 $\vx \sim   \dNorm(\vzero, \vU\vU^{\top})$，其中 \(\vU \in \O(D, P) \subseteq \R^{D \times P}\) 是一个正交矩阵。根据 \eqref{eq:gmm_lowrank_denoiser}，贝叶斯最优去噪器是
\begin{align}
    \bar{\vx}^{\ast}(t, \vx_{t}) = \frac{1}{1 + t^{2}} \vU\vU^\top\vx_t. 
\end{align}
为了学习这个贝叶斯最优去噪器，我们可以相应地将去噪算子 $\bar{\vx}(t,\vx_t)$ 参数化如下：
\begin{align}
    \bar{\vx}(t,\vx_t) = \frac{1}{1 + t^{2}} \vV\vV^\top\vx_t,
\end{align}
其中 $\vV \in \O(D, P)$ 是可学习的参数。将此代入训练损失 \eqref{eq:denoising_loss} 得到
\begin{equation}\label{eq:training loss PCA}
	  \min_{\vV \in \O(D, P)}\Ex_{\vx, \vx_{t}}\left\|\vx - \frac{1}{1 + t^{2}} \vV\vV^\top\vx_t\right\|_{2}^{2} = \Ex_{\vx, \vg}\left\|\vx - \frac{1}{1 + t^{2}} \vV\vV^\top(\vx + t\vg)\right\|_{2}^{2},
\end{equation}
其中等式是由于 \eqref{eq:additive_gaussian_noise_model}。以 $\vx$ 为条件，我们计算
\begin{align}
    & \Ex_{\vg}\left\|\vx - \frac{1}{1 + t^{2}} \vV\vV^\top(\vx + t\vg)\right\|_{2}^{2} \\
    = & \left\|\vx - \frac{1}{1 + t^{2}} \vV\vV^\top \vx\right\|_{2}^{2} - \frac{t}{1+t^2}\Ex_{\vg} \left\langle \vx - \frac{1}{1 + t^{2}} \vV\vV^\top \vx,  \vV\vV^\top\vg \right\rangle + \frac{t^2}{(1+t^2)^2} \Ex_{\vg}\left\| \vV\vV^\top \vg \right\|_2^2 \\
    = & \left\|\vx - \frac{1}{1 + t^{2}} \vV\vV^\top \vx\right\|_{2}^{2} + \frac{t^2P}{(1+t^2)^2} 
\end{align}
其中第二个等式是由于 \(\vg \sim \dNorm(\vzero, \vI)\) 和 $ \Ex_{\vg}\left\| \vV\vV^\top \vg \right\|_2^2 = \Ex_{\vg}\left\|\vV^\top \vg \right\|_2^2 = P$（因为 $\vV \in \O(D, P)$）。因此，问题 \eqref{eq:training loss PCA} 等价于
\begin{align}
    \min_{\vV \in \O(D, P)}\Ex_{\vx} \left\|\vx - \frac{1}{1 + t^{2}} \vV\vV^\top \vx\right\|_{2}^{2} = \Ex_{\vx} \|\vx\|_2^2 + \left( \frac{1}{(1+t^2)^2} - \frac{2}{1+t^2} \right) \Ex_{\vx}\|\vV^\top \vx\|_2^2.
\end{align}
这进一步等价于
\begin{align}
    \max_{\vV \in \O(D, P)} \Ex_{\vx}\|\vV^\top \vx\|_2^2,
\end{align}
这本质上是问题 \eqref{eq:PPCA}。
\end{remark}

总的来说，学习到的去噪器构成了给定数据的一个（隐式参数化）编码方案，因为它可以用于去噪/投影到数据分布上。训练一个去噪器等同于寻找一个更好的编码方案，这部分满足了 \Cref{sub:min_entropy} 末尾提出的一个期望（\textit{第二个}）。在下文中，我们将讨论如何满足另一个期望（\textit{第一个}）。


\subsection{通过迭代去噪学习和采样分布}\label{sub:sampling_denoising}

回想一下，在 \Cref{sub:min_entropy} 的末尾，我们讨论了追求具有低维结构的分布的一对期望。第一个期望是从一个正态分布开始，比如具有高熵的分布，然后逐渐降低其熵，直到达到数据的分布。我们将这个过程称为\textit{采样}，因为我们正在生成新的样本。现在是我们讨论如何用我们已经建立的工具包来做到这一点的时候了。

我们知道如何对非常嘈杂的样本 \(\vx_{T}\) 进行去噪，以获得与目标随机变量 \(\vx\) 具有相似分布的近似 \(\hat{\vx}\)。但期望是，为了采样，我们希望从一个\textit{没有}受到 \(\vx\) 分布影响的模板分布开始，并使用去噪器引导迭代值朝向 \(\vx\) 的分布。我们如何做到这一点？一种方法如下所示：
\begin{equation}\label{eq:ve_converge_to_large_gaussian}
	\frac{\vx_{T}}{T} = \frac{\vx + T\vg}{T} = \frac{\vx}{T} + \vg \to \vg \sim \dNorm(\vzero, \vI).
\end{equation}
因此，\(\vx_{T} \approx \dNorm(\vzero, T^{2}\vI)\)。这个近似对于几乎所有实际分布都相当好，并在 \Cref{fig:xT_vs_noise} 中可视化。
\begin{figure}
	\centering 
	\includegraphics[width=0.75\textwidth]{\toplevelprefix/chapters/chapter3/figs/xT_vs_noise.png}
	\caption{\small\textbf{可视化 \(\vx_{T}\) 与 \(\dNorm(\vzero, T^{2}\vI)\)。} \textit{左图：} 高斯混合模型数据 \(\vx\) 的图。 \textit{右图：} \(\vx\) 以及 \(\vx_{T}\) 和一个独立的 \(\dNorm(\vzero, T^{2}\vI)\) 样本的图，其中 \(T = 10\)。在右图中，\(\vx\) 的颜色与左图相同：然而，来自 \(\vx_{T}\) 和 \(\dNorm(\vzero, T^{2}\vI)\) 的样本平均比来自 \(\vx\) 的样本大得多，因此由于缩放的原因，它看起来小得多。尽管有这种大的放大，我们清楚地观察到 \(\vx_{T}\) 和 \(\dNorm(\vzero, T^{2}\vI)\) 分布的相似性。}
	\label{fig:xT_vs_noise}
\end{figure}

因此，将 \([0, T]\) 均匀离散化为 \(0 = t_{0} < t_{1} < \cdots < t_{L} = T\)，使用 \(t_{\ell} = T\ell / L\)（如前一节），一种从纯噪声中采样的方法是：
\begin{itemize}
	\item 从 \(\dNorm(\vzero, T^{2}\vI)\) 中采样 \(\hat{\vx}_{T}\)（与其他一切独立同分布）
	\item 运行去噪迭代，如 \Cref{sub:intro_diffusion_denoising} 中所述，即
		\begin{equation}\label{eq:denoising-iteration-basic}
		\hat{\vx}_{t_{\ell - 1}} = \bp{1 - \frac{1}{\ell}}\cdot\hat{\vx}_{t_{\ell}} + \frac{1}{\ell}\cdot\bar{\vx}^{\ast}(t_{\ell}, \hat{\vx}_{t_{\ell}}).
	\end{equation}
	\item 输出 \(\hat{\vx} = \hat{\vx}_{0}\)。
\end{itemize}
这在概念上就是\textit{扩散模型}的全部内容，它根据第一个期望将噪声转换为数据样本。然而，在我们得到能够根据实际资源限制从像图像这样的真实数据分布中采样的模型之前，还有几步要走。在下文中，我们将介绍并论证几个这样的步骤。

\paragraph{步骤 1：不同的离散化方法。} 我们做的第一步是基于以下观点：\textit{我们不需要在大的 \(t\) 值上花费那么多的去噪迭代}。如果我们看 \Cref{fig:ve_gmm_denoising}，我们观察到在 500 次采样迭代中，前 200 或 300 次迭代只是将噪声整体收缩到数据分布上，而其余的迭代则将样本推向一个子空间。在固定的迭代次数 \(L\) 下，这表明我们应该在 \(t = 0\) 附近花费更多的时间步 \(t_{\ell}\)，而不是在 \(t = T\) 附近。因此，在采样（和训练）期间，我们可以使用 \([0, T]\) 的另一种离散化方法，将其离散化为 \(0 \leq t_{0} < t_{1} < \cdots < t_{L} \leq T\)，例如\textit{指数离散化}：
\begin{equation}\label{eq:denoising_exponential_discretization}
	t_{\ell} = C_{1}(e^{C_{2}\ell} - 1), \qquad \forall \ell \in \{0, 1, \dots, L\}
\end{equation}
其中 \(C_{1}, C_{2} > 0\) 是可以在实践中调整以获得最佳性能的常数；理论分析通常也会指定这样的最优常数。然后去噪/采样迭代变为
\begin{equation}\label{eq:denoising_iteration}
	\hat{\vx}_{t_{\ell - 1}} \doteq \frac{t_{\ell - 1}}{t_{\ell}}\hat{\vx}_{t_{\ell}} + \bp{1 - \frac{t_{\ell - 1}}{t_{\ell}}}\bar{\vx}^{\ast}(t_{\ell}, \hat{\vx}_{t_{\ell}}),
\end{equation}
同样，\(\hat{\vx}_{t_{L}} \sim \dNorm(\vzero, t_{L}^{2}\vI)\)。

\paragraph{步骤 2：不同的噪声模型。} 第二步是考虑与 \eqref{eq:additive_gaussian_noise_model} 略有不同的模型。这样做的基本动机如下。在实践中，噪声分布 \(\dNorm(\vzero, t_{L}^{2}\vI)\) 在高维情况下成为对真实协方差越来越差的估计，即 \eqref{eq:ve_converge_to_large_gaussian} 成为越来越差的近似，尤其是在各向异性的高维数据中。 \(\dNorm(\vzero, t_{L}^{2}\vI)\) 与 \(\vx_{t_{L}}\) 的真实分布之间的距离增加，可能会导致去噪器在这种情况下表现更差。理论上，随着 \(t_{L}\) 的增加，\(\vx_{t_{L}}\) 不会收敛到任何分布，因此这种设置难以进行端到端的分析。在这种情况下，我们的补救措施是\textit{同时添加噪声并缩小 \(\vx\) 的贡献，使得 \(\vx_{T}\) 随着 \(T \to \infty\) 而收敛}。添加噪声的速率表示为 \(\sigma \colon [0, T] \to \R_{\geq 0}\)，收缩的速率表示为 \(\alpha \colon [0, T] \to \R_{\geq 0}\)，使得 \(\sigma\) 是\textit{递增}的，\(\alpha\) 是（非严格）\textit{递减}的，并且
\begin{equation}\label{eq:gen_additive_gaussian_noise_model}
	\vx_{t} \doteq \alpha_{t}\vx + \sigma_{t}\vg, \qquad \forall t \in [0, T].
\end{equation}
之前的设置有 \(\alpha_{t} = 1\) 和 \(\sigma_{t} = t\)，这被称为\textit{方差爆炸（VE）过程}。一个流行的选择是减少 \(\vx\) 的贡献，正如我们最初描述的，它有 \(T = 1\)（因此 \(t \in [0, 1]\)），\(\alpha_{t} = \sqrt{1 - t^{2}}\) 和 \(\sigma_{t} = t\)；这是\textit{方差保持（VP）过程}。注意，在 VP 过程下，\(\vx_{1} \sim \dNorm(\vzero, \vI)\) 是精确的，所以我们可以直接从这个标准分布中采样并迭代去噪。因此，VP 过程在理论上更容易分析，在经验上也更稳定。\footnote{为什么要使用整个 \(\alpha, \sigma\) 设置？正如我们将在 \Cref{exercise:implement_denoising_processes} 中看到的，它封装并统一了许多提出的过程，包括最近流行的所谓\textit{流匹配}过程。尽管如此，VE 和 VP 过程在经验和理论上仍然是最流行的（到目前为止），因此我们将在本节中考虑它们。}

有了这个更通用的设置，Tweedie 公式 \eqref{eq:tweedie} 变为
\begin{equation}\label{eq:gen_tweedie}
	\Ex[\vx \mid \vx_{t}] = \frac{1}{\alpha_{t}}\bp{\vx_{t} + \sigma_{t}^{2}\nabla \log p_{t}(\vx)}.
\end{equation}
去噪迭代 \eqref{eq:denoising_iteration} 变为
\begin{equation}\label{eq:gen_denoising_iteration}
	\hat{\vx}_{t_{\ell - 1}} = \frac{\sigma_{t_{\ell - 1}}}{\sigma_{t_{\ell}}}\hat{\vx}_{t_{\ell}} + \bp{\alpha_{t_{\ell - 1}} - \frac{\sigma_{t_{\ell - 1}}}{\sigma_{t_{\ell}}}\alpha_{t_{\ell}}}\bar{\vx}^{\ast}(t_{\ell}, \hat{\vx}_{t_{\ell}}).
\end{equation}
最后，高斯混合模型去噪器 \eqref{eq:gmm_bayes_optimal_denoiser} 变为
\begin{equation}\label{eq:gen_gmm_bayes_optimal_denoiser}
	\bar{\vx}^{\ast}(t, \vx_{t}) = \sum_{k = 1}^{K}\frac{\pi_{k}\phi(\vx_{t}
	; \alpha_{t}\vmu_{k}, \alpha_{t}^{2}\vSigma_{k}
	+ \sigma_{t}^{2}\vI)}{\sum_{i = 1}^{K}\pi_{i}\phi(\vx_{t} ; \alpha_{t}\vmu_{i}, \alpha_{t}^{2}\vSigma_{i} + \sigma_{t}^{2}\vI)}\cdot\bp{\vmu_{k} + \alpha_{t}\vSigma_{k}(\alpha_{t}^{2}\vSigma_{k} + \sigma_{t}^{2}\vI)^{-1}(\vx_{t} - \alpha_{t}\vmu_{k})}.
\end{equation}
\Cref{fig:vp_gmm_denoising} 展示了采样过程的迭代。注意，去噪迭代 \eqref{eq:gen_denoising_iteration} 给出了一个名为 DDIM（“去噪扩散隐式模型”）采样器的采样算法 \cite{song2020denoising}，是当今扩散模型中最流行的采样算法之一。我们在这里将其总结在 \Cref{alg:iterative_denoising} 中。

\begin{figure}
	\centering 
	\includegraphics[width=\textwidth]{\toplevelprefix/chapters/chapter3/figs/vp_gmm_denoising.png}
	\caption{\small\textbf{使用 VP 扩散过程对高斯混合模型进行去噪。} 我们使用与 \Cref{fig:ve_gmm_denoising} 相同的图形设置和数据分布。注意，与 \Cref{fig:ve_gmm_denoising} 相比，噪声分布更集中在原点附近。}
	\label{fig:vp_gmm_denoising}
\end{figure}

\begin{algorithm}
	\caption{使用去噪器进行采样。}
	\label{alg:iterative_denoising}
	\begin{algorithmic}[1]
		\Require{一个用于采样的有序时间步列表 \(0 \leq t_{0} < \cdots < t_{L} \leq T\)。}
		\Require{一个去噪器 \(\bar{\vx} \colon \{t_{\ell}\}_{\ell = 1}^{L} \times \R^{D} \to \R^{D}\)。}
		\Require{尺度和噪声水平函数 \(\alpha, \sigma \colon \{t_{\ell}\}_{\ell = 0}^{L} \to \R_{\geq 0}\)。}
		\Ensure{一个样本 \(\hat{\vx}\)，近似来自 \(\vx\) 的分布。}
		\Function{DDIMSampler}{$\bar{\vx}, (t_{\ell})_{\ell = 0}^{L}$}
		\State{初始化 \(\tilde{\vx}_{t_{L}} \sim\) \(\vx_{t_{L}}\) 的近似分布} \Comment{VP \(\implies \dNorm(\vzero, \vI)\), VE \(\implies \dNorm(\vzero, t_{L}^{2}\vI)\)。}
		\For{\(\ell = L, L - 1, \dots, 1\)}
		\State{计算
			\begin{equation*}
				\hat{\vx}_{t_{\ell - 1}} \doteq \frac{\sigma_{t_{\ell - 1}}}{\sigma_{t_{\ell}}}\hat{\vx}_{t_{\ell}} + \bp{\alpha_{t_{\ell - 1}} - \frac{\sigma_{t_{\ell - 1}}}{\sigma_{t_{\ell}}}\alpha_{t_{\ell}}}\bar{\vx}(t_{\ell}, \hat{\vx}_{t_{\ell}})
			\end{equation*}
		}
		\EndFor
		\State{\Return{\(\hat{\vx}_{t_{0}}\)}}
		\EndFunction
	\end{algorithmic}
\end{algorithm}

\paragraph{步骤 3：优化训练流程。} 如果我们使用 \Cref{sub:intro_diffusion_denoising} 规定的程序来为采样算法中使用的每个时间 \(t\) 学习一个单独的去噪器 \(\bar{\vx}(t, \cdot)\)，\textit{我们将不得不学习 \(L\) 个单独的去噪器！} 这是非常低效的——通常情况下，我们必须训练 \(L\) 个独立的神经网络，占用 \(L\) 倍的训练时间和存储内存，然后永远被锁定在使用这些时间步进行采样。相反，我们可以\textit{训练一个单一的神经网络}来在所有时间 \(t\) 上进行去噪，将连续变量 \(\vx_{t}\) 和 \(t\) 作为输入（而不仅仅是之前的 \(\vx_{t}\)）。从机制上讲，我们的训练损失在 \(t\) 上取平均，即解决以下问题：
\begin{equation}
	\min_{\bm\theta}\Ex_{t, \vx, \vx_{t}}\norm{\bar{\vx}_{\bm\theta}(t, \vx_{t}) - \vx}_{2}^{2}.
\end{equation}
与步骤 1 类似，我们在那里使用更多接近 \(t = 0\) 的时间步来确保更好的采样过程，我们可能希望确保去噪器在接近 \(t = 0\) 时质量更高，从而\textit{对损失进行加权}，使得接近 \(0\) 的 \(t\) 具有更高的权重。设 \(w_{t}\) 是时间 \(t\) 的权重，加权损失将如下所示：
\begin{equation}\label{eq:parametric_denoising_loss}
	\min_{\bm\theta}\Ex_{t}w_{t}\Ex_{\vx, \vx_{t}}\norm{\bar{\vx}_{\bm\theta}(t, \vx_{t}) - \vx}_{2}^{2}.
\end{equation}
在实践中，一个合理的权重选择是 \(w_{t} = \alpha_{t}/\sigma_{t}\)。确切的原因将在下一段中介绍，但总的来说，它用于提高对应于接近 \(0\) 的 \(t\) 的损失的权重，同时保持合理的数值稳定性。当然，在实践中我们无法计算期望，所以我们使用最直接的蒙特卡洛平均来估计它。这里所做的一系列改变有几个概念上和计算上的好处：我们不需要训练多个去噪器，我们可以在一组时间步上训练，并使用一个子集（或完全其他的）进行采样，等等。完整的流程在 \Cref{alg:learning_denoiser} 中讨论。

\begin{algorithm}
	\begin{algorithmic}[1]
		\Require{数据集 \(\cD \subseteq \R^{D}\)。}
		\Require{一个用于采样的有序时间步列表 \(0 \leq t_{0} < \cdots < t_{L} \leq T\)。}
		\Require{一个加权函数 \(w \colon \{t_{\ell}\}_{\ell = 1}^{L} \to \R_{\geq 0}\)。}
		\Require{尺度和噪声水平函数 \(\alpha, \sigma \colon \{t_{\ell}\}_{\ell = 0}^{L} \to \R_{\geq 0}\)。}
		\Require{一个参数空间 \(\bm\Theta\) 和一个去噪器架构 \(\bar{\vx}_{\bm \theta}\)。}
		\Require{一个用于参数的优化算法。}
		\Require{优化迭代次数 \(M\)。}
		\Require{每次迭代的蒙特卡洛抽样次数 \(N\)（用于近似 \eqref{eq:parametric_denoising_loss} 中的期望）}
		\Ensure{一个训练好的去噪器 \(\bar{\vx}_{\bm\theta^{\ast}}\)。}

		\Function{TrainDenoiser}{$\cD, \bm\Theta$}
		\State{初始化 \(\bm\theta^{(1)} \in \bm\Theta\)}
		\For{\(i \in [M]\)}
		\For{\(n \in [N]\)}
		\State{\(\vx_{n}^{(i)} \sim \cD\)} \Comment{从数据集中抽取一个样本。}
		\State{\(t_{n}^{(i)} \simiid \dUnif(\{t_{\ell}\}_{\ell = 1}^{L})\)} \Comment{采样一个时间步。}
		\State{\(\vg_{n}^{(i)} \simiid \dNorm(\vzero, \vI)\)} \Comment{采样一个噪声向量。}
		\State{\(\vx_{t, n}^{(i)} \doteq \alpha_{t_{n}^{(i)}}\vx_{n}^{(i)} + \sigma_{t_{n}^{(i)}}\vg_{n}^{(i)}\)} \Comment{计算加噪样本。}
		\State{\(w_{n}^{(i)} \doteq w_{t_{n}^{(i)}}\)} \Comment{计算损失权重。}
		\EndFor
		\State{\(\hat{\cL}^{(i)} \doteq \displaystyle \frac{1}{N}\sum_{n = 1}^{N}w_{n}^{(i)}\norm{\vx_{n}^{(i)} - \bar{\vx}_{\bm\theta^{(i)}}(t_{n}^{(i)}, \vx_{t, n}^{(i)})}_{2}^{2}\)} \Comment{计算损失估计。}
		\State{\(\bm\theta^{(i + 1)} \doteq \texttt{OptimizationUpdate}^{(i)}(\bm\theta^{(i)}, \nabla_{\bm\theta^{(i)}}\hat{\cL}^{(i)})\)} \Comment{更新参数。}
		\EndFor
		\State{\Return{\(\bar{\vx}_{\bm\theta^{(K + 1)}}\)}}
		\EndFunction
	\end{algorithmic}
	\caption{从数据中学习一个去噪器。}
	\label{alg:learning_denoiser}
\end{algorithm}

\paragraph{（可选）步骤 4：改变估计目标。} 注意，通常的做法是将整个去噪流程重新定位到\textit{噪声预测器}上，即对 \(\Ex[\vg \mid \vx_{t}]\) 的估计。在实践中，噪声预测器稍微更容易训练，因为它们的输出（几乎）总是与高斯随机变量的大小相当，所以训练在数值上更稳定。注意，由 \eqref{eq:gen_additive_gaussian_noise_model} 我们有
\begin{equation}
	\vx_{t} = \alpha_{t}\Ex[\vx \mid \vx_{t}] + \sigma_{t}\Ex[\vg \mid \vx_{t}] \implies \Ex[\vg \mid \vx_{t}] = \frac{1}{\sigma_{t}}\bp{\vx_{t} - \alpha_{t}\Ex[\vx \mid \vx_{t}]},
\end{equation}
因此，任何对 \(\vx\) 的预测器都可以使用上述关系转化为对 \(\vg\) 的预测器，即
\begin{equation}
	\bar{\vg}(t, \vx_{t}) = \frac{1}{\sigma_{t}}\vx_{t} - \frac{\alpha_{t}}{\sigma_{t}}\bar{\vx}(t, \vx_{t}),
\end{equation}
反之亦然。因此，一个好的估计 \(\bar{\vg}\) 的网络与一个好的估计 \(\bar{\vx}\) 的网络是相同的，\textit{外加一个残差连接}（例如在 Transformer 中看到的）。它们的损失也与去噪器相同，只是多了一个因子 \(\alpha_{t}/\sigma_{t}\)，即
\begin{equation}
	\Ex_{t}w_{t}\Ex_{\vg, \vx_{t}}\norm{\vg - \bar{\vg}(t, \vx_{t})}_{2}^{2} = \Ex_{t}w_{t}\frac{\alpha_{t}^{2}}{\sigma_{t}^{2}}\Ex_{\vx, \vx_{t}}\norm{\vx - \bar{\vx}(t, \vx_{t})}_{2}^{2}.
\end{equation}
为了完整起见，我们将提到其他目标也已被提出用于不同的任务，例如 \(\Ex[\odv*{\vx_{t}}{t} \mid \vx_{t}]\)（称为 \(v\)-预测或速度预测）等，但去噪和噪声预测仍然是常用的。在本书的其余部分，我们将只考虑去噪。


我们对我们最初的理想化加噪/去噪过程做了很多改变。为了确保新过程在实践中仍然有效，我们可以计算数值例子（例如 \Cref{fig:vp_gmm_denoising}）。为了确保它在理论上是合理的，我们可以证明采样算法的\textit{错误率界限}，这表明采样器的输出分布在所谓的\textit{全变分（TV）距离}上收敛到真实分布。两个随机变量 \(\vx\) 和 \(\vy\) 之间的 TV 距离定义为：
\begin{equation}
	\TV(\vx, \vy) \doteq \sup_{A \subseteq \R^{d}}\abs*{\Pr[\vx \in A] - \Pr[\vy \in A]}.
\end{equation}
如果 \(\vx\) 和 \(\vy\) 非常接近（一致地），那么上确界将很小。所以 TV 距离衡量随机变量的接近程度。（它确实是一个度量，正如其名所示：证明是一个练习。）

\begin{theorem}[\citep{li2024d} 定理 1，简化版]\label{thm:diffusion_sampler_convergence}
	假设 \(\Ex\norm{\vx}_{2} < \infty\)。如果 \(\vx\) 根据 VP 过程进行去噪，并采用指数离散化\footnote{精确的定义在我们的记法中相当冗长，并且只定义到各种绝对常数，因此为了简洁起见，我们在此省略。当然，它在原始论文 \citep{li2024d} 中。}，如 \eqref{eq:denoising_exponential_discretization} 所示，\Cref{alg:iterative_denoising} 的输出 \(\hat{\vx}\) 满足全变分界
	\begin{equation}\label{eq:diffusion_sampling_error}
		\TV(\vx, \hat{\vx}) = \tilde{\cO}\rp{\underbrace{\frac{D}{L}}_{\text{离散化误差}} + \underbrace{\sqrt{\frac{1}{L}\sum_{\ell = 1}^{L}\frac{\alpha_{t_{\ell}}}{\sigma_{t_{\ell}}^{2}}\Ex_{\vx, \vx_{t_{\ell}}}\norm{\bar{\vx}^{\ast}(t_{\ell}, \vx_{t_{\ell}}) - \bar{\vx}(t_{\ell}, \vx_{t_{\ell}})}_{2}^{2}}}_{\text{去噪器的平均超额误差}}}
	\end{equation}
	其中 \(\bar{\vx}^{\ast}\) 是 \(\vx\) 的贝叶斯最优去噪器，\(\tilde{\cO}\) 是大-\(\cO\) 记法的一个版本，它忽略了 \(L\) 中的对数因子。
\end{theorem}
非常高层次的证明技巧是，如前所述，在每一步都对误差进行界定，区分误差来源（离散化和去噪器误差），并仔细确保误差不会累积太多（甚至相互抵消）。

注意，如果 \(L \to \infty\) 并且我们正确地学习了贝叶斯最优去噪器 \(\bar{\vx} = \bar{\vx}^{\ast}\)（使得超额误差为 \(0\)），那么 \Cref{alg:iterative_denoising} 中的采样过程将产生一个加噪过程的\textit{完美（在分布意义上）逆过程}，因为 \Cref{thm:diffusion_sampler_convergence} 中的错误率趋于 \(0\)，\footnote{对于 VE 过程也有类似的结果，尽管据我们所知，没有一个像这个一样精确。} 正如之前启发式地论证的那样。

\begin{remark}
	如果数据是低维的，比如说支持在高维空间 \(\R^{D}\) 的一个\textit{低秩子空间}上，会怎么样？如果数据分布是紧支撑的——比如说，如果数据被归一化到单位超立方体，这通常是作为真实数据（如图像）的预处理步骤来确保的——那么有可能做得更好。也就是说，\cite{li2024d} 的作者还使用所谓的覆盖数的渐近性定义了一个\textit{近似内在维度}的度量，这在直觉上（如果不是在实现上）与下一节中介绍的率失真函数非常相似。然后他们表明，使用 \Cref{alg:iterative_denoising} 中 DDIM 采样器的一个特定的小修改（即，稍微扰动更新系数），离散化误差变为
	\begin{equation}
		\tilde{\cO}\rp{\frac{\text{近似内在维度}}{L}}
	\end{equation}
	而不是像 \Cref{thm:diffusion_sampler_convergence} 中那样的 \(\frac{D}{L}\)。因此，使用这个修改后的算法，即使 \(D\) 达到数千或数百万，\(L\) 也不必太大，因为真实数据具有低维结构。然而，在实践中我们使用 DDIM 采样器，所以 \(L\) 应该对 \(D\) 有一个温和的依赖性，以实现一致的错误率。\(L\) 的确切选择是在采样的计算复杂性（例如，运行时间或内存消耗）和学习低维结构的去噪器的统计复杂性之间进行权衡。\(L\) 的值在训练时（其中较大的 \(L\) 允许更好地覆盖区间 \([0, T]\)，这有助于网络学习一个在 \(t\) 上泛化的关系）和采样时（其中较小的 \(L\) 意味着更高效的采样）通常是不同的。人们甚至可以在采样时自适应地选择时间步，以优化这种权衡 \cite{bao2022analytic}。
\end{remark}

\begin{remark}
	各种其他工作将逆过程定义为使用显式差分方程或在极限 \(L \to \infty\) 下的微分方程在时间索引 \(t\) 上向后移动，或者使用变换 \(\vy_{t} = \vx_{T - t}\) 在时间上向前移动，这样如果 \(t\) 增加，\(\vy_{t}\) 就变得更接近 \(\vx_{0}\)。在这项工作中，我们努力保持一致性：我们向前移动时间来加噪，向后移动时间来去噪。如果你正在阅读另一篇在时间索引上不清楚的著作，或者试图实现一个同样不清楚的算法，有一种方法可以每次都做对：采样过程在从一步到下一步移动时，在去噪器项和当前迭代值上都应该有一个\textit{正}系数。但总的来说，许多论文定义了自己的记法，这对用户不友好。
\end{remark}

\begin{remark}
	上一节 \Cref{sub:intro_diffusion_denoising} 末尾的理论似乎（粗略地说）表明，在实践中，使用类 Transformer 网络是学习或近似去噪器的一个好选择。这是合理的，但是使用任何旧的神经网络（例如多层感知器（MLP））并试图将其无限放大有什么问题呢？为了观察这个问题，让我们看一个在 \Cref{example:denoising_gaussian_mixture} 中研究的高斯混合模型的另一个特例。也就是说，\textit{经验分布}是退化高斯混合模型的一个实例，有 \(K = N\) 个分量 \(\dNorm(\vx_{i}, \vzero)\)，以相等的概率 \(\pi_{i} = \frac{1}{N}\) 采样。在这种情况下，贝叶斯最优去噪器是
	\begin{equation}\label{eq:memorizing_denoiser}
		\bar{\vx}^{\star}(t, \vx_{t}) = \sum_{i = 1}^{N}\frac{e^{-\|\vx_{t} - \alpha_{t}\vx_{i}\|_{2}^{2}/(2\sigma_{t}^{2})}}{\sum_{j = 1}^{N}e^{-\|\vx_{t} - \alpha_{t}\vx_{j}\|_{2}^{2}/(2\sigma_{t}^{2})}}\vx_{i}.
	\end{equation}
	这是数据 \(\vx_{i}\) 的一个凸组合，并且随着 \(t \to 0\)，系数变得“更尖锐”（即更接近 \(0\) 或 \(1\)）。注意，当我们根据从一个固定的有限数据集 \(\vX = \{\vx_{i}\}_{i = 1}^{N}\) 中均匀随机抽取 \(\vx\) 来计算损失时，这个去噪器\textit{最优地解决了}去噪优化问题 \eqref{eq:parametric_denoising_loss}，这是一个非常现实的设置。因此，如果我们的网络架构 \(\bar{\vx}_{\theta}\) 足够有表现力，以至于可以很好地近似上述形式 \eqref{eq:memorizing_denoiser} 的最优去噪器，那么学习到的去噪器可能就会这样做。然后，我们的迭代去噪 \Cref{alg:iterative_denoising} 将精确地从经验分布中采样，重新生成训练数据中的样本，正如 \Cref{thm:diffusion_sampler_convergence} 所证明的。这是一个糟糕的采样器，并不比一个包含所有样本的数据库更有趣，因此理解如何在实践中避免这种情况很重要。关键是提出一个能够很好地近似真实去噪器（例如对应于 \eqref{eq:gmm_lowrank_denoiser} 中的低秩分布）但不能很好地近似 \eqref{eq:memorizing_denoiser} 中的经验贝叶斯去噪器的网络架构。一些工作已经探讨了这条细微的界线，以及为什么使用基于 Transformer 和卷积的网络架构的现代扩散模型可以在不同机制下记忆和泛化 \citep{kamb2024analytic,niedoba2024towards}。

	在较高的层次上，一个记忆所有训练点的去噪器，如 \eqref{eq:memorizing_denoiser} 所示，对应于一个参数化的分布模型，该模型具有绝对最小的编码率，并通过单独编码每个样本来实现这一点。我们将在下一节从信息论的角度讨论这个问题（以及与我们在 \Cref{sub:min_entropy} 末尾的初始期望的看似矛盾之处）。
\end{remark}




% \subsubsection{Denoising, and Learning a Denoiser}

% , say within a function class \(\cF_{t}\) of square-integrable functions from \(\R^{D}\) to \(\R^{D}\), which forms the best possible approximation (on average) of the true \(\vx\), given \(t\) and \(\vx_{t}\):
% \begin{equation}\label{eq:denoising_loss_one_t}
% 	\bar{\vx}^{\ast}(t, \cdot) \in \argmin_{\bar{\vx}(t, \cdot) \in \cF_{t}}\Ex_{\vx, \vx_{t}}\norm{\vx - \bar{\vx}(t, \vx_{t})}_{2}^{2}.
% \end{equation}
% In short, the solution to the denoising problem is a \textit{denoiser}: a map which attempts to remove all the noise (and inverts the scaling) from the noisy random variable \(\vx_{t}\) to get an approximation to \(\vx\). As the notation suggests, we can also attempt to learn a denoiser which \textit{jointly} denoises from all times \(t\) simultaneously. Namely, given a distribution of \(t\)'s, a weighting function \(w \colon [0, T] \to \R_{\geq 0}\), and a function class \(\cF\) of square-integrable functions from \([0, T] \times \R^{D}\) to \(\R^{D}\), we can attempt to learn a denoiser from this function class:
% \begin{equation}\label{eq:denoising_loss_all_t}
% 	\bar{\vx}^{\ast} \in \argmin_{\bar{\vx} \in \cF}\Ex_{t, \vx, \vx_{t}}w_{t}\norm{\vx - \bar{\vx}(t, \vx_{t})}_{2}^{2}.
% \end{equation}
% The function classes \(\cF_{t}\) and \(\cF\) can be variously defined. For instance, when \(\cF_{t}\) or \(\cF\) is the whole set of square-integrable functions, we obtain an analytically tractable solution in the \textit{Bayes optimal denoiser}:
% \begin{equation}\label{eq:optimal_denoiser}
% 	\bar{\vx}^{\ast}(t, \vxi) \doteq \Ex[\vx \mid \vx_{t} = \vxi].
% \end{equation}
% Indeed, this notation is why it is justified to use the notation \(\bar{\vx}\) as the Bayes optimal denoiser computes a conditional expectation (i.e., conditional average or mean).
% \yima{Why not introduce the Tweedie's formula and property here already?... and then describe how to compute it?}


% Of course, in practice it is impossible to actually compute this conditional expectation without perfect knowledge of the distribution of \(\vx\) (and even then it may still be intractable). So in practice we look to the second case where the denoiser is parameterized, say \(\cF = \{f_{\theta} \colon \theta \in \Theta\}\) where \(\Theta\) is the parameter space. Formally, we can train such denoisers using the method in \Cref{alg:learning_denoiser}.

% So the question is, what is a good parametric function family \(\cF\)? For special cases of distributions, such as Gaussians, a functional form containing the optimal denoiser can be explicitly derived.

% \yima{Please change the environment for Example to something similar to proof, with a QED mark so that we know when an example is finished.}
% \begin{example}[Denoising Gaussian Noise from a Gaussian Mixture Model]\label{example:gmm_denoising}
% 	Suppose that \(\vx\) has the following data-generating process:
% 	\begin{itemize}
% 		\item First, an auxiliary (``label'') variable \(y\) is sampled according to the probability vector \(\vpi\), that is, \(y = k \in [K]\) with probability \(\pi_{k}\).
% 		\item Then, \(\vx\) is sampled as \(\vx \sim \dNorm(\vmu_{y}, \vSigma_{y})\).
% 	\end{itemize}
% 	Marginally, \(\vx\) is a Gaussian Mixture Model, namely
% 	\begin{equation}
% 		\vx \sim \sum_{k = 1}^{K}\pi_{k}\dNorm(\vmu_{k}, \vSigma_{k}).
% 	\end{equation}
% 	Then it is easy to see that
% 	\begin{equation}
% 		\vx_{t} \sim \sum_{k = 1}^{K}\pi_{k}\dNorm(\alpha_{t}\vmu_{k}, \alpha_{t}^{2}\vSigma_{k} + \sigma_{t}^{2}\vI).
% 	\end{equation}
% 	Conditioned on the label \(y\), the variables \(\vx\) and \(\vx_{t}\) are jointly Gaussian, i.e.,
% 	\begin{equation}
% 		\mat{\vx \\ \vx_{t}} \mid y \sim \dNorm\rp{\mat{\vmu_{y} \\ \alpha_{t}\vmu_{y}}, \mat{\vSigma_{y} & \alpha_{t}\vSigma_{y} \\ \alpha_{t}\vSigma_{y} & \alpha_{t}^{2}\vSigma_{y} + \sigma_{t}^{2}\vI}},
% 	\end{equation}
% 	and the posterior distribution of \(\vx\) given \(\vx_{t}\) and \(y\) is (famously)
% 	\begin{equation}
% 		\vx \mid (\vx_{t}, y) \sim \dNorm(\vmu_{y} + \alpha_{t}\vSigma_{y}(\alpha_{t}^{2}\vSigma_{y} + \sigma_{t}^{2}\vI)^{-1}(\vx_{t} - \alpha_{t}\vmu_{y}), \vSigma_{y} - \alpha_{t}^{2}\vSigma_{y}(\alpha_{t}^{2}\vSigma_{y} + \sigma_{t}^{2}\vI)^{-1}\vSigma_{y}).
% 	\end{equation}
% 	Then the optimal denoiser (among all square-integrable functions) given the label \(y\) is the conditional expectation:
% 	\begin{equation}\label{eq:gaussian_denoiser}
% 		\Ex[\vx \mid \vx_{t}, y] = \vmu_{y} + \alpha_{t}\vSigma_{y}(\alpha_{t}^{2}\vSigma_{y} + \sigma_{t}^{2}\vI)^{-1}(\vx_{t} - \alpha_{t}\vmu_{y}).
% 	\end{equation}
% 	To find the Bayes optimal denoiser \(\bar{\vx}^{\star}(t, \cdot) = \Ex[\vx \mid \vx_{t}]\), we use iterated expectation:
% 	\begin{align}
% 		\bar{\vx}^{\star}(t, \vx_{t})
% 		 & = \Ex[\Ex[\vx \mid \vx_{t}, y] \mid \vx_{t}]                         \\
% 		 & = \sum_{k = 1}^{K}\Ex[\vx \mid \vx_{t}, y = k]p(y = k \mid \vx_{t}).
% 	\end{align}
% 	Let \(\phi(\vx \mid \vmu, \vSigma)\) be the density of a Gaussian random variable with mean \(\vmu\) and covariance \(\vSigma\) evaluated at \(\vx\). By Bayes' rule we can calculate the latter as:
% 	\begin{align}
% 		p(y = k \mid \vx_{t})
% 		 & = \frac{p(\vx_{t} \mid y = k)p(y = k)}{\sum_{i = 1}^{K}p(\vx_{t} \mid y = i)p(y = i)}                                                                                                                                \\
% 		 & = \frac{\pi_{k} p(\vx_{t} \mid y = k)}{\sum_{i = 1}^{K}\pi_{i} p(\vx_{t} \mid y = i)}                                                                                                                                \\
% 		 & = \frac{\pi_{k} \phi(\vx_{t} \mid \alpha_{t}\vmu_{k}, \alpha_{t}^{2}\vSigma_{k} + \sigma_{t}^{2}\vI)}{\sum_{i = 1}^{K}\pi_{i} \phi(\vx_{t} \mid \alpha_{t}\vmu_{i}, \alpha_{t}^{2}\vSigma_{i} + \sigma_{t}^{2}\vI)}.
% 	\end{align}
% 	Putting this all together, we obtain
% 	\begin{equation}\label{eq:gaussian_mixture_denoiser}
% 		\bar{\vx}^{\star}(t, \vx_{t}) = \sum_{k = 1}^{K}\frac{\pi_{k} \phi(\vx_{t} \mid \alpha_{t}\vmu_{k}, \alpha_{t}^{2}\vSigma_{k} + \sigma_{t}^{2}\vI)}{\sum_{i = 1}^{K}\pi_{i} \phi(\vx_{t} \mid \alpha_{t}\vmu_{i}, \alpha_{t}^{2}\vSigma_{i} + \sigma_{t}^{2}\vI)}\bp{\vmu_{k} + \alpha_{t}\vSigma_{k}(\alpha_{t}^{2}\vSigma_{k} + \sigma_{t}^{2}\vI)^{-1}(\vx_{t} - \alpha_{t}\vmu_{k})}.
% 	\end{equation}
% 	If we want to learn the parameters \(\vpi, \vmu_{k}, \vSigma_{k}\), then the expression above suggests that one possible good parameterization for the denoiser is
% 	\begin{equation}
% 		\bar{\vx}_{(\tilde{\vpi}, \tilde{\vmu}, \tilde{\vSigma})}^{\star}(t, \vx_{t}) = \sum_{k = 1}^{K}\frac{\tilde{\pi}_{k} \phi(\vx_{t} \mid \alpha_{t}\tilde{\vmu}_{k}, \alpha_{t}^{2}\tilde{\vSigma}_{k} + \sigma_{t}^{2}\vI)}{\sum_{i = 1}^{K}\tilde{\pi}_{i} \phi(\vx_{t} \mid \alpha_{t}\tilde{\vmu}_{i}, \alpha_{t}^{2}\tilde{\vSigma}_{i} + \sigma_{t}^{2}\vI)}\bp{\tilde{\vmu}_{k} + \alpha_{t}\tilde{\vSigma}_{k}(\alpha_{t}^{2}\tilde{\vSigma}_{k} + \sigma_{t}^{2}\vI)^{-1}(\vx_{t} - \alpha_{t}\tilde{\vmu}_{k})},
% 	\end{equation}
% 	whereby we can use the function class
% 	\begin{equation}
% 		\cF = \{\bar{\vx}_{(\tilde{\vpi}, \tilde{\vmu}, \tilde{\vSigma})} \colon (\tilde{\vpi}, \tilde{\vmu}, \tilde{\vSigma}) \in \Theta \doteq \Delta_{[K]} \times \R^{K \times D} \times \PSD(D)^{K}\}
% 	\end{equation}
% 	But, the denoising problem under this parameterization is difficult, not least because of the simplex and PSD cone constraints, so we can actually reparameterize this problem and obtain an alternate function class of denoisers:
% 	\begin{equation}
% 		\cF = \{\bar{\vx}_{(\softmax(\tilde{\vw}), \tilde{\vmu}, \tilde{\vU}\tilde{\vU}^{\top})} \colon (\tilde{\vw}, \tilde{\vmu}, \tilde{\vU}) \in \Theta \doteq \R^{K} \times \R^{K \times D} \times \R^{K \times D \times D}\},
% 	\end{equation}
% 	where (by an abuse of notation) we ``broadcast'' \(\tilde{\vU}\tilde{\vU}^{\top} = (\tilde{\vU}_{k}\tilde{\vU}_{k}^{\top})_{k = 1}^{K}\).
% 	The latter parameterization can be plugged into modern end-to-end unconstrained optimization frameworks, and used to find a good ``Gaussian mixture model'' approximation to the training data.

% 	There are several noteworthy special cases of this model, which we will go over slightly later. For now let us get some geometric intuition about how this denoiser works. Suppose for now that there is one component, e.g., if \(K = 1\) and \(\vx \sim \dNorm(\vmu, \vSigma)\), then
% 	\begin{equation}
% 		\bar{\vx}^{\star}(t, \x_{t}) = \vmu + \alpha_{t}\vSigma(\alpha_{t}^{2}\vSigma + \sigma_{t}^{2}\vI)^{-1}(\vx_{t} - \alpha_{t}\vmu).
% 	\end{equation}
% 	To get some good geometric intuition about this model, diagonalize \(\vSigma = \vV\vLambda\vV^{\top}\). Then
% 	\begin{equation}
% 		\bar{\vx}^{\star}(t, \vx_{t}) = \vmu + \vV\mat{\alpha_{t}\lambda_{1}/(\alpha_{t}^{2}\lambda_{1} + \sigma_{t}^{2}) & & \\ & \ddots & \\ & & \alpha_{t}\lambda_{D}/(\alpha_{t}^{2}\lambda_{D} + \sigma_{t}^{2})}\vV^{\top}(\vx_{t} - \alpha_{t}\vmu).
% 	\end{equation}
% 	Namely, we can see from the above functional form that the denoiser contracts the residual \(\vx_{t} - \alpha_{t}\vmu\) in every direction, but most of all in the directions of the smallest eigenvalues of \(\vSigma\). In particular if \(\vSigma\) is low-rank (so that some of its eigenvectors correspond to the eigenvalue \(0\)), then the denoiser instantly contracts the components of the residual in those directions to \(0\). This contraction is the estimate of \(\vx - \vmu\), so after computing it we can add back \(\vmu\) to get an estimate of \(\vx\).

% 	This reveals that the whole Gaussian mixture model denoiser \textit{locally} projects the residual onto the support of each component distribution, but these projections are mixed using the posterior probabilities \(p(y \mid \vx_{t})\). So if the distributions are well-separated then, near the support of each distribution, one can approximate the whole Gaussian mixture model denoiser by a single Gaussian denoiser (and then endow it with the same geometric interpretation as above).
% \end{example}
% In many cases, we do not know the functional form of the optimal denoiser, and so we must approximate it, say via neural networks. Such learned denoisers still constitute approximate (and implicit) models for the data distribution. Later in this Section, we will briefly discuss good neural network architectures for this problem. But for now we will discuss the actual use of these denoisers to reduce entropy by approximately inverting the noising process.

% \subsubsection{Iterative Denoising}

% So, how can we carefully use a denoiser to approximately invert the noising process? The key idea is that in order to invert a \textit{gradual} noising process, we use a \textit{gradual} denoising process: denoise a little bit at a time (a computationally easier problem) and chain the results together, in order to denoise from an extremely noisy distribution (a hard problem). So now how can we denoise a little bit at a time? There are several possible methods for how to do this; the one that we use is particularly elegant and performs well in practice.

% To be concrete, let us fix times \(0 < s < t\). The distribution at time \(t\), i.e., of the random variable \(\vx_{t}\), is noisier than the distribution at time \(s\), i.e., of the random variable \(\vx_{s}\), since the noise level \(\sigma\) is an increasing function. Thus one good way to incrementally denoise from \(\vx_{t}\) is to try to (approximately) recover the distribution of \(\vx_{s}\) from \(\vx_{t}\) using our denoiser \(\hat{\vx}\).

% Now, given \(\vx\), the quantities \(\vx_{s}\) and \(\vx_{t}\) are related as follows:
% \begin{align}
% 	\vx_{s}
% 	 & = \alpha_{s}\vx + \sigma_{s}\vg,                                                                       \\
% 	\vx_{t}
% 	 & = \alpha_{t}\vx + \sigma_{t}\vg                                                                        \\
% 	\implies \vx_{s}
% 	 & = \alpha_{s}\vx + \sigma_{s} \cdot \frac{\vx_{t} - \alpha_{t}\vx}{\sigma_{t}}                          \\
% 	 & = \frac{\sigma_{s}}{\sigma_{t}}\vx_{t} + \bp{\alpha_{s} - \frac{\sigma_{s}}{\sigma_{t}}\alpha_{t}}\vx.
% \end{align}
% Now, if we want to denoise \(\vx_{t}\) to \(\vx_{s}\) given \(\vx_{t}\) and \(\vx\), we can do that exactly using the above equation.

% However, if we are denoising for \(\vx_{s}\) given only the noisy observation \(\vx_{t}\), we do not have access to \(\vx\). Here is where our learned denoiser comes in: we can use it as an estimate of \(\vx\). So we have
% \begin{equation}
% 	\vx_{s} \approx \frac{\sigma_{s}}{\sigma_{t}}\vx_{t} + \bp{\alpha_{s} - \frac{\sigma_{s}}{\sigma_{t}}\alpha_{t}}\bar{\vx}(t, \vx_{t}).
% \end{equation}
% Indeed, in the limit \(s \nearrow t\) and taking \(\bar{\vx}\) as an (approximately) optimal denoiser for the data distribution, this approximation becomes \textit{exact}. Thus, in order to denoise \(\vx_{t}\) all the way down to \(\vx\), we should take \textit{lots of little steps}, say of length \(\delta\), to get from \(\vx_{t} \to \vx_{t - \delta} \to \vx_{t - 2\delta} \to \cdots \to \vx_{0} = \vx\), so that the error of each individual step is very small and each approximation is close to exact.

% This provides an iterative denoising algorithm: given \(\vx_{t}\) and a denoiser \(\bar{\vx}\) obtained from solving \eqref{eq:denoising_loss_all_t}, discretize \([0, t]\) into \(0 = t_{1} < t_{2}< \cdots < t_{L} < t_{L + 1} = t\). Then, initialize \(\tilde{\vx}_{t_{L + 1}} \doteq \vx_{t}\) and run (in reverse, from \(\ell = L\) to \(\ell = 1\)):\footnote{To clarify, \(\vx\) and \(\tilde{\vx}\) are vectors, while \(\bar{\vx}\) is a function (that attempts to approximate \(\vx\)).}
% \begin{equation}
% 	\tilde{\vx}_{t_{\ell}} \doteq \frac{\sigma_{t_{\ell}}}{\sigma_{t_{\ell + 1}}}\tilde{\vx}_{t_{\ell + 1}} + \bp{\alpha_{t_{\ell}} - \frac{\sigma_{t_{\ell}}}{\sigma_{t_{\ell + 1}}}\alpha_{t_{\ell + 1}}}\bar{\vx}(t_{\ell + 1}, \tilde{\vx}_{t_{\ell + 1}}), \qquad \forall \ell \in [L].
% \end{equation}
% The output \(\hat{\vx} \doteq \tilde{\vx}_{t_{1}} = \tilde{\vx}_{0}\) is a good approximation for \(\vx\), and so we have successfully (approximately) inverted the noising process that produced \(\vx_{t}\)!

% \subsubsection{From Iterative Denoising to Sampling and Diffusion Models}

% We are able to iteratively denoise \(\vx_{t}\) to (approximately) \(\vx\). But this is not quite enough, yet, to accomplish our goals for the section as laid out at the end of \Cref{sub:min_entropy}. The more salient such goal is to start with a high-entropy distribution which is \textit{pure noise} and gradually denoise (i.e., lower the entropy) until it reaches the data distribution. So how do we do this with what we have so far?

% The key is to set up our noising process (i.e., set up \(T, \alpha, \sigma\)) such that \(\vx_{T}\) is completely noise. Then we can use the tools we have built so far in order to denoise this initial noise into a sample from the data distribution.

% This procedure is at the heart of so-called \textit{diffusion models}. There are many types of diffusion models, but three that fit elegantly in our framework are:
% \begin{itemize}
% 	\item \textit{Variance-exploding} (VE) diffusion models, which correspond to noising processes with \(T\) large and
% 	      \begin{equation}
% 		      \alpha_{t} = 1, \qquad \sigma_{t} = t, \qquad \forall t \in [0, T].
% 	      \end{equation}
% 	      Recall that we studied the \(\alpha = 1\) case at the beginning of the section, and it has the most concrete ties to entropy increase and decrease. Also note that \(\vx_{t} / t = \vx/t + \vg \to \dNorm(\vzero, \vI)\) in distribution as \(t \to \infty\), so we can approximate \(\vx_{T} \approx \dNorm(\vzero, T^{2}\vI)\).
% 	\item \textit{Variance-preserving} (VP) diffusion models, which correspond to noising processes with \(T = 1\) and
% 	      \begin{equation}
% 		      \alpha_{t} = \sqrt{1 - t^{2}}, \qquad \sigma_{t} = t, \qquad  \forall t \in [0, 1].
% 	      \end{equation}
% 	      Notice that there exist several different time-parameterizations, for example \(\alpha_{t} = \cos(\pi t)\) and \(\sigma_{t} = \sin(\pi t)\) for \(t \in [0, 1]\) or \(\alpha_{t} = \sqrt{1 - e^{-2t}}\) and \(\sigma_{t} = e^{-t}\) for \(t \in [0, \infty)\). In this case, we have the exact equality in distribution \(\vx_{T} = \dNorm(\vzero, \vI)\).
% 	\item \textit{Flow matching} (FM) diffusion models, which correspond to noising processes with \(T = 1\) and
% 	      \begin{equation}
% 		      \alpha_{t} = 1 - t, \qquad \sigma_{t} = t, \qquad  \forall t \in [0, 1].
% 	      \end{equation}
% 	      This means that \(\vx_{t}\) is a \textit{convex combination} of \(\vx\) and \(\vg\). We also have in this case that \(\vx_{T} = \dNorm(\vzero, \vI)\).
% \end{itemize}

% To specify a diffusion model we also need to fix a sampling method and a discretization of \([0, T]\) into \(0 = t_{1} < t_{2} < \cdots < t_{L} < t_{L + 1} = T\). We can use many sampling methods, but here we choose the one previously outlined in this section; it is popularly called the \textit{DDIM sampler} (where DDIM stands for ``Denoising Diffusion Implicit Model''). For temporal discretization, we can choose from many possible schemes,\footnote{Although one usually trains the denoiser on exactly the timesteps used for sampling, there is evidence that the timesteps can be chosen adaptively during sampling to obtain more precise results; we do not explore it here but a reference is \cite{bao2022analytic}.} but in practice either a uniform discretization or exponential decay discretization suffices:
% \begin{equation}
% 	t_{\ell} = \frac{\ell - 1}{L}\cdot T \qquad \text{or} \qquad t_{\ell} = C_{1}(e^{C_{2}\ell} - 1), \qquad \forall \ell \in [L + 1].
% \end{equation}
% These all lead to the following diffusion model sampling algorithm.

% Does this algorithm actually work? To answer this question, we furnish a convergence bound from the literature which shows that the output distribution of the sampler converges in the so-called \textit{total variation (TV) distance} to the true distribution, defined between two random variables \(\vx\) and \(\vy\) as:

% \begin{figure}[h]
% 	\centering
% 	\includegraphics[width=\textwidth]{\toplevelprefix/chapters/chapter3/figs/gaussian_mixture_denoising.png}
% 	\caption{\textbf{Denoising to sample from a mixture of low-rank Gaussians.} \textit{Top left:} We start with three Gaussians, colored gray, orange, and red, supported on a \(2\)-d subspace and two \(1\)-d subspaces respectively in \(\R^{3}\). \textit{Top middle:} We begin the (variance-preserving) denoising process \eqref{eq:denoising_ve_iter} by sampling some noise from \(\dNorm(\vzero, \vI)\). \textit{Top right:} After \(\ell = 200\) iterations, the current iterate \(\tilde{\vx}_{t_{\ell}}\)  starts to develop some structure aligned with the true distribution. \textit{Bottom left, bottom middle:} The same figure with some more denoising iterations, showing that the sampled data structure increases further, aligning more with the true data distribution. \textit{Bottom right:} The output of the sampling procedure, which is distributed very similarly to the original dataset. \yima{Can we make distributions in these plots larger?}}
% 	\label{fig:denoising-twolines-oneplane}
% \end{figure}



% \subsubsection{Parameterizing the Denoiser}

% The sampling error in \Cref{eq:diffusion_sampling_error} is proportional to the average error of the denoiser used for sampling. Usually in practice (for example, when sampling images) we do not know how to specify the denoiser to achieve the minimum possible error. Thus, we parameterize it by using neural networks. Here, we will briefly talk about what our existing theory tells us is a reasonable architecture for such neural networks. For this, we will examine \textit{two} special cases of the Gaussian mixture model denoiser discussed in \Cref{example:gmm_denoising}, since any distribution (of high or low dimension) can be well-approximated by a Gaussian mixture model.

% First, let us attempt to model high-dimensional data with low-dimensional structure by considering \(\vx\) as distributed as an equiprobable mixture of  \(K\) \textit{low-rank} Gaussians, i.e., \(\vx \sim \frac{1}{K}\sum_{k = 1}^{K}\dNorm(\vzero, \vU_{k}\vU_{k}^{\top})\) where \(\vU_{k} \in \R^{D \times P}\) where \(P \ll D\). In particular, suppose that \(\vU_{k} \in \O(D, P)\) is orthogonal, i.e., \(\vU_{k}^{\top}\vU_{k} = \vI\). In this case, the (Bayes) optimal denoiser is
% \begin{equation}
% 	\bar{\vx}^{\ast}(t, \vx_{t}) = \sum_{k = 1}^{K}\frac{\phi(\vx_{t} \mid \vzero, \alpha_{t}^{2}\vU_{k}\vU_{k}^{\top} + \sigma_{t}^{2}\vI)}{\sum_{i = 1}^{K}\phi(\vx_{t} \mid \vzero, \alpha_{t}^{2}\vU_{i}\vU_{i}^{\top} + \sigma_{t}^{2}\vI)}\bp{\alpha_{t}\vU_{k}\vU_{k}^{\top}(\alpha_{t}^{2}\vU_{k}\vU_{k}^{\top} + \sigma_{t}^{2}\vI)^{-1}\vx_{t}}.
% \end{equation}
% To simplify the expression, we can use the Sherman-Morrison-Woodbury identity, namely
% \begin{equation}
% 	(\vA + \vU\vC\vV)^{-1} = \vA^{-1} - \vA^{-1}\vU(\vC^{-1} + \vV\vA^{-1}\vU)^{-1}\vV\vA^{-1}
% \end{equation}
% (for all matrices where the given inverses are well-defined), with \(\vA = \sigma_{t}^{2}\vI\), \(\vU = \vU_{k}\), \(\vV = \vU_{k}^{\top}\), and \(\vC = \alpha_{t}^{2}\vI\), and we obtain
% \begin{align}
% 	(\alpha_{t}^{2}\vU_{k}\vU_{k}^{\top} + \sigma_{t}^{2}\vI)^{-1}
% 	 & = \bp{\frac{1}{\sigma_{t}^{2}}\vI - \frac{1}{\sigma_{t}^{4}}\vU_{k}\bp{\frac{1}{\alpha_{t}^{2}}\vI + \frac{1}{\sigma_{t}^{2}}\vU_{k}^{\top}\vU_{k}}^{-1}\vU_{k}^{\top}} \\
% 	 & = \bp{\frac{1}{\sigma_{t}^{2}}\vI - \frac{1}{\sigma_{t}^{4}}\bp{\frac{1}{\alpha_{t}^{2}} + \frac{1}{\sigma_{t}^{2}}}^{-1}\vU_{k}\vU_{k}^{\top}}                         \\
% 	 & = \frac{1}{\sigma_{t}^{2}}\bp{\vI - \frac{1}{\sigma_{t}^{2}(\frac{1}{\alpha_{t}^{2}} + \frac{1}{\sigma_{t}^{2}})}\vU_{k}\vU_{k}^{\top}}                                 \\
% 	 & = \frac{1}{\sigma_{t}^{2}}\bp{\vI - \frac{1}{1 + (\sigma_{t}/\alpha_{t})^{2}}\vU_{k}\vU_{k}^{\top}}.
% \end{align}
% Since the \(\vU_{k}\)'s are all orthogonal, \(\det(\alpha_{t}^{2}\vU_{k}\vU_{k}^{\top} + \sigma_{t}^{2}\vI)\) are all the same, so
% \begin{align}
% 	\frac{\phi(\vx_{t} \mid \vzero, \alpha_{t}^{2}\vU_{k}\vU_{k}^{\top} + \sigma_{t}^{2}\vI)}{\sum_{i = 1}^{K}\phi(\vx_{t} \mid \vzero, \alpha_{t}^{2}\vU_{i}\vU_{i}^{\top} + \sigma_{t}^{2}\vI)}
% 	 & = \frac{\exp\rp{-\vx_{t}^{\top}(\vI - \frac{1}{1 + (\sigma_{t}/\alpha_{t})^{2}}\vU_{k}\vU_{k}^{\top})\vx_{t}/(2\sigma_{t}^{2})}}{\sum_{i = 1}^{K}\exp\rp{-\vx_{t}^{\top}(\vI - \frac{1}{1 + (\sigma_{t}/\alpha_{t})^{2}}\vU_{i}\vU_{i}^{\top})\vx_{t}/(2\sigma_{t}^{2})}}                                                                                   \\
% 	 & = \frac{\exp\rp{-\frac{1}{2\sigma_{t}^{2}}\vx_{t}^{\top}\vx_{t} + \frac{1}{2\sigma_{t}^{2}[1 + (\sigma_{t}/\alpha_{t})^{2}]}\vx_{t}^{\top}\vU_{k}\vU_{k}^{\top}\vx_{t}}}{\sum_{i = 1}^{K}\exp\rp{-\frac{1}{2\sigma_{t}^{2}}\vx_{t}^{\top}\vx_{t} + \frac{1}{2\sigma_{t}^{2}[1 + (\sigma_{t}/\alpha_{t})^{2}]}\vx_{t}^{\top}\vU_{i}\vU_{i}^{\top}\vx_{t}}}   \\
% 	 & = \frac{\exp\rp{-\frac{1}{2\sigma_{t}^{2}}\norm{\vx_{t}}_{2}^{2} + \frac{1}{2\sigma_{t}^{2}[1 + (\sigma_{t}/\alpha_{t})^{2}]}\norm{\vU_{k}^{\top}\vx_{t}}_{2}^{2}}}{\sum_{i = 1}^{K}\exp\rp{-\frac{1}{2\sigma_{t}^{2}}\norm{\vx_{t}}_{2}^{2} + \frac{1}{2\sigma_{t}^{2}[1 + (\sigma_{t}/\alpha_{t})^{2}]}\norm{\vU_{i}^{\top}\vx_{t}}_{2}^{2}}}             \\
% 	 & = \frac{\exp\rp{-\frac{1}{2\sigma_{t}^{2}}\norm{\vx_{t}}_{2}^{2}}\exp\rp{\frac{1}{2\sigma_{t}^{2}[1 + (\sigma_{t}/\alpha_{t})^{2}]}\norm{\vU_{k}^{\top}\vx_{t}}_{2}^{2}}}{\sum_{i = 1}^{K}\exp\rp{-\frac{1}{2\sigma_{t}^{2}}\norm{\vx_{t}}_{2}^{2}}\exp\rp{\frac{1}{2\sigma_{t}^{2}[1 + (\sigma_{t}/\alpha_{t})^{2}]}\norm{\vU_{i}^{\top}\vx_{t}}_{2}^{2}}} \\
% 	 & = \frac{\exp\rp{\frac{1}{2\sigma_{t}^{2}[1 + (\sigma_{t}/\alpha_{t})^{2}]}\norm{\vU_{k}^{\top}\vx_{t}}_{2}^{2}}}{\sum_{i = 1}^{K}\exp\rp{\frac{1}{2\sigma_{t}^{2}[1 + (\sigma_{t}/\alpha_{t})^{2}]}\norm{\vU_{i}^{\top}\vx_{t}}_{2}^{2}}}.
% \end{align}
% Also, each component denoiser becomes
% \begin{align}
% 	\alpha_{t}\vU_{k}\vU_{k}^{\top}(\alpha_{t}^{2}\vU_{k}\vU_{k}^{\top}(\alpha_{t}^{2}\vU_{k}\vU_{k}^{\top} + \sigma_{t}^{2}\vI)^{-1}\vx_{t})
% 	 & = \alpha_{t}\vU_{k}\vU_{k}^{\top}\cdot\frac{1}{\sigma_{t}^{2}}\bp{\vI - \frac{1}{1 + (\sigma_{t}/\alpha_{t})^{2}}\vU_{k}\vU_{k}^{\top}}\vx_{t} \\
% 	 & = \frac{\alpha_{t}}{\sigma_{t}^{2}}\bp{1 - \frac{1}{1 + (\sigma_{t}/\alpha_{t})^{2}}}\vU_{k}\vU_{k}^{\top}\vx_{t}                              \\
% 	 & = \frac{\alpha_{t}}{\sigma_{t}^{2}}\cdot\frac{(\sigma_{t}/\alpha_{t})^{2}}{1 + (\sigma_{t}/\alpha_{t})^{2}}\cdot \vU_{k}\vU_{k}^{\top}\vx_{t}  \\
% 	 & = \frac{1}{\alpha_{t}[1 + (\sigma_{t}/\alpha_{t})^{2}]}\vU_{k}\vU_{k}^{\top}\vx_{t}.
% \end{align}
% Putting this together,
% \begin{equation}\label{eq:lowrank_gmm_denoiser}
% 	\bar{\vx}^{\ast}(t, \vx_{t}) = \frac{1}{\alpha_{t}[1 + (\sigma_{t}/\alpha_{t})^{2}]}\sum_{k = 1}^{K}\frac{\exp\rp{\frac{1}{2\sigma_{t}^{2}[1 + (\sigma_{t}/\alpha_{t})^{2}]}\norm{\vU_{k}^{\top}\vx_{t}}_{2}^{2}}}{\sum_{i = 1}^{K}\exp\rp{\frac{1}{2\sigma_{t}^{2}[1 + (\sigma_{t}/\alpha_{t})^{2}]}\norm{\vU_{i}^{\top}\vx_{t}}_{2}^{2}}}\vU_{k}\vU_{k}^{\top}\vx_{t}.
% \end{equation}

% This is a softmax which weights each component denoiser based on the projection magnitude of \(\vx_{t}\) onto \(\vU_{k}\), similar to the attention mechanism in transformers. As we will see in \Cref{ch:representation}, this resemblance is not a mere coincidence: denoising against mixtures of Gaussians is very closely related to compression mechanisms and ultimately the attention mechanism in transformer networks.

% So, this theory seems to suggest (loosely speaking) that in practice, using a transformer-like network is reasonable for denoising. This is reasonable, but what is the problem with using any old neural network (such as a multi-layer perceptron (MLP)) and just trying to scale it up to infinity? To observe the problem with this, let us look at another special case of the Gaussian mixture model. Namely, the \textit{empirical distribution} is also an instance of a degenerate Gaussian mixture model, with \(K = N\) components \(\dNorm(\vx_{i}, \vzero)\) sampled with equal probability \(\pi_{i} = \frac{1}{N}\). In this case the Bayes optimal denoiser is
% \begin{equation}\label{eq:memorizing_denoiser}
% 	\bar{\vx}^{\star}(t, \vx_{t}) = \sum_{i = 1}^{N}\frac{e^{-\|\vx_{t} - \alpha_{t}\vx_{i}\|_{2}^{2}/(2\sigma_{t}^{2})}}{\sum_{j = 1}^{N}e^{-\|\vx_{t} - \alpha_{t}\vx_{j}\|_{2}^{2}/(2\sigma_{t}^{2})}}\vx_{i}.
% \end{equation}
% This is a convex combination of the data \(\vx_{i}\), and the coefficients get ``sharper'' (i.e., closer to \(0\) or \(1\)) as \(t \to 0\). Notice that this denoiser \textit{optimally solves} the denoising optimization problem \eqref{eq:denoising_loss_all_t} when we compute the loss based on drawing \(\vx\) uniformly at random from a fixed finite dataset \(\vX = \{\vx_{i}\}_{i = 1}^{N}\). This is the realistic setting. Thus, if the function family \(\cF\) we use in practice is large enough such that optimal denoisers of the above form \eqref{eq:memorizing_denoiser} may be approximated, then the learned denoiser may do just that. Then, our iterative denoising algorithm will sample exactly from the empirical distribution, re-generating samples in the training data. This is a bad sampler, not really more interesting than a database of all samples, and so it is important to understand how to avoid this in practice. The key is to come up with a function family which can well-approximate the true denoiser (say corresponding to a low-rank distribution as in \eqref{eq:lowrank_gmm_denoiser}) but not the empirical Bayesian denoiser as in \eqref{eq:memorizing_denoiser}. Some work has explored this fine line and why modern diffusion models, which use transformer- and convolutional-based network architectures, can memorize and generalize in different regimes \citep{kamb2024analytic,niedoba2024towards}.

% At a high level, a denoiser which memorizes all the training points, as in \eqref{eq:memorizing_denoiser}, corresponds to a parametric model of the distribution which has absolutely minimal coding rate, and achieves this by just coding every sample separately. We will discuss this problem (and seeming paradox with our initial desiderata at the end of \Cref{sub:min_entropy}) from the perspective of information theory in the next Section.

% \subsubsection{Learning a Denoiser vs.~Learning a Density}

% So far we have talked about learning the distribution by training a denoiser. This uses a particular definition of ``learning''. However, in the literature, it is often the case that ``learning the distribution'' corresponds to learning the probability measure or density of the data distribution. We now discuss a direct connection between our denoising formulation and a way to learn the distribution in this density-centric paradigm. The connection follows from \textit{Tweedie's formula} \cite{Robbins1956AnEB}, which we repeat below.

% \begin{theorem}[Tweedie's Formula]\label{thm:tweedie}
% 	Let the notation used in \eqref{eq:gen_additive_gaussian_noise_model} prevail. Then
% 	\begin{equation}
% 		\Ex[\vx \mid \vx_{t}] = \frac{1}{\alpha_{t}}\bp{\vx_{t} + \sigma_{t}^{2}\nabla \log p_{t}(\vx_{t})},
% 	\end{equation}
% 	where \(p_{t}\) is the density of \(\vx_{t}\).
% \end{theorem}
% \begin{proof}
% 	For the sake of this proof, suppose that \(\vx\) has probability density function \(p\) (though the result is true even in the case \(\vx\) does not have a density). Then, note that \(p_{t}\) can be factored into
% 	\begin{equation}
% 		p_{t}(\vx_{t}) = \int_{\R^{D}}p(\vx)p_{t \mid 0}(\vx_{t} \mid \vx)\odif{\vx},
% 	\end{equation}
% 	where \(p_{t \mid 0}\) is the probability density of \(\vx_{t}\) given an observation of \(\vx = \vx_{0}\). Define \(p_{0 \mid t}\) correspondingly. Now, we know from the noise model that
% 	\begin{equation}
% 		p_{t \mid 0}(\vx_{t} \mid \vx) = \phi(\vx_{t} \mid \alpha_{t}\vx, \sigma_{t}^{2}\vI),
% 	\end{equation}
% 	where \(\phi(\vmu, \vSigma)\) is the density of the normal distribution with mean \(\vmu\) and covariance \(\vSigma\). Then
% 	\begin{align}
% 		\nabla \log p_{t}(\vx_{t})
% 		 & = \frac{1}{p_{t}(\vx_{t})}\nabla p_{t}(\vx_{t})                                                                                                                                                      \\
% 		 & = \frac{1}{p_{t}(\vx_{t})}\nabla_{\vx_{t}}\int_{\R^{D}}p(\vx)\phi(\vx_{t} \mid \alpha_{t}\vx, \sigma_{t}^{2}\vI)\odif{\vx}                                                                           \\
% 		 & = \frac{1}{p_{t}(\vx_{t})}\int_{\R^{D}}p(\vx)\bs{\nabla_{\vxi}\phi(\vx_{t} \mid \alpha_{t}\vx, \sigma_{t}^{2}\vI)}\odif{\vx}                                                                         \\
% 		 & = \frac{1}{p_{t}(\vx_{t})}\int_{\R^{D}}p(\vx)\bs{\phi(\vx_{t} \mid \alpha_{t}\vx, \sigma_{t}^{2}\vI)\cdot \frac{\alpha_{t}\vx - \vx_{t}}{\sigma_{t}^{2}}}\odif{\vx}                                  \\
% 		 & = \frac{1}{\sigma_{t}^{2}p_{t}(\vx_{t})}\int_{\R^{D}}p(\vx)\phi(\vx_{t} \mid \alpha_{t}\vx, \sigma_{t}^{2}\vI)[\alpha_{t}\vx - \vx_{t}]\odif{\vx}                                                    \\
% 		 & = \frac{\alpha_{t}}{\sigma_{t}^{2}p_{t}(\vx_{t})}\int_{\R^{D}}p(\vx)\phi(\vx_{t} \mid \alpha_{t}\vx, \sigma_{t}^{2}\vI)\vx\odif{\vx}                                                                 \\
% 		 & \qquad - \frac{1}{\sigma_{t}^{2}p_{t}(\vx_{t})}\vx_{t}\int_{\R^{D}}p(\vx)\phi(\vx_{t} \mid \alpha_{t}\vx, \sigma_{t}^{2}\vI)\odif{\vx}                                                               \\
% 		 & = \frac{\alpha_{t}}{\sigma_{t}^{2}p_{t}(\vx_{t})}\int_{\R^{D}}p(\vx)\phi(\vx_{t} \mid \alpha_{t}\vx, \sigma_{t}^{2}\vI)\vx\odif{\vx} - \frac{1}{\sigma_{t}^{2}p_{t}(\vx_{t})}\vx_{t} p_{t}(\vx_{t}), \\
% 		 & = \frac{\alpha_{t}}{\sigma_{t}^{2}p_{t}(\vx_{t})}\int_{\R^{D}}p(\vx)\phi(\vx_{t} \mid \alpha_{t}\vx, \sigma_{t}^{2}\vI)\vx\odif{\vx} - \frac{1}{\sigma_{t}^{2}}\vx_{t}.
% 	\end{align}
% 	Let us consider the remaining integral. Remember that \(\phi(\vx_{t} \mid \alpha_{t}\vx, \sigma_{t}^{2}\vI) = p_{t \mid 0}(\vx_{t} \mid \vx)\). By Bayes' theorem, we write
% 	\begin{equation}
% 		p(\vx)\phi(\vx_{t} \mid \alpha_{t}\vx, \sigma_{t}^{2}\vI) = p(\vx)p_{t \mid 0}(\vx_{t} \mid \vx) = p_{t}(\vx_{t})p_{0 \mid t}(\vx \mid \vx_{t}),
% 	\end{equation}
% 	which allows us to rewrite the integrand in more favorable terms, overall obtaining
% 	\begin{align}
% 		\nabla \log p_{t}(\vx_{t})
% 		 & = \frac{\alpha_{t}}{\sigma_{t}^{2}p_{t}(\vx_{t})}\int_{\R^{D}}p(\vx)\phi(\vx_{t} \mid \alpha_{t}\vx, \sigma_{t}^{2}\vI)\vx\odif{\vx} - \frac{1}{\sigma_{t}^{2}}\vx_{t} \\
% 		 & = \frac{\alpha_{t}}{\sigma_{t}^{2}p_{t}(\vx_{t})}\int_{\R^{D}}p_{t}(\vx_{t})p_{0 \mid t}(\vx \mid \vx_{t})\vx\odif{\vx} - \frac{1}{\sigma_{t}^{2}}\vx_{t}              \\
% 		 & = \frac{\alpha_{t}}{\sigma_{t}^{2}}\int_{\R^{D}}p_{0 \mid t}(\vx \mid \vx_{t})\vx\odif{\vx} - \frac{1}{\sigma_{t}^{2}}\vx_{t}                                          \\
% 		 & = \frac{\alpha_{t}}{\sigma_{t}^{2}}\Ex[\vx \mid \vx_{t}] - \frac{1}{\sigma_{t}^{2}}\vx_{t}.
% 	\end{align}
% 	Rearranging proves the theorem.
% \end{proof}

% This result forms a connection between estimating the conditional expectation \(\Ex[\vx \mid \vx_{t}]\), i.e., learning a denoiser \(\bar{\vx}(t, \cdot)\), and estimating the (gradients of the log-density of) the data distribution \(\nabla \log p_{t}\), also known as the \textit{(Hyv{\"a}rinen) score \cite{hyvarinen05a}}. We already have a recipe for estimating \(\Ex[\vx \mid \vx_{t}]\) (i.e., \Cref{alg:learning_denoiser}), and we can thus convert this estimate into an estimate for \(\nabla \log p_{t}\). As we have seen in \Cref{ch:intro}, \Cref{fig:score-function} illustrates the geometric intuition of the score in the case of a 1D distribution. We have already discussed how to use estimates for \(\Ex[\vx \mid \vx_{t}]\) to iteratively transform high-entropy distributions towards the (low-dimensional) data distribution; we can do the same with \(\nabla \log p_{t}\) using Tweedie's formula in \Cref{thm:tweedie}. In this sense, by learning a denoiser using a parametric function family \(\cF = \{\bar{\vx}_{\theta} \colon \theta \in \Theta\}\), we learn a parametric encoding of the distribution\, as desired in accordance with the stated goals in \Cref{sub:min_entropy}.

% \subsubsection{Recap}

% Let us recap what we have covered so far. We have discussed how to fit a \textit{denoiser} \(\bar{\vx}\) from a parametric function class \(\cF\) using finite samples. We have shown that this denoiser encodes a distribution in two ways: first, we can use it to gradually transform a pure noise (high-entropy) distribution towards the learned distribution via \textit{iterative denoising}, and second, it is equivalent to the target distribution's (gradient of log) density via Tweedie's formula (\Cref{thm:tweedie}). This iterative denoising process is thus a realization of \textit{both} ways of learning or pursuing a distribution laid out at the end of \Cref{sub:min_entropy}.

% Nevertheless, in this methodology the encoding of the distribution is more or less implicit. In the sequel, we discuss a framework for making our model of the distribution more explicit and computationally accessible.


% \begin{exercise}\label{exercise:noise_prediction}
% 	In practice, a common technique is to not predict \(\vx\) but rather predict \(\vg\) --- this is fittingly called \textit{noise prediction}. Analogous results and algorithms can be derived for noise prediction. In this exercise, please derive a training procedure for noise prediction, a sampling procedure which uses noise prediction, find the error rate in \Cref{thm:diffusion_sampler_convergence} in terms of noise prediction, and modify \Cref{thm:tweedie} to express \(\Ex[\vg \mid \vx_{t}]\) in terms of \(\nabla \log p_{t}\).

% 	\textit{PS:} Another technique is to predict \(\odv*{\vx_{t}}{t}\) instead of \(\vx\) --- this is called \textit{velocity prediction} or \textit{\(v\)-prediction}. Surprisingly, each of velocity prediction, noise prediction, and denoising are completely equivalent tasks under the additive noise model \eqref{eq:gen_additive_gaussian_noise_model}. You can repeat the same exercise above for \(v\)-prediction.
% \end{exercise}

% \begin{example}\label{exercise:}
% 	Reproduce \Cref{fig:denoising-twolines-oneplane}.

% 	\textit{Hint:} You may want to set \(t_{L + 1} < 1\), so that \(\sigma_{t_{L + 1}} \neq 0\) (avoiding divide-by-\(0\) issues).
% \end{example}


% Suppose that \(\vz_{\sigma}\) is a perturbation of \(\vz_{o}\) by additive Gaussian noise, i.e.,
% \begin{equation}\label{eq:additive_gaussian_noise_model}
%     \vz_{\sigma} = \vz_{o} + \sigma \vg, \qquad \vg \sim \dNorm(\vzero, \vI).
% \end{equation}
% The following theorem shows that, by adding Gaussian noise, the differential entropy of the
% resulting distribution indeed increases. \sdb{TODO: hammer transitions a bit.
% the resolution to the negative infinity diffent paradox is the lossy coding/rate
% distortion scheme later... think about the transition. also cf the modern
% practice of VQ-VAE. for this part, it's okay if this is the ``how to do it in
% the very clean setting'' (practical later via discretization), as long as the
% transition/context is good}
% \begin{theorem}[Diffusion and Entropy]\label{thm:diffusion-and-entropy}
%     Suppose that \(\vz_{o} \in \R^{d}\) is a random variable with a well-defined
%     differential entropy $h(\vz_o)$, %
%     %with \(\Ex\norm{\vz_{o}}_{2}^{2} < \infty\),
%     \(\vg \sim \dNorm(\vzero, \vI)\) is a standard Gaussian random variable which is independent of \(\vz_{o}\), \(\sigma > 0\) is a positive real number, and \(\vz_{\sigma} = \vz_{o} + \sigma \vg\). Then \(h(\vz_{\sigma}) > h(\vz_{o})\).
% \end{theorem}
% The justification is concise but requires somewhat sophisticated mathematics that is beyond the scope of this book. We have left the details in Appendix \ref{app:diffusion-denoising}.
% The proof demonstrates a rigorous way to establish, by a limiting argument, that
% $h(\vz_o) = -\infty$ for large classes of degenerate distributions, as we
% claimed earlier, and therefore the same increase of entropy asserted in
% \Cref{thm:diffusion-and-entropy} applies to this important class of
% distributions.
% % \yima{probably state as a theorem or proposition.}

% The inverse operation to removing isotropic Gaussian noise is known as \textit{denoising}. It is a classical and well-studied topic in signal processing and system theory, such as the Wiener filter and the Kalman filter. The several problems discussed in Chapter \ref{ch:classic} such as PCA, ICA, and Dictionary Learning, are specific instances of the denoising problem. Specifically, the denoising problem can be formulated as attempting to learn a function \(\vmu_{\sigma} \colon \R^{d} \to \R^{d}\) which solves the maximum likelihood estimation problem to estimate \(\vz_{o}\) from \(\vmu_{\sigma}\):
% \begin{equation}\label{eq:denoiser_maximum_likelihood}
%     \vmu_{\sigma} \in \argmax_{\substack{f \colon \R^{d} \to \R^{d},\
%     %\\%f\ \text{measurable} \\
%     \Ex\norm{f(\vz_{\sigma})}_{2}^{2} < \infty}}\Ex \log p_{\sigma \mid o}(f(\vz_{\sigma})),
% \end{equation}
% In particular, the denoiser \(\vmu_{\sigma}\) finds the value of the true data which makes the noisy observations most likely. It is easy to show that finding the function \(\vmu_{\sigma}\), with the Gaussian noise model \eqref{eq:additive_gaussian_noise_model}, is equivalent to solving the problem
% %\footnote{If you aren't sure what \textit{measurable} means, don't worry about it; it's merely a technical condition, which holds for any function you can write down. Its exact definition can be looked up in any graduate-level real analysis or probability textbook.}
% \begin{equation}\label{eq:denoiser_conditional_expectation}
%     \vmu_{\sigma} \in \argmin_{\substack{f \colon \R^{d} \to \R^{d},\ %\\% \text{\(f\) measurable} \\
%     \Ex\norm{f(\vz_{\sigma})}_{2}^{2} < \infty}}\Ex\norm{\vz_{o} - f(\vz_{\sigma})}_{2}^{2}.
% \end{equation}
% Namely, for some function class \(\cF\) containing some potential denoising functions, say a family of neural networks with the same architecture and different weights, we can solve the denoising problem
% \begin{equation}\label{eq:denoiser_learning_problem}
%     f^{\ast} \in \argmin_{f \in \cF}\Ex\norm{\vz_{o} - f(\vz_{\sigma})}_{2}^{2}.
% \end{equation}
% % It turns out that solving the minimization problem in \eqref{eq:denoiser_conditional_expectation} is \textit{much} more computationally tractable than solving the maximum likelihood problem \eqref{eq:denoiser_maximum_likelihood}.
% To solve this problem, we use the following algorithm outlined \Cref{alg:denoiser}.
% \begin{algorithm}
%     \caption{Learning a Denoiser}
%         \label{alg:denoiser}
%     \begin{algorithmic}[1]
%         \Require{An optimization algorithm, a parametric function class \(\cF = \{f_{\vtheta} \colon \vtheta \in \vTheta\}\), an initial parameter \(\vtheta^{1}\), a noise level \(\sigma > 0\), and data \(\vz_{o}^{1}, \dots, \vz_{o}^{N} \simiid \vz_{o}\).}
%         \Ensure{A denoiser \(f_{\vtheta^{\ast}}\).}

%         \Procedure{TrainDenoiser}{\texttt{OptUpdate}, $\cF$, $\vtheta^{1}$, $\sigma$, $\{\vz_{o}^{i}\}_{i = 1}^{N}$}
%         \For{each iteration \(j \in [M]\)}
%             \For{each sample \(i \in [N]\)}
%                 \State{\(\vg^{i, j} \gets \) sample from \(\dNorm(\vzero, \vI)\)} \Comment{\(\vg^{i, j}\) is independent of everything}
%                 \State{\(\vz_{\sigma}^{ij} \gets \displaystyle \vz_{o}^{i} + \sigma \vg^{i, j}\)}
%             \EndFor
%             \State{\(\ell^{j} \gets \displaystyle\frac{1}{N}\sum_{i = 1}^{N}\norm{\vz_{o}^{i} - f_{\vtheta^{j}}(\vz_{\sigma, j}^{i})}_{2}^{2}\)} \Comment{\(\ell^{j}\) is estimate of ``true'' denoising loss}
%             \State{\(\vtheta^{j + 1} \gets \displaystyle \texttt{OptUpdate}(\vtheta^{j}; \ell^{j})\)} \Comment{Update \(\vtheta^{j + 1}\) via optimization, i.e., \(\vtheta^{j + 1} \gets \vtheta^{j} - \alpha \nabla_{\vtheta}\ell^{j}\) for SGD.}
%             \EndFor
%         \State{\Return{\(f_{\vtheta^{M+1}}\)}}
%         \EndProcedure
%     \end{algorithmic}
% \end{algorithm}

% The only thing that needs to be specified in the algorithm is the form of the function class $\mathcal{F}$. For special classes of distributions, such as Gaussian, the function form can be explicitly derived.

% \begin{example}[Denoising Gaussian noise from a Gaussian]\label{example:gaussian_denoising}
%     Consider the case where \(\vz_{o}\) is a Gaussian, say \(\vz_{o} \sim \dNorm(\vu, \vSigma)\). Then \(\vz_{\sigma} \sim \dNorm(\vu, \vSigma + \sigma^{2}\vI)\) where \(\vSigma\) is nonsingular. Then \(\vz_{o}\) and \(\vz_{\sigma}\) are jointly Gaussian, i.e.,
%     \begin{equation}
%         \mat{\vz_{o} \\ \vz_{\sigma}} \sim \dNorm\rp{\mat{\vu \\ \vu}, \mat{\vSigma & \vSigma \\ \vSigma & \vSigma + \sigma^{2}\vI}},
%     \end{equation}
%     and the posterior distribution of \(\vz_{o}\) given \(\vz_{\sigma}\) is (famously)
%     \begin{equation}
%         \vz_{o} \mid \vz_{\sigma} \sim \dNorm(\vu + \vSigma(\vSigma + \sigma^{2}\vI)^{-1}(\vz_{\sigma} - \vu), \vSigma - \vSigma (\vSigma + \sigma^{2}\vI)^{-1}\vSigma).
%     \end{equation}
%     Thus we have
%     \begin{equation}\label{eq:gaussian_denoiser}
%         \vmu_{\sigma}(\vxi) = \vu + \vSigma(\vSigma + \sigma^{2}\vI)^{-1}(\vxi - \vu),
%     \end{equation}
%     which provides an answer for both denoising problems. Letting \(\vSigma = \vV\vLambda\vV^{\top}\) where \(\vLambda = \diag(\lambda_{1}, \dots, \lambda_{d})\), we have
%     \begin{equation}
%         \vmu_{\sigma}(\vxi) = \vu + \vV\diag\mat{\lambda_{1}/(\lambda_{1} + \sigma^{2}) & & \\ & \ddots & \\ & & \lambda_{d}/(\lambda_{d} + \sigma^{2})}\vV^{\top}(\vxi - \vu).
%     \end{equation}
%     which shows that as \(\sigma\) grows, the denoiser collapses the residual \(\vxi - \vu\) to \(0\) fastest in the directions where the eigenvalues of \(\vSigma\) are smallest. Also, if we are trying to learn a Gaussian (for instance), we can parameterize the denoiser by the mean \(\vu\) and covariance \(\vSigma\), so that \(\vtheta = (\vu, \vSigma)\), and consider denoisers of the functional form \eqref{eq:gaussian_denoiser}.
% \end{example}

% Using the definition in \eqref{eq:denoiser_conditional_expectation}, one can show that \(\vmu_{\sigma}\) is the conditional expectation of \(\vz_{o}\) given \(\vz_{\sigma}\),\footnote{We leave this as an exercise for the reader to verify.} i.e.,
% \begin{equation}
%     \vmu_{\sigma}(\vxi) = \Ex[\vz_{o} \mid \vz_{\sigma} = \vxi].
% \end{equation}
% One may use this to show that denoising reduces the entropy $h(\vz)$, as desired.
% \begin{theorem}[Denoising and Entropy]\label{thm:conditioning_reduces_entropy}
%     Suppose that \(\vz_{o} \in \R^{d}\) is a random variable,
%     %with \(\Ex\norm{\vz_{o}}_{2}^{2} < \infty\),
%     \(\vg \sim \dNorm(\vzero, \vI)\) is a standard Gaussian random variable
%     which is independent of \(\vz_{o}\), \(\sigma > 0\) is a positive real
%     number, \(\vz_{\sigma} = \vz_{o} + \sigma \vg\), and \(\vmu_{\sigma}(\vz)
%     = \Ex[\vz_{o} \mid \vz_{\sigma} = \vz]\). Then under certain regularity
%     conditions on $\vz_o$ (detailed in \Cref{app:diffusion-denoising}), it holds
%     that \(h(\vmu_{\sigma}(\vz_{\sigma})) < h(\vz_{\sigma})\).
% \end{theorem}
% The proof is slightly technical and deferred to
% \Cref{app:diffusion-denoising}. Regarding the necessary regularity conditions on
% $\vz_o$, it suffices to assume that $\vz_o$ is any random variable with an arbitrarily small amount of independent Gaussian noise added to it. In the sequel, we will use \textit{iterative denoising} to lower the entropy of
% a high-entropy distribution until it reaches the data distribution, realizing
% the second approach to pursuing a distribution discussed at the end of \Cref{sub:min_entropy}: this makes \Cref{thm:conditioning_reduces_entropy}
% applicable to stages of the iterative denoising process corresponding to
% removing previously added noise from the data distribution.

% So far we have talked about learning the distribution using a particular definition of ``learning''. However, in the literature, it is often the case that ``learning the distribution'' corresponds to learning the probability measure or density of the data distribution. We now discuss a direct connection between our denoising formulation and a way to learn the distribution in this density-centric paradigm. The connection follows from \textit{Tweedie's formula} \cite{Robbins1956AnEB}, which we repeat below.

% \begin{theorem}[Tweedie's Formula]\label{thm:tweedie}
%     Suppose that \(\vz_{o} \in \R^{d}\) is a random variable with \(\Ex\norm{\vz_{o}}_{2}^{2} < \infty\), \(\vg \sim \dNorm(\vzero, \vI)\) is a standard Gaussian random variable which is independent of \(\vz_{o}\), \(\sigma > 0\) is a positive real number, \(\vz_{\sigma} = \vz_{o} + \sigma \vg\), and \(p_{\sigma}\) is the probability density function of \(\vz_{\sigma}\). Then
%     \begin{equation}
%         \Ex[\vz_{o} \mid \vz_{\sigma} = \vxi] = \vxi + \sigma^{2}\nabla \log p_{\sigma}(\vxi).
%     \end{equation}
% \end{theorem}
% \begin{proof}
%     For the sake of this proof, suppose that \(\vz_{o}\) has probability density function \(p_{o}\) (though the result is true even in the case \(\vz_{o}\) does not have a density). Then, note that \(p_{\sigma}\) can be factored into
%     \begin{equation}
%         p_{\sigma}(\vxi) = \int_{\R^{d}}p_{o}(\vx)p_{\sigma \mid o}(\vxi \mid \vx)\odif{\vx},
%     \end{equation}
%     where \(p_{\sigma \mid o}\) is the probability density of \(\vz_{\sigma}\) given an observation of \(\vz_{o}\). Define \(p_{o \mid \sigma}\) correspondingly. Now, we know from the noise model that
%     \begin{equation}
%         p_{\sigma \mid o}(\vxi \mid \vx) = \phi(\vxi \mid \vx, \sigma^{2}\vI),
%     \end{equation}
%     where \(\phi(\vmu, \vSigma)\) is the density of the normal distribution with mean \(\vmu\) and covariance \(\vSigma\). Then
%     \begin{align}
%         \nabla \log p_{\sigma}(\vxi)
%         &= \frac{1}{p_{\sigma}(\vxi)}\nabla p_{\sigma}(\vxi) \\
%         &= \frac{1}{p_{\sigma}(\vxi)}\nabla_{\vxi}\int_{\R^{d}}p_{o}(\vx)\phi(\vxi \mid \vx, \sigma^{2}\vI)\odif{\vx} \\
%         &= \frac{1}{p_{\sigma}(\vxi)}\int_{\R^{d}}p_{o}(\vx)\bs{\nabla_{\vxi}\phi(\vxi \mid \vx, \sigma^{2}\vI)}\odif{\vx} \\
%         &= \frac{1}{p_{\sigma}(\vxi)}\int_{\R^{d}}p_{o}(\vx)\bs{\phi(\vxi \mid \vx, \sigma^{2}\vI)\frac{\vx - \vxi}{\sigma^{2}}}\odif{\vx} \\
%         &= \frac{1}{\sigma^{2}p_{\sigma}(\vxi)}\int_{\R^{d}}p_{o}(\vx)\phi(\vxi \mid \vx, \sigma^{2}\vI)[\vx - \vxi]\odif{\vx} \\
%         &= \frac{1}{\sigma^{2}p_{\sigma}(\vxi)}\int_{\R^{d}}p_{o}(\vx)\phi(\vxi \mid \vx, \sigma^{2}\vI)\vx\odif{\vx} \\
%         &\qquad - \frac{1}{\sigma^{2}p_{\sigma}(\vxi)}\vxi\int_{\R^{d}}p_{o}(\vx)\phi(\vxi \mid \vx, \sigma^{2}\vI)\odif{\vx} \\
%         &= \frac{1}{\sigma^{2}p_{\sigma}(\vxi)}\int_{\R^{d}}p_{o}(\vx)\phi(\vxi \mid \vx, \sigma^{2}\vI)\vx\odif{\vx} - \frac{1}{\sigma^{2}p_{\sigma}(\vxi)}\vxi p_{\sigma}(\vxi), \\
%         &= \frac{1}{\sigma^{2}p_{\sigma}(\vxi)}\int_{\R^{d}}p_{o}(\vx)\phi(\vxi \mid \vx, \sigma^{2}\vI)\vx\odif{\vx} - \frac{1}{\sigma^{2}}\vxi.
%     \end{align}
%     Let us consider the remaining integral. Remember that \(\phi(\vxi \mid \vx, \sigma^{2}\vI) = p_{\sigma \mid o}(\vxi \mid \vx)\). By Bayes' theorem, we write
%     \begin{equation}
%         p_{o}(\vx)\phi(\vxi \mid \vx, \sigma^{2}\vI) = p_{o}(\vx)p_{\sigma \mid o}(\vxi \mid \vx) = p_{\sigma}(\vxi)p_{o \mid \sigma}(\vx \mid \vxi),
%     \end{equation}
%     which allows us to rewrite the integrand in more favorable terms, overall obtaining
%     \begin{align}
%         \nabla \log p_{\sigma}(\vxi)
%         &= \frac{1}{\sigma^{2}p_{\sigma}(\vxi)}\int_{\R^{d}}p_{o}(\vx)\phi(\vxi \mid \vx, \sigma^{2}\vI)\vx\odif{\vx} - \frac{1}{\sigma^{2}}\vxi \\
%         &= \frac{1}{\sigma^{2}p_{\sigma}(\vxi)}\int_{\R^{d}}p_{\sigma}(\vxi)p_{o \mid \sigma}(\vx \mid \vxi)\vx\odif{\vx} - \frac{1}{\sigma^{2}}\vxi \\
%         &= \frac{1}{\sigma^{2}}\int_{\R^{d}}p_{o \mid \sigma}(\vx \mid \vxi)\vx\odif{\vx} - \frac{1}{\sigma^{2}}\vxi \\
%         &= \frac{1}{\sigma^{2}}\bp{\Ex[\vz_{o} \mid \vz_{\sigma} = \vxi] - \vxi}.
%     \end{align}
%     Rearranging proves the theorem.
% \end{proof}
%\yima{Tweedie's formula for the scalar case... as an example of an exercise.}
%\DP{Exercise \ref{exercise:vp_tweedie_denoising} gives a better exercise about these contents. Scalar Tweedie's formula is not a good question since it just repeats the same calculation as here.}

% This result forms a connection between estimating the conditional expectation \(\Ex[\vz_{o} \mid \vz_{\sigma}]\) and estimating the (gradients of the log-density of) the data distribution \(\nabla \log p_{\sigma}\), also known as the \textit{(Hyv{\"a}rinen) score \cite{hyvarinen05a}}. We already have a recipe for estimating \(\Ex[\vz_{0} \mid \vz_{\sigma}]\), so in turn we have a recipe for estimating \(\nabla \log p_{\sigma}\). As we have seen in Chapter \ref{ch:intro}, Figure \ref{fig:score-function} illustrates the geometric intuition of the score in the case of a 1D distribution. In the sequel, we will discuss how to use our estimates for \(\nabla \log p_{\sigma}\) to iteratively transform high-entropy distributions towards the (low-dimensional) data distribution.

% \yima{I have some concerns with the notation here. A few notations are related but are used without clarifying their relationships:  we use $f_{\bm_{\theta}}$ in Algorithm \ref{alg:denoiser}, but use $\bm{\mu}_\sigma$ in Algorithm \ref{alg:iterative_denoising}... We use the notation $\bm{s}_\sigma$ in Algorithm \ref{alg:iterative_denoising} but never quite explain its relation with the score $\nabla \log p_{\sigma}$. Anyway, currently the presentation is not written for first timers. Even experienced need to sort out or guess the notation...  }

% \yima{How to solve this? Does one need to parameterize $f$? The reader needs to solve this for each $\z_o$?}

%\yima{This paragraph. Please do not make comments as if the reader already knows the answers to the denoising problem. This is a book not a paper. We should directly describe how to solve the denoising problem. Some comments could be made AFTER the facts have been presented.}
%At a very high level, denoising follows the general approach we declared in order to learn a distribution. However, there \textit{also} exist very concrete connections between the conditional expectation, i.e., the optimal denoiser, to (perturbed versions of) the data distribution. These mostly follow from \textit{Tweedie's formula} \DP{cite: Robbins 1957, or before?} \DP{cite: Jong-Chul Ye DPM, which has the most general form of Tweedie's formula that I know} \DP{cite: Druv's MAE paper}, whose simplest form is as follows. Consider the model \eqref{eq:additive_gaussian_noise_model}, let \(p_{\sigma}\) be the probability density of \(\vz_\sigma\), and let \(\vs_{\sigma} \doteq \nabla \log p_{\sigma}\). Then
% \begin{equation}\label{eq:tweedie_simple}
%     \vmu_{\sigma}(\vz) \doteq \Ex[\vz_{o} \mid \vz_{\sigma} = \vz] = \vz + \sigma^{2}\nabla \log p_{\sigma}(\vz) \doteq \vz + \sigma^{2}\vs_{\sigma}(\vz).
% \end{equation}
% Thus, it is equivalent to learn a perturbed version of the data distribution via \(\nabla \log p^{\sigma}\), also called the (Hyvarinen) score \DP{cite: aapo score matching} \DP{cite: sohl-dickstein, ganguli}, and to solve the simple conditional expectation regression problem \eqref{eq:denoiser_learning_problem}, which is much more tractable. \yima{this sentence is very awkward: what are equivalent? which is more tractable? We need to describe exactly how the conditional expectation can be solved and the score function can be represented and estimated, at least for the Gaussian noise case. Describe clearly one way and probably comment on other alterative ways to estimate the score etc. Or leave as exercises.}



% It can be shown that the resulting data distribution following the above denoising (with the score) always has entropy reduced: $$h(\boldsymbol{\mu}_\sigma) < h(\z_\sigma).$$
% Again, see Appendix \ref{app:diffusion-denoising} for the justification. \yima{probably state as a theorem.}

% \subsection{Sampling via Iterative Denoising}

% Previously, we have discussed a recipe for learning a parametric encoding of the data distribution, via fitting a denoiser from a parametric family of functions. We also observed, in \ref{thm:conditioning_reduces_entropy}, that applying the denoiser would reduce the entropy of a noisy distribution. In this section, we will discuss how to carefully use this denoiser to gradually reduce the entropy of a noisy distribution until it reaches the data distribution, thereby \textit{sampling} from the distribution. This procedure, known as \textit{iterative denoising} or \textit{iterative compression}, is a realization of the first approach to learning or pursuing a distribution laid out at the end of Section \ref{sub:min_entropy}.

% \subsection{Sampling via Iterative Denoising}

%\yima{Maybe adding an example about how this applies to a single subspace/Gaussian, i.e., the PCA case? Maybe at the end of the subsection, as the next subsection deals with the more general mixture of Gaussians. Then this section would parallel the flow of section 3.3.}

% Previously, we have discussed a recipe for learning a parametric encoding of the data distribution, via fitting a denoiser from a parametric family of functions. We also observed, in Theorem \ref{thm:conditioning_reduces_entropy}, that applying the denoiser would reduce the entropy of a noisy distribution. In this section, we will discuss how to carefully use this denoiser to gradually reduce the entropy of a noisy distribution until it reaches the data distribution, thereby \textit{sampling} from the distribution. This procedure, known as \textit{iterative denoising} or \textit{iterative compression}, is a realization of the first approach to learning or pursuing a distribution laid out at the end of Section \ref{sub:min_entropy}.

% To start, notice that our denoiser maps from \(\vz_{\sigma}\) to an estimate of \(\vz_{o}\). And it is true that \(h(\Ex[\vz_{o} \mid \vz_{\sigma}]) < h(\vz_{\sigma})\). But the quantity \(\Ex[\vz_{o} \mid \vz_{\sigma}]\) is not necessarily a point on the support of the data distribution in general, but rather an average of such points, and thus is not a true data point. Thus to reduce entropy carefully enough to sample, we \textit{cannot} use only a single application of a denoiser.

% Instead of a single application, we choose to break up our problem into \textit{many small steps} --- denoise (or reduce entropy) a little bit at a time (a computationally easier problem) and chain the results together, to denoise/reduce entropy from large noise/entropy distributions (a hard problem). More precisely, we have
% \begin{equation}
%     \vz_{\sigma} = \vz_{o} + \sigma \vg \implies \vg = \frac{\vz_{\sigma} - \vz_{o}}{\sigma}.
% \end{equation}
% Then for any \(\delta > 0\) we have
% \begin{align}
%     \Ex[\vz_{\sigma - \delta} \mid \vz_{\sigma}]
%     &= \Ex[\vz_{o} + (\sigma - \delta)\vg \mid \vz_{\sigma}] \\
%     &= \Ex[\vz_{\sigma} - \delta \vg \mid \vz_{\sigma}] \\
%     &= \vz_{\sigma} - \delta \Ex[\vg \mid \vz_{\sigma}] \\
%     &= \vz_{\sigma} - \delta \Ex\rs{\frac{\vz_{\sigma} - \vz_{o}}{\sigma} \given\ \vz_{\sigma}} \\
%     &= \bp{1 - \frac{\delta}{\sigma}}\vz_{\sigma} + \frac{\delta}{\sigma}\Ex[\vz_{o}\ \given\ \vz_{\sigma}].
% \end{align}
% Thus we can do iterative denoising --- recovering \(\vz_{\sigma - \delta}\) from \(\vz_{\sigma}\) repeatedly --- and eventually we will try to recover \(\vz_{o}\) from \(\vz_{\sigma}\) where \(\sigma\) is small. At this point, the conditional expectation vector \(\Ex[\vz_{o} \mid \vz_{\sigma}]\) will be very close to the support of the data distribution, allowing us to approximately sample. Formally, we can set up the following method: set up a terminal iteration \(M\) and \textit{noise schedule} \((\sigma^{(m)})_{m = 0}^{M - 1}\), such that \(\sigma^{(0)} > \sigma^{(1)} > \cdots > \sigma^{(M - 1)} > 0\), and the differences \(\delta^{(m)}\) between subsequent noise levels in the schedule can be written
% \begin{equation}
%     \delta^{(m)} \doteq \begin{cases}\sigma^{(m)} - \sigma^{(m + 1)}, & \text{if}\ 0 \leq m < M - 1 \\ \sigma^{(M - 1)}, & \text{if}\ m = M - 1.\end{cases}
% \end{equation}
% Then we have the iteration
% \begin{align}
%     \vz^{(0)}
%     &\sim \dNorm(\vzero, (\sigma^{(0)})^{2}\vI) \label{eq:denoising_ve_init}, \\
%     \vz^{(m + 1)}
%     &= \bp{1 - \frac{\delta^{(m)}}{\sigma^{(m)}}}\vz^{(m)} + \frac{\delta^{(m)}}{\sigma^{(m)}}\Ex[\vz_{o} \mid \vz_{\sigma^{(m)}} = \vz^{(m)}], \label{eq:denoising_ve_iter} \\
%     &\qquad  \qquad \forall m \in \{0, 1, \dots, M - 1\}
% \end{align}
% This method denoises by the amount \(\delta^{(m)}\) in each of \(M\) steps, such that \(\vz^{(m)}\) has (approximate) noise level \(\sigma^{(m)}\). For historical reasons, the score reformulation is more popular and we can obtain it using Tweedie's formula, i.e.,
% \begin{align}
%     \vz^{(m + 1)}
%     &= \bp{1 - \frac{\delta^{(m)}}{\sigma^{(m)}}}\vz^{(m)} + \frac{\delta^{(m)}}{\sigma^{(m)}}\Ex[\vz_{o} \mid \vz_{\sigma^{(m)}} = \vz^{(m)}] \\
%     &=  \bp{1 - \frac{\delta^{(m)}}{\sigma^{(m)}}}\vz^{(m)} + \frac{\delta^{(m)}}{\sigma^{(m)}}\bs{\vz^{(m)} + (\sigma^{(m)})^{2}\nabla \log p_{\sigma^{(m)}}(\vz^{(m)})} \\
%     &= \vz^{(m)} + \delta^{(m)} \sigma^{(m)}\nabla \log p_{\sigma^{(m)}}(\vz^{(m)}).
% \end{align}
% Thus, we have reformulated each denoising step as taking a gradient step with step size \(\delta^{(m)} \sigma^{(m)}\) on the log-density \(\log p_{\sigma^{(m)}}(\vz^{(m)})\). See Figure \ref{fig:diffusion-chapter3} for a visual description of such gradient steps, and the geometry of iterative denoising. For your convenience, an algorithm is concisely presented below.
% \begin{algorithm}
%     \caption{Iterative Denoising}\label{alg:iterative_denoising}
%     \begin{algorithmic}[1]
%         \Require{Noise schedule \((\sigma^{(m)})_{m = 0}^{M - 1}\) s.t.~\(\sigma^{(0)} > \sigma^{(1)} > \cdots > \sigma^{(M - 1)} > 0\), samples \(\{\vz_{o}^{i}\}_{i = 1}^{N} \simiid \vz_{o}\).}
%         \Ensure{A sample \(\vz^{(M)}\) drawn (approximately) from the distribution of \(\vz_{o}\).}

%         \Procedure{IterativeDenoising}{$(\sigma^{(m)})_{m = 0}^{M - 1}$, $\{\vz_{o}^{i}\}_{i = 1}^{N}$}

%         \For{\(m \in \{0, 1, \dots, M-2\}\)} \Comment{Calculate the noise differences \(\delta^{(m)}\)}
%             \State{\(\delta^{(m)} \gets \sigma^{(m)} - \sigma^{(m + 1)}\)}
%         \EndFor
%         \State{\(\delta^{(M - 1)} \gets \sigma^{(M-1)}\)}
%         \Statex{}

%         \State{Initialize \(\vz^{(0)} \gets \dNorm(\vzero, \vI)\)} \Comment{Initialize denoising}
%         \For{\(m \in \{0, 1, \dots, M-1\}\)}
%             \State{\(\hat{\vmu}_{\sigma^{(m)}} \gets \displaystyle \textsc{TrainDenoiser}(\sigma^{(m)}, \{\vz_{o}^{i}\}_{i = 1}^{N})\)} \Comment{From \Cref{alg:denoiser}}
%             \State{\(\hat{\vs}_{\sigma^{(m)}} \gets \displaystyle \frac{\hat{\vmu}_{\sigma^{(m)}}(\vz^{(m)}) - \vz^{(m)}}{(\sigma^{(m)})^{2}}\)} \Comment{Convert to approx.~scores using Tweedie's formula (\Cref{thm:tweedie})}
%             \State{\(\vz^{(m + 1)} \gets \displaystyle \vz^{(m)} + \delta^{(m)}\sigma^{(m)}\hat{\vs}_{\sigma^{(m)}}(\vz^{(m)})\)} \Comment{Denoising iteration}
%         \EndFor

%         \State{\Return{\(\vz^{(M)}\)}}
%         \EndProcedure
%     \end{algorithmic}
% \end{algorithm}

% Some common choices of noise schedules are:
% \begin{itemize}
%     \item \(\sigma^{(m)} = C(M - m)\) where \(C > 0\), which is the ``linear noise schedule''.
%     \item \(\sigma^{(m)} = C(\e^{\alpha (M - m)} - 1)\) where \(C > 0\) and \(\alpha > 0\), which is the ``exponential noise schedule''.
% \end{itemize}
% Previous work\footnote{\cite{karras2022elucidating} empirically evaluates different noise schedules, while \cite{bao2022analytic} constructs theoretically optimal noise schedules, albeit needing to be estimated via Monte Carlo.} has studied ``optimal'' notions of noise schedule.

% Algorithm \ref{alg:iterative_denoising} converges to within good precision, even when the scores are estimated (as long as the estimation is good enough).
% \begin{theorem}[Theorem 2.2 of \cite{lee2022convergence}, Abreviated]\label{thm:distribution-convergence}
%     Under certain technical conditions on \(\vz_{o}\) the noise levels \((\sigma^{(m)})_{m = 0}^{M - 1}\), and the quality of the score estimates \(\hat{\vs}_{\sigma^{(m)}}\) learned from samples \(\{\vz_{o}^{i}\}_{i = 1}^{N}\), if \(\vz^{(M)}\) is the output of the sampling process in Algorithm \ref{alg:iterative_denoising}, then \(\vz^{(M)} \to \vz_{o}\) (in the so-called \textit{total variation distance}) as \(M, N \to \infty\).
% \end{theorem}


% \begin{remark}
%     In the case that the data \(\vz_{o}\) have low-dimensional structures such as those described in Chapter \ref{ch:classic}, the density of \(\vz_{o}\) may not exist. Regardless, the techniques here apply just as well, because \(\vz_{\sigma}\) has a density for any \(\sigma > 0\).\footnote{This is a non-trivial fact to prove in general, requiring the so-called \textit{H\"ormander condition} to hold, and proving that H\"ormander's condition implies the existence of a density requires Malliavin calculus. Refer to any advanced stochastic calculus textbook for details.} However, the scores \(\hat{\vs}_{\sigma}\) become increasingly numerically unstable (in the sense that their Jacobians are ill-conditioned or even non-Lipschitz-continuous) for small \(\sigma\). Thus, it is recommended to increase \(M\) when sampling from low-dimensional distributions.
% \end{remark}


% \begin{remark}
%     This ``iterative denoising'' process is \textit{exactly} the sampling algorithm used in diffusion models. (Experts or the curious may know this as the ``variance-exploding probability flow'' process.) The sampling process in Algorithm \eqref{alg:iterative_denoising}, though deterministic, is equivalent to a whole host of deterministic and stochastic processes which can be used to sample from the data distribution.\footnote{Some examples: \cite{montanari2023sampling} and \cite{song2020denoising}.   %\DP{Just keeping this around, does not need to go to final version.}
%     } Figure \ref{fig:denoising-twolines-oneplane} shows an example of sampling from a mixture of three degenerate Gaussians (two lines and one plane) by iterative denoising random samples from a Gaussian with a very large variance by the algorithm.
% \end{remark}

% \begin{example}[Sampling from a Gaussian]
%     In Example \ref{example:gaussian_denoising}, we worked through the optimal denoiser for Gaussian ground truth distributions. Above, we showed that denoising and sampling are tightly related. Here, we will work out the score function and denoising iteration for Gaussians, and draw a connection to PCA (c.f.~Chapter \ref{ch:classic}).

%     More specifically, let \(\vz_{o} \sim \dNorm(\vu, \vSigma)\), and let \(\vz_{\sigma} = \vz_{o} + \sigma \vg\). We showed in Example \ref{example:gaussian_denoising} that
%     \begin{equation}
%         \Ex[\vz_{o} \mid \vz_{\sigma} = \vxi] = \vmu_{\sigma}(\vxi) = \vu + \vSigma(\vSigma + \sigma^{2}\vI)^{-1}(\vxi - \vu).
%     \end{equation}
%     By Tweedie's formula (Theorem \ref{thm:tweedie}) we obtain the score:
%     \begin{align}
%         \nabla \log p_{\sigma}(\vxi)
%         &= \frac{\vmu_{\sigma}(\vxi) - \vxi}{\sigma^{2}} \\
%         &= \frac{1}{\sigma^{2}}\bs{\bp{\vu + \vSigma(\vSigma + \sigma^{2}\vI)^{-1}(\vxi - \vu)} - \vxi} \\
%         &= \frac{1}{\sigma^{2}}\bs{\vSigma(\vSigma + \sigma^{2}\vI)^{-1}(\vxi - \vu) - (\vxi - \vu)} \\
%         &= \frac{1}{\sigma^{2}}\bs{\vSigma(\vSigma + \sigma^{2}\vI)^{-1} - \vI}(\vxi - \vu).
%     \end{align}
%     Now, take the spectral decomposition \(\vSigma = \vV\vLambda\vV^{\top}\) where \(\vV \in \O(d)\) and \(\vLambda = \diag(\lambda_{1}, \dots, \lambda_{d})\) where \(\lambda_{i} \geq 0\). We have
%     \begin{align}
%         \nabla \log p_{\sigma}(\vxi) &= \frac{1}{\sigma^{2}}\bs{\vV\vLambda\vV^{\top}(\vV\vLambda \vV^{\top} + \sigma^{2}\vI)^{-1} - \vI}(\vxi - \vu) \\
%         &= \frac{1}{\sigma^{2}}\bs{\vV\vLambda\vV^{\top}(\vV(\vLambda + \sigma^{2}\vI) \vV^{\top})^{-1} - \vI}(\vxi - \vu) \\
%         &= \frac{1}{\sigma^{2}}\bs{\vV\vLambda\vV^{\top}\vV(\vLambda + \sigma^{2}\vI)^{-1} \vV^{\top} - \vI}(\vxi - \vu) \\
%         &= \frac{1}{\sigma^{2}}\vV\bs{\vLambda(\vLambda + \sigma^{2}\vI)^{-1} - \vI}\vV^{\top}(\vxi - \vu) \\
%         &= \frac{1}{\sigma^{2}}\vV\mat{\lambda_{1}/(\lambda_{1} + \sigma^{2}) - 1 & & \\ & \ddots & \\ & & \lambda_{d}/(\lambda_{d} + \sigma^{2}) - 1}\vV^{\top}(\vxi - \vu) \\
%         &= \frac{1}{\sigma^{2}}\vV\mat{-\sigma^{2}/(\lambda_{1} + \sigma^{2}) & & \\ & \ddots & \\ & & -\sigma^{2}/(\lambda_{d} + \sigma^{2})}\vV^{\top}(\vxi - \vu) \\
%         &= -\vV\mat{1/(\lambda_{1} + \sigma^{2}) & & \\ & \ddots & \\ & & 1/(\lambda_{d} + \sigma^{2})}\vV^{\top}(\vxi - \vu) \\
%         &= -\vV(\vLambda + \sigma^{2}\vI)^{-1}\vV^{\top}(\vxi - \vu) \\
%         &= -(\vSigma + \sigma^{2}\vI)^{-1}(\vxi - \vu).
%     \end{align}

%     In this case, the score is really simple, even when the denoiser looks relatively complicated. Following the Algorithm \ref{alg:iterative_denoising}, an iterative denoising step would then be
%     \begin{equation}
%         \vz^{+} \doteq \vz + \delta \sigma \nabla \log p_{\sigma}(\vz) = \vz - \delta \sigma(\vSigma + \sigma^{2}\vI)^{-1}(\vz - \vu).
%     \end{equation}
%     Finally, we restrict ourselves to a notable special case. Let \(\vu = \vzero\), let \(r \in [d]\) be a positive integer, and let \(\vSigma = \vU\vU^{\top}\) for some \(\vU \in \O(d, r)\). Define \(\cF_{\sigma, r}\) as
%     \begin{equation}
%         \cF_{\sigma, r} = \{\vx \mapsto \hat{\vSigma}(\hat{\vSigma} + \sigma^{2}\vI)^{-1}\vx \colon \hat{\vSigma} = \hat{\vU}\hat{\vU}^{\top}, \hat{\vU} \in \O(d, r)\},
%     \end{equation}
%     namely, the set of denoisers corresponding to Gaussians with the true mean \(\vzero\) and an estimated true covariance \(\hat{\vU}\hat{\vU}^{\top} + \sigma^{2}\vI\), which differs from the true model via the support estimation (i.e., \(\hat{\vU}\) versus \(\vU\)). Then, as \cite{wang2024diffusion} shows, the denoising problem
%     \begin{equation}\label{eq:denoise}
%         \argmin_{f_{\sigma} \in \cF_{\sigma, r}}\Ex\norm{\vz_{o} - f_{\sigma}(\vz_{\sigma})}_{2}^{2}
%     \end{equation}
%     is equivalent to PCA, i.e., identifying \(\hat{\vU} \in \O(d, r)\) which supports the data distribution. In this way, distribution learning via denoising can be viewed as a generalization of PCA for nonlinear data distributions.
% \end{example}

% \begin{exercise}
% Please show that Problem \eqref{eq:denoise} is equivalent to the probabilistic PCA problem. Here are some hints: \\
% (i) Matrix inversion lemma: $(\vI + \vQ\vQ^\top)^{-1} = \vI - \vQ(\vI + \vQ^\top\vQ)^{-1}\vQ^\top$ for any $\vQ \in \R^{d\times r}$. \\
% (ii) It holds that $\mathbb{E}_{\vg \sim \mathcal{N}(\vzero, \vI)}[\|\vU^\top \vg\|^2] = \sum_{i=1}^r \mathbb{E}_{\vg}[\|\vu_i^\top\vg\|^2] = r$ for any $\vU \in \O(d, r)$, where $\vu_i$ denotes the $i$-th column of $\vU$. \\
% (iii) You can condition on $\vz_o$.
% \end{exercise}

% \subsection{Denoising a Mixture of Low-Dimensional Gaussians}

% % \yima{%Made this a separate subsection to pair well with Section 3.3.3. %Consider to incorporate the following things to improve this subsection:
% % \begin{enumerate}
% %     %\item State the best known results regarding sampling complexity for this class of models, from memorization to generalization etc. Make connection to Section 3.3.3. \DP{Sample complexity is known for \textit{isotropic} Gaussians, not low-rank Gaussians (Peng's paper also uses some thresholding operators, not the true denoisers, but there are some more by e.g. Kulin Shah and Sitan Chen). Similarly, memorization-generalization transition is not known, although we are working on it.}
% %     %\item Use this class of model to illustrate exactly how the previous denoising and sampling algorithms are applied. \DP{done in the earlier simple case of single Gaussian.}
% %     \item Add an example of simulation with a specific mixed distribution, say for the same model used in Section 3.3.3: one plane and two lines in $\mathbb{R}^3$. Also design and leave some programming exercises for students to implement the diffusion process for learning a similarly simple distribution.
% % \end{enumerate}
% % }

% %\begin{example}\label{example:gmm_denoising}
%     In this section, we will work through the denoising process analytically for a very important family of distributions where the data are from a low-rank Gaussian mixture model and the estimator tries to estimate the supporting subspaces. Figure \ref{fig:denoising-twolines-oneplane} has shown numerically how the denoising process works for a specific example of such an distribution. We have studied some special cases from this family of distributions in the previous Chapter \ref{ch:linear-independent} and we will see in the next Chapter \ref{ch:representation} how this class of distributions can be used to model well most general distributions of practical importance.

%     More particularly, suppose that the data are drawn from the distribution
%     \begin{equation}
%         \vz_{o} \sim \frac{1}{K}\sum_{k = 1}^{K}\dNorm(\vzero, \vU_{k}\vU_{k}^{\top}),
%     \end{equation}
%     where \(\vU_{[K]} = (\vU_{k})_{k = 1}^{K}\) are \(d \times p\) orthogonal matrices.
%     A calculation using Tweedie's formula (left as exercise) yields that the optimal denoiser of \(\vz_{o}\) at noise level \(\sigma\) is given by
%     \begin{equation}
%         f_{\sigma}^{\ast}(\vz) = \frac{\sum_{k = 1}^{K}\phi(\vz \mid \vzero, \vU_{k}\vU_{k}^{\top} + \sigma^{2}\vI)\cdot (\vU_{k}\vU_{k}^{\top} + \sigma^{2}\vI)^{-1}\vz}{\sum_{k = 1}^{K}\phi(\vz \mid \vzero, \vU_{k}\vU_{k}^{\top} + \sigma^{2}\vI)},
%     \end{equation}
%     where \(\phi(\vx \mid \vmu, \vSigma)\) is the density of the normal distribution with mean \(\vmu\) and \(\vSigma\) evaluated at point \(\vx\).
%     Now, suppose that we use a class of denoisers corresponding to a similar measure with \(\wh{K}\) components, with estimated subspaces \(\hat{\vU}_{k}\), namely belonging to the class \(\cF_{\wh{K}, \sigma}\) containing functions
%     \begin{equation}\label{eq:estimated_lowrank_denoiser}
%         f_{\sigma}(\vz \mid \hat{\vU}_{[\hat{K}]}) \doteq \frac{\sum_{k = 1}^{\hat{K}}\phi(\vz \mid \vzero, \hat{\vU}_{k}\hat{\vU}_{k}^{\top} + \sigma^{2}\vI)\cdot (\hat{\vU}_{k}\hat{\vU}_{k}^{\top} + \sigma^{2}\vI)^{-1}\vz}{\sum_{k = 1}^{K}\phi(\vz \mid \vzero, \hat{\vU}_{k}\hat{\vU}_{k}^{\top} + \sigma^{2}\vI)}.
%     \end{equation}
%     Recent work such as in \cite{wang2024diffusion} has shown that it is possible to learn the \(\hat{\vU}_{[\hat{K}]}\) via gradient descent on denoising objectives, and that with enough samples \(N\) and \(K = \hat{K}\) (say), we have \(\hat{\vU}_{k}\hat{\vU}_{k}^{\top} \approx \vU_{k}\vU_{k}^{\top}\). However, that's not all there is to say --- in fact, this expression for a denoiser, \eqref{eq:estimated_lowrank_denoiser}, is secretly almost the form of a neural network layer. To see this, notice that we can calculate (proof left as exercise):
%     \begin{equation}
%         f_{\sigma}(\vz \mid \hat{\vU}_{[\hat{K}]}) = \sigma^{-2}\vz - \frac{\sigma^{-2}}{1 + \sigma^{2}}\cdot\frac{\sum_{k = 1}^{\hat{K}}\phi(\vz \mid \vzero, \hat{\vU}_{k}\hat{\vU}_{k}^{\top} + \sigma^{2}\vI)\hat{\vU}_{k}\hat{\vU}_{k}^{\top}\vz}{\sum_{k = 1}^{K}\phi(\hat{\vz} \mid \vzero, \hat{\vU}_{k}\hat{\vU}_{k}^{\top} + \sigma^{2}\vI)}.
%     \end{equation}
%     This is similar to a scaling of an attention layer in a transformer, with the following caveats:
%     \begin{itemize}
%         \item There is only one ``token'', though this is more of a consequence of the data model.
%         \item The query-key-value matrices in each of \(K\) heads are replaced with a simple linear projection matrix \(\vU_{k}\).
%     \end{itemize}
%     This is not a coincidence, or an accident! The following Section \ref{sec:lossy_compression} and Chapter \ref{ch:representation} will give a reason for why transformer-like networks will use these denoising operators from the perspective of compression.
% %\end{example}


% Let us take a moment to reflect on what we have achieved:
% \begin{itemize}
%     \item We discovered a way to learn an encoding of a distribution by several denoisers within a parametric function class.
%     \item We discovered a way to use that parametric encoding (e.g., denoisers) to iteratively decrease the entropy of a high-entropy noise sample until it lands on the data distribution.
% \end{itemize}
% These two approaches actually fulfill, in some sense, the candidate approaches presented in \eqref{sub:min_entropy}. However, everything presented is more or less implicit: the denoisers only form an implicit encoding of the distribution, and the sampling is only on a distribution-wise level. In the next Section, we discuss a framework for making our models of the distribution more explicit and computationally accessible.

% \begin{exercise}
%     Complete the proofs alluded to in this subsection. Namely:
%     \begin{enumerate}
%         \item Show via Tweedie's formula (Theorem \ref{thm:tweedie}) that the optimal denoiser corresponding to a mixture of Gaussians \(\mu = \frac{1}{K}\sum_{k = 1}^{K}\dNorm(\vzero, \vU_{k}\vU_{k}^{\top})\), where \(\vU_{k} \in \O(d, p)\), is
%         \begin{equation}
%             f_{\sigma}^{\ast}(\vx \mid \vU_{[K]}) = \frac{\sum_{k = 1}^{K}\phi(\vx \mid \vzero, \vU_{k}\vU_{k}^{\top} + \sigma^{2}\vI)(\vU_{k}\vU_{k}^{\top} + \sigma^{2}\vI)^{-1}\vx}{\sum_{i = 1}^{K}\phi(\vx \mid \vzero, \vU_{k}\vU_{k}^{\top} + \sigma^{2}\vI)}
%         \end{equation}
%         where \(\phi(\vx \mid \vmu, \vSigma)\) is the density of \(\dNorm(\vmu, \vSigma)\) evaluated at \(\vx\).
%         \item Prove the Woodbury matrix identity, i.e.,
%         \begin{equation}
%             (\vA + \vU\vC\vV)^{-1} = \vA^{-1} - \vA^{-1}\vU(\vC^{-1} + \vV\vA^{-1}\vU)^{-1}\vV\vA^{-1}
%         \end{equation}
%         where \(\vA \in \R^{n \times n}\) is an invertible matrix, \(\vU \in \R^{n \times k}\), \(\vV \in \R^{k \times n}\), and \(\vC \in \R^{k \times k}\) is an invertible matrix.
%         \item Use the Woodbury matrix identity to show that
%         \begin{align}
%             (\vU_{k}\vU_{k}^{\top} + \sigma^{2}\vI)^{-1}
%             &= \sigma^{-2}\vI - \sigma^{-4}\vU_{k}(\vI + \sigma^{-2}\vI)^{-1}\vU_{k}^{\top} \\
%             &= \sigma^{-2}\vI - \frac{\sigma^{-4}}{1 + \sigma^{-2}}\vU_{k}\vU_{k}^{\top} \\
%             &= \sigma^{-2}\vI - \frac{\sigma^{-2}}{1 + \sigma^{2}}\vU_{k}\vU_{k}^{\top} \\
%             &= \sigma^{-2}\bp{\vI - \frac{1}{1 + \sigma^{2}}\vU_{k}\vU_{k}^{\top}}.
%         \end{align}
%         %\DP{Don't need to include all steps of the simplification, this is just how I worked it out.}
%         \item Use the above form for the inverse to write the optimal denoiser as
%         \begin{equation}
%             \scalebox{0.95}{\(\displaystyle f_{\sigma}^{\ast}(\vx \mid \vU_{[K]}) = \sigma^{-2}\vx - \frac{\sigma^{-2}}{1 + \sigma^{2}}\cdot\frac{\sum_{k = 1}^{K}\phi(\vx \mid \vzero, \vU_{k}\vU_{k}^{\top} + \sigma^{2}\vI)\vU_{k}\vU_{k}^{\top}\vx}{\sum_{k = 1}^{K}\phi(\vx \mid \vzero, \vU_{k}\vU_{k}^{\top} + \sigma^{2}\vI)}.\)}
%         \end{equation}
%     \end{enumerate}
% \end{exercise}

% \begin{exercise}\label{exercise:vp_tweedie_denoising}
%     Consider the new data generating process
%     \begin{equation}
%         \vz_{\sigma} = \alpha(\sigma)\vz_{o} + \sigma \vg, \qquad \vg \sim \dNorm(\vzero, \vI),
%     \end{equation}
%     where \(\alpha \colon \R_{> 0} \to \R_{> 0}\) is an integrable positive non-increasing function. Find an appropriate version of Tweedie's formula, e.g., compute \(\Ex[\vz_{o} \mid \vz_{\sigma}]\), and compute an incremental denoising iteration, e.g., compute \(\Ex[\vz_{\sigma - \delta} \mid \vz_{\sigma}]\), for this process. We remark that this is not an arbitrary extension of our framework: it actually extends the framework to cover all popular diffusion models setups, including those which use the so-called ``Variance-Preserving'' and Ornstein-Uhlenbeck diffusions (which mandate \(\sigma \in [0, 1)\) and take \(\alpha(\sigma) = \sqrt{1 - \sigma^{2}}\)) or the ``Flow Matching'' process (which mandates \(\sigma \in [0, 1]\) and takes \(\alpha(\sigma) = 1 - \sigma\)).
% \end{exercise}

% But, as we can observe from this interpretation, the process is liable to be stuck in local modes or local maxima of the density. In order to circumvent this, the usual sampling algorithm adds an appropriately small amount of Gaussian noise to each iteration, obtaining the iteration
% \begin{equation}
%     \vz^{(m + 1)} = \vz^{(m)} + \delta \sigma^{(m)}\nabla \log p_{\sigma^{(m)}}(\vz^{(m)}) + \sqrt{2\delta \sigma^{(m)}}\vg^{(m)}
% \end{equation}
% where \(\vg^{(m)}\) are i.i.d.~standard Gaussian variables.

% \yima{Again, please avoid comment on things. We are not writing a paper, but a book. Directly describe what can be or should be done next. Leave citation or comments later. Book should be largely self-contained. }
% Learning \(\vs_{\sigma} = \nabla \log p_{\sigma}\) is useful because it provides a sample-efficient way to learn a parametric statistical model \DP{cite: aapo}, and also gives ways to sample from the perturbed distribution \(p^{\sigma}\) (and, if \(\sigma\) is small or approaches to \(0\), then also sample from the ground truth distribution \(p\)).  One such classical sampling procedure is obtained by the so-called \textit{Langevin dynamics}, recently supplanted by the more modern \textit{diffusion models}. In what follows, we give a brief overview of both methods.

% To sample from a distribution \(p_{\sigma}\) using Langevin dynamics \DP{not actually sure the correct citation here... in the worst case, look at sources in Lee/Risteski blog}, we use the following iterative procedure:
% \begin{equation}\label{eq:langevin}
%     \vz_{t + 1} = \vz_{t} + \eta_{t} \vs_{\sigma}(\vz_{t}) + \sqrt{2\eta_{t}} \vg_{t}, \qquad \vg_{t} \sim \dNorm(\vzero, \vI), \quad \forall t \in [T].
% \end{equation}
% Here \(\eta_{t}\) are step-sizes and \(\vg_{t}\) is sampled independently from the whole trajectory up until time step \(t\). Some brief intuition for this iteration is as follows. When sampling from a desired distribution \(p_{\sigma}\), we want to move our iterate \(\vz_{t}\) closer to the regions of high probability for \(p_{\sigma}\) (equivalently, \(\log p_{\sigma}\), whose gradient we use because it does not require the intractable computation of a normalizing constant present in \(p_{\sigma}\) itself). Thus, we take a gradient step on \(\log p_{\sigma}\). But we do not want to just concentrate our samples on local or global optima of \(\log p_{\sigma}\), so to perturb the current iterate out of such optima, we add some appropriately-scaled noise. One can prove that the iteration \eqref{eq:langevin} will converge to a random variable which has the target distribution of interest.

% \begin{theorem}
% \yima{Present as a theorem that the above Langevin dynamics converges to the desired (Gibbs)   distribution.}
% \end{theorem}

%\DP{figure denoting gradient descent on smoothed form of low-dimensional density: like MAE paper picture, but with curved data} \DP{series of pictures: generic data, linear data, piecewise linear data, transforming the data into linear structure}


% \paragraph{A path of iterative denoising.}
% \yima{Before presenting the solution for approximating and sampling the data distribution, please describe the approach and set up the stage for it. Describe the iterative denoising process from a standard Gaussian to the data distribution. Describe precisely what the sequence of scores $\vs_{\sigma^{\ell}}(\vz_{t}^{\ell})$ are and how they are learned, by applying previous results.}

% \yima{When does the denoising process stop? Why not converging to the empirical distribution of the samples, which seem to have the lowest intrinsic dimension, 0?}


% \subsection{Annealed Langevin Dynamics}
% The main issue that we take with the Langevin dynamics here is that it does not allow us to sample from real-world data distributions, i.e., those which are high-dimensional but with low-dimensional structure. In particular, those distributions do not have well-defined densities, and so Langevin dynamics is not well-defined. For this reason, \DP{cite: song \& ermon 2019} proposed a form of \textit{annealed Langevin dynamics}: run trajectories of \eqref{eq:langevin} with smaller and smaller \(\sigma = \sigma^{\ell}\), using the previous trajectory's terminal state as the current trajectory's input state, until the output has a distribution close to the data distribution, i.e.,
% \begin{align}
%     \vz_{t + 1}^{\ell}
%     &= \vz_{t}^{\ell} + \eta_{t}^{\ell} \vs_{\sigma^{\ell}}(\vz_{t}^{\ell}) + \sqrt{2\eta_{t}^{\ell}} \vg_{t}^{\ell}, \qquad \vg_{t}^{\ell} \sim \dNorm(\vzero, \vI), \\
%     \vz_{1}^{\ell}
%     &= \vz_{T + 1}^{\ell - 1}, \qquad \forall t \in [T], \quad \forall \ell \in [L].
% \end{align}
% By choosing the \(\sigma^{\ell}\) properly, we obtain the following simplified process:
% \begin{equation}\label{eq:diffusion_sampling}
%     \vz^{\ell + 1} = \vz^{\ell} + \eta^{\ell} \vs_{\sigma^{\ell}}(\vz^{\ell}) + \sqrt{2\eta^{\ell}} \vg^{\ell}, \qquad \vg^{\ell} \sim \dNorm(\vzero, \vI), \quad \forall \ell \in [L].
% \end{equation}
% This is an iteration scheme associated with \textit{diffusion models}, which provably sample from the data distribution, assuming the score \(\nabla \log p^{\sigma}\) is estimated well, even if the distribution is low-dimensional. \yima{Any reference for this statement?}

% In particular, the iteration \eqref{eq:diffusion_sampling} provides a way to \textit{sample by denoising} from even arbitrary distributions. That is, by using \eqref{eq:tweedie_simple}, we can write \eqref{eq:diffusion_sampling} as
% \begin{align}
%     \vz^{\ell + 1}
%     &= \vz^{\ell} + \eta^{\ell}\left(\frac{\vmu_{\sigma^{\ell}}(\vz^{\ell}) - \vz^{\ell}}{(\sigma^{\ell})^{2}}\right) + \sqrt{2\eta^{\ell}}\vg^{\ell}, \quad \vg^{\ell} \sim \dNorm(\vzero, \vI), \quad \forall \ell \in [L], \\
%     &= \left(1 - \frac{\eta^{\ell}}{(\sigma^{\ell})^{2}}\right)\vz^{\ell} + \frac{\eta^{\ell}}{(\sigma^{\ell})^{2}}\vmu_{\sigma^{\ell}}(\vz^{\ell}) + \sqrt{2\eta^{\ell}}\vg^{\ell}.
% \end{align}
% \yima{probably present results from Jun Zhu's group on how to make the all the quantities explicit, such as variances etc.}
% In particular, each iteration takes a convex combination of the current iterate and the optimal denoiser, gradually moving towards the denoised sample, along with some appropriately scaled noise to sample along the posterior distribution. Hence, diffusion models learn the distribution via estimating the optimal denoiser, and sample by iterative denoising.

% \yima{Describe the process as pseudocode for an implementable  algorithm.} \DP{}


% % \DP{outline and some old notes}These also are the key ways to learn a distribution in a high-dimensional space with very low intrinsic dimension. More specifically denoising (and score function), or completion, or prediction...

% % Empirical Bayesian denoising, Tweedie's formula, score function, Lagevine dynamics, MCMC etc... (Can parameterize score function, but don't need to... denoising is well-defined in general. \yima{Do not understand the last sentence.})

% \DP{TODO: DP}



% \subsection{Denoising for Low-rank Gaussians}
% \yima{Note that the above framework is for an arbitrary distribution. In general, we do not know exactly what the score function $\boldsymbol{s}_\sigma$ is. Of course, one could approximate it with some neural networks as it was often done in practice. Here we show that when the underlying distribution can be modeled by low-rank Gaussian, the score function takes an explicit form that could help us understand the structures of modern deep networks. }

% \yima{Why doesn't the denoising process converge to the (discrete) empirical distribution which has the lowest intrinsic dimension, 0? If no rigorous justification, at least some plausible explanations. Probably leading to the lossy compression approach.}

% Diffusion models and denoisers are said to learn the distribution. It is a natural question to ask whether the information they learn is in some sense equivalent to the correct information learned by other classical methods to learn the distribution. It turns out that, in some restricted cases, it is possible to prove that the answer is yes. \DP{continue...}

% \DP{outline}
% Interpretation of PCA from the perspective of diffusion and denoising for a single low-rank Gaussian.

% Diffusion and denoising for a mixture of low-rank Gaussians, and subspace clustering.

% \DP{TODO: DP}

\section{有损编码与压缩} \label{sec:lossy_compression}

让我们回顾一下到目前为止我们所涵盖的内容。我们已经讨论了如何从有限样本中拟合一个\textit{去噪器} \(\bar{\vx}_{\theta}\)。我们展示了这个去噪器编码了一个分布，因为它通过 Tweedie 公式 \eqref{eq:tweedie} 与其对数密度直接相关。然后，我们用它通过\textit{迭代去噪}将一个纯噪声（高熵）分布逐渐转换为学习到的分布。因此，我们已经发展了在 \Cref{sub:min_entropy} 末尾提出的学习或追求分布的第一种方式。

然而，在这种方法论中，分布的编码隐含在去噪器的函数形式和参数（如果有的话）中。事实上，敏锐的读者可能已经注意到，对于一个一般的分布，我们从未明确指定去噪器的函数形式。在实践中，人们通常用一些具有经验设计的架构的深度神经网络来建模它。此外，尽管我们知道上述去噪过程降低了熵，但我们不知道降低了多少，也不知道中间和最终分布的熵。回想一下，我们的总体目标是识别和建模一个具有低维支撑的（连续）分布。这类分布的微分熵总是负无穷大；它们的支撑集的体积总是零。那么，在所有能够生成相同数据样本的分布中，我们如何比较哪一个更好呢？本节旨在解决这些重要问题。

在本章的其余部分，我们将讨论一个框架，它允许我们将学习到的分布模型与一个明确可计算的编码和解码方案联系起来，遵循 \Cref{sub:min_entropy} 末尾建议的第二种方法。正如我们将看到的，这种方法本质上允许我们根据与编码方案相关的（有损）编码长度或编码率来准确地近似学习到的分布的熵。有了这样的度量，我们不仅可以准确地衡量任何处理（包括去噪）分布所降低的熵，从而获得的信息增益，而且我们还可以推导出能够以最有效的方式执行此类操作的最优算子的显式形式。正如我们将在下一章 \Cref{ch:representation} 中看到的，这将为深度网络架构提供一个有原则的解释，并导致更高效的深度架构设计。

\subsection{有损编码的必要性}

%\yima{I need to completely rewrite the  transition below as motivation for rate distortion... }

%\yima{Lossy coding does more than just quantization. It allows us to avoid pathological solution by simply using the empirical distribution as the optimal solution that minimizes entropy. How to elaborate this point?}

我们之前多次讨论过一个难题：如果我们最终从有限样本中学习分布，并且我们的去噪器函数类包含足够多的函数，我们如何确保我们从\textit{真实}分布（具有低维支撑）中采样，而不是从任何其他可能以高概率产生这些有限样本的分布中采样？让我们通过一些具体的例子来揭示一些概念上和技术上的困难。

\begin{example}[体积、维度和熵]\label{eg:measures-of-complexity}
对于图 \ref{fig:1d-line} 顶部所示的例子，假设我们从一条线（比如在二维平面上）上的均匀分布中抽取了一些样本。这条线或样本集的体积为零。
从几何上看，在产生的有限样本集上的经验分布是能够产生该有限样本集的{\em 最小维度}分布。\footnote{一组离散样本的维度都为零，而支撑线的维度为一。} 但这与另一个复杂性度量——熵——似乎形成了对比。这条线的（微分）熵是负无穷，但这个样本集的（离散）熵是有限且为正的。因此，根据这几种复杂性度量，我们似乎陷入了一个两难的境地：有些度量根本无法区分低维支撑的分布，有些则似乎以完全相反的方式区分它们。\footnote{当然，严格来说，微分熵和离散熵是不能直接比较的。}
\end{example}

\begin{example}[密度]\label{eg:density} 考虑图 \ref{fig:1d-line} 中所示的两组采样数据点。从几何上看，它们本质上是相同的：每组都包含八个点，每个点以相等的频率 $1/8$ 出现。唯一的区别是，对于第二组数据，一些点“足够近”，可以被看作在其各自的“簇”周围具有更高的密度。哪一个与可能生成这些样本的真实分布更相关？我们如何调和在解释这种（经验）分布时遇到的这类困境？
\begin{figure}[t]
	\centering
	\includegraphics[width=0.7\linewidth]{\toplevelprefix/chapters/chapter3/figs/one-dim-distribution.png}
	\caption{在一条线上观察到的八个点。}
	\label{fig:1d-line}
\end{figure}
\end{example}


% If the (geometric) volume is not a suitable measure of parsimony or compression for a (finite-sampled empirical) distribution, then maybe the {entropy}, or {\em the minimum coding rate}, associated with true probability (density or distribution) of those samples $p(\vx_i)$ is a better measure. Although many results like Theorem \ref{thm:distribution-convergence} \yima{missing reference...} ensure convergence to the true distribution (hence entropy) in the asymptotic regime\footnote{That is, when the sample size goes to infinity.}, it is not so clear when and how one can simultaneously infer the distribution and conduct the compression correctly in the {\em non-asymptotic} regime when we are given only a fixed set of samples, as the cases shown in Figure \ref{fig:1d-line}. It is observed empirically that sampling Algorithm \ref{alg:iterative_denoising} (with diffusion/denoising models) tends to generalize beyond just generating samples from the training set.

% Furthermore, besides the entropy of the data distribution, there are in fact other fundamental constraints and costs associated with implementing an encoding-decoding scheme for any given data set. This section attempts to address some of these issues from a constructive perspective regarding learning and representing a data distribution from a finite set of samples.


%\yima{From computable (entropy) to implementable. That is, in addition to a computable or achievable encoding scheme, there should also exist an implementable decoding scheme. Lossy quantization becomes necessary for real-valued data.}

在为一个数据集构建显式编码和解码方案时，还存在另一个技术难题。给定一个采样数据集 $\X = [\vx_1, \ldots, \vx_N]$，如何设计一个可以在具有有限内存和计算资源的机器上实现的编码方案？注意，即使表示一个一般的实数也需要无限多的数字或比特。因此，人们可能会想，一个分布的熵是否是其（最优）编码方案复杂度的直接度量。我们用另一个简单的例子来探讨这个问题。
\begin{example}[精度] \label{eg:two-inrational}
	考虑一个离散分布 $\X = [e, \pi]$，它以相等的概率 $1/2$ 取欧拉数 $e \approx 2.71828$ 或圆周率 $\pi \approx 3.14159$ 的值。这个分布的熵是 $H =1$，这表明可以用一个一位的二进制数 $0$ 或 $1$ 来分别编码这两个数。但是你能在有限状态机上实现这个编码的解码方案吗？答案其实是否定的，因为精确描述这两个数中的任何一个都需要无限多的比特。
\end{example}

因此，通常不可能有一个能够精确再现任意实值分布样本的编码和解码方案。\footnote{也就是说，如果想要精确编码这样的样本，唯一的方法就是记住每一个样本。} 但是，如果一个编码方案不能解码出从同一分布中抽取的样本，那么它几乎没有实际价值。

因此，为了确保任何编码/解码方案在有限的内存和计算资源下是可计算和可实现的，我们需要量化样本 $\x$，并仅以一定的精度（比如 $\epsilon > 0$）对其进行编码。{\em 这样做，本质上，如果两个数据点的距离小于 $\epsilon$，我们就将它们视为等价的。} 更准确地说，我们希望考虑编码方案
\begin{equation}
	\x \mapsto \hat \x
\end{equation}
使得由量化引起的期望误差以 $\epsilon$ 为界。在数学上更方便，并且在概念上几乎相同的是，将期望\textit{平方}误差以 \(\epsilon^{2}\) 为界，即
\begin{equation}
	\Ex[d(\vx, \hat \vx)^{2}] \le \epsilon^{2}.
\end{equation}
通常，距离 \(d\) 被选为欧几里得距离，或 2-范数。\footnote{更一般地，我们可以用任何所谓的\textit{散度}来代替 \(d^{2}\)。}

\subsection{率失真}
当然，在所有满足上述约束的编码方案中，我们希望选择能最小化最终编码率的那个。对于一个给定的随机变量 $\x$ 和一个精度 $\epsilon$，这个率被称为{\em 率失真}，记为 $\cR_{\epsilon}(\x)$。事实证明，对于一个实值连续随机向量，这个率是 $\x$ 的熵 $H(\x)$ 的一个可实现的近似，意义如下：
\begin{equation}
	\cR_{\epsilon}(\x) = \min_{p(\hat \x \mid \x): \Ex[d(\x, \hat \x)^{2}] \le \epsilon^{2}} [H(\x) - H(\x \mid \hat{\x})],
    \label{eqn:rate-distortion-general}
\end{equation}
其中最小化是在所有满足失真约束 $\Ex_{\x, \hat \x}[d(\x, \hat \x)^{2}] \le \epsilon^{2}$ 的条件分布 $p(\hat \x \mid \x)$ 上进行的。从定义中我们知道，$\cR_{\epsilon}(\x)$ 是一个关于 $\epsilon$ 的{\em 非增}函数。

\begin{remark}
	注意，在信息论中，量 $H(\x) - H(\x \mid \hat \x)$ 被称为 $\x$ 和 $\hat \x$ 两个变量之间的{\em 互信息} $I(\x; \hat \x)$。更多细节请参见 \cite{Cover-Thomas}。因此，当 $\x$ 和 $\hat \x$ 之间的互信息在所有满足约束 $\mathbb{E} [d(\x, \hat \x)] \le \epsilon$ 的分布中被最小时，达到最小编码率。根据互信息的性质 $I(\x; \hat \x) = I(\hat \x; \x)$，我们有
	\begin{equation}
        H(\x) - H(\x \mid \hat{\x}) = H(\hat\x) - H(\hat\x \mid {\x}).
    \end{equation}
    因此，上述率失真 \eqref{eqn:rate-distortion-general} 也可以重写为：
    \begin{equation}
		\cR_{\epsilon}(\x) = \min_{p(\hat \vx \mid \vx): \mathbb{E} [d(\x, \hat \x)^{2}] \le \epsilon^{2}} [H(\hat \x) - H(\hat \x \mid \x)].  \label{eqn:rate-distortion-general-2}
	\end{equation}
\end{remark}

\begin{remark}
	注意，给定一组数据点 $\X = [\x_1,\ldots, \x_N]$，总可以将其解释为来自一个在这些 $N$ 个向量上具有相等概率 $1/N$ 的均匀离散分布的样本。这样一个分布的熵是 $H(\X) = \frac{1}{N}\log_2 N$。\footnote{再次注意，即使我们可以用这个编码率来编码这些向量，我们也无法以任意精度解码它们。} 然而，即使 $\X$ 是其样本上的一个均匀分布，如果这些样本分布不那么均匀，并且许多样本聚集在一起，那么用有损编码方案可实现的编码率 $\cR_{\epsilon}(\X)$ 可能会显著低于 $H(\X)$。因此，对于图 \ref{fig:1d-line} 中所示的第二种分布，对于一个适当选择的量化误差 $\epsilon$，可实现的有损编码率可以显著低于将其编码为均匀分布的率。\footnote{然而，对于这个离散均匀分布，当 $\epsilon$ 足够小时，我们总是有 $H(\X) = \cR_{\epsilon}(\X)$。} 另外注意，有了率失真的概念，在例 \ref{eg:two-inrational} 中讨论的困难也消失了：我们可以选择两个足够接近每个无理数的有理数。由此产生的编码方案将具有有限的复杂度。
\end{remark}

\begin{example}\label{example:sphere-covering-rate-distortion}
	有时，我们可能会面临相反的情况，即我们想先固定编码率，然后试图找到一个最小化失真的编码方案。例如，假设我们只想对从一个分布中采样的点使用固定数量的码，并且我们想知道如何设计这些码，使得在编码/解码方案中平均或最大失真最小化。例如，给定一个单位正方形上的均匀分布，我们想知道用比如说 $n$ 比特能多精确地编码从这个分布中抽取的点。这个问题等价于问，用 $2^n$ 个半径为多少的圆盘可以覆盖这个单位正方形，使得这个半径（即失真）最小。图 \ref{fig:seven-circles-packing} 显示了用 \(n = 4, 7, 10\) 个圆盘（即 \(2^{n} = 16, 128, 1024\) 个圆盘）近似最优地填充一个正方形的情况。注意，随着圆盘数量 \(2^{n}\) 的增加，圆盘的最优半径减小。
	\begin{figure}
		\centering
		\includegraphics[width=0.3\textwidth]{\toplevelprefix/chapters/chapter3/figs/sphere_covering_n4.png}
		\hfill
		\includegraphics[width=0.3\textwidth]{\toplevelprefix/chapters/chapter3/figs/sphere_covering_n7.png}
		\hfill
		\includegraphics[width=0.3\textwidth]{\toplevelprefix/chapters/chapter3/figs/sphere_covering_n10.png}

		\caption{用 \(2^{4}\)、\(2^{7}\) 和 \(2^{10}\) 个圆盘填充一个正方形的最优解，以及相应的半径。}
		\label{fig:seven-circles-packing}
	\end{figure}

	% \DP{I actually do not understand this calculation... definitely I don't understand why these problems are equivalent (though obviously they are related). Let us chat soon about this and clarify.}
\end{example}

上述例子可以推广，以表明随机变量 \(\vx\) 的率失真与 \(\vx\) 支撑集的所谓\textit{覆盖数}非常密切相关，前提是这个支撑集是一个紧集（例如，有界的）。实际上，覆盖数正是能够覆盖 \(\vx\) 支撑集的有限半径球的数量。我们在下面的命题中形式化这个概念，其证明将在 \Cref{exer:prop cover} 中给出。

\begin{proposition}\label{prop:covering-number-rate-distortion}
	假设 \(\vx\) 是一个随机变量，其支撑集 \(K \doteq \Supp(\vx)\) 是一个紧集。定义\textit{覆盖数} \(\cN_{\epsilon}(K)\) 为能够覆盖 \(K\) 的半径为 \(\epsilon\) 的球的最小数量，即
	\begin{equation}
		\cN_{\epsilon}(K) \doteq \min\left\{n \in \bN \colon \exists \vp_{1}, \dots, \vp_{n} \in K\ \text{s.t.}\ K \subseteq \bigcup_{i = 1}^{n}B_{\epsilon}(\vp_{i})\right\},
	\end{equation}
	其中 \(B_{\epsilon}(\vp)\) 通常是中心在 \(\vp\) 的半径为 \(\epsilon\) 的欧几里得球。
	那么有
	\begin{equation}
		\cR_{\epsilon}(\vx) \leq \log_{2} \cN_{\epsilon}(K).
	\end{equation}
	此外，当 \(\vx\) 在 \(K\) 上均匀分布时，这个不等式是紧的。
\end{proposition}
如果 \(\vx\) 没有紧支撑集，但是是良性的（例如，具有高斯类尾部），那么它可以被一个紧支撑的随机变量近似（比如说，通过将其归一化到某个半径为 \(R\) 的大单位球上，即 \(\vx \mapsto \vx / \min(\|\vx\|_2, R)\)），此时上述定理适用。因此，率失真可以被认为是一种“概率感知”的方式，用许多小球的混合来近似 \(\vx\) 分布的支撑集。

\begin{remark}
	对于小而有限的 \(\epsilon\)，使用有损编码 \(\hat \vx\) 的分布而不是原始数据 \(\vx\) 的分布要容易得多。虽然它近似于 \(\vx\) 的分布，但它是离散的，因此具有明确定义的熵；它也具有明确定义的体积。因此，这种通过球填充的有限率失真方法重新启用或推广了所有先前的分布复杂性度量，使我们能够以统一的方式区分和排序不同的分布。

	特别是，为简单起见，假设 \(\vx\) 在一个紧集（可能是低维的）上均匀分布，那么在 \(\hat{\vx}\) 的性质与 \(\vx\) 的性质之间存在以下显著的恒等式。回想一下，\(\hat{\vx}\) 依赖于 \(\epsilon\)，并设 \(V_{D}(\epsilon) = C_{D}\epsilon^{D}\) 是 \(\R^{D}\) 中一个 \(\epsilon\)-球的体积（其中 \(C_{D}\) 是一个依赖于维度 \(D\) 的常数），我们在一些一般情况下有
	\begin{align}
		\operatorname{vol}(\Supp(\vx))
		&= \lim_{\epsilon \to 0}\operatorname{vol}(\Supp(\hat \vx)) = \lim_{\epsilon \to 0}V_{D}(\epsilon)2^{\cR_{\epsilon}(\vx)}, \\
		\dim(\Supp(\vx))
		&= \lim_{\epsilon \to 0}\frac{\cR_{\epsilon}(\vx)}{\log(1/\epsilon)}, \\
		h(\vx)
		&= \lim_{\epsilon \to 0}\bs{\cR_{\epsilon}(\vx) + H(\hat{\vx} \mid \vx) + \log V_{D}(\epsilon)}.
	\end{align}
	第一个和第二个表达式来自于将编码率或率失真解释为覆盖。特别是第二个表达式来自于 Minkowski 维度的定义 \citep{bishop2017fractals}，并且在 \(\vx\) 的支撑集不太病态（例如，一个低维流形）时成立。第三个表达式来自于展开 \(\vx\) 的微分熵的积分定义，并用一个适当的黎曼和来近似它，然后为得到的熵写出 \(H(\hat{\vx}) = \cR_{\epsilon}(\vx) + H(\hat{\vx} \mid \vx)\)；填写这个证明的细节留作练习。

	虽然这三个极限的某些参数随着 \(\epsilon \to 0\) 而发散到 \(\infty\)，但它们仍然可以在有限的 \(\epsilon\) 下作为好的近似进行比较。因此，我们可以严格地根据有限 \(\epsilon\) 下的率失真来理解所有先前的复杂性度量。
\end{remark}

对于一个一般的分布，通常不可能以解析形式找到其率失真函数。人们通常求助于数值计算\footnote{感兴趣的读者可以参考 \cite{Blahut-1972}，了解一个经典算法，该算法为离散分布数值计算率失真函数。}。然而，正如我们将看到的，在我们的背景下，我们通常需要知道率失真作为一组数据点或其表示的显式函数。这是因为我们想用编码率作为表示优劣的度量。一个显式的解析形式使得确定如何变换数据分布以改进表示变得容易。因此，我们应该处理那些率失真函数具有显式解析形式的分布。为此，我们从最简单，也是最重要的分布族开始。

\subsection{低维高斯分布的有损编码率}\label{subsec:lossy DR}
%\yima{Lossy coding length, encoding and decoding via sphere packing, rate distortion for a Gaussian or a low-dimensional subspace. }
现在假设我们给定了一组来自任何分布的数据样本 $\X = [\x_1, \ldots, \x_N]$。\footnote{或者这些数据点本身可以被看作一个（经验）分布。} 我们希望提出一个构造性的方案，能够以一定的精度编码数据，比如说
\begin{equation}
	\x_i \mapsto \hat \x_i, \quad \mbox{满足} \quad \|\x_i - \hat \x_i\|_2 \le \epsilon.
\end{equation}
注意，这是一个充分、明确且可解释的条件，它确保了数据被编码，使得 \(\frac{1}{N}\sum_{i = 1}^{N} d(\x_i, \hat \x_i)^{2} \le \epsilon^{2}\)。后一个不等式正是所提供的经验分布及其编码的率失真约束。例如，在 \Cref{example:sphere-covering-rate-distortion} 中，我们使用这个简化的准则来明确地找到给定编码率下的最小失真和显式编码方案。

不失一般性，我们假设 $\X$ 的均值为零，即 $\frac{1}{N} \sum_{i = 1}^{N} \x_i = \boldsymbol{0}$。在没有任何关于 $\X$ 背后分布性质的先验知识的情况下，我们可以将 $\X$ 视为从一个高斯分布 $\mathcal{N}(\boldsymbol{0}, {\boldsymbol{\Sigma}})$ 中采样的，其协方差为\footnote{众所周知，在给定固定方差的情况下，高斯分布的熵最大。也就是说，它给出了在可能的编码率方面的最坏情况的上界。}：
\begin{equation}
	{\boldsymbol{\Sigma}} = \frac{1}{N} \X\X^\top.
\end{equation}
从几何上看，${\boldsymbol{\Sigma}}$ 刻画了一个椭球区域，大多数样本 $\x_i$ 都位于其中。

我们可以将 $\hat \X = [\hat \x_1,\ldots, \hat \x_N]$ 视为 $\X = [\x_1, \ldots, \x_N]$ 的一个带噪版本：
\begin{equation}
	\hat \x_i = \x_i + \vw_i
\end{equation}
其中 $\vw_i$ 是一个高斯噪声 $\vw_i  \sim \mathcal{N}(\boldsymbol{0} , {\epsilon^2}  \boldsymbol{I}/{D})$，独立于 $\vx_i$。那么 $\hat \x_i$ 的协方差由下式给出：
\begin{equation}
	\hat{\boldsymbol{\Sigma}} = \mathbb{E}\left[\hat \x_i \hat \x_i^\top\right] = \frac{\epsilon^2}{D} \boldsymbol{I} + \frac{1}{N} \X\X^\top.
\end{equation}
注意，由向量 $\x_i$ 张成的区域的体积与协方差矩阵的行列式的平方根成正比
\begin{equation}
	\mbox{volume}(\hat \x_i) \propto \sqrt{\det \big(\hat{\boldsymbol{\Sigma}}\big)} = \sqrt{\det\left(\frac{\epsilon^2}{D} \boldsymbol{I} + \frac{1}{N} \X\X^\top \right)}.
\end{equation}
每个随机向量 $\boldsymbol{w}_i$ 张成的体积与
\begin{equation}
	\mbox{volume}(\boldsymbol{w}_i) \propto   \sqrt{\det\left(\frac{\epsilon^2}{D} \boldsymbol{I} \right)}.
\end{equation}
成正比。

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{\toplevelprefix/chapters/chapter3/figs/Gaussian-compression.png}
	\caption{在数据向量张成的区域内填充 $\epsilon$-球。空间的体积越大，需要的球就越多，因此需要更多的比特来编码和枚举这些球。每个实值向量 $\vx$ 都可以被编码为它落入的球的编号。}
	\label{fig:ball-packing}
\end{figure}

为了编码落入由 $\hat \x_i$ 张成的区域的向量，我们可以用半径为 $\epsilon$ 的不重叠的球来覆盖该区域，如图 \ref{fig:ball-packing} 所示。当由 $\hat \x_i$ 张成的区域的体积显著大于 $\epsilon$-球的体积时，我们需要覆盖该区域的总球数近似等于两个体积的比值：
\begin{equation}
	\# \,\epsilon\mbox{-球} \approx \frac{\mbox{volume}(\hat \vx_i)}{\mbox{volume}(\vw_i)} = \sqrt{\det\left(\boldsymbol{I} + \frac{D}{N\epsilon^2} \X\X^\top  \right)}.
\end{equation}

如果我们用二进制数来标记感兴趣区域中的所有 $\epsilon$-球，所需的总二进制比特数是 %\DP{suggest using notation \(R_{\eps}\) since the ``conditional'' notation will become overused in future.}
\begin{equation} 
	\cR_{\epsilon}(\X) \doteq \log_2 (\# \,\epsilon\mbox{-球}) \approx R_{\epsilon}(\X) \doteq \frac{1}{2} \log \det \left(\boldsymbol{I} + \frac{D}{N\epsilon^2} \X\X^\top \right).
	\label{eqn:rate-Gaussian}
\end{equation}

\begin{example}
	图 \ref{fig:ball-packing} 展示了一个具有椭球支撑的二维分布的例子——近似于一个二维高斯分布的支撑。该区域被大小为 $\epsilon$ 的小球覆盖。所有的球从 $1$ 编号到比如说 $n$。然后，给定该区域中的任何向量 $\x$，我们只需要确定它最接近哪个 $\epsilon$-球的中心，记为 $\operatorname{ball}_{\epsilon}(\x)$。为了记住 $\x$，我们只需要记住这个球的编号，这需要 $\log(n)$ 比特来存储。如果我们需要从这个编号解码 $\x$，我们只需将 $\hat \x$ 作为球的中心。这导致了一个明确的编码和解码方案：
	\begin{equation}
		\x \longrightarrow \operatorname{ball}_{\epsilon}(\x) \longrightarrow \hat \x = \mbox{center of} \operatorname{ball}_{\epsilon}(\x).
	\end{equation}
	人们可以将这些球心称为编码方案的码本或字典中的“码字”。很容易看出，这个（有损）编码-解码方案的精度大约是球的半径 $\epsilon$。显然，$\cR_{\epsilon}(\Z)$ 是用这个编码方案编码每个向量 $\z$ 的球编号所需的平均比特数，因此被称为与此方案相关的{\em 编码率}。
\end{example}


从上面的推导中，我们知道编码率 $\cR_{\epsilon}(\X)$ 是（近似）可以通过一个明确的编码（和解码）方案实现的。它有两个有趣的性质：
\begin{itemize}
	\item 首先，人们可能会注意到 $\cR_{\epsilon}(\X)$ 与高斯源的率失真函数非常相似 \cite{Cover-Thomas}。实际上，当 $\epsilon$ 很小时，上述表达式是高斯源率失真的一个很好的近似，正如 \cite{MaY2007-PAMI} 所指出的。
	\item 其次，如果数据 $\X$ 被假定来自一个线性子空间，那么同样的闭式编码率 $R_{\epsilon}(\X)$ 也可以被推导为 \(\cR_{\epsilon}(\vX)\) 的一个近似。这可以通过适当地量化 $\X$ 的奇异值分解（SVD）$\X = \boldsymbol{U} \boldsymbol{\Sigma}\boldsymbol{V}^\top$ 并为由 $\boldsymbol{U}$ 张成的子空间中的向量构建一个有损编码方案来证明 \cite{MaY2007-PAMI}。
\end{itemize}
在我们的背景下，闭式表达式 $R_{\epsilon}(\X)$ 是相当基础的：它是与一个明确且自然的有损编码方案相关的编码率，该方案适用于从高斯分布或线性子空间中抽取的数据。正如我们将在下一章中看到的，这个公式在理解深度神经网络的架构方面扮演着重要的角色。


\subsection{聚类低维高斯混合模型}
\label{sec:clustering-Gaussians}
正如我们之前讨论的，给定的数据集 $\X$ 通常具有低维的内在结构。因此，将其编码为一个一般的高斯分布将是非常冗余的。如果我们能识别出 $\X$ 中的那些内在结构，我们就可以设计出更好的编码方案，从而得到更低的编码率。或者等价地说，用于编码这样的 $\X$ 的码可以被压缩。我们将看到，压缩提供了一种统一的可计算方法来识别这些结构。在本节中，我们将用最基本的低维结构族——（低维）高斯或子空间的混合——来展示这个重要的思想。

\begin{example}
	图 \ref{fig:two-subspaces} 展示了一个例子，其中数据 $\X$ 分布在两个子空间（或低维高斯）周围。如果将它们视为一个单一的高斯分布并一起编码，相关的离散（有损）码本，由所有蓝色球表示，显然是非常冗余的。我们可以尝试识别两个子空间的位置，记为 $S_1$ 和 $S_2$，并设计一个只覆盖这两个子空间的码本，即绿色球。如果我们能正确地将数据 $\X$ 中的样本划分到两个子空间中：$\X = [\X_1, \X_2]\bm \Pi$，其中 $\X_1 \in S_1$ 和 $\X_2 \in S_2$，$\bm \Pi$ 表示一个置换矩阵，那么数据的最终编码率将会低得多。这为数据提供了一个更简约，因此更理想的表示。
\end{example}

\begin{figure}
	\centering
	\includegraphics[width=0.5\linewidth]{\toplevelprefix/chapters/chapter3/figs/Two-subspaces.png}
	\caption{两种有损编码方案的比较，用于分布在两个子空间周围的数据。一种是为两个子空间张成的整个空间填充（蓝色）$\epsilon$-球；另一种是只在两个子空间周围的管状邻域内填充球。后者显然具有更小的码本，并导致子空间上样本的编码率低得多。}
	\label{fig:two-subspaces}
\end{figure}

所以，更一般地说，如果数据是从任何子空间或低维高斯混合模型中抽取的，那么识别这些分量并根据这些分量的内在维度对数据进行编码将是可取的。事实证明，假设数据是从低维高斯混合模型中抽取的，我们不会失去太多的一般性。这是因为高斯混合模型可以很好地近似大多数一般分布 \cite{borkar2016gaussian}。

\paragraph{聚类问题。}
现在对于这个特定的分布族，我们如何从一组样本中有效且高效地识别那些低维分量
\begin{equation}
	\X = \left[\x_1, \x_2, \ldots, \x_N\right],
\end{equation}
这些样本是从它们中抽取的？换句话说，给定整个数据集 $\X$，我们希望将其划分或聚类成多个，比如说 $K$ 个子集：
\begin{equation}
	\X\bm \Pi = [\X_1, \X_2, \dots, \X_K],
\end{equation}
其中每个子集仅包含从一个低维高斯或子空间中抽取的样本，而 $\bm \Pi$ 是一个置换矩阵，用于指示划分的成员关系。注意，根据情况，划分可以是确定性的，也可以是概率性的。如 \cite{ma2007segmentation} 所示，对于高斯混合模型，概率性划分不会导致更低的编码率。因此，为简单起见，我们这里只考虑确定性划分。

\paragraph{通过有损压缩进行聚类。}
解决上述聚类问题的主要困难在于，我们通常不知道聚类的数量 $K$，也不知道每个分量的维度。对这个聚类问题的研究历史悠久。教科书 \cite{GPCA} 对这个问题的不同方法给出了系统而全面的介绍。为了找到解决这个问题的有效方法，我们首先需要理解和阐明我们为什么要进行聚类。换句话说，与不聚类相比，我们从聚类中究竟获得了什么？我们如何衡量这种增益？从数据压缩的角度来看，一个正确的聚类应该能带来更高效的编码（和解码）方案。

对于任何给定的数据集 $\X$，已经有两种明显的编码方案作为基准。它们代表了两种极端的编码数据的方式：
\begin{itemize}
	\item 简单地将所有样本视为从一个单一的高斯分布中抽取的。相关的编码率，如前所述，由下式给出：
	      \begin{equation}
		      \cR_{\epsilon}(\vX) \approx R_{\epsilon}(\X) = \frac{1}{2} \log \det \left(\boldsymbol{I} + \frac{D}{N\epsilon^2} \X\X^\top \right).
	      \end{equation}
	\item 简单地通过为每个样本分配一个不同的编号来单独记忆所有样本。编码率将是：
	      \begin{equation}
		      \cR_0(\X) = \log(N).
	      \end{equation}
\end{itemize}


注意，对于某些（极端的）量化误差 $\epsilon$ 的选择，这两种编码方案中的任何一种都可能成为“最优”解：
\begin{enumerate}
	\item {\em 懒惰机制}：如果我们选择的 $\epsilon$ 极大，那么 $\X$ 中的所有样本都可以被一个球覆盖。此时的编码率为 $\lim_{\epsilon \rightarrow \infty} \cR_\epsilon \rightarrow \frac{1}{2}\log\det (\boldsymbol{I}) = 0$。
	\item {\em 记忆机制}：如果 $\epsilon$ 极小，那么 $\X$ 中的每个样本都会被一个不同的 $\epsilon$-球覆盖，因此总共有 $N$ 个球。此时的编码率为 $\lim_{\epsilon \rightarrow 0} \cR_\epsilon \rightarrow \log(N)$。
\end{enumerate}
注意，第一种方案对应于人们完全不关心分布的任何有趣特性的情况。人们不想为任何有信息量的内容花费任何比特。我们称之为“懒惰机制”。第二种方案对应于人们希望以极高的精度解码每个样本的情况。所以最好“记住”每个样本。我们称之为“记忆机制”。
\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{\toplevelprefix/chapters/chapter3/figs/circle-packing.png}
	\caption{在二维平面上的一些随机样本。考虑为每个样本分配一个以该样本为中心的 $\epsilon$-圆盘。样本的密度从左到右增加。}
	\label{fig:circle-packing}
\end{figure}
\begin{example}
	为了看看何时偏好记忆机制，让我们考虑在二维平面的一个单位面积内随机分布的 $N$ 个样本。\footnote{比如说，这些点是由密度为每单位面积 $N$ 个点的泊松过程抽取的。} 想象一下，我们试图设计一个具有固定量化误差 $\epsilon$ 的有损编码方案。这等同于在每个样本周围放置一个 $\epsilon$-圆盘，如图 \ref{fig:circle-packing} 所示。当 $N$ 很小时，所有圆盘相互重叠的几率为零。在这种情况下，大小为 $N$ 的码本是必要且最优的。当 $N$ 或密度达到某个临界值 $N_c$ 时，所有圆盘很可能开始重叠并连接成一个覆盖整个平面的簇——这种现象被称为连续“逾渗” \cite{Gilbert-1961,Mertens-Moore-2012}。当 $N$ 变得大于这个值时，圆盘会严重重叠。$N$ 个圆盘的数量变得非常冗余，因为我们只想以给定的精度 $\epsilon$ 编码平面上的点。覆盖所有样本所需的圆盘数量远小于 $N$。\footnote{事实上，有高效的算法可以找到这样的覆盖 \cite{Booth-2001}。}
\end{example}

懒惰机制和记忆机制都有些琐碎，也许在理论或实践上兴趣不大。当用于编码从一个具有{\em 紧凑且低维支撑}的分布中抽取的大量样本时，这两种方案都远非最优。有趣的机制存在于这两者之间。
\begin{example}
	\begin{figure}[t]
		\centering
		\includegraphics[width=0.4\linewidth]{\toplevelprefix/chapters/chapter3/figs/Two-lines-and-plane.png}
		\includegraphics[width=0.9\linewidth]{\toplevelprefix/chapters/chapter3/figs/Coding-Rate.jpg}
		\caption{上图：从 $\mathbb{R}^3$ 中的两条线和一个平面抽取的 358 个带噪样本。下图：改变 $\epsilon$ 对聚类结果和编码率的影响。红线标记了添加到样本中的高斯噪声的方差 $\epsilon_0$。}
		\label{fig:two-lines-and-plane}
		\label{fig:two-lines-and-plane-epsilon}
	\end{figure}
	图 \ref{fig:two-lines-and-plane} 展示了一个例子，其中带噪样本是从 $\mathbb{R}^3$ 中的两条线和一个平面抽取的。正如我们从右侧的图 (c) 中注意到的，最优编码率随着我们增加 $\epsilon$ 而单调递减，这与率失真函数的性质相符。图 (a) 和 (b) 显示，当将 $\epsilon$ 从非常小（接近零）变化到非常大（趋于无穷）时，当编码率最小时的最优聚类数。我们可以清楚地看到图两端的懒惰机制和记忆机制。但人们也可以在图 (b) 中注意到，当量化误差 $\epsilon$ 被选择在真实噪声方差 $\epsilon_0$ 的水平附近时，最优聚类数是“正确”的数字三，代表两个平面和一个子空间。我们非正式地将这个中间机制称为“泛化机制”。注意，在这些机制之间发生了急剧的相变。\footnote{到目前为止，据我们所知，对于这些相变行为还没有严格的理论解释。}
\end{example}


从上面的讨论和例子中，我们看到，当量化误差相对于样本密度\footnote{或者样本密度相对于量化误差}在一个合适的范围内时，最小化有损编码率将使我们能够揭示采样数据的底层（低维）分布。{\em 因此，最初作为实用性选择的量化，似乎正在成为从其有限样本的经验分布中学习连续分布所必需的。} 尽管解释这种现象的严格理论仍然难以捉摸，但在这里，为了学习的目的，我们关心的是如何利用这种现象来设计能够找到正确分布的算法。

让我们用图 \ref{fig:two-subspaces} 中所示的简单例子来说明基本思想。如果能将 $\X$ 中的所有样本划分为 $\X_1$ 和 $\X_2$ 两个簇，分别有 $N_1$ 和 $N_2$ 个样本，那么相关的编码率将是\footnote{我们这里忽略了编码每个样本成员关系所需的一些开销比特，比如说通过霍夫曼编码。}
\begin{equation}
	R_{\epsilon}^c(\X\mid \boldsymbol{\Pi}) = \frac{N_1}{N}R_{\epsilon}(\X_1) + \frac{N_2}{N}R_{\epsilon}(\X_2),
\end{equation}
其中我们用 $\boldsymbol{\Pi}$ 来表示划分的成员关系。如果划分尊重分布的低维结构，在这种情况下，$\X_1$ 和 $\X_2$ 分别属于两个子空间，那么得到的编码率应该显著小于上述两个基本方案：
\begin{equation}
	R_{\epsilon}^c(\X \mid \boldsymbol{\Pi}) \ll R_{\epsilon}(\X), \quad     R_{\epsilon}^c(\X \mid \boldsymbol{\Pi}) \ll R_0(\X).
\end{equation}
总的来说，我们可以将聚类问题转化为一个最小化编码率的优化问题：
\begin{equation}
	\min_{\boldsymbol{\Pi}}  \bc{ R_{\epsilon}^c(\X \mid \boldsymbol{\Pi}) = \sum_{k=1}^K \frac{N_k}{N}R_{\epsilon}(\X_k)}.
\end{equation}

\paragraph{聚类的优化策略。}
剩下的问题是我们如何优化上述编码率目标以找到最优的簇。对于这个目标，有三种自然的途径：
\begin{enumerate}
	\item 我们可以从将整个集合 $\X$ 作为一个单一的簇开始（即懒惰机制），然后搜索（比如随机地）将其划分，以期得到更小的编码率。
	\item 相反，我们可以从将每个样本 $\x_i$ 作为其自己的簇开始（即记忆机制），然后搜索合并那些能导致更小编码率的簇。
	\item 或者，如果我们能将成员关系 $\boldsymbol{\Pi}$ 表示为一些连续参数，我们可以使用诸如梯度下降（GD）之类的优化方法。
\end{enumerate}
第一种方法在计算上不那么吸引人，因为需要尝试的可能划分数量是样本数量的指数级。例如，将 $\X$ 划分为两个大小相等的子集的数量是 $N \choose N/2$，当 $N$ 变大时，这个数量会爆炸式增长。我们将在下一章 \ref{ch:representation} 中探讨第三种方法。在那里，我们将看到深度神经网络，特别是 Transformer，的角色如何与编码率目标联系起来。



第二种方法最初由 \cite{ma2007segmentation} 的工作提出。它展示了能够高效地（比如用解析形式）评估编码率的好处。有了它，数据的（低维）簇可以通过最小化编码长度（MCL）的原则相当高效且有效地找到。注意，对于一个有 $N_k$ 个样本的簇 $\X_k$，编码 $\X_k$ 中所有样本所需的二进制比特长度由下式给出：\footnote{事实上，一个更精确的编码长度估计是 $L(\X_k) = (N_k+D) R_\epsilon(\X_k)$，其中额外的比特用于编码子空间的基 \cite{ma2007segmentation}。为简单起见，我们这里省略了这个开销。}
\begin{equation}
	L(\X_k) = N_k R_\epsilon(\X_k).
\end{equation}
如果我们有两个簇 $\X_k$ 和 $\X_l$，如果我们想将样本编码为两个独立的簇，所需的二进制比特长度是
\begin{equation*}
	L^c(\X_k, \X_l) = N_k R_\epsilon(\X_k) + N_l R_\epsilon(\X_l) - N_k \log\frac{N_k}{N_k + N_l} - N_l \log\frac{N_l}{N_k + N_l}.
\end{equation*}
最后两项是根据霍夫曼编码编码样本成员关系所需的比特数。

然后，给定任何两个独立的簇 $\X_1$ 和 $\X_2$，我们可以根据两个编码长度的差值来决定是否合并它们：
\begin{equation}
	L(\X_k \cup \X_l) - L^c(\X_k, \X_l)
\end{equation}
是正还是负，其中 $\X_k \cup \X_l$ 表示 $\bm X_k$ 和 $\bm X_l$ 中样本集的并集。如果是负的，意味着如果我们把这两个簇合并成一个，编码长度会变小。这个简单的事实导致了 \cite{ma2007segmentation} 提出的以下聚类算法：
\begin{algorithm}[!htbp]
	\caption{编码长度的成对最速下降}\label{alg:steepest_descent_coding_length}
	\begin{algorithmic}[1]
		\Require{\(N\) 个数据点 \(\{\vx_{i}\}_{i = 1}^{N}\)}
		\Ensure{一个簇的集合 \(\cC\)}

		\Procedure{PairwiseSteepestDescentOfCodingLength}{$\{\vx_{i}\}_{i = 1}^{N}$}
		\State{\(\cC \gets \{\{\bm x_{i}\}\}_{i = 1}^{N}\)} \Comment{初始化 \(N\) 个簇 \(\vX_{k}\)，每个簇含一个元素}
		\While{\(\abs{\cC} > 1\)}
		\If{\(\displaystyle \min_{\vX_{k}, \vX_{l} \in \cC}[L(\vX_{k} \cup \vX_{l}) -L^{c}(\vX_{k}, \vX_{l})] \geq 0\)} \Comment{如果任何合并都不能节省比特}
		\State{\Return{\(\cC\)}} \Comment{提前返回 \(\cC\) 并退出}
		\Else
		\State{\(\displaystyle \vX_{k^{\ast}}, \vX_{l^{\ast}} \gets \argmin_{\vX_{k}, \vX_{l} \in \cC}[L(\vX_{k} \cup \vX_{l}) - L^{c}(\vX_{k}, \vX_{l})]\)} \Comment{合并节省最多比特的簇}
		\State{\(\displaystyle \cC \gets [\cC \setminus \{\vX_{k^{\ast}}, \vX_{l^{\ast}}\}] \cup \{\vX_{k^{\ast}} \cup \vX_{l^{\ast}}\}\)} \Comment{移除未合并的簇并加回合并后的簇}
		\EndIf
		\EndWhile
		\State{\Return{\(\cC\)}}  \Comment{如果所有合并都产生节省，则返回一个簇}
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

注意，这个算法是易于处理的，因为（成对）比较和合并的总数大约是 $O(N^2\log N)$。然而，由于其贪婪的性质，没有理论保证该过程会收敛到全局最优的聚类解。尽管如此，正如 \cite{ma2007segmentation} 所报道的，在实践中，这个看似简单的算法效果非常好。图 \ref{fig:two-lines-and-plane} 中绘制的聚类结果实际上就是由这个算法计算的。

%\subsection{Applications of Subspace  Clustering}

\begin{example}[图像分割]\label{eg:image-segmentation} 上述编码长度的度量和相关的聚类算法假设数据分布是（低维）高斯混合模型。尽管这看起来有些理想化，但当模型（近似）有效时，该度量和算法已经可以非常有用甚至强大。

	例如，一张自然图像通常由多个具有近乎同质纹理的区域组成。如果我们从每个区域取许多小窗口，它们应该类似于从一个（低维）高斯分布中抽取的样本，如图 \ref{fig:image-patch} 所示。图 \ref{fig:image-segmentation} 显示了基于将上述聚类算法直接应用于图像块的图像分割结果。关于将该算法定制到图像分割问题的更多技术细节可以在 \cite{Mobahi-IJCV2011} 中找到。
\end{example}


\begin{figure}
	\centering
	\includegraphics[width=0.4\linewidth]{\toplevelprefix/chapters/chapter3/figs/image-segmentation-tiles.png}
	\caption{大小为 $w\times w$ 像素的图像块。}
	\label{fig:image-patch}
\end{figure}

\begin{figure}[th]
	\centering
	\includegraphics[width=0.8\linewidth]{\toplevelprefix/chapters/chapter3/figs/image-segmentation.png}
	\caption{基于应用于图像块的聚类算法的分割结果。}
	\label{fig:image-segmentation}
\end{figure}

\section{通过信息增益获得信息丰富的表示}
\label{sec:chap4-representation-learning-problem}

% \yaodong{TODO: Connect representation properties defined in Chapter 2. (suggested by Druv)}

% \yaodong{TODO: cut the citations to the ``Notes'' part in the later of the chapter.}

%\pw{add some sentences to connect!}

到目前为止，在本章中，我们已经讨论了如何通过压缩的原则来识别具有低维结构的分布。正如我们从前两节所看到的，计算压缩可以通过去噪操作或聚类来实现。图 \ref{fig:Gaussian-Subspaces} 用我们最喜欢的例子阐述了这个概念。
\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{\toplevelprefix/chapters/chapter3/figs/Gaussian-Subspaces.png}
    \caption{通过去噪或聚类，从一个通用的随机高斯分布（右）开始，识别一个具有两个子空间的低维分布（左）。}
    \label{fig:Gaussian-Subspaces}
\end{figure}
当然，识别数据分布的最终目标是利用它来促进某些后续任务，如分割、分类或（图像）生成。因此，最终分布的“表示”方式极大地影响了与这些后续任务相关的信息如何被高效、有效地检索和利用。这自然引出了一个基本问题：什么使得一个表示对于下游使用真正“好”？在下文中，我们将探讨一个有意义且有用的表示应该具备的基本属性，以及如何通过信息增益明确地刻画和追求这些属性。

% In recent years, deep learning has seen tremendous empirical success in processing
% and modeling massive amounts of high-dimensional and multi-modal data. Much of this success has been attributed to deep networks' ability in effectively learning  ``good representations'' that facilitate many downstream tasks, e.g., classification, recognition and segmentation, and generation of visual data.

\paragraph{如何衡量表示的优劣。}
为了更正式地陈述所有这些实践背后的共同问题，我们可以将给定的数据集视为一个随机向量 $\x$ 的样本，该向量在某个高维空间（比如 $\mathbb{R}^D$）中具有一定的分布。通常，$\x$ 的分布的内在维度远低于环境空间。一般来说，{\em 学习一个表示}指的是学习一个连续映射，比如说 $f(\cdot)$，它将 $\x$ 转换为另一个（通常是低维）空间（比如 $\mathbb{R}^d$，其中 $d < D$）中的所谓{\em 特征向量} $\z$。希望通过这样的映射
\begin{equation}
	\x \in \mathbb{R}^D \xrightarrow{\hspace{2mm} f(\x)\hspace{2mm}} \z  \in \mathbb{R}^d,
	\label{eqn:chap4-1-encoding}
\end{equation}
$\x$ 的低维内在结构被识别并以更紧凑和结构化的方式由 $\z$ 表示，从而促进后续任务，如分类或生成。特征 $\z$ 可以被看作是原始数据 $\x$ 的一个（学习到的）紧凑编码，所以映射 $f$ 也被称为\textit{编码器}。
表示学习的基本问题是
\begin{center}
	\noindent{\em 什么是衡量表示优劣的有原则且有效的方法？}
\end{center}

从概念上讲，表示 $\z$ 的质量取决于它为后续任务识别 $\x$ 的最相关和充分信息的程度，以及它表示这些信息的效率。
长期以来，人们相信并认为，学习到的特征表示的“充分性”或“优劣”应该根据特定任务来定义。例如，在分类问题中，$\z$ 只需要足以预测类别标签 $\y$。下面，让我们从经典的图像分类问题开始，论证为什么这种特定于任务的“表示”概念是有限的，需要被推广。

%To understand the role of deep learning or deep networks in this type of representation learning, \cite{Tishby-ITW2015} proposed the \textit{information bottleneck} framework, which suggests that a measure of feature goodness is to maximize the mutual information between $\z$ and $\y$ while minimizing the mutual information between $\z$ and $\x$.

% Nevertheless, in recent years the predominant practice has been to learn first a \textit{task-agnostic} representation by pre-training a large deep neural network, in some cases known as a \textit{foundation model} \cite{Bommasani2021-vm}.
% The so-learned representation can subsequently be fine-tuned for multiple specific tasks.
% This has been shown to be more effective and efficient for many practical tasks across diverse data modalities, including speech, language, and natural images.
% Notice that representation learning in this context is different from that for a specific task, where $\z$ only needs to be good enough for predicting a specific $\y$. In a task-agnostic setting, the learned representation $\z$ needs to encode \textit{almost all essential information about the distribution of the data $\x$}. That is, the learned representation $\z$ not only is a more compact and structured representation for the intrinsic structures of $\x$, but can also recover $\x$ to a certain degree of faithfulness.
% Hence, it is natural to ask, in the task-agnostic context, what a principled measure of goodness for a learned (feature) representation should be.
% As we know, in recent practice of learning task-agnostic representations, one type of deep architectures, known as transformers \cite{vaswani2017attention}, have emerged as an almost universal choice for the backbone of deep networks, for either discriminative or generative tasks, from language to vision.
% %We will provide more details on how to derive transformer-like architecture via a white-box approach in Section~\ref{sec:chap4-white-box-transformer}.
% As we will see in this chapter, clarifying the principled measure for feature goodness is also the key to fully understand why a transformer-like architecture is suitable for task-agnostic pretraining, as well as to reveal the precise role and function of each layer in transformer-like deep networks.


% \subsection{Principles and Objectives for Representation Learning}


% TODO: compression and sparsity.

% \subsection{Deep Network Architectures: Nonlinear to Linear}


% \section{White-box Deep Networks via Unrolling}\label{sec:chap4-white-box-model-via-unrolling}

% \subsection{LISTA - Sparsity}

% \subsection{ReduNet - Compression}

% \subsection{CRATE - Compression \& Sparsity}



\subsection{线性判别表示}\label{subsec:LDR}
%{\color{red} Outline: Introducing and formulating the principle of {\em information gain} or {\em rate reduction}. Transform the data distribution to maximize information gain. More specifically rate reduction in the important case of mixture of low-rank Gaussians.}

假设 $\bm{x} \in \mathbb{R}^D$ 是一个从 $K$ 个（分量）分布 $\mathcal{D} = \{\mathcal{D}_k\}_{k=1}^K$ 的混合中抽取的随机向量。给定随机向量 $\bm x$ 的一组有限的独立同分布样本 $\X = [\x_1, \x_2, \ldots, \x_N] \in \Re^{D\times N}$，我们{\em 寻求一个好的表示}，通过一个连续映射 $f(\x): \mathbb{R}^D \rightarrow \mathbb{R}^d$ 来捕捉 $\x$ 的内在结构，并最好地促进后续的分类任务。\footnote{分类是深度学习最初取得成功并引发对深度网络爆炸性兴趣的领域。尽管我们的研究侧重于分类，但我们相信这些思想和原则可以自然地推广到其他设置，如回归。} 为了简化学习分布 $\mathcal{D}$ 的任务，在流行的监督分类设置中，为每个样本 $\x_i$ 提供一个真实的类别标签（或每个类别的码字），通常由一个独热向量 $\y_i \in \mathbb{R}^K$ 表示。

 

% Given a random vector $\bm{x} \in \mathbb{R}^D$ that is drawn from a mixture of $k$ (component) distributions  $\mathcal{D} = \{\mathcal{D}^j\}_{j=1}^k$, one of the most fundamental problems is how to effectively and efficiently {\em learn the underlying distribution} from a finite set of i.i.d. samples, say $\X = [\x_1, \x_2, \ldots, \x_N] \in \Re^{D\times N}$. 
% To this end, we {\em seek a good representation}  through a continuous mapping $f(\x): \mathbb{R}^D \rightarrow \mathbb{R}^d$ that captures intrinsic structures of $\x$ and best facilitates the subsequent classification task.\footnote{Classification is where deep learning demonstrated the initial success that has catalyzed the explosive interest in deep networks. Although our study focuses on classification, we believe the ideas and principles can be naturally generalized to other settings, such as regression.} 

\paragraph{通过交叉熵编码类别信息。}  大量研究表明，对于许多实际数据集（例如，图像、音频和自然语言），从数据 $\bm{x}$ 到其类别标签 $\bm{y}$ 的（编码）映射可以通过训练一个深度网络来有效地建模，\footnote{这里我们暂时不担心应该使用哪个网络以及为什么。这里的目的是考虑任何经验上测试过的深度网络。我们将把网络架构的论证留到下一章。} 这里表示为 $$f(\x, \bm \theta):\x \mapsto \y$$，网络参数为 $\bm \theta \in \bm \Theta$，其中 $\bm \Theta$ 表示参数空间。为了使输出 $f(\x, \bm \theta)$ 与标签 $\y$ 很好地匹配，我们希望在训练集 $\{(\x_i, \y_i)\}_{i=1}^N$ 上最小化{\em 交叉熵损失}：
\begin{equation}
   \min_{\bm \theta \in \bm \Theta} \; - \mathbb{E}[\langle \y, \log(f(\x, \bm \theta)) \rangle] \, \approx - \frac{1}{N}\sum_{i=1}^N \langle \y_i, \log\left(f(\x_i, \bm \theta)\right) \rangle.
   \label{chap4-eqn:cross-entropy}
\end{equation}
最优网络参数 $\bm \theta $ 通常通过高效的梯度下降方案优化上述目标来找到，梯度通过反向传播（BP）计算，如附录 \ref{app:optimization} 的 \ref{app:BP-section} 节所述。

尽管这种方法有效且非常流行，但它有两个严重的局限性：1) 它只旨在预测标签 $\y$，即使它们可能被错误标记。经验研究表明，用作“黑箱”的深度网络甚至可以拟合随机标签~\cite{zhang2017understanding}。
2) 通过这种端到端的数据拟合，尽管在试图解释如此学习到的特征方面付出了大量的经验努力~\cite{Zeiler-ECCV2014}，但尚不清楚网络学习到的中间特征在多大程度上捕捉了数据的内在结构，而这些结构正是使有意义的分类成为可能的基础。学习到的特征的精确几何和统计特性也常常被掩盖，这导致深度学习缺乏可解释性和后续的性能保证（例如，泛化性、可迁移性和鲁棒性等）。
因此，{\em 本章的目标之一是通过重新表述目标，以学习对数据 $\x$ 具有明确意义和有用性的表示，而不仅限于分类，来解决这些局限性。}

\begin{figure}
	\centering
	\includegraphics[width=0.95\linewidth]{\toplevelprefix/chapters/chapter3/figs/neural_collapse.pdf}
	\caption{在 CIFAR10 数据集上用 3 个随机选择的类别训练 VGG13 神经网络时，倒数第二层输出的演变。图来自 \cite{papyan2020prevalence}。}
	\label{chap4-fig:neural-collapse}
\end{figure}

\paragraph{通过信息瓶颈获得最小判别特征。}
一种流行的解释深度网络作用的方法是将网络中间层的输出视为选择数据的某些潜在特征 $\z = f(\x, \bm \theta) \in \Re^d$，这些特征在多个类别之间具有判别性。学习到的表示 $\z$ 随后通过优化一个分类器 $g(\z)$ 来促进后续的分类任务，以预测类别标签 $\y$：
\begin{equation}
	\x   \xrightarrow{\hspace{2mm} f(\x, \bm \theta)\hspace{2mm}} \z  \xrightarrow{\hspace{2mm} g(\z) \hspace{2mm}} \y.
\end{equation}
我们从信息论~\cite{Cover-Thomas} 中知道，两个随机变量（比如 $\x,\z$）之间的{\em 互信息}定义为
\begin{equation}
	I(\x, \z) = H(\x) - H(\x\mid \z),
\end{equation}
其中 $H(\x \vert \z)$ 是给定 $\z$ 的 $\x$ 的条件熵。互信息也称为{\em 信息增益}：它衡量一旦给定 $\z$，随机变量 $\x$ 的熵可以减少多少。或者等价地说，它衡量 $\z$ 包含关于 $\x$ 的多少信息。{\em 信息瓶颈}（IB）公式 \cite{Tishby-ITW2015} 进一步假设网络的作用是学习 $\z$ 作为预测 $\y$ 的最小充分统计量。形式上，它寻求最大化 $\z$ 和 $\y$ 之间的互信息 $I(\z, \y)$，同时最小化 $\x$ 和 $\z$ 之间的互信息 $I(\x, \z)$：
\begin{equation}
	\max_{\bm \theta\in \bm \Theta}\; \mbox{IB}(\x, \y, \z) \doteq I(\z, \y) - \beta I(\x, \z) \quad\ \mathrm{s.t.}\ \z = f(\x, \bm \theta),
	\label{chap4-eqn:information-bottleneck}
\end{equation}
其中 $\beta >0$。

如果能克服与此框架相关的一些注意事项 \cite{kolchinsky2018caveats-ICLR2018}，例如如何用有限的退化分布样本准确评估互信息，这个框架可以帮助解释深度网络的某些行为。
例如，最近的工作 \cite{papyan2020prevalence} 确实表明，通过交叉熵损失 \eqref{chap4-eqn:cross-entropy} 学习到的表示表现出一种\emph{神经坍缩}现象。
也就是说，每个类别的特征被映射到一个一维向量，而该类别的所有其他信息都被抑制了，如图~\ref{chap4-fig:neural-collapse} 所示。
% % \footnote{Essentially, once an over-parameterized network fits the training data, regularization (such as weight decay) would collapse weight components or features that are not the most relevant for fitting the class labels. Besides the most salient feature, informative and discriminative features that also help define a class can be suppressed.} 
% where within-class variability and structural information are getting suppressed and ignored, as illustrated in Figure~\ref{chap4-fig:neural-collapse}. 
\begin{remark}
    神经坍缩是指在为分类任务训练的深度神经网络中观察到的一种现象，其中学习到的特征表示和分类器权重在训练的最后阶段表现出高度对称和结构化的行为 \cite{papyan2020prevalence,zhu2021geometric}。具体来说，在每个类别内，特征坍缩到它们的类别均值，而在类别之间，这些均值变得最大程度地分离，形成一个单纯形等角配置。线性分类器与类别均值对齐，直到重新缩放。此外，最后一层分类器收敛到选择具有最近训练类别均值的类别。神经坍缩揭示了优化动力学、泛化和监督学习中出现的几何结构之间的深层联系。
\end{remark}

从上面的分类例子中，我们看到，如此学习到的表示给出了一个非常简单的编码器，它本质上将每一类数据映射到仅一个码字：代表每个类别的独热向量。从有损压缩的角度来看，这样的编码器对于保留数据分布中的信息来说损失太大了。其他信息，例如对图像生成等任务有用的信息，在这样的监督学习过程中严重丢失。为了弥补这种情况，我们希望学习一种不同的编码方案，使得最终的特征表示能够捕捉到关于数据分布的更丰富的信息，而不仅限于对分类有用的信息。

% Here by being task-dependent (depending on the label $\y$) and seeking a {\em minimal} set of most informative features for the task at hand (for predicting the label $\y$ only), the so learned network may sacrifice robustness in case the labels can be corrupted or transferability when we want to use the features for different tasks. To address this, we

% {\em the framework -- we will present in this chapter -- uses the label $\y$ as only side information to assist learning discriminative yet diverse (not minimal) representations; these representations optimize a different intrinsic objective based on }

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.99\linewidth]{\toplevelprefix/chapters/chapter3/figs/Expansion2.png}
	\caption{在识别出低维数据分布后，我们希望进一步将数据分布转换为更具信息量的结构表示：$R$ 是填充在整个空间中的 $\epsilon$-球的数量，$R^c$ 是所有子空间（绿色球）的数量之和。$\Delta R$ 是它们的差值（蓝色球的数量）。}\label{fig:sphere-packing}
	\label{fig:informative-representation}
\end{figure}


\paragraph{线性判别表示。}
给定的混合分布 $\mathcal{D}$ 的数据 $\X$ 是否能被有效分类或聚类，取决于分量分布 $\mathcal{D}_k$ 的可分性（或判别性）有多强（或可以变得多强）。一个流行的工作假设是，每个类别的分布都具有相对{\em 低维}的内在结构。因此，我们可以假设每个类别的分布 $\mathcal{D}_k$ 的支撑集位于一个低维子流形上，比如说 $\mathcal{M}_k$，其维度为 $d_k \ll D$，而 $\x$ 的分布 $\mathcal D$ 的支撑集位于这些子流形的混合体上，即 $\mathcal M = \cup_{k=1}^K \mathcal{M}_k$，在高维环境空间 $\Re^D$ 中。

我们不仅需要识别低维分布，还希望以一种最能促进后续任务（如分类、聚类和条件生成，我们将在未来看到）的形式来表示该分布。为此，我们要求我们学习到的特征表示具有以下属性：
\begin{enumerate}
	\item {\em 类内可压缩性：} 来自同一类别的样本的特征应该在它们属于一个低维线性子空间的意义上是强{\em 相关}的。
	\item {\em 类间判别性：} 来自不同类别的样本的特征应该高度{\em 不相关}，并属于不同的低维线性子空间。
	\item {\em 最大多样性表示：} 只要每个类别的特征与其他类别不相干，其维度（或方差）应{\em 尽可能大}。
\end{enumerate}
我们将这样的表示称为{\em 线性判别表示}（LDR）。注意，第一个属性与我们在第 \ref{sub:pca} 章中讨论的经典{\em 主成分分析}（PCA）的目标非常一致。第二个属性类似于经典的{\em 线性判别分析}（LDA）~\cite{HastieTiFr09}。图 \ref{fig:informative-representation} 用一个简单的例子说明了这些属性，当数据分布实际上是两个子空间的混合时。通过压缩（去噪或聚类），我们首先识别出真实的数据分布是两个低维子空间的混合（中），而不是一个通用的高斯分布（左）。然后我们希望变换分布，使得两个子空间最终变得相互不相干/独立（右）。

\begin{remark}
    线性判别分析（LDA）~\cite{HastieTiFr09} 是一种监督降维技术，旨在找到数据的线性投影，以最大化类别可分性。具体来说，给定带标签的数据，LDA 寻求一个线性变换，将高维输入投影到一个低维空间，在该空间中类别被最大程度地分离。注意，PCA 是一种无监督方法，它将数据投影到最大方差的方向上，而不考虑类别标签。虽然 PCA 纯粹关注于保留全局方差结构，但 LDA 明确利用标签信息来增强判别能力；参见 \Cref{fig:LDA} 中的比较。
\end{remark}

\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{\toplevelprefix/chapters/chapter3/figs/LDA.png}\vspace{-0.1in}
	\caption{PCA 与 LDA 的比较。图改编自 \url{https://sebastianraschka.com/Articles/2014_python_lda.html}。}
	\label{fig:LDA}
\end{figure}


第三个属性也很重要，因为我们希望学习到的特征能揭示一个类别与所有其他类别不同的所有可能原因。例如，为了区分“苹果”和“橙子”，我们不仅关心颜色，还关心形状和叶子。理想情况下，每个子空间 $\{\mathcal{S}_k\}$ 的维度应该等于相应子流形 $\mathcal{M}_k$ 的维度。如果我们希望映射 $f(\x,\bm\theta)$ 对于像图像生成这样的任务是{\em 可逆的}，这个属性将很重要。例如，如果我们从“苹果”的特征子空间中抽取不同的样本点，我们应该能够将它们解码以生成各种各样的苹果图像。从最小化交叉熵 \eqref{chap4-eqn:cross-entropy} 中学习到的特征显然不具有这个属性。


总的来说，尽管每个类别/簇的内在结构可能是低维的，但它们在其原始表示 $\x$ 中绝非简单的线性（或高斯），它们需要首先通过一些非线性变换被线性化。\footnote{我们将在第 \ref{ch:autoencoding} 章中讨论如何明确地做到这一点。} 因此，总的来说，我们使用非线性变换 $f(\x,\bm\theta)$ 来寻求数据的表示，使得代表所有类别的子空间是最大程度不相干的线性子空间。更准确地说，我们希望学习一个映射 {$\z = f(\x,\bm\theta)$}，它将每个子流形 $\mathcal{M}_k \subset \Re^D$（图~\ref{chap4-fig:mcr-diagram} 左）映射到一个{\em 线性}子空间 $\mathcal{S}_k \subset \Re^d$（图~\ref{chap4-fig:mcr-diagram} 右）。在某种程度上，得到的多个子空间 $\{\mathcal{S}_k\}$ 可以被视为原始数据 $\bm x$ 的判别性{\em 广义主成分} \cite{GPCA}，或者，如果是正交的，则为{\em 独立成分} \cite{hyvarinen2000independent}。
正如我们将在下一章 \ref{ch:representation} 中看到的，深度网络恰恰扮演了建模和实现从数据分布到线性判别表示的这种非线性变换的角色。


%\subsection{The Principle of Maximal Information Gain}
\subsection{最大编码率降低原则}\label{subsec:MCR2}



%\paragraph{Classic  Information Gain.}  The classic  {\em Information Gain} (IG), which aims to maximize the reduction of entropy of a random variable, say $\bm z$, with respect to an observed attribute, say $\bm \pi$:$\max_{\bm \pi} \; \mbox{IG}(\bm z, \bm \pi) \doteq H(\bm z) - H(\bm z \mid \bm \pi),$i.e., the {\em mutual information} between $\z$ and $\bm \pi$ \cite{Cover-Thomas}. Maximal information gain has been widely used in areas such as decision trees \cite{decision-trees}. However, MCR$^2$ is used differently in several ways: 1) A typical setting of MCR$^2$ is when the data class labels are given, i.e. $\bm \Pi$ is known, MCR$^2$ focuses on learning representations $\bm z(\theta)$ rather than fitting labels. 2) In traditional settings of IG, the number of attributes in $\bm z$ cannot be so large and their values are discrete (typically binary). Here, the ``attributes'' $\bm \Pi$  represent the probability of a multi-class partition for all samples and their values can even be continuous. 3) As mentioned before, entropy $H(\bm z)$ or mutual information $I(\bm z, \bm \pi)$ \cite{hjelm2018learning} is not well-defined for degenerate continuous distributions whereas the rate distortion $R(\bm z, \epsilon)$ is and can be accurately and efficiently computed for (mixed)  subspaces, at least.


尽管线性判别表示（LDRs）的三个属性——{\em 类间判别性}、{\em 类内可压缩性}和{\em 最大多样性表示}——都是学习表示 $\z$ 的高度期望的属性，但它们绝非易事：这些属性是否兼容，以至于我们可以期望一次性实现它们？如果是，是否存在一个{\em 简单但有原则的}目标，可以根据所有这些属性来衡量最终表示的优劣？这些问题的关键是{找到}一个有原则的“紧凑性度量”，用于衡量随机变量 $\z$ 的分布或其有限样本 $\{\bm z_i\}_{i=1}^N$ 的紧凑性。这样的度量应该直接而准确地刻画分布的内在几何或统计特性，即其内在维度或{体积}。与交叉熵 \eqref{chap4-eqn:cross-entropy} 或信息瓶颈 \eqref{chap4-eqn:information-bottleneck} 不同，这样的度量不应完全依赖于类别标签，以便它可以在更一般的设置中工作，如监督、自监督、半监督和无监督设置。

\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{\toplevelprefix/chapters/chapter3/figs/mcr_diagram.png}
	\caption{高维数据 $\x\in \Re^D$ 的分布 $\mathcal D$ 支持在一个流形 $\mathcal{M}$ 上，其类别支持在低维子流形 $\mathcal{M}_k$ 上。我们的目标是学习一个由 $\bm \theta$ 参数化的映射 $f(\x, \bm  \theta)$，使得 $\z_i = f(\x_i, \bm \theta)$ 位于最大程度不相关的子空间 $\{\mathcal{S}_k\}$ 的并集上。}
	\label{chap4-fig:mcr-diagram}
\end{figure}
% As we discussed above, we would like the learned representation to preserve as much information about $\x$. If so, one can approximately recover $\x$ from the corresponding $\z$. In other words, there exists a decoding map $g(\z)$ such that
% \begin{equation}
%     \x   \xrightarrow{\hspace{2mm} f(\x)\hspace{2mm}} \z  \xrightarrow{\hspace{2mm} g(\z) \hspace{2mm}} \hat{\x},
% \end{equation}
% where $\hat{\x}$ closely approximates $\x$. Hence we would like to maximize the mutual information between $\x$ and $\hat \x$ while we want $\z$ to be as compact as possible. A natural measure of compactness for $\z$ is its entropy $H(\z)$. We want it to be minimized. Therefore, we could argue that to seek a good representation, we would like to maximize the following objective:
% \begin{equation}
%     \max I(\x, \hat{\x}) - \alpha H(\z).
%     \label{eqn:info-gain-obj1}
% \end{equation}
% Notice that $I(\x, \hat{\x}) \ge R(\hat \x,\epsilon)$ if the decoded $\hat \x$ satisfies $\mathbb E[\|\x - \hat \x \|_2] \le \epsilon$. Hence the above objective can be replaced as
% \begin{equation}
%     \max R(\hat \x,\epsilon) - \alpha H(\z).
%     \label{eqn:info-gain-obj2}
% \end{equation}

% However, differential entropy $H(\z)$ is not well-defined for continuous random variables with degenerate distributions.  This is unfortunately the case here for $\z$. In this case, strictly speaking $H(\z)$ can be negative infinity, as we can see from the differential entropy of a (nearly degenerate) Gaussian distribution in \eqref{eqn:entropy-Gaussian-multi}. To alleviate this difficulty, as we have discussed earlier in this chapter, we may use another measure of ``compactness'' of a distribution, the {\em rate distortion}: Recall that, given a random variable $\z$ and a prescribed precision $\epsilon >0$, the rate distortion $R(\z, \epsilon)$ is the minimal number of binary bits needed to encode $\z$ such that the expected decoding error is less than $\epsilon$, i.e., the decoded $\hat \z$ satisfies $\mathbb E[\|\z - \hat \z \|_2] \le \epsilon$. Hence the above objective \eqref{eqn:info-gain-obj2} can be replaced as:
% \begin{equation}
%     \max R(\hat \x,\epsilon) - \alpha R(\z, \epsilon).
%     \label{eqn:info-gain-obj3}
% \end{equation}




%To learn a discriminative linear representation for intrinsic low-dimensional structures from high-dimensional data, we here propose an information-theoretic measure that maximizes the coding rate difference between the whole dataset and the sum of each individual class, known as {\em rate reduction}. This new objective provides a more unifying view of the above objectives such as cross-entropy, information bottleneck, and contractive and contrastive learning. {\em We will rigorously show that when the intrinsic dimensions of the submanifolds are known and this objective is optimized, the resulting representation indeed has the desired properties listed above.}


% However, entropy is not well-defined for continuous random variables with degenerate distributions. The same difficulty resides with evaluating mutual information $I(\x, \z)$ for degenerate distributions.
% This is unfortunately the case here.
% To alleviate this difficulty, as we have discussed earlier in this chapter, we may use another measure of ``compactness'' of a distribution, the {\em rate distortion}:
% Given a random variable $\z$ and a prescribed precision $\epsilon >0$, the rate distortion $R(\z, \epsilon)$ is the minimal number of binary bits needed to encode $\z$ such that the expected decoding error is less than $\epsilon$, i.e., the decoded $\hat \z$ satisfies $\mathbb E[\|\z - \hat \z \|_2] \le \epsilon$.

%\yaodong{TODO: change the notion for $i$-th sample from $\bm{z}_{i}$ to $\bm{z}^{i}$, since $\bm{z}_{\ell}^{i}$ denotes the $i$-th sample at the $\ell$-th layer.}

不失一般性，假设随机向量 $\x$ 的分布 $\mathcal D$ 支持在一个分布的混合体上，即 $\mathcal D = \cup_{k=1}^K \mathcal{D}_k$，其中每个 $\mathcal{D}_k \subset \Re^D$ 在高维环境空间 $\Re^D$ 中具有低的内在维度。设 $\bm X_k \in \Re^{D\times N_k}$ 表示数据矩阵，其列是从分布 $\mathcal{D}_k$ 中抽取的样本，其中 $N_k$ 表示每个 $k=1,\dots,K$ 的样本数量。然后，我们用 $\bm X=[\bm X_1,\dots,\bm X_K] \in \Re^{D\times N}$ 表示所有样本，其中 $N=\sum_{k=1}^K N_k$。
%Suppose that we have a set of samples $\X = \cup_{j=1}^k \X_j$ drawn from the distribution $\mathcal D$, where $\X_j$ contains samples drawn from the corresponding distribution $\mathcal{D}_j$ for each $j=1,\dots,k$. 
回想一下，我们也用 $\vx_i$ 表示 $\X$ 的第 $i$ 个样本，即 $\X=[\vx_1,\dots,\vx_N]$。在一个编码映射下：
\begin{equation}
	\x   \xrightarrow{\hspace{2mm} f(\x)\hspace{2mm}} \z,
\end{equation}
输入样本被映射为 $\vz_i = f(\vx_i)$，对于每个 $i=1,\dots,N$。滥用一下记号，我们也写 $\Z_k = f(\X_k)$ 和 $\Z = f(\X)$。因此，我们有 $\Z = [\bm Z_1,\dots,\bm Z_K]$ 和 $\Z = [\vz_1,\dots\vz_N]$。

一方面，为了使学习到的特征具有判别性，不同类别/簇的特征最好是相互{\em 最大程度不相干}的。因此，它们一起应该张成一个尽可能大体积（或维度）的空间，并且整个集合 $\Z$ 的编码率应该尽可能大。另一方面，同一类别/簇的学习特征应该高度相关和相干。因此，每个类别/簇应该只张成一个体积非常小的空间（或子空间），并且编码率应该尽可能小。现在，我们将介绍如何衡量学习特征的编码率。

\paragraph{特征的编码率。} 值得注意的是，评估编码率的一个实际挑战是，特征表示 $\Z$ 的底层分布通常是未知的。为了解决这个问题，我们可以将特征 $\Z = [\z_1, \ldots, \z_N]$ 近似为从一个多元高斯分布中抽取的样本。在这个假设下，正如在第 \ref{subsec:lossy DR} 章中讨论的，特征 $\Z$ {\em 作为一个整体}的紧凑性可以用每个样本的平均编码长度来衡量，称为{\em 编码率}，受精度水平 $\epsilon > 0$ 的限制（见 \eqref{eqn:rate-Gaussian}），定义如下：
\begin{equation}
	R_{\epsilon}(\Z) = \frac{1}{2}\log\det\left(\I + \frac{d}{N\epsilon^{2}}\Z\Z^{\top}\right).
	\label{chap4-eqn:coding-length-eval}
\end{equation}

另一方面，我们希望一个非线性变换 $f(\x)$ 将每个特定于类别的子流形 $\mathcal{M}_k \subset \mathbb{R}^D$ 映射到一个最大程度不相干的线性子空间 $\mathcal{S}_k \subset \mathbb{R}^d$，使得学习到的特征 $\Z$ 位于一个低维子空间的并集上。这种结构允许通过分别分析每个子空间来更准确地评估编码率。% To this end, we partition the data $\Z$ into multiple subsets: $\Z = \Z_1 \cup \cdots \cup \Z_k$, where each subset $\Z_j$ lies within in one low-dimensional subspace. 
% The above coding rate \eqref{chap4-eqn:coding-length-eval} can then be accurately computed for each subset as follows:
% \begin{align}
%     R_{\epsilon}(\Z_j) = \frac{1}{2} \log\det\left(\I + \frac{d}{N_j\epsilon^{2}}\Z_j\Z_j^{\top}\right),
% \end{align}
% where $N_j$ denotes the number of samples in $\Z_j$. 
% For convenience, let $\bm{\Pi} = \{\bm{\Pi}^j \in \Re^{N \times N}\}_{j=1}^{k}$ be a set of diagonal matrices whose diagonal entries encode the membership of the $N$ samples in the $k$ classes. More specifically, the diagonal entry $\bm \Pi^j(i,i)$ of $\bm \Pi^j$ indicates the probability of sample $i$ belonging to subset $j$. Therefore $\bm{\Pi}$ lies in a simplex: ${\Omega} \doteq \{\bm{\Pi} \mid \bm{\Pi}^j \ge \mathbf{0}, \; \bm{\Pi}^1 + \cdots + \bm{\Pi}^k = \I\}$. 
回想一下，$\Z_k$ 的列表示 $\X_k$ 中样本的特征，对于每个 $k=1,\dots,K$。$\bm Z_k$ 中特征的编码率可以如下计算：
\begin{align}
    R_{\epsilon}(\Z_k) = \frac{N_k}{2N}\log\det\left(\I + \frac{d}{N_k\epsilon^{2}}\Z_k\Z_k^{\top}\right)
\end{align}
然后，每个类别中特征的平均编码率之和为
\begin{equation}
	 R_{\epsilon}^c(\Z) \doteq \sum_{k=1}^K R_{\epsilon}(\Z_k),
	\label{chap4-eqn:compress-loss-eval}
\end{equation}
% Note that when $\Z$ is given, $R_{\epsilon}^c(\Z \mid \bm{\Pi})$ is a concave function of $\bm{\Pi}$.
% The function $\log\det(\cdot)$ in the above expressions has been long known as an effective heuristic for rank minimization problems, with guaranteed convergence to local minimum \cite{fazel2003log-det}. {As it nicely characterizes the coding rate of Gaussian or subspace-like distributions, $\log\det(\cdot)$ can be very effective in clustering or classification of data that are mixed Gaussians \cite{ma2007segmentation}.} Note that for the clustering purpose alone, one may only care about the sign of $\Delta R$ to decide whether to partition the data or not, which leads to the greedy clustering Algorithm \ref{alg:steepest_descent_coding_length} studied in the previous section. More specifically, in the context of clustering {\em finite} samples, one needs to use the more precise measure of the coding length mentioned earlier in Section \ref{sec:clustering-Gaussians}. See \cite{ma2007segmentation} for more details.
% As discussed in Chapter \ref{subsec:lossy DR}, we can use the metric  \eqref{eq:coding rate} to measure the compactness (i.e., coding rate) for the whole set of the features and want $R_{\epsilon}(\bm Z)$ to be large. 
% Hence we want the coding rate (or length) for the whole set of features $R_{\epsilon}(\Z)$ to be large (see \eqref{chap4-eqn:coding-length-eval}). \pw{We should relate here to Chpa 3.3.3!} 
% Hence we want the coding rate for each subset $\Z_j$ to be small. We denote the sum of all these coding rates as
 % where $\bm \Pi$ denotes the partition $\Z \rightarrow \cup_{j=1}^k \Z_j$.

因此，$\X$ 的一个好的表示 $\Z$ 是那个在整体编码率和所有类别编码率之和之间实现大差异的表示：
\begin{equation}
	\Delta R_{\epsilon}(\Z) \doteq R_{\epsilon}(\Z) - R_{\epsilon}^c(\Z).
	\label{chap4-eqn:coding-length-reduction}
\end{equation}
注意，根据我们本章早些时候的讨论，这个差异可以被解释为通过识别整体集合 $\Z$ 内的正确低维簇 $\Z_k$ 所获得的“信息增益”量。

如果我们选择我们的特征映射 $f(\cdot)$ 为一个具有网络参数 $\bm \theta$ 的深度神经网络 $f(\cdot,\bm \theta)$，那么特征表示和由此产生的率降低的整个过程可以用以下图表来说明：
\begin{equation}
	\X
	\xrightarrow{\hspace{2mm} f(\x, \bm \theta)\hspace{2mm}} \Z  \xrightarrow{\hspace{2mm} \epsilon \hspace{2mm}} \Delta R_{\epsilon}(\Z).
	\label{chap4-eqn:flow}
\end{equation}
注意，$\Delta R_{\epsilon}$ 在特征 $\Z$ 的尺度上是{\em 单调的}。为了确保不同表示之间的公平比较，对学习到的特征的尺度进行{\em 归一化}至关重要。这可以通过对每个类别 $\Z_k$ 的 Frobenius 范数施加与 $\Z_k \in \mathbb R^{d \times N_k}$ 中特征数量成比例的尺度来实现，即 $\|\Z_k\|_F^2 = N_k$，或者通过将每个特征归一化到单位球面上来实现，即 $\z_i \in \mathbb{S}^{d-1}$，其中 $N_k=\mathrm{tr}(\bm \Pi_k)$ 表示第 $k$ 个类别中的样本数量。这个公式为在训练深度神经网络的实践中需要“批量归一化”提供了自然的理由 \cite{ioffe2015batch}。% An alternative, arguably simpler, way to normalize the scale of learned representations is to ensure that the mapping of each layer of the network is approximately {\em isometric} \cite{ISOnet}.

一旦表示是可比较的，目标就变成了学习一组特征 $\Z  = f(\X, \bm \theta)$，使得它们最大化所有特征的编码率与各类别特征之和的编码率之间的差值：
\begin{equation}
	\begin{aligned}
		\max_{\bm \theta } & \;  \Delta R_{\epsilon}\big(\Z \big) \doteq R_{\epsilon}(\Z) - R_{\epsilon}^c(\Z ), \\
		\mbox{s.t.} & \ \ \, \Z = f(\bm X, \bm \theta),\  \|\Z_k\|_F^2                                          = N_k,\ k=1,\dots,K. 
	\end{aligned}
	\label{eqn:maximal-rate-reduction}
\end{equation}
我们称之为{\em 最大编码率降低}（MCR$^2$）原则，
这是亚里士多德名言的真实体现：
\begin{quote}
	\centering
	“{\em 整体大于部分之和。}”
\end{quote}
为了学习最好的表示，我们要求{\em 整体最大程度地大于其部分之和}。让我们再看一下图 \ref{fig:informative-representation} 中所示的例子。从压缩的角度来看，右边的表示是{\em 最紧凑的}，因为当所有特征被编码为一个单一的高斯分布（蓝色）时的编码率与当特征被适当地聚类并编码为两个独立的子空间（绿色）时的编码率之间的差异是最大的。\footnote{直观地说，所有特征张成的整个空间的“体积”与特征实际占用的体积之间的比率是最大的。}

 
    注意，上述 MCR$^2$ 原则是为监督学习问题设计的，其中组成员关系（或类别标签）是已知的。然而，这个原则可以自然地扩展到无监督学习问题，通过引入一个成员关系矩阵，该矩阵编码了每个数据点到潜在组或簇的（可能是软的）分配。具体来说，设 $\bm \Pi = \{\bm \Pi_k\}_{k=1}^K \subset \R^{N\times N}$ 是一组对角矩阵，其对角线元素编码了 $N$ 个样本到 $K$ 个类别的成员关系。也就是说，$\bm \Pi$ 位于一个单纯形 $\Omega \doteq \{\bm \Pi: \bm \Pi_k \ge \bm 0: \sum_{k=1}^K \bm \Pi_k = \bm I_N\}$ 中。然后，我们可以定义相对于划分 $\bm \Pi$ 的平均编码率如下：
    \begin{align}\label{eq:MCRc}
        R_{\epsilon}^c(\Z \mid \bm \Pi) \doteq \sum_{k=1}^K \frac{\mathrm{tr}(\bm \Pi_k)}{2N}\log\det\left(\bm I + \frac{d}{\mathrm{tr}(\bm \Pi_k)\epsilon^2}\bm Z\bm \Pi_k \bm Z^\top \right).
    \end{align}
    当 $\bm Z$ 给定时，$R_{\epsilon}^c(\Z \vert \bm \Pi)$ 是 $\bm \Pi$ 的一个凹函数。那么无监督学习问题的 MCR$^2$ 原则如下：
    \begin{align}\label{eq:MCR pi}
        \max_{\bm \Pi, \bm \theta} & \  \Delta R_{\epsilon}\big(\Z  \mid \bm \Pi) \doteq R_{\epsilon}(\bm Z) - R_{\epsilon}^c(\Z \mid \bm \Pi) \notag \\ 
       \mathrm{s.t.}  & \ \ \ \bm Z = f(\bm X, \bm \theta),\ \|\bm Z \bm \Pi_k\|_F^2 = N_k,\ k = 1,\dots,K,\ \bm\Pi \in \Omega. 
    \end{align}
    与 \eqref{eqn:maximal-rate-reduction} 相比，这里的公式允许对组成员关系和网络参数进行联合优化。特别是，当 $\bm \Pi$ 固定为一个将 $N$ 个数据点分配到 $K$ 个组的组成员关系矩阵时，问题 \eqref{eq:MCR pi} 可以恢复为问题 \eqref{eqn:maximal-rate-reduction}。
 


% \paragraph{Relationship to Classic  Information Gain.}
%  The maximal coding rate reduction can be viewed as a generalization to the classic  {\em Information Gain} (IG), which aims to maximize the reduction of entropy of a random variable, say $\bm z$, with respect to an observed attribute, say $\bm \pi$:
% $
% \max_{\bm \pi} \; \mbox{IG}(\bm z, \bm \pi) \doteq H(\bm z) - H(\bm z \mid \bm \pi),
% $
% i.e., the {\em mutual information} between $\z$ and $\bm \pi$ \cite{Cover-Thomas}. Maximal information gain has been widely used in areas such as decision trees \cite{decision-trees}.
% However, MCR$^2$ is used differently in several ways: 1) A typical setting of MCR$^2$ is when the data class labels are given, i.e. $\bm \Pi$ is known, MCR$^2$ focuses on learning representations $\bm z(\theta)$ rather than fitting labels. 2) In traditional settings of IG, the number of attributes in $\bm z$ cannot be so large and their values are discrete (typically binary).
% Here, the ``attributes'' $\bm \Pi$  represent the probability of a multi-class partition for all samples and their values can even be continuous. 3) As mentioned before, entropy $H(\bm z)$ or mutual information $I(\bm z, \bm \pi)$ \cite{hjelm2018learning} is not well-defined for degenerate continuous distributions whereas the rate distortion $R(\bm z, \epsilon)$ is and can be accurately and efficiently computed for (mixed)  subspaces, at least.


% \footnote{Strictly speaking, in the context of clustering {\em finite} samples, one needs to use the more precise measure of the coding length mentioned earlier, see \cite{ma2007segmentation} for more details.}
%\cs{I believe this sentence is not strict. In greedy algorithm, we use a difference between the loss encoding before and after merge. But the difference is not \eqref{eqn:maximal-rate-reduction}. In fact, in the clustering context, the first term $R(\Z(\theta), \epsilon)$ in  \eqref{eqn:maximal-rate-reduction} is a constant.}


\begin{figure}[t]
	\centering
	\includegraphics[width=0.8\linewidth]{\toplevelprefix/chapters/chapter3/figs/mcr2-global.png}
	\caption{{\bf 局部优化景观：} 根据定理 \ref{thm:MCR2-properties}，率降低目标的全局最大值对应于一个具有相互不相干子空间的解。}
	\label{fig:mcr-global}
\end{figure}




% %\begin{wrapfigure}{r}{0.5\textwidth}
% \begin{figure}
% \begin{center}
% % \vskip -0.3in
% \includegraphics[width=0.65\textwidth]{figures_compress/pack_CompressPdf.pdf}
% \end{center}
% \vskip -0.05in
% \caption{\small Comparison of two learned representations $\Z$ and $\Z'$ via reduced rates: $R$ is the number of $\epsilon$-balls packed in the joint distribution and $R^c$ is the sum of the numbers for all the subspaces (the green balls). $\Delta R$ is their difference (the number of blue balls). The MCR$^2$ principle prefers $\Z$ (the left one).}\label{fig:sphere-packing}
% \vskip -0.15in
% \end{figure}
% %\end{wrapfigure}

% \begin{wrapfigure}{r}{0.5\textwidth}
% \vspace{-10mm}
%   \begin{center}
%     \includegraphics[width=0.47\textwidth]{figures_compress/pack_CompressPdf.pdf}%\vspace{-5mm}
%   \end{center}
%   %\addtocounter{figure}{1}
% \caption{\small Comparison of two learned representations $\Z$ and $\Z'$ via reduced rates: $R$ is the number of $\epsilon$-balls packed in the joint distribution and $R^c$ is the sum of the numbers for all the subspaces (the green balls). $\Delta R$ is their difference (the number of blue balls). The MCR$^2$ principle prefers $\Z$ (the left one).}\label{fig:sphere-packing}
% \end{wrapfigure}

\vspace{-0.1in}


\subsection{编码率降低的优化性质}
\label{sec:MCR-landscape}
% {\color{red}Geometric properties of the global optimization landscape of the rate reduction objective. MAE paper theorem that connects maximizing rate distortion to Gaussian score.}

在本小节中，我们通过分析 MCR$^2$ 函数的最优解及其优化景观的结构来研究其优化性质。为了绕过神经网络引入的技术难题，我们考虑问题 \eqref{eqn:maximal-rate-reduction} 的一个简化版本如下：
\begin{align}\label{eqn1:maximal-rate-reduction}
    \max_{\bm Z}\ R_{\epsilon}(\Z) - R_{\epsilon}^c(\Z)\qquad \mathrm{s.t.}\quad \|\bm Z_k\|_F^2 = N_k,\ k =1,\dots,K. 
\end{align}
理论上，MCR$^2$ 原则 \eqref{eqn1:maximal-rate-reduction} 受益于很好的泛化性，并且可以应用于{\em 任何}分布的表示 $\Z$，只要这些分布的率 $R_\epsilon$ 和 $R^c_\epsilon$ 能够被准确且高效地评估。最优表示 $\Z^{\ast}$ 应该具有一些有趣的几何和统计性质。我们在这里揭示了子空间这种特殊情况下的最优表示的良好性质，子空间在机器学习中有许多重要的用例。当 $\Z$ 的期望表示是多个子空间时，\eqref{eqn1:maximal-rate-reduction} 中的率 $R_\epsilon$ 和 $R^c_\epsilon$ 分别由 \eqref{chap4-eqn:coding-length-eval} 和 \eqref{chap4-eqn:compress-loss-eval} 给出。在最大率降低时，MCR$^2$ 达到其最优表示，记为 $\Z^{\ast} = [\Z_1^*,\dots,\Z_K^*]$，其中 $\rank{(\Z_{k}^*)}\le d_k$。可以证明 $\Z^{\ast}$ 具有以下期望的性质（正式陈述和详细证明见 \cite{yu2020learning}）。

\begin{theorem}[\bf 全局最优解的刻画]
	假设 $\Z^{\ast} = [\Z_1^*,\dots,\Z_K^*]$ 是问题~\eqref{eqn1:maximal-rate-reduction} 的一个全局最优解。以下陈述成立：
	\begin{itemize}
		\item {\em 类间判别性}：只要环境空间足够大（$d \ge \sum_{k=1}^{K} d_k$），子空间就都是相互正交的，{\em 即}，对于 $k \not= l$，有 $(\Z_{k}^*)^{\top} \Z_{l}^* = \bm{0}$。
		\item {\em 最大多样性表示}：
		      只要编码精度足够高，即 $\epsilon ^4 < c\cdot \min_{k}\left\{ \frac{N_k}{N}\frac{d^2}{d_k^2}\right\}$，其中 $c>0$ 是一个常数。每个子空间都达到其最大维度，即 $\mathrm{rank}{(\Z_{k}^*)}= d_k$。此外，$\Z_{k}^*$ 的最大的 $d_k-1$ 个奇异值是相等的。
		      \label{thm:MCR2-properties}
	\end{itemize}
	% \vspace{-2mm}
\end{theorem}

% \begin{figure}[t]
% % \vspace{-5mm}
%     % \includegraphics[width=0.7\textwidth]{\toplevelprefix/chapters/chapter3/figs/pack_Compress.pdf}
% % \vspace{-15mm}
% \caption{\small Comparison of two learned representations $\Z$ and $\Z'$ via reduced rates: $R$ is the number of $\epsilon$-balls packed in the joint distribution and $R^c$ is the sum of the numbers for all the subspaces (the green balls). $\Delta R$ is their difference (the number of blue balls). The MCR$^2$ principle prefers $\Z$ (the left one).}\label{fig:sphere-packing}
% \vspace{-3mm}
% \end{figure}

这个定理表明，MCR$^2$ 原则促进了将数据嵌入到多个独立的子空间中（如图 \ref{fig:mcr-global} 所示），并且特征在每个子空间中是{\em 各向同性}分布的（除了可能的一个维度）。值得注意的是，这个定理也证实了由 MCR$^2$ 原则学习到的特征表现出在 \Cref{subsec:LDR} 中讨论的期望的低维判别性质。此外，在所有这样的判别表示中，它偏好在环境空间中具有最高维度的那个。这与信息瓶颈~\eqref{chap4-eqn:information-bottleneck} 的目标有本质的不同。
% \pw{add some transition here to connect MCR$^2$ to the regularized MCR$^2$}\yaodong{TODO: motivation: an equivalent formulation allows more fine-grained optimization landscape analysis?}



\begin{example}[CIFAR-10 上的图像分类]
	我们在这里展示 MCR$^2$ 目标如何帮助学习比交叉熵 \eqref{chap4-eqn:cross-entropy} 更好的图像分类表示。这里我们采用流行的神经网络架构，ResNet-18~\cite{he2016deep}，来建模特征映射 $\z = f(\x,\bm \theta)$。我们优化神经网络参数 $\bm \theta$ 以最大化编码率降低。我们用 CIFAR10 图像分类数据集~\cite{krizhevsky2009learning} 来评估性能。

	\begin{figure}[t]
		\begin{subfigure}[t]{0.42\textwidth}
			\centering
			\includegraphics[width=\textwidth]{\toplevelprefix/chapters/chapter3/figs/loss_log.pdf}
			\caption{训练过程中 $R_\epsilon$、$R^c_\epsilon$、$\Delta R_\epsilon$ 的演变。}
			\label{fig:train-test-loss-pca-1}
		\end{subfigure}
		\hfill
		\begin{subfigure}[t]{0.42\textwidth}
			\centering
			\includegraphics[width=\textwidth]{\toplevelprefix/chapters/chapter3/figs/pca_mainline.pdf}
			\caption{PCA：{\small (\textbf{红色}) 整体数据；(\textbf{蓝色}) 单个类别。}}
			\label{fig:train-test-loss-pca-3}
		\end{subfigure}
		\caption{\small 训练过程中 MCR$^2$ 的率的演变，学习到的特征的主成分。}
		\label{fig:train-test-loss-pca}
	\end{figure}

	% \begin{figure}[t]
	%     %\includegraphics[width=0.47\textwidth]{\toplevelprefix/chapters/chapter3/figs/heatmap_mcr2.pdf}
	%     %\includegraphics[width=0.47\textwidth]{\toplevelprefix/chapters/chapter3/figs/heatmap_ce.pdf}
	%     \caption{\small Cosine similarity between  learned features by using the MCR$^2$ objective  (\textbf{left}) and CE loss (\textbf{right}).}
	%     \label{fig:heatmap-plot}
	% \end{figure}

	\begin{figure*}[b]
		\begin{center}

			\includegraphics[width=0.42\textwidth]{\toplevelprefix/chapters/chapter3/figs/heatmap_mcr2.pdf}
			\hspace{0.25cm}
			\includegraphics[width=0.42\textwidth]{\toplevelprefix/chapters/chapter3/figs/heatmap_ce.png}
			\caption{\small 使用 MCR$^2$ 目标（\textbf{左}）和 CE 损失（\textbf{右}）学习到的特征之间的余弦相似度。}
			\label{fig:heatmap-plot}
		\end{center}
		\vskip -0.1in
	\end{figure*}

	图~\ref{fig:train-test-loss-pca-1} 说明了两个率及其差值（对于训练和测试数据）在训练的各个时期如何演变：在初始阶段之后，$R_\epsilon$ 逐渐增加，而 $R^c_\epsilon$ 减少，表明特征 $\bm Z$ 作为一个整体在扩展，而每个类别 $\bm Z_k$ 正在被压缩。
	图~\ref{fig:train-test-loss-pca-3} 显示了每个 $\Z_k$ 的奇异值分布。图~\ref{fig:heatmap-plot} 显示了按类别排序的学习特征之间的余弦相似度。我们比较了使用交叉熵 \eqref{chap4-eqn:cross-entropy} 和 MCR$^2$ 目标 \eqref{eqn:maximal-rate-reduction} 学习到的特征的相似性。从图中可以清楚地看到，使用 MCR$^2$ 损失学习到的表示比使用交叉熵损失学习到的表示多样得多。关于这个实验的更多细节可以在 \cite{chan2021redunet} 中找到。
	%In addition, we find that we are able to select diverse images from the same class according to the ``principal'' components of the learned features (see Figure~\ref{fig:visual-class-2-8} and Figure~\ref{fig:visual-overall-data}).
	\label{eg:Rate-Reduction-CIFAR10}
\end{example}

%\yima{I would suggest the above example is better moved to the Chapter 3. To justify the rate reduction objective function and its landscape.}

然而，上述实验中使用的网络架构一直缺乏明显的理由。目前尚不清楚为什么这里采用的网络（ResNet-18）适合表示映射 $f(\x, \bm \theta)$，更不用说解释其内部学习到的层算子和参数 $\bm \theta$ 了。在下一章中，{\em 我们将展示如何完全从期望的目标（比如率降低）中“白盒”地推导出网络架构和组件。}



\paragraph{正则化的 MCR$^2$。}
上述定理刻画了率降低目标的全局最优解的性质。那么其他最优解，比如局部最优解呢？由于 Frobenius 范数的约束，从优化理论的角度分析问题 \eqref{eqn1:maximal-rate-reduction} 是一项极其困难的任务。因此，我们考虑 \eqref{eqn1:maximal-rate-reduction} 的拉格朗日形式。这可以被看作是 \eqref{eqn1:maximal-rate-reduction} 的一个紧松弛甚至等价问题，其最优解在正则化参数的特定设置下是一致的；参见 \cite[Proposition 1]{wang2024global}。
具体来说，我们研究的公式，此后称为\textit{正则化的 MCR$^2$ 问题}，如下：
\begin{align}\label{eq:MCR-reg}
	\max_{\bm Z}\ R_{\epsilon}(\Z) - R_{\epsilon}^c(\Z) - \frac{\lambda}{2}\|\bm Z\|_F^2,
\end{align}
其中 $\lambda > 0$ 是正则化参数。尽管程序 \eqref{eq:MCR-reg} 是高度非凹的，并且在其梯度计算中涉及矩阵逆，我们仍然可以明确地刻画其局部和全局最优解如下。

\begin{theorem}[\bf 局部和全局最优解]\label{thm:mcr-global-opt}
	设 $N_k$ 表示第 $k$ 个类别中的训练样本数量，对于每个 $k \in \{1,\dots,K\}$，$N_{\max} \doteq \max\{N_1,\dots,N_K\}$，$\alpha=d/(N\epsilon^2)$，以及对于每个 $k \in \{1,\dots,K\}$，$\alpha_{k} = d/(N_k\epsilon^2)$。给定一个编码精度 $\epsilon > 0$，如果正则化参数满足
	\begin{align}\label{eq:lambda}
		\lambda \in \left(0, \frac{d(\sqrt{N/N_{\max}}-1)}{N(\sqrt{N/N_{\max}}+1)\epsilon^2} \right],
	\end{align}
	那么以下陈述成立：\\
	(i) ({\bf 局部最大化子}) $\bm Z^* = \left[\bm Z_1^*,\dots,\bm Z_K^* \right]$ 是问题 \eqref{eq:MCR-reg} 的一个局部最大化子，当且仅当第 $k$ 个块承认以下分解
	\begin{align}\label{eq:Zk opti}
		\bm Z_k^* = \left(\frac{ \eta_k + \sqrt{\eta_k^2 - 4\lambda^2N/N_k}}{2\lambda \alpha_{k}}\right)^{1/2} \bm U_k \bm V_k^\top,
	\end{align}
	其中 (a) $r_k = \mathrm{rank}(\bm Z_k^*)$ 满足 $r_k \in [0,\min\{N_k,d\})$ 且 $\sum_{k=1}^K r_k \le \min\{N,d\}$，(b) $\bm U_k \in \mathcal{O}^{d \times r_k}$ 满足对于所有 $k \neq l$，$\bm U_k^{\top}\bm U_l = \bm 0$，$\bm V_k \in \mathcal{O}^{N_k \times r_k}$，以及 (c) 对于每个 $k\in \{1,\dots,K\}$，$\eta_k=(\alpha_k-\alpha) - \lambda(N/N_k+1)$。
	\\
	(ii) ({\bf 全局最大化子}) $\bm Z^* = \left[\bm Z_1^*,\dots,\bm Z_K^* \right]$ 是问题 \eqref{eq:MCR-reg} 的一个全局最大化子，当且仅当 (a) 它满足上述所有条件且 $\sum_{k=1}^K r_k = \min\{m,d\}$，以及 (b) 对于所有满足 $N_k < N_l$ 和 $r_l > 0$ 的 $k \neq l \in [K]$，我们有 $r_k = \min\{N_k,d\}$。
\end{theorem}

\begin{figure}[t]
	\centering
	\includegraphics[width=0.8\linewidth]{\toplevelprefix/chapters/chapter3/figs/mcr2-global-local.png}
	\caption{{\bf 全局优化景观：} 根据 \cite{sun2015nonconvex,lee2016gradient}，\Cref{thm:mcr-global-opt,thm:mcr-benign-opt-landscape}，（正则化的）率降低目标的全局和局部最大值都对应于一个具有相互不相干子空间的解。所有其他临界点都是严格鞍点。}
	\label{fig:mcr-global-local}
\end{figure}

我们将证明推迟到 \cite{wang2024global}。这个定理明确地刻画了问题 \eqref{eq:MCR-reg} 的局部和全局最优解。直观地说，这表明由问题 \eqref{eq:MCR-reg} 的每个局部最大化子表示的特征是低维且具有判别性的。尽管我们在定理~\ref{thm:mcr-global-opt} 中刻画了局部和全局最优解，但仍然未知这些解是否可以通过使用 GD 来有效计算以解决问题 \eqref{eq:MCR-reg}，因为 GD 可能会卡在其他临界点，如鞍点。
幸运的是，\cite{sun2015nonconvex,lee2016gradient} 表明，如果一个函数是二次连续可微的并且满足{\em 严格鞍点性质}，即每个临界点要么是局部极小值，要么是严格鞍点\footnote{我们说一个临界点是问题 \eqref{eq:MCR-reg} 的一个严格鞍点，如果它有一个具有严格正曲率的方向~\cite{sun2015nonconvex}。这包括具有严格正曲率的经典鞍点以及局部极小值。}，那么 GD 从随机初始化开始几乎肯定会收敛到其局部极小值。我们通过刻画问题 \eqref{eq:MCR-reg} 的所有临界点来研究其全局优化景观如下。

\begin{theorem}[\bf 良性优化景观]\label{thm:mcr-benign-opt-landscape}
	给定一个编码精度 $\epsilon > 0$，如果正则化参数满足 \eqref{eq:lambda}，
	那么问题 \eqref{eq:MCR-reg} 的任何临界点 $\bm Z$ 要么是一个局部最大化子，要么是一个严格鞍点。
\end{theorem}
我们将证明推迟到 \cite{wang2024global}。总的来说，上述两个定理表明，与率降低目标的每个局部最大化子（不仅仅是全局最大化子）相关的学习特征被构造为不相干的低维子空间。此外，（正则化的）率降低目标 \eqref{eqn:maximal-rate-reduction} 具有一个非常良性的景观，其临界点只有局部最大值和严格鞍点，如图 \ref{fig:mcr-global-local} 所示。据我们所知，\Cref{thm:mcr-global-opt,thm:mcr-benign-opt-landscape} 构成了对 MCR$^2$ 目标的局部最优解和优化景观的首次分析。根据 \cite{sun2015nonconvex,lee2016gradient}，\Cref{thm:mcr-global-opt,thm:mcr-benign-opt-landscape} 意味着低维且具有判别性的表示（LDRs）可以通过从随机初始化开始对率降低目标 \eqref{eqn:maximal-rate-reduction} 应用（随机）梯度下降来有效找到。这些结果也间接解释了为什么在例 \ref{eg:Rate-Reduction-CIFAR10} 中，如果选择的网络足够有表现力并且训练得很好，得到的表示通常会给出一个不相干的线性表示，这很可能对应于全局最优解。



%\subsection{Classification via minimal incremental coding length}
%\href{http://people.eecs.berkeley.edu/~yima/psfile/MICL_SJIS.pdf}{supervised classification} from the perspective of (lossy) compression.

\section{总结与注释}

去噪和扩散用于采样的历史非常丰富。第一个明确关于扩散模型的工作可能是 \cite{Sohl-Dickstein2015}，但在此之前，有许多关于去噪作为计算和统计问题的工作。其中最相关的可能是 \cite{hyvarinen05a}，它明确使用分数函数进行去噪（以及执行独立成分分析）。最流行的后续工作基本上是同时出现的：\cite{ho2020denoising,song2019}。从那时起，成千上万的论文建立在扩散模型之上；我们将在 \Cref{ch:autoencoding} 中重新讨论这个主题。

许多这些工作使用了与简单线性组合 \eqref{eq:gen_additive_gaussian_noise_model} 不同的随机过程。事实上，上面列出的所有工作都强调了在前向过程的每一步开始时添加\textit{独立的}高斯噪声的必要性。理论导向的工作实际上使用布朗运动或随机微分方程来构建前向过程 \cite{song2020score}。然而，由于高斯的线性组合仍然是高斯，这些过程的\textit{边际分布}仍然采用 \eqref{eq:gen_additive_gaussian_noise_model} 的形式。我们的大部分讨论只需要边际分布是它们的样子，因此我们过于简化的模型实际上对于几乎所有事情都足够了。事实上，唯一边际分布不够的时候是我们推导 \(\Ex[\vx_{s} \mid \vx_{t}]\) 关于 \(\Ex[\vx \mid \vx_{t}]\) 的表达式时。不同的（加噪）过程给出不同的此类表达式，这些表达式可用于采样（当然还有其他方法可以推导高效的采样器，例如在 \cite{song2020score} 中）。然而，\eqref{eq:gen_additive_gaussian_noise_model} 中的过程是一个真正的随机过程，其“自然”的去噪迭代采用流行的 DDIM 算法的形式 \cite{song2020denoising}。（即使是这种等价性也不是微不足道的；我们引用 \cite{de2025distributional} 作为理由。）

% {Lossy coding does more than just quantization. It allows us to avoid a pathological solution by simply using the empirical distribution as the optimal solution that minimizes entropy. }

% {From the achievable (entropy) to an implementable coding scheme. That is, in addition to a computable or achievable encoding scheme, there should also exist an implementable decoding scheme. Lossy quantization becomes necessary for real-valued data.} 
 
最近的研究越来越关注当图像数据具有低维结构时扩散模型的训练和采样效率。在训练方面，\cite{chen2023score} 提供了一个理论框架，用于理解当数据位于低维流形上或附近时扩散模型的表现。它严格分析了流形设置下分数函数的近似和估计误差，并证明了扩散模型可以实现高效的分布恢复，其样本和计算复杂度取决于数据的内在维度，而不是环境维度。后来，\cite{wang2024diffusion} 扩展了分析，并提供了令人信服的理论和经验证据，表明当数据分布位于低维子空间的并集附近时，可以有效地训练扩散模型。具体来说，他们表明，准确学习低秩高斯分布混合所需的训练样本数量与数据的内在维度成线性关系。这些工作表明，扩散模型能够利用图像数据中固有的低维结构，从而显著提高训练效率。在采样方面，\cite{li2024adapting} 表明，当底层分布位于高维空间中的低维流形上或附近时，去噪扩散概率模型（DDPM）采样器可以适应低维结构，并实现 $O(k^2/\sqrt{T})$ 阶的采样率，其中 $k$ 是内在维度，$T$ 是步数。最近，\cite{liang2025low} 分析了扩散生成模型如何利用未知的低维结构来加速采样。他们表明，DDPM 样本实现了 $O(k/T)$ 阶的采样率。\cite{tang2024adaptivity} 表明，朗之万扩散和前向-后向扩散模型都可以适应内在流形结构，并且诱导分布估计器的采样率仅取决于数据的内在维度。这些结果共同证明，扩散模型可以有效地利用数据中的内在低维结构来显著提高采样效率。











% \pw{Add some references on computational and sampling efficiency of diffusion models and discuss these results.}




\section{练习与扩展}

\begin{exercise}
    请证明 \eqref{eq:optimal_denoiser} 是问题 \eqref{eq:denoising_loss} 的最优解。
\end{exercise}

\begin{exercise}\label{exercise:conditional_gaussian}
  考虑随机向量 $\vx \in \bR^D$ 和 $\vy \in \bR^d$，使得对 $(\vx, \vy) \in \bR^{D + d}$ 是联合高斯的。这意味着
  \begin{equation*}
    \begin{bmatrix}
      \vx \\
      \vy
    \end{bmatrix}
    \sim
    \cN \left(
      \begin{bmatrix}
        \vmu_{\vx} \\
        \vmu_{\vy}
      \end{bmatrix}
      ,
      \begin{bmatrix}
        \vSigma_{\vx} & \vSigma_{\vx\vy} \\
        \vSigma_{\vx\vy}^\top & \vSigma_{\vy}
      \end{bmatrix}
    \right),
  \end{equation*}
  其中均值和协方差参数由下式给出
  \begin{equation*}
    \vmu_{\vx} = \bE[\vx],\quad \vmu_{\vy} = \bE[\vy],\quad
    \begin{bmatrix}
      \vSigma_{\vx} & \vSigma_{\vx\vy} \\
      \vSigma_{\vx\vy}^\top & \vSigma_{\vy}
    \end{bmatrix}
    =
    \bE\left[
      \begin{bmatrix}
        \vx - \bE[\vx] \\
        \vy - \bE[\vy]
      \end{bmatrix}
      \begin{bmatrix}
        \vx - \bE[\vx] \\
        \vy - \bE[\vy]
      \end{bmatrix}^\top
      \right]
  \end{equation*}
  假设 $\vSigma_{\vy}$ 是正定的（因此是可逆的）；那么协方差矩阵的正半定性等价于舒尔补条件 $\vSigma_{\vx} - \vSigma_{\vx\vy} \vSigma_{\vy}^{-1}
  \vSigma_{\vx\vy}^\top \succeq \Zero$。

  在本练习中，我们将证明条件分布 $p_{\vx \mid
  \vy}$ 是高斯的：即，
  \begin{equation}\label{eq:gaussian-conditional-eqn}
    p_{\vx \mid \vy} \sim \cN\left(
      \vmu_{\vx} + \vSigma_{\vx\vy} \vSigma_{\vy}^{-1} (\vy - \vmu_{\vy}),
      \vSigma_{\vx} - \vSigma_{\vx\vy} \vSigma_{\vy}^{-1}
      \vSigma_{\vx\vy}^{\top}
    \right).
  \end{equation}
  证明此结果的直接途径是操作密度比 $p_{\vx, \vy} / p_{\vy}$。我们下面概述这种形式的一个代数上简洁的论证。

  \begin{enumerate}
    \item 验证协方差的以下矩阵恒等式：
      \begin{equation}\label{eq:gaussian-conditional-covariance-block}
        \begin{bmatrix}
          \vSigma_{\vx} & \vSigma_{\vx\vy} \\
          \vSigma_{\vx\vy}^\top & \vSigma_{\vy}
        \end{bmatrix}
        =
        \begin{bmatrix}
          \vI_D & \vSigma_{\vx\vy}\vSigma_{\vy}^{-1} \\
          \Zero & \vI_d
        \end{bmatrix}
        \begin{bmatrix}
          \vSigma_{\vx} - \vSigma_{\vx\vy} \vSigma_{\vy}^{-1}
          \vSigma_{\vx\vy}^{\top} & \Zero \\
          \Zero & \vSigma_{\vy}
        \end{bmatrix}
        \begin{bmatrix}
          \vI_D & \Zero\\
          \vSigma_{\vy}^{-1}\vSigma_{\vx\vy}^\top & \vI_d
        \end{bmatrix}.
      \end{equation}
      通过对协方差矩阵进行两轮（分块）高斯消元可以得到这个恒等式。
    \item 基于前面的恒等式，证明
      \begin{equation}\label{eq:gaussian-conditional-covariance-inverse-block}
        \begin{bmatrix}
          \vSigma_{\vx} & \vSigma_{\vx\vy} \\
          \vSigma_{\vx\vy}^\top & \vSigma_{\vy}
        \end{bmatrix}^{-1}
        =
        \begin{bmatrix}
          \vI_D & \Zero\\
          -\vSigma_{\vy}^{-1}\vSigma_{\vx\vy}^\top & \vI_d
        \end{bmatrix}
        \begin{bmatrix}
          \left(\vSigma_{\vx} - \vSigma_{\vx\vy} \vSigma_{\vy}^{-1}
          \vSigma_{\vx\vy}^{\top}\right)^{-1} & \Zero \\
          \Zero & \vSigma_{\vy}^{-1}
        \end{bmatrix}
        \begin{bmatrix}
          \vI_D & -\vSigma_{\vx\vy}\vSigma_{\vy}^{-1} \\
          \Zero & \vI_d
        \end{bmatrix}
      \end{equation}
      只要相关的逆矩阵有定义。\footnote{在舒尔补项不可逆的情况下，同样的结果也成立，其逆被替换为摩尔-彭若斯伪逆。特别是，条件分布 \eqref{eq:gaussian-conditional-eqn} 变成一个退化的高斯分布。}
      结论是
      \begin{align}
        &\begin{bmatrix}
          \vx-\vmu_{\vx} \\
          \vy-\vmu_{\vy}
        \end{bmatrix}^{\top}
        \begin{bmatrix}
          \vSigma_{\vx} & \vSigma_{\vx\vy} \\
          \vSigma_{\vx\vy}^\top & \vSigma_{\vy}
        \end{bmatrix}^{-1}
        \begin{bmatrix}
          \vx-\vmu_{\vx} \\
          \vy-\vmu_{\vy}
        \end{bmatrix}
        \\
        &\qquad=
        \begin{bmatrix}
          \vx - \left(\vmu_{\vx} + \vSigma_{\vx\vy}\vSigma_{\vy}^{-1}(\vy
          - \vmu_{\vy})\right) \\
          \vy - \vmu_{\vy}
        \end{bmatrix}^\top
        \begin{bmatrix}
          \left(\vSigma_{\vx} - \vSigma_{\vx\vy} \vSigma_{\vy}^{-1}
          \vSigma_{\vx\vy}^{\top}\right)^{-1} & \Zero \\
          \Zero & \vSigma_{\vy}^{-1}
        \end{bmatrix}
        \begin{bmatrix}
          \vx - \left(\vmu_{\vx} + \vSigma_{\vx\vy}\vSigma_{\vy}^{-1}(\vy
          - \vmu_{\vy})\right) \\
          \vy - \vmu_{\vy}
        \end{bmatrix}.
      \end{align}
      （\textit{提示：为了简化代数操作，注意
      \Cref{eq:gaussian-conditional-covariance-block} 右侧的第一个和最后一个矩阵是彼此的转置。}）
    \item 通过除以 $p_{\vx, \vy} / p_{\vy}$，证明
      \Cref{eq:gaussian-conditional-eqn}。（\textit{提示：使用前面的恒等式，只需要最少的代数运算。对于归一化常数，使用 \Cref{eq:gaussian-conditional-covariance-inverse-block} 类似地分解行列式。}）

  \end{enumerate}


\end{exercise}

\begin{exercise}\label{exercise:sherman_morrison_woodbury_identity}
    证明 Sherman-Morrison-Woodbury 恒等式，即对于矩阵 \(\vA\)、\(\vC\)、\(\vU\)、\(\vV\)，使得 \(\vA\)、\(\vC\) 和 \(\vA + \vU\vC\vV\) 可逆，
    \begin{equation}
        (\vA + \vU\vC\vV)^{-1} = \vA^{-1} - \vA^{-1}\vU(\vC^{-1} + \vV\vA^{-1}\vU)^{-1}\vV\vA^{-1}
    \end{equation}
\end{exercise}

\begin{exercise}\label{exercise:generalizing_results_to_different_noise_models}
    假设 \(\vx_{t}\) 遵循广义噪声模型 \eqref{eq:gen_additive_gaussian_noise_model}，重新推导以下内容。
    \begin{itemize}
        \item Tweedie 公式：\eqref{eq:gen_tweedie}。
        \item DDIM 迭代：\eqref{eq:gen_denoising_iteration}。
        \item 高斯混合模型的贝叶斯最优去噪器：\eqref{eq:gen_gmm_bayes_optimal_denoiser}。
    \end{itemize}
\end{exercise}

\begin{exercise}\label{exercise:implement_denoising_processes}
\begin{enumerate}
    \item 实现 \Cref{exercise:generalizing_results_to_different_noise_models} 中推导的公式，为高斯混合模型构建一个采样器。
    \item 复现 \Cref{fig:ve_forward_denoising} 和 \Cref{fig:vp_gmm_denoising}。
    \item 我们现在引入一个称为\textit{流匹配（FM）}的独立过程，如下：
    \begin{equation}
        \alpha_{t} = 1 - t, \qquad \sigma_{t} = t.
    \end{equation}
    使用相同的框架实现这个过程，并测试其在高维采样中的效果。哪个过程似乎给出更好或更稳定的结果？
\end{enumerate}
\end{exercise}

\begin{exercise}\label{exer:prop cover}
	证明 \Cref{prop:covering-number-rate-distortion}。回想一下，我们有 \(\vx\) 是一个具有紧支撑集 \(K\) 的随机变量。
	\begin{enumerate}
		\item 首先，证明覆盖数 \(\cN\) 和率失真 \(\cR\)（覆盖数的定义请参考 \Cref{prop:covering-number-rate-distortion}）具有以下不等式：
		\begin{equation}
			\cR_{\epsilon}(\vx) \leq \log_{2} \cN_{\epsilon}(K).
		\end{equation}
		\item 其次，证明当 \(\vx\) 在 \(K\) 上均匀分布时，这个不等式是\textit{紧的}。
		\item 第三，证明一般情况下不存在形如 \(\log_{2} \cN_{c\epsilon}(K) \leq \cR_{\epsilon}(\vx)\) 的下界，对于任何常数 \(c > 0\)。\textit{提示：}为简单起见，假设 \(\vx\) 取有限个值，假设这些值非常分散，并且它以非常高的概率取其中一个值。根据你如何精确定义 \(\vx\)，最优编码方案就是这个高概率值本身，因此在这种情况下率失真函数为 \(0\)。与此同时，覆盖数可以很大。填写细节以完成这部分问题。
	\end{enumerate}
\end{exercise}

\begin{exercise}
	请证明 $\log\det(\cdot)$ 函数的以下性质。
	\begin{enumerate}
		\item 证明
		      \begin{align*}
			      f(\vX) = \log\det\left(\vX\right)
		      \end{align*}
		      是一个凹函数。（{\bf 提示：} 函数 $f(\vx)$ 是凸的当且仅当对于所有 $\vx$ 和 $\vh$，函数 $f(\vx+t\vh)$ 是凸的。）

		\item 证明：
		      \begin{align*}
			      \log\det(\vI + \vX^\top\vX) = \log\det(\vI + \vX\vX^\top)
		      \end{align*}

		\item 设 $\vA \in \R^{n\times n}$ 是一个正定矩阵。请证明：
		      \begin{align}
			      \log\det\left(\vA \right) = \sum_{i=1}^n \log(\lambda_i),
		      \end{align}
		      其中 $\lambda_1,\lambda_2,\dots,\lambda_n$ 是 $\vA$ 的特征值。
	\end{enumerate}

\end{exercise}

% \begin{exercise}
% 	\sdb{Exercise about proving the Gaussian is max entropy (in a remark in
% 		appendix B rn)}
% \end{exercise}

 

\end{document}
