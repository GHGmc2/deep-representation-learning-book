\providecommand{\toplevelprefix}{../..}
\documentclass[../../book-main_ro.tex]{subfiles}

\begin{document}

\chapter{Entropie, Difuzie, Denoising și Codare cu Pierderi}\label{app:entropy}\label{app:diffusion-denoising}

\begin{quote}
``{\em Creșterea dezordinii sau entropiei cu timpul este un exemplu a ceea ce se numește o săgeată a timpului, ceva care distinge trecutul de viitor, dând o direcție timpului.}''

$~$\hfill -- O scurtă istorie a timpului, Stephen Hawking
 \end{quote}
\vspace{5mm}

În această anexă oferim demonstrații pentru mai multe fapte, menționate în
\Cref{ch:general-distribution}, care sunt legate de entropia diferențială, cum
evoluează aceasta sub procese de difuzie și conexiunile sale cu codarea cu pierderi. Vom
face următoarea presupunere blândă despre variabila aleatoare care reprezintă
sursa de date, notată \(\vx\).

\begin{assumption}\label{assumption:entropy_x_compact_support}
    \(\vx\) este suportată pe o mulțime compactă \(\cS \subseteq \R^{D}\) de rază cel mult \(R\), adică \(R \doteq \sup_{\vxi \in \cS}\norm{\vxi}_{2}\).
\end{assumption}

În particular, deoarece mulțimile compacte în spațiul euclidian sunt mărginite, avem \(R < \infty\). Vom folosi consistent notația \(B_{r}(\vxi) \doteq \{\vu \in \R^{D} \colon \norm{\vxi - \vu}_{2} \leq r\}\) pentru a denota bila euclidiană de rază \(r\) centrată în \(\vxi\). În acest sens, \Cref{assumption:entropy_x_compact_support} are \(\cS \subseteq B_{R}(\vzero)\).

Observați că această presupunere este valabilă pentru (aproape) toate variabilele care ne interesează în practică, deoarece este (adesea) impusă de un pas de normalizare în timpul pre-procesării datelor. 

\section{Entropia Diferențială a Distribuțiilor de Dimensiune Mică}\label{sec:low_dim_entropy}

În această scurtă anexă discutăm entropia diferențială a distribuțiilor de dimensiune mică. Prin definiție, entropia diferențială a unei variabile aleatoare \(\vx\) care nu are o densitate este \(-\infty\); aceasta include toate variabilele aleatoare suportate pe mulțimi de dimensiune mică. Obiectivul acestei secțiuni este să discutăm de ce aceasta este o valoare ``moral corectă''.

În fapt, fie \(\vx\) orice variabilă aleatoare astfel încât \Cref{assumption:entropy_x_compact_support} este valabilă, suportul \(\cS\) al lui \(\vx\) are volum \(0\).\footnote{Formal acest lucru înseamnă că \(\cS\) este măsurabil Borel cu măsură Borel \(0\).} Vom considera cazul în care \(\vx\) este uniform pe \(\cS\).\footnote{Să zicem, în raport cu măsura Hausdorff pe \(\cS\).} Scopul nostru este să calculăm \(h(\vx)\).

În acest caz, \(\vx\) nu ar avea o densitate; în lumea contrafactuală unde nu am ști că \(h(\vx) = -\infty\), nu am putea-o defini direct folosind definiția standard a entropiei diferențiale. În schimb, ca în restul analizei și teoriei informației ar fi rezonabil să considerăm limita entropiilor aproximărilor succesiv mai bune \(\vx_{\eps}\) ale lui \(\vx\) care au densități, adică,
\begin{equation}
    h(\vx)\ \text{``=''}\ \lim_{\eps \searrow 0}h(\vx_{\eps}).
\end{equation}


Pentru aceasta, ideea de bază este să luăm o \(\eps\)-îngroșare a lui \(\cS\), să zicem \(\cS_{\eps}\) definită ca 
\begin{equation}
    S_{\eps} = \bigcup_{\vxi \in \cS}B_{\eps}(\vxi)
\end{equation}
și vizualizată în \Cref{fig:entropy_eps_thickening}.
\begin{figure}[th]
    \centering
    \begin{tikzpicture}
        \def\radius{0.2cm} %
        \def\curve{(0,0) .. controls (1,1.5) and (3,-0.5) .. (4,1)} %

        \path[decoration={markings, mark=between positions 0 and 1 step 0.02 with {
                \fill[red!30, opacity=0.5, draw=none] (0,0) circle (\radius);
            }}, postaction={decorate}] \curve;

        \draw[blue, thick] \curve node[right, blue] {$\cS$};

        \coordinate (p_on_curve) at (2, 0.5); %
        \fill[red!50, opacity=0.7] (p_on_curve) circle (\radius);
        \fill[black] (p_on_curve) circle (1pt) node[below left] {$x$};

            \node[red, below right] at (3.5, -0.2) {$\cS_{\eps} = \bigcup_{\vxi \in S} B_{\eps}(\vxi)$};

    \end{tikzpicture}
    \caption{Ilustrarea \(\eps\)-îngroșării \(\cS_\eps\) a unei curbe \(\cS \subseteq \R^{2}\).}
    \label{fig:entropy_eps_thickening}
\end{figure}
Vom lucra cu variabile aleatoare al căror suport este \(\cS_{\eps}\), care este complet dimensional, și vom lua limita când \(\eps \to 0\). Într-adevăr, definiți \(\vx_{\eps} \sim \dUnif(\cS_{\eps})\). Deoarece \(\cS_{\eps}\) are volum pozitiv, \(\vx_{\eps}\) are o densitate \(p_{\eps}\) egală cu
\begin{equation}
    p_{\eps}(\vxi) = \indvar(\vxi \in \cS_{\eps}) \cdot \frac{1}{\volume(\cS_{\eps})}.
\end{equation}
Calculând entropia lui \(\vx_{\eps}\) folosind convenția că \(0 \log 0 = 0\), avem
\begin{align}
    h(\vx_{\eps}) 
    &= -\int_{\R^{D}}p_{\eps}(x)\log p_{\eps}(x)\odif{x} \\ 
    &= -\int_{\cS_{\eps}}\frac{1}{\volume(\cS_{\eps})} \log\rp{\frac{1}{\volume(\cS_{\eps})}}\odif{\vxi} \\ 
    &= \frac{\log(\volume(\cS_{\eps}))}{\volume(\cS_{\eps})}\int_{\cS_{\eps}}\odif{\vxi} \\ 
    &= \log(\volume(\cS_{\eps})).
\end{align}
Deoarece \(\cS\) este compact \(\volume(\cS_{\eps})\) este finit și tinde către \(0\) când \(\eps \searrow 0\). Așadar
\begin{equation}
    h(\vx) = \lim_{\eps \searrow 0}h(\vx_{\eps}) = \lim_{\eps \searrow 0}\log(\volume(\cS_{\eps})) = -\infty,
\end{equation}
așa cum se dorea.

Calculul de mai sus este de fapt un corolar al unui set mult mai celebru și celebrat de rezultate despre entropia maximă posibilă a lui \(\vx\) supusă anumitor constrângeri asupra distribuției lui \(\vx\). Ar fi o omisiune să nu oferim rezultatele aici; demonstrațiile sunt furnizate în Capitolul 2 din \cite{poliyanski2024information}, de exemplu.
\begin{theorem}\label{thm:max_entropy}
    Fie \(\vx\) o variabilă aleatoare pe \(\R^{D}\).
    \begin{enumerate}
        \item Dacă \(\vx\) este suportată pe o mulțime compactă \(\cS \subseteq \R^{D}\) (adică \Cref{assumption:entropy_x_compact_support}) atunci
        \begin{equation}
            h(\vx) \leq h(\dUnif(\cS)) = \log \volume(\cS).
        \end{equation}
        \item Dacă \(\vx\) are covarianță finită astfel încât, pentru o matrice PSD \(\vSigma \in \PSD(D)\), avem \(\Cov(\vx) \preceq \vSigma\) (în raport cu ordinea PSD, adică \(\vSigma - \Cov(\vx)\) este PSD), atunci
        \begin{equation}
            h(\vx) \leq h(\dNorm(\vzero, \vSigma)) = \frac{1}{2}\log((2\pi e)^{D}\det\vSigma).
        \end{equation}
        \item Dacă \(\vx\) are moment de ordinul doi finit astfel încât, pentru o constantă \(a \geq 0\), avem \(\Ex\norm{\vx}_{2}^{2} \leq a\), atunci
        \begin{equation}
            h(\vx) \leq h\rp{\dNorm\rp{\vzero, \frac{a}{D}\vI}} = \frac{D}{2}\log\frac{2\pi e a}{D}.
        \end{equation}
    \end{enumerate}
\end{theorem}


\section{Procese de Difuzie și Denoising}\label{sec:entropy_diffusion}

În corpul principal (\Cref{ch:general-distribution}), am considerat o variabilă aleatoare \(\vx\), și un proces stocastic definit de \eqref{eq:additive_gaussian_noise_model}, adică,
\begin{equation}\label{eq:app_additive_gaussian_noise_model}
    \vx_{t} = \vx + t\vg, \qquad  \forall t \in [0, T]
\end{equation}
unde \(\vg \sim \dNorm(\vzero, \vI)\) independent de \(\vx\). 

Structura acestei secțiuni este următoarea. În \Cref{sub:diffusion_entropy_increases} oferim o teoremă formală și o demonstrație clară care arată că sub \Cref{eq:app_additive_gaussian_noise_model} entropia crește, adică \(\odv*{h(\vx_{t})}{t} > 0\). În \Cref{sub:denoising_entropy_decreases} oferim o teoremă formală și o demonstrație clară care arată că sub \Cref{eq:app_additive_gaussian_noise_model}, entropia scade în timpul denoising-ului, adică \(h(\Ex[\vx_{s} \given \vx_{t}]) < h(\vx_{t})\) pentru toți \(s < t\). În \Cref{sub:app_diffusion_intermediate_results} oferim demonstrații pentru lemele tehnice care sunt necesare pentru a stabili afirmațiile din subsecțiunile anterioare.

Înainte de a începe, introducem câteva notații cheie. Mai întâi, fie \(\phi_{t}\) densitatea lui \(\dNorm(\vzero, t^{2}\vI)\), adică,
\begin{equation}\label{eq:gaussian_noise_time_t}
    \phi_{t}(\vxi) \doteq \frac{1}{(2\pi)^{D/2}t^{D}}\exp\rp{-\frac{\norm{\vxi}_{2}^{2}}{2t^{2}}}.
\end{equation}
În continuare, \(\vx_{t}\) este suportat pe tot \(\R^{D}\), deci are o \textit{densitate}, pe care o notăm \(p_{t}\) (ca în corpul principal). Un calcul rapid arată că
\begin{equation}\label{eq:p_t_representation}
    p_{t}(\vxi) = \Ex[\phi_{t}(\vxi - \vx)],
\end{equation}
și din această reprezentare este posibil să deducem (adică din \Cref{prop:diff_convolution}) că \(p_{t}\) este netedă (adică infinit diferențiabilă) în \(\vxi\), de asemenea netedă în \(t\), și pozitivă peste tot. Acest fapt este oarecum remarcabil la prima vedere: chiar pentru o variabilă aleatoare complet neregulată \(\vx\) (să zicem, o variabilă aleatoare Bernoulli, care nu are o densitate), netezirea sa gaussiană admite o densitate pentru fiecare (arbitrar mic) \(t > 0\). Demonstrația este lăsată ca exercițiu pentru cititorii versați în analiza matematică.

Totuși, trebuie să adăugăm și o presupunere despre \textit{netezimea} distribuției lui \(\vx\), care va elimina unele probleme tehnice care apar în jurul lui \(t = 0\) cu distribuții de dimensiune mică.\footnote{Deoarece atunci diverse cantități devin foarte neregulate și tratarea lor ar necesita analiză suplimentară semnificativă.} Cu toate acestea, ne așteptăm ca rezultatele noastre să fie valabile sub presupuneri mai blânde cu muncă suplimentară. Pentru moment, să presupunem:
\begin{assumption}\label{assumption:entropy_x_density}
    \(\vx\) are o densitate de două ori continuu diferențiabilă, notată \(p\).
\end{assumption}


\subsection{Procesul de Difuzie Crește Entropia în Timp}\label{sub:diffusion_entropy_increases}

În această secțiune anexă oferim o demonstrație a \Cref{thm:diffusion_entropy_increases}. Pentru convenieță, o reiterăm după cum urmează. 

\begin{theorem}[Difuzia Crește Entropia]\label{thm:diffusion_entropy_increases}
    Fie \(\vx\) orice variabilă aleatoare astfel încât \Cref{assumption:entropy_x_compact_support,assumption:entropy_x_density} sunt valabile, și fie \((\vx_{t})_{t \in [0, T]}\) procesul stocastic \eqref{eq:app_additive_gaussian_noise_model}. Atunci 
    \begin{equation}\label{eq:diffusion_entropy_increases}
        h(\vx_{s}) < h(\vx_{t}), \qquad \forall s, t \colon 0 \leq s < t \leq T.
    \end{equation}
\end{theorem}
\begin{proof}
    Înainte de a începe, trebuie să ne întrebăm: când are sens inegalitatea din \eqref{eq:diffusion_entropy_increases}? Vom arăta în \Cref{lem:diffusion_entropy_exists} că sub presupunerile noastre, entropia diferențială este bine definită, nu este niciodată \(+\infty\), și pentru \(t > 0\) este finită, deci inegalitatea (strictă) din \eqref{eq:diffusion_entropy_increases} are sens.

    Punând la o parte chestiunea bună-definiției, esența acestei demonstrații este să arătăm că densitatea \(p_{t}\) a lui \(\vx_{t}\) satisface o ecuație cu derivate parțiale particulară, care este foarte similară cu \textit{ecuația căldurii}. Ecuația căldurii este o EDP celebră care descrie difuzia căldurii prin spațiu. Acest lucru ar trebui să aibă sens intuitiv și pictează o imagine mentală: pe măsură ce timpul \(t\) crește, probabilitatea din originalul (poate strâns concentrat) \(\vx\) se dispersează în tot \(\R^{D}\) ca căldura care radiază dintr-o sursă în vid.
    
    Astfel de EDP-uri pentru \(p_{t}\), cunoscute ca \textit{ecuații Fokker-Planck} pentru procese stocastice mai generale, sunt unelte foarte puternice, deoarece ne permit să descriem derivatele temporale instantanee ale lui \(p_{t}\) în termenii derivatelor spațiale instantanee ale lui \(p_{t}\), și viceversa, oferind o descriere concisă a regularității și dinamicii lui \(p_{t}\). Odată ce obținem dinamica pentru \(p_{t}\), putem apoi folosi sistemul pentru a obține altul care descrie dinamica lui \(h(\vx_{t})\), care până la urmă este doar o funcțională a lui \(p_{t}\).  

    Descrierea EDP implică un obiect matematic numit Laplacianul \(\Delta\). Amintiți-vă din cursul de calcul multivariabil că Laplacianul care operează pe o funcție diferențiabilă-în-timp și de două ori-diferențiabilă-în-spațiu \(f \colon (0, T) \times \R^{D} \to \R\) este dat de
    \begin{equation}
        \Delta f_{t}(\vxi) = \tr(\nabla^{2}f_{t}(\vxi)) = \sum_{i = 1}^{D}\pddv{f_{t}}{\xi_{i}}(\vxi).
    \end{equation}
    
    Anume, folosind reprezentarea integrală a lui \(p_{t}\) și diferențiind sub integrală, putem calcula derivatele lui \(p_{t}\) (ceea ce facem în \Cref{prop:p_t_derivatives}) și observăm că \(p_{t}\) satisface EDP asemănătoare cu ecuația căldurii
    \begin{equation}
        \pdv{p_{t}}{t}(\vxi) = t\Delta p_{t}(\vxi).
    \end{equation}
    Apoi pentru a găsi dinamica lui \(h(\vx_{t})\), putem folosi \Cref{prop:dutis} din nou precum și EDP asemănătoare cu ecuația căldurii pentru a obține
    \begin{align}
        \odv*{h(\vx_{t})}{t}
        &= -\odv*{\int_{\R^{D}}p_{t}(\vxi)\log p_{t}(\vxi)\odif{\vxi}}{t} \\
        &= -\int_{\R^{D}}\pdv*{\bs{p_{t}(\vxi)\log p_{t}(\vxi)}}{t}\odif{\vxi} \\
        &= -\int_{\R^{D}}\pdv{p_{t}}{t}(\vxi)[1 + \log p_{t}(\vxi)]\odif{\vxi} \\
        &= -t\int_{\R^{D}}\Delta p_{t}(\vxi)[1 + \log p_{t}(\vxi)]\odif{\vxi}.
    \end{align}
    Folosind un argument de integrare prin părți puțin implicat (\Cref{lem:diffusion_ibp}), obținem 
    \begin{align}
        \odv*{h(\vx_{t})}{t}
        &= t\int_{\R^{D}}\ip{\nabla \log p_{t}(\vxi)}{\nabla p_{t}(\vxi)}\odif{\vxi} \\
        &= t\int_{\R^{D}}\frac{\norm{\nabla p_{t}(\vxi)}_{2}^{2}}{p_{t}(\vxi)}\odif{\vxi} \\
        &> 0
    \end{align}
    unde inegalitatea strictă este valabilă în ultima linie deoarece, pentru ca ea să nu fie valabilă, \(\nabla p_{t}(\vxi)\) ar trebui să se anuleze aproape peste tot (adică peste tot cu excepția posibil pe o mulțime de volum zero), dar aceasta ar implica că \(p_{t}\) ar fi constantă aproape peste tot, o contradicție cu faptul că \(p_{t}\) este o densitate.

    Pentru a completa demonstrația folosim doar teorema fundamentală a calculului
    \begin{equation}
        h(\vx_{t}) = h(\vx_{s}) + \int_{s}^{t}\odv*{h(\vx_{u})}{u}\odif{u} > h(\vx_{s}),
    \end{equation}
    care demonstrează afirmația. (Notăm că aceasta nu are sens când \(h(\vx_{s}) = -\infty\), ceea ce se poate întâmpla doar când \(s = 0\) și \(h(\vx) = -\infty\), dar în acest caz \(h(\vx_{t}) > -\infty\) deci afirmația este oricum adevărată în mod vacuu.)
\end{proof}

\subsection{Procesul de Denoising Reduce Entropia în Timp}\label{sub:denoising_entropy_decreases}

Amintiți-vă că în \Cref{sub:intro_diffusion_denoising} pornim cu variabila aleatoare \(\vx_{T}\) și o denoisăm iterativ folosind iterații de forma
\begin{equation}\label{eq:app_denoising_iteration}
    \hat{\vx}_{s} \doteq \Ex[\vx_{s} \mid \vx_{t} = \hat{\vx}_{t}] = \frac{s}{t}\hat{\vx}_{t} + \bp{1 - \frac{s}{t}}\bar{\vx}^{\ast}(t, \hat{\vx}_{t}).
\end{equation}
pentru \(s, t \in \{t_{0}, t_{1}, \dots, t_{L}\}\) cu \(s < t\) și \(\vx_{T} = \hat{\vx}_{T}\). Dorim să demonstrăm că \(h(\hat{\vx}_{s}) < h(\hat{\vx}_{t})\), arătând că procesul de denoising reduce de fapt entropia.

Înainte de a face aceasta, facem câteva remarci despre enunțul problemei. Întâi, formula lui Tweedie \eqref{eq:tweedie} spune că 
\begin{equation}
    \bar{\vx}^{\ast}(t, \vx_{t}) = \vx_{t} + t^{2}\nabla p_{t}(\vx_{t}),
\end{equation}
care face ca un pas complet de denoising de la timpul \(t\) la timpul \(0\) să semene cu un pas de gradient pe log-densitatea lui \(\vx_{t}\). Putem obține un rezultat similar pentru pasul complet de denoising de la timpul \(t\) la timpul \(s\) în \eqref{eq:app_denoising_iteration}? Se pare că într-adevăr putem, și este destul de simplu. Folosind \eqref{eq:app_denoising_iteration} și formula lui Tweedie \eqref{eq:tweedie}, obținem
\begin{equation}\label{eq:app_denoising_iteration_score}
    \Ex[\vx_{s} \mid \vx_{t}] = \frac{s}{t}\vx_{t} + \bp{1 - \frac{s}{t}}\bp{\vx_{t} + t^{2}\nabla_{\vx_{t}}\log p_{t}(\vx_{t})} = \vx_{t} + \bp{1 - \frac{s}{t}}t^{2}\nabla_{\vx_{t}}\log p_{t}(\vx_{t}).
\end{equation}
Deci acest pas iterativ de denoising este din nou un pas de gradient pe log-densitatea perturbată \(\log p_{t}\) cu o dimensiune a pasului micsorată. În particular, acest pas poate fi văzut ca o perturbare a distribuției variabilei aleatoare \(\vx_{t}\) prin \textit{câmpul vectorial al funcției scor}, sugerând o conexiune cu ecuațiile diferențiale stocastice (SDE) și teoria modelelor de difuzie \cite{song2020score}. Într-adevăr, o demonstrație a următorului rezultat \Cref{thm:conditioning_reduces_entropy} poate fi dezvoltată folosind acest mecanism puternic și un argument de limită (de exemplu, urmând abordarea tehnică din expunerea \cite{DBLP:conf/iclr/ChenC0LSZ23}). Vom da o demonstrație mai simplă aici, care va folosi doar unelte elementare și prin aceasta va ilumina unele dintre cantitățile cheie din spatele procesului de reducere a entropiei prin denoising. Pe de altă parte, va trebui să tratăm unele calcule ușor tehnice din cauza faptului că procesul de denoising din \Cref{thm:conditioning_reduces_entropy} \textit{nu} corespunde exact procesului \textit{invers} asociat procesului de adăugare de zgomot care generează observația \(\vx_{t}\).\footnote{Pentru cei familiarizați cu modelele de difuzie, ne referim aici la faptul că procesul înainte inversat în timp nu coincide cu secvența de iterații generate de procesul definit de \Cref{thm:conditioning_reduces_entropy}. Aceste procese coincid într-o anumită limită unde se iau infinit de mulți pași ai \Cref{thm:conditioning_reduces_entropy} cu niveluri infinit mici de zgomot adăugate la fiecare pas; pentru pași generali, finiți, trebuie să introducem unele aproximări indiferent de nivelul de sofisticare al uneltelor noastre.}

Vrem să demonstrăm că \(h(\Ex[\vx_{s} \mid \vx_{t}]) < h(\vx_{t})\), adică formal:
\begin{theorem}\label{thm:conditioning_reduces_entropy}
    Fie \(\vx\) orice variabilă aleatoare astfel încât \Cref{assumption:entropy_x_compact_support,assumption:entropy_x_density} sunt valabile, și fie \((\vx_{t})_{t \in [0, T]}\) procesul stocastic \eqref{eq:app_additive_gaussian_noise_model}. Atunci 
    \begin{equation}
        h(\Ex[\vx_{s} \mid \vx_{t}]) < h(\vx_{t}), \qquad \forall s, t \in [0, T] \colon \quad 0 < t \leq \frac{R}{\sqrt{2D}}, \quad 0 \leq s  < t\cdot\min\bc{1, \frac{R^{2}/D - 2t^{2}}{R^{2}/D - t^{2}}}.
    \end{equation}
\end{theorem}
\begin{proof}
    Această demonstrație folosește două idei principale:
    \begin{enumerate}
        \item Mai întâi, scriem o densitate pentru \(\Ex[\vx_{s} \mid \vx_{t}]\) folosind o formulă de schimbare de variabile.
        \item În al doilea rând, mărginim această densitate pentru a controla entropia.
    \end{enumerate}
    Schimbarea de variabile este justificată de \Cref{cor:gribonval_A2}, care a fost derivată inițial în \cite{Gribonval2011-pf}.

    Executăm aceste idei acum. Din \Cref{cor:gribonval_A2}, obținem că funcția \(\bar{\vx}\) definită ca \(\bar{\vx}(\vxi) \doteq \Ex[\vx_{s} \given \vx_{t} = \vxi]\) este diferențiabilă, injectivă, și astfel inversabilă pe domeniul său, pe care îl notăm de acum \(\cX \subseteq \R^{D}\). Notăm inversa sa ca \(\bar{\vx}^{-1}\). Folosind o formulă de schimbare de variabile, densitatea \(\bar{p}\) a lui \(\bar{\vx}(\vx_{t})\) este dată de 
    \begin{equation}
        \bar{p}(\vxi) \doteq \frac{(p_{t} \circ \bar{\vx}^{-1})(\vxi)}{\det(\bar{\vx}^{\prime}(\bar{\vx}^{-1}(\vxi)))},
    \end{equation}
    unde (amintiți-vă, din \Cref{sec:autodiff}) \(\bar{\vx}^{\prime}\) este jacobianul lui \(\bar{\vx}\). Deoarece din \Cref{lem:gribonval_A1} știm că \(\bar{\vx}^{\prime}\) este o matrice pozitiv definită, determinantul este pozitiv și deci întreaga densitate este pozitivă. Atunci urmează că 
    \begin{align}
        h(\bar{\vx}(\vx_{t}))
        &= -\int_{\cX}\frac{(p_{t} \circ \bar{\vx}^{-1})(\vxi)}{\det(\bar{\vx}^{\prime}(\bar{\vx}^{-1}(\vxi)))} \log \frac{(p_{t} \circ \bar{\vx}^{-1})(\vxi)}{\det(\bar{\vx}^{\prime}(\bar{\vx}^{-1}(\vxi)))} \odif{\vxi} \\ 
        &= -\int_{\cX}\frac{(p_{t} \circ \bar{\vx}^{-1})(\vxi)}{\det(\bar{\vx}^{\prime}(\bar{\vx}^{-1}(\vxi)))} \log((p_{t} \circ \bar{\vx}^{-1})(\vxi))\odif{\vxi} \\ 
        &\qquad + \int_{\cX}\frac{(p_{t} \circ
        \bar{\vx}^{-1})(\vxi)}{\det(\bar{\vx}^{\prime}(\bar{\vx}^{-1}(\vxi)))}\log\det\rp{\bar{\vx}^{\prime}(\bar{\vx}^{-1}(\vxi))}\odif{\vxi} \\ 
        &= -\int_{\R^{D}}p_{t}(\vxi)\log p_{t}(\vxi)\odif{\vxi}
        + \int_{\cX}\frac{(p_{t} \circ
        \bar{\vx}^{-1})(\vxi)}{\det(\bar{\vx}^{\prime}(\bar{\vx}^{-1}(\vxi)))}\log\det\rp{\bar{\vx}^{\prime}(\bar{\vx}^{-1}(\vxi))}\odif{\vxi} \\ 
        &= h(\vx_{t}) - \int_{\cX}\frac{(p_{t} \circ \bar{\vx}^{-1})(\vxi)}{\det(\bar{\vx}^{\prime}(\bar{\vx}^{-1}(\vxi)))}\log\rp{\frac{1}{\det(\bar{\vx}^{\prime}(\bar{\vx}^{-1}(\vxi)))}}\odif{\vxi}.
    \end{align}
    Vom studia ultimul termen (incluzând \(-\)), și vom arăta că este negativ.

    Prin concavitate, avem \(-x\log x \leq 1 - x\) pentru orice \(x \geq 0\). Prin urmare 
    \begin{align}
        h(\bar{\vx}(\vx_{t})) - h(\vx_{t})
        &= - \int_{\cX}\frac{(p_{t} \circ \bar{\vx}^{-1})(\vxi)}{\det(\bar{\vx}^{\prime}(\bar{\vx}^{-1}(\vxi)))}\log\rp{\frac{1}{\det(\bar{\vx}^{\prime}(\bar{\vx}^{-1}(\vxi)))}}\odif{\vxi} \\ 
        &\leq  \int_{\cX}(p_{t} \circ \bar{\vx}^{-1})(\vxi)\cdot \bp{1 - \frac{1}{\det(\bar{\vx}^{\prime}(\bar{\vx}^{-1}(\vxi)))}}\odif{\vxi} \\ 
        &= \int_{\cX}(p_{t} \circ \bar{\vx}^{-1})(\vxi)\odif{\vxi} - \int_{\cX}\frac{(p_{t} \circ \bar{\vx}^{-1})(\vxi)}{\det(\bar{\vx}^{\prime}(\bar{\vx}^{-1}(\vxi)))}\odif{\vxi} \\
        &= \int_{\R^{D}}p_{t}(\vxi)\det\rp{\bar{\vx}^{\prime}(\bar{\vx}^{-1}(\vxi))}\odif{\vxi} - \int_{\cX}\bar{p}(\vxi)\odif{\vxi} \\
        &= \int_{\R^{D}}p_{t}(\vxi)\det\rp{\vI + \bp{1 - \frac{s}{t}}t^{2}\nabla^{2}\log p_{t}(\vxi)}\odif{\vxi} - 1.
    \end{align}
    Acum, prin inegalitatea AM-GM pe valori proprii, avem pentru orice matrice simetrică pozitiv definită \(\vM \in \PSD(D)\) marginea 
    \begin{equation}
        \det(\vM)^{1/D} = \prod_{i = 1}^{D}\lambda_{i}(\vM)^{1/D} \leq \frac{\sum_{i = 1}^{D}\lambda_{i}(\vM)}{D} = \frac{\tr(\vM)}{D},
    \end{equation}
    pe care o putem aplica expresiei de mai sus și obținem 
    \begin{align}
        &\int_{\R^{D}}p_{t}(\vxi)\det\rp{\vI + \bp{1 - \frac{s}{t}}t^{2}\nabla^{2}\log p_{t}(\vxi)}\odif{\vxi} \\
        &\leq \int_{\R^{D}} p_{t}(\vxi) \tr\rp{\frac{1}{D}\bs{\vI + \bp{1 - \frac{s}{t}}t^{2}\nabla^{2}\log p_{t}(\vxi)}}^{D}\odif{\vxi} \\
        &= \int_{\R^{D}} p_{t}(\vxi)\bp{1 + \frac{\bp{1 - \frac{s}{t}}t^{2}}{D}\tr(\nabla^{2}\log p_{t}(\vxi))}^{D}\odif{\vxi} \\
        &= \int_{\R^{D}} p_{t}(\vxi)\bp{1 + \frac{\bp{1 - \frac{s}{t}}t^{2}}{D}\Delta \log p_{t}(\vxi)}^{D}\odif{\vxi}.
    \end{align}
    Din \Cref{lem:app_diffusion_laplacian_control}, avem (unde, amintiți-vă, \(R\) este raza suportului lui \(\vx\) ca în \Cref{assumption:entropy_x_compact_support})
    \begin{equation}
        \abs{\Delta \log p_{t}(\vxi)} \leq \max\rp{\frac{D}{t^{2}}, \abs*{\frac{R^{2}}{t^{4}} - \frac{D}{t^{2}}}} =: U_{t}.
    \end{equation}
    Atunci avem
    \begin{equation}
        -\frac{\bp{1 - \frac{s}{t}}t^{2}}{D}U_{t} \leq \frac{\bp{1 - \frac{s}{t}}t^{2}}{D}\Delta \log p_{t}(\vxi) \leq \frac{\bp{1 - \frac{s}{t}}t^{2}}{D}U_{t}.
    \end{equation}
    În același timp, funcția \(x \mapsto (1 + x)^{D}\) este convexă pe \([-1, \infty)\), deci pentru
    \(-(1-s/t)t^{2}U_{t}/D \leq x \leq (1-s/t)t^{2}U_{t}/D\) avem 
    \begin{align}
        (1 + x)^{d} 
        &\leq \bp{1 - \frac{\bp{1 - \frac{s}{t}}t^{2}U_{t}}{D}}^{D} + \underbrace{\bs{\bp{1 + \frac{\bp{1 - \frac{s}{t}}t^{2}U_{t}}{D}}^{D} - \bp{1 - \frac{\bp{1 - \frac{s}{t}}t^{2}U_{t}}{D}}^{D}}}_{M(s, t, D)}x \\ 
        &\leq 1 + M(s, t, D)x.
    \end{align}
    Aici \(M(s, t, D) > 0\) deoarece \(U_{t} > 0\). În marginea de mai sus, trebuie să verificăm că marginea inferioară pentru \(x\) este \(\geq -1\). Într-adevăr,
    \begin{align}
        -\frac{\bp{1 - \frac{s}{t}}t^{2}}{D}U_{t}
        &= -\frac{\bp{1 - \frac{s}{t}}t^{2}}{D}\max\rp{\frac{D}{t^{2}}, \abs*{\frac{R^{2}}{t^{4}} - \frac{D}{t^{2}}}} \\ 
        &= -\bp{1 - \frac{s}{t}}\max\rp{1, \abs*{\frac{R^{2}}{Dt^{2}} - 1}}
    \end{align}
    Observăm că aceasta este \(\geq -1\) dacă și numai dacă \(\bp{1 - \frac{s}{t}}\cdot\bp{\frac{R^{2}}{Dt^{2}} - 1} \geq 1\), adică \(0 < t < R/\sqrt{2D}\) și \(0 \leq s \leq t\cdot\frac{R^{2}/D - 2t^{2}}{R^{2}/D - t^{2}}\), așa cum este acordat de presupuneri.
    
    Aplicând această margine, obținem
    \begin{align}
        &\int_{\R^{D}} p_{t}(\vxi)\bp{1 + \frac{\bp{1 - \frac{s}{t}}t^{2}}{D}\Delta \log p_{t}(\vxi)}^{D}\odif{\vxi} \\ 
        &\leq \int_{\R^{D}}p_{t}(\vxi)\bp{1 + M(s, t, D)\Delta \log p_{t}(\vxi)}\odif{\vxi} \\
        &= 1 + M(s, t, D)\int_{\R^{D}}p_{t}(\vxi)\Delta \log p_{t}(\vxi)\odif{\vxi} \\
        &= 1 - M(s, t, D)\int_{\R^{D}}\ip{\nabla p_{t}(\vxi)}{\nabla\log p_{t}(\vxi)}\odif{\vxi} \\
        &= 1 - M(s, t, D)\int_{\R^{D}}\frac{\norm{\nabla p_{t}(\vxi)}_{2}^{2}}{p_{t}(\vxi)}\odif{\vxi},
    \end{align}
    unde ultimele câteva linii sunt aceleași ca în demonstrația \Cref{thm:diffusion_entropy_increases}. Combinând acest rezultat cu estimarea noastră anterioară,
    \begin{equation}
        h(\bar{\vx}(\vx_{t})) - h(\vx_{t}) \leq - M(s, t, D)\int_{\R^{D}}\frac{\norm{\nabla p_{t}(\vxi)}_{2}^{2}}{p_{t}(\vxi)}\odif{\vxi} < 0
    \end{equation}
    unde inegalitatea este strictă prin același argument ca în \Cref{thm:diffusion_entropy_increases}.
\end{proof}

Observați că marginile pentru \(s\) și \(t\) depind de raza \(R\) a distribuției datelor, și nu sunt atât de generale ca marginile din \Cref{thm:diffusion_entropy_increases}. Rezultatul este de fapt ``atât de general cât este necesar'' în următorul sens. Notăm că dacă \(\vx\) are o densitate de două ori continuu diferențiabilă suportată pe bila de rază \(R\) centrată în \(\vzero\), atunci o are pentru \(2R\), și \(3R\), și așa mai departe, adică pentru orice bilă de rază \(R^{\prime} > R\). Așadar o strategie pentru a obține garanția de denoising corespunzătoare este: fixați o dimensiune a datelor \(D\) și un program de discretizare, și apoi setați (în analiză) raza datelor \(R\) să fie foarte mare astfel încât fiecare pas de denoising să satisfacă cerințele pentru scăderea entropiei date în \Cref{thm:conditioning_reduces_entropy}. Atunci fiecare pas al procesului de denoising va reduce într-adevăr entropia, așa cum se dorea.

 
\subsection{Leme Tehnice și Rezultate Intermediare}\label{sub:app_diffusion_intermediate_results}

În această subsecțiune prezentăm rezultate tehnice care susțin cele două teoreme conceptuale principale ale noastre. Prezentarea noastră va fi mai mult sau mai puțin standard pentru matematică; vom începe cu rezultatele de nivel mai înalt mai întâi, și vom merge treptat înapoi la rezultatele mai incrementale. Rezultatele de nivel mai înalt vor folosi rezultatele incrementale, și în acest fel avem o ordonare ușor de citit a dependențelor rezultatelor: niciun rezultat nu depinde de cele dinaintea sa. Rezultatele care nu depind unele de altele sunt în general ordonate după locul în care apar în perechea de demonstrații de mai sus. 


\subsubsection{Finitudinea Entropiei Diferențiale}

Mai întâi arătăm că entropia există de-a lungul procesului stocastic și este finită.

\begin{lemma}\label{lem:diffusion_entropy_exists}
    Fie \(\vx\) orice variabilă aleatoare, și fie \((\vx_{t})_{t \in [0, T]}\) procesul stocastic \eqref{eq:app_additive_gaussian_noise_model}. 
    \begin{enumerate}
        \item Pentru \(t > 0\), entropia diferențială \(h(\vx_{t})\) există și este \(> -\infty\).
        \item Dacă în plus \Cref{assumption:entropy_x_compact_support} este valabilă pentru \(\vx\), atunci \(h(\vx) < \infty\) și \(h(\vx_{t})\ < \infty\).
    \end{enumerate}
\end{lemma}
\begin{proof}
    Pentru a demonstra \Cref{lem:diffusion_entropy_exists}.1, folosim un argument de analiză clasic dar obositor. Deoarece \(\vx_{t}\) are o densitate, putem scrie 
    \begin{equation}
        h(\vx_{t}) = -\int_{\R^{D}}p_{t}(\vxi)\log p_{t}(\vxi) \odif{\vxi}.
    \end{equation}
    În consecință, fie \(g \colon \R^{D} \to \R\) definită ca 
    \begin{equation}
        g(\vxi) \doteq -p_{t}(\vxi)\log p_{t}(\vxi) \implies h(\vx_{t}) = \int_{\R^{D}}g(\vxi)\odif{\vxi}.
    \end{equation}
    Ca de obicei pentru a mărgini valoarea unei integrale în analiză, definim 
    \begin{equation}
        g_{+}(\vxi) \doteq \max(g(\vxi), 0), \quad g_{-}(\vxi) \doteq \max(-g(\vxi), 0) \quad \implies \quad g = g_{+} - g_{-}\quad \text{și} \quad g_{+}, g_{-} \geq 0.
    \end{equation}
    Atunci 
    \begin{equation}
        h(\vx_{t}) = \int_{\R^{D}}g_{+}(\vxi)\odif{\vxi} - \int_{\R^{D}}g_{-}(\vxi)\odif{\vxi},
    \end{equation}
    și ambele integrale sunt garantate să fie ne-negative deoarece integranții lor sunt. 
    
    Pentru a arăta că \(h(\vx_{t})\) este bine definită, trebuie să arătăm că \(\int_{\R^{D}}g_{+}(\vxi)\odif{\vxi} < \infty\) sau \(\int_{\R^{D}}g_{-}(\vxi) \odif{\vxi} < \infty\). Pentru a arăta că \(h(\vx_{t}) > -\infty\), este suficient doar să arătăm că \(\int_{\R^{D}}g_{-}(\vxi)\odif{\vxi} < \infty\). Pentru a mărgini integrala lui \(g_{-}\) trebuie să înțelegem cantitatea \(g_{-}\), anume, vrem să caracterizăm când \(g\) este negativă.
    \begin{equation}
        g(\vxi) \leq 0 \iff p_{t}(\vxi)\log p_{t}(\vxi) \geq 0 \iff \log p_{t}(\vxi) \geq 0 \iff p_{t}(\vxi) \geq 1.
    \end{equation}
    Așadar, avem că 
    \begin{equation}
        g_{-}(\vxi) = \indvar(p_{t}(\vxi) \geq 1)\cdot (-g(\vxi)) = \indvar(p_{t}(\vxi) \geq 1)p_{t}(\vxi)\log p_{t}(\vxi).
    \end{equation}
    Pentru a mărgini integrala lui \(g_{-}(\vxi)\), trebuie să arătăm că \(p_{t}\) ``nu este prea concentrată,'' anume că \(p_{t}\) nu este prea mare. Pentru a demonstra aceasta, în acest caz suntem norocoși să putem mărgini funcția \(g_{-}(\vxi)\) însăși. Anume, observăm că 
    \begin{equation}
        \max_{\vxi \in \R^{D}}\phi_{t}(\vxi - \vx) = \phi_{t}(\vzero) = \frac{1}{(2\pi)^{D/2}t^{D}} =: C_{t}.
    \end{equation}
    care explodează când \(t \to 0\) dar este finită pentru toți \(t\) finiți. Prin urmare 
    \begin{equation}
        p_{t}(\vxi) = \Ex \phi_{t}(\vxi - \vx) \leq \Ex C_{t} = C_{t}.
    \end{equation}
    Acum există două cazuri.
    \begin{itemize}
        \item Dacă \(C_{t} < 1\), atunci \(p_{t}(\vxi) < 1\), deci indicatorul nu este niciodată \(1\), prin urmare \(g_{-} = 0\) identic și integrala sa este de asemenea \(0\).
        \item Dacă \(C_{t} \geq 1\), atunci \(\log C_{t} \geq 0\), deci deoarece logaritmul este monoton crescător,
        \begin{align}
            \int_{\R^{D}}g_{-}(\vxi)\odif{\vxi}
            &= \int_{\R^{D}}\indvar(p_{t}(\vxi) \geq 1)p_{t}(\vxi)\log p_{t}(\vxi)\odif{\vxi}  \\ 
            &= \Ex[\indvar(p_{t}(\vx_{t}) \geq 1)\log p_{t}(\vx_{t})]  \\ 
            &\leq \Ex[\indvar(p_{t}(\vx_{t}) \geq 1) \log C_{t}] \\ 
            &= \Pr[p_{t}(\vx_{t}) \geq 1]\log C_{t}.
        \end{align}
    \end{itemize}
    Prin urmare avem \(\int_{\R^{D}}g_{-}(\vxi)\odif{\vxi} < \infty\), deci entropia diferențială \(h(\vx_{t})\) există și este \(> -\infty\).

    Pentru a demonstra \Cref{lem:diffusion_entropy_exists}.2, să presupunem că \Cref{assumption:entropy_x_compact_support} este valabilă. Vrem să arătăm că \(h(\vx) < \infty\) și \(h(\vx_{t}) < \infty\). Mecanismul pentru a face aceasta este același, și implică rezultatul de entropie maximă \Cref{thm:max_entropy}. Anume, deoarece \(\vx\) este absolut mărginită, are o covarianță finită pe care o vom nota \(\vSigma\). Atunci covarianța lui \(\vx_{t}\) este \(\vSigma + t^{2}\vI\). Așadar entropia lui \(\vx\) și \(\vx_{t}\) poate fi mărginită superior de entropia distribuțiilor normale cu covarianțele respective, adică \(\log[(2\pi e)^{D}\det(\vSigma)]\) sau \(\log[(2\pi e)^{D}\det(\vSigma + t^{2}\vI)]\), și ambele sunt \(< \infty\).
\end{proof}

\subsubsection{Integrare prin Părți în Identitatea De Brujin}

În final, completăm argumentul de integrare prin părți la care s-a făcut aluzie în demonstrațiile \Cref{thm:diffusion_entropy_increases,thm:conditioning_reduces_entropy}. Argumentul este conceptual destul de simplu dar necesită unele estimări tehnice pentru a arăta că termenul de frontieră în integrarea prin părți dispare.

\begin{lemma}\label{lem:diffusion_ibp}
    Fie \(\vx\) orice variabilă aleatoare astfel încât \Cref{assumption:entropy_x_compact_support,assumption:entropy_x_density} sunt valabile, și fie \((\vx_{t})_{t \in [0, T]}\) procesul stocastic \eqref{eq:app_additive_gaussian_noise_model}. Pentru \(t \geq 0\), fie \(p_{t}\) densitatea lui \(\vx_{t}\). Atunci pentru o constantă \(c \in \R\) avem 
    \begin{equation}
        \int_{\R^{D}}\Delta p_{t}(\vxi)[c + \log p_{t}(\vxi)]\odif{\vxi} = -\int_{\R^{D}}\ip{\nabla \log p_{t}(\vxi)}{\nabla p_{t}(\vxi)}\odif{\vxi}.
    \end{equation}
\end{lemma}
\begin{proof}
    Ideea de bază a acestei demonstrații este în doi pași:
    \begin{itemize}
        \item Mai întâi, aplicăm teorema lui Green pentru a face integrare prin părți peste o mulțime compactă;
        \item În al doilea rând, trimitem raza acestei mulțimi compacte la \(+\infty\), pentru a obține integrale peste tot \(\R^{D}\).
    \end{itemize}
    Teorema lui Green spune că pentru orice mulțime compactă \(\cK \subseteq \R^{D}\), \(\plainphi \colon \R^{D} \to \R\) de două ori continuu diferențiabilă, și \(\psi \colon \R^{D} \to \R\) continuu diferențiabilă,
    \begin{equation}
        \int_{\cK}\bc{\psi(\vxi) \Delta \plainphi(\vxi) + \ip{\nabla \psi(\vxi)}{\nabla \plainphi(\vxi)}}\odif{\vxi} = \int_{\partial \cK}\psi(\vxi)\ip{\nabla \plainphi(\vxi)}{\vn(\vxi)}\odif{\sigma(\vxi)}
    \end{equation}
    unde \(\odif{\sigma(\vxi)}\) denotă o integrală peste ``măsura de suprafață'', adică măsura moștenită pe \(\partial \cK\), anume frontiera lui \(\cK\), și în consecință \(\vxi\) ia valori pe această suprafață și \(\vn(\vxi)\) este vectorul normal unitar la \(\cK\) în punctul de pe suprafață \(\vxi\). Acum, luând \(\plainphi(\vxi) \doteq p_{t}(\vxi)\) și \(\psi(\vxi) \doteq c + \log p_{t}(\vxi)\), peste o bilă \(B_{r}(\vzero)\) de rază \(r > 0\) centrată în \(\vzero\) (astfel încât \(\partial B_{r}(\vzero)\) este sfera de rază \(r\) centrată în \(\vzero\) și \(\vn(\vxi) = \vxi/\norm{\vxi}_{2} = \vxi/r\)):
    \begin{align}
        &\int_{B_{r}(\vzero)}\bc{\Delta p_{t}(\vxi)[c + \log p_{t}(\vxi)] + \ip{\nabla \log p_{t}(\vxi)}{\nabla p_{t}(\vxi)}}\odif{\vxi} \\
        &= \int_{\partial B_{r}(\vzero)}[c + \log p_{t}(\vxi)]\ip*{\nabla p_{t}(\vxi)}{\frac{\vxi}{r}}\odif{\sigma(\vxi)} \\
        &= \frac{1}{r}\int_{\partial B_{r}(\vzero)}[c + \log p_{t}(\vxi)]\ip*{\nabla p_{t}(\vxi)}{\vxi}\odif{\sigma(\vxi)}.
    \end{align}
    Trimițând \(r \to \infty\), avem că 
    \begin{align}
        &\int_{\R^{D}}\bc{\Delta p_{t}(\vxi)[c + \log p_{t}(\vxi)] + \ip{\nabla \log p_{t}(\vxi)}{\nabla p_{t}(\vxi)}}\odif{\vxi} \label{eq:app_diffusion_r_infinity_limit} \\
        &= \lim_{r \to \infty}\int_{B_{r}(\vzero)}\bc{\Delta p_{t}(\vxi)[c + \log p_{t}(\vxi)] + \ip{\nabla \log p_{t}(\vxi)}{\nabla p_{t}(\vxi)}}\odif{\vxi} \\
        &= \lim_{r \to \infty}\frac{1}{r}\int_{\partial B_{r}(\vzero)}[c + \log p_{t}(\vxi)]\ip*{\nabla p_{t}(\vxi)}{\vxi}\odif{\sigma(\vxi)},
    \end{align}
    unde prima inegalitate urmează prin convergență dominată pe integrand. Rămâne să calculăm ultima limită. Pentru aceasta, luăm expansiuni asimptotice ale fiecărui termen. Dispozitivul principal este următorul: pentru \(\vxi \in \partial B_{r}(\vzero)\), avem \(\norm{\vxi}_{2} = r\). Prin inegalitatea Cauchy-Schwarz, pentru orice \(\vx\) cu \(\norm{\vx}_{2} \leq R\), avem
    \begin{equation}
        -2r\norm{\vx}_{2} - \norm{\vx}_{2}^{2} \leq 2\ip{\vxi}{\vx} - \norm{\vx}_{2}^{2} \leq 2r\norm{\vx}_{2} - \norm{\vx}_{2}^{2}.
    \end{equation}
    Reamintim că prin \Cref{assumption:entropy_x_compact_support}, \(\vx\) este suportat pe o mulțime compactă \(\cS\) de rază \(R\). Astfel
    \begin{equation}
        -2R(r + R) \leq 2\ip{\vxi}{\vx} - \norm{\vx}_{2}^{2} \leq 2Rr.
    \end{equation}
    Cu alte cuvinte, avem
    \begin{equation}
        C_{t}e^{-[r^{2} + 2R(r + R)]/(2t^{2})} \leq p_{t}(\vxi) \leq C_{t}e^{[-r^{2} + 2Rr]/(2t^{2})}.
    \end{equation}
    Acum pentru a calcula gradientul, putem folosi \Cref{prop:p_t_derivatives} și liniaritatea așteptării pentru a calcula
    \begin{align}
        \ip{\nabla p_{t}(\vxi)}{\vxi}
        &= \ip*{-\frac{1}{t^{2}}\Ex\rs{\bp{\vxi - \vx}\phi_{t}(\vxi - \vx)}}{\vxi} \\
        &= -\frac{1}{t^{2}}\Ex\rs{\ip{\vxi - \vx}{\vxi}\phi_{t}(\vxi - \vx)} \\
        &= -\frac{1}{t^{2}}\Ex\rs{\bp{\norm{\vxi}_{2}^{2} - \ip{\vxi}{\vx}}\phi_{t}(\vxi - \vx)} \\
        &= -\frac{1}{t^{2}}\Ex\rs{\bp{r^{2} - \ip{\vxi}{\vx}}\phi_{t}(\vxi - \vx)} \\
        &= \frac{1}{t^{2}}\Ex\rs{\bp{\ip{\vxi}{\vx} - r^{2}}\phi_{t}(\vxi - \vx)}.
    \end{align}
    Folosind Cauchy-Schwarz și reprezentarea \(p_{t}(\vxi) \doteq \Ex[\phi_{t}(\vxi - \vx)]\) din nou, avem
    \begin{align}
        &\frac{1}{t^{2}}\Ex\rs{\bp{-Rr - r^{2}}\phi_{t}(\vxi - \vx)} \leq \ip{\nabla p_{t}(\vxi)}{\vxi} \leq \frac{1}{t^{2}}\Ex\rs{\bp{Rr - r^{2}}\phi_{t}(\vxi - \vx)} \\
        &\frac{1}{t^{2}}\bp{-Rr - r^{2}}\Ex\rs{\phi_{t}(\vxi - \vx)} \leq \ip{\nabla p_{t}(\vxi)}{\vxi} \leq \frac{1}{t^{2}}\bp{Rr - r^{2}}\Ex\rs{\phi_{t}(\vxi - \vx)} \\
        &-\frac{r(R + r)}{t^{2}}p_{t}(\vxi) \leq \ip{\nabla p_{t}(\vxi)}{\vxi} \leq -\frac{r(r - R)}{t^{2}}p_{t}(\vxi).
    \end{align}
    Pentru \(r > R > 0\) (cum este potrivit, deoarece vom lua limita \(r \to \infty\) în timp ce \(R\) este fix), ambele părți sunt negative. Acest lucru are sens: cea mai mare parte a masei de probabilitate este conținută în bila de rază \(R\) și astfel scorul indică spre interior, având un produs scalar negativ cu vectorul care indică spre exterior \(\vxi\). Astfel, folosind limitele corespunzătoare pentru \(p_{t}(\vxi)\),
    \begin{equation}
        -\frac{r(R + r)}{t^{2}}\cdot C_{t}e^{[-r^{2} + 2Rr]/(2t^{2})} \leq \ip{\nabla p_{t}(\vxi)}{\vxi} \leq -\frac{r(r - R)}{t^{2}}\cdot C_{t}e^{-[r^{2} + 2R(r + R)]/(2t^{2})}.
    \end{equation}
    Apoi, notând că \(C_{t} = \mathrm{poly}(t^{-1})\), putem calcula
    \begin{equation}
        [c + \log p_{t}(\vxi)]\ip{\nabla p_{t}(\vxi)}{\vxi} = \mathrm{poly}(r, R, t^{-1}, c)e^{-\Theta_{r}(r^{2})}
    \end{equation}
    Deci se poate vedea că, notând aria suprafeței \(\partial B_{r}(\vzero)\) ca \(\omega_{D} r^{D - 1}\) unde \(\omega_{D}\) este o funcție de \(D\), avem
    \begin{equation}
        \frac{1}{r}\int_{\partial B_{r}(\vzero)}[c + \log p_{t}(\vxi)]\ip{\nabla p_{t}(\vxi)}{\vxi}\odif{\vxi} = \mathrm{poly}(r, R, t^{-1}, c)e^{-\Theta_{r}(r^{2})}
    \end{equation}
    și prin urmare cozile exponențial descăzătoare înseamnă
    \begin{equation}
        \lim_{r \to \infty}\frac{1}{r}\int_{\partial B_{r}(\vzero)}[c + \log p_{t}(\vxi)]\ip{\nabla p_{t}(\vxi)}{\vxi}\odif{\vxi} = 0.
    \end{equation}
    În final, înlocuind în \eqref{eq:app_diffusion_r_infinity_limit}, avem
    \begin{align}
        &\int_{\R^{D}}\bc{\Delta p_{t}(\vxi)[c + \log p_{t}(\vxi)] + \ip{\nabla \log p_{t}(\vxi)}{\nabla p_{t}(\vxi)}}\odif{\vxi} = 0 \\
        \implies 
        &\int_{\R^{D}}\Delta p_{t}(\vxi)[c + \log p_{t}(\vxi)]\odif{\vxi} = -\int_{\R^{D}}\ip{\nabla \log p_{t}(\vxi)}{\nabla p_{t}(\vxi)}\odif{\vxi}
    \end{align}
    așa cum s-a afirmat.
\end{proof}

\subsubsection{Invertibilitatea Locală a Denoiser-ului \(\bar{\vx}\)}

Aici oferim câteva rezultate folosite în demonstrația \Cref{thm:conditioning_reduces_entropy} care sunt generalizări corespunzătoare ale rezultatelor corespondente din \cite{Gribonval2011-pf}.

\begin{lemma}[Generalizare a \cite{Gribonval2011-pf}, Lema A.1]\label{lem:gribonval_A1}
    Fie \(\vx\) orice variabilă aleatoare astfel încât \Cref{assumption:entropy_x_compact_support,assumption:entropy_x_density} sunt valabile, și fie \((\vx_{t})_{t \in [0, T]}\) procesul stocastic \eqref{eq:app_additive_gaussian_noise_model}. Fie \(s, t \in [0, T]\) astfel încât \(0 \leq s < t \leq T\), și fie \(\bar{\vx}(\vxi) \doteq \Ex[\vx_{s} \mid \vx_{t} = \vxi]\). Jacobianul \(\bar{\vx}^{\prime}(\vxi)\) este simetric pozitiv definit.
\end{lemma}
\begin{proof}
    Avem 
    \begin{equation}
        \bar{\vx}^{\prime}(\vxi) = \vI + \bp{1 - \frac{s}{t}}t^{2}\nabla^{2}\log p_{t}(\vxi).
    \end{equation}
    Aici expandăm 
    \begin{equation}
        \nabla^{2}\log p_{t}(\vxi) = \frac{p_{t}(\vxi)\nabla^{2}p_{t}(\vxi) - (\nabla p_{t}(\vxi))(\nabla p_{t}(\vxi))^{\top}}{p_{t}(\vxi)^{2}}.
    \end{equation}
    Deci trebuie să asigurăm că 
    \begin{align}
        \bar{\vx}^{\prime}(\vxi)
        &= \vI + \bp{1 - \frac{s}{t}}t^{2}\frac{p_{t}(\vxi)\nabla^{2}p_{t}(\vxi) - (\nabla p_{t}(\vxi))(\nabla p_{t}(\vxi))^{\top}}{p_{t}(\vxi)^{2}} \\
        &= \frac{p_{t}(\vxi)^{2}\vI + \bp{1 - \frac{s}{t}}t^{2}\bs{p_{t}(\vxi)\nabla^{2}p_{t}(\vxi) - (\nabla p_{t}(\vxi))(\nabla p_{t}(\vxi))^{\top}}}{p_{t}(\vxi)^{2}}
    \end{align}
    este simetric pozitiv semidefinit. Într-adevăr este evident simetric (prin teorema lui Clairaut). Pentru a arăta pozitiv semidefinirea sa, înlocuim reprezentarea de așteptare a lui \(p_{t}\) dată de \eqref{eq:p_t_representation} (și \(\nabla p_{t}\), \(\Delta p_{t}\) prin \Cref{prop:p_t_derivatives}) pentru a obține (unde \(\vx\) este așa cum este definit și \(\vy\) este i.i.d.~ca \(\vx\)),
    \begin{align}
        &\vv^{\top}[\bar{\vx}^{\prime}(\vxi)]\vv \\
        &= p_{t}(\vxi)^{-2}\vv^{\top}\Bigg\{p_{t}(\vxi)^{2}\vI + \bp{1 - \frac{s}{t}}t^{2}\Ex[\phi_{t}(\vxi - \vx)]\Ex\rs{\phi_{t}(\vxi - \vx)\cdot\frac{(\vxi - \vx)(\vxi - \vx)^{\top} - t^{2}\vI}{t^{4}}} \\
        & \qquad \qquad \qquad -\bp{1 - \frac{s}{t}}t^{2}\Ex\rs{-\phi_{t}(\vxi - \vx)\cdot\frac{\vxi - \vx}{t^{2}}}\Ex\rs{-\phi_{t}(\vxi - \vx)\cdot\frac{\vxi - \vx}{t^{2}}}^{\top}\Bigg\}\vv \\
        &= p_{t}(\vxi)^{-2}\vv^{\top}\Bigg\{\Ex[\phi_{t}(\vxi - \vx)\phi_{t}(\vxi - \vy)\vI] \\
        & \qquad \qquad \qquad + \bp{1 - \frac{s}{t}}t^{2}\Ex\rs{\phi_{t}(\vxi - \vx)\phi_{t}(\vxi - \vy)\cdot\frac{(\vxi - \vy)(\vxi - \vy)^{\top} - t^{2}\vI}{t^{4}}} \\
        & \qquad \qquad \qquad -\bp{1 - \frac{s}{t}}t^{2}\Ex\rs{\phi_{t}(\vxi - \vx)\phi_{t}(\vxi - \vy)\cdot\frac{(\vxi - \vx)(\vxi - \vy)^{\top}}{t^{4}}}\Bigg\} \\ 
        &= \frac{1 - s/t}{p_{t}(\vxi)^{2}}\vv^{\top}\Ex\mathopen{}\Bigg[\phi_{t}(\vxi - \vx)\phi_{t}(\vxi - \vy)\bc{\frac{1}{1 - s/t}\vI + \frac{(\vxi - \vy)(\vxi - \vy)^{\top}}{t^{2}} - \vI - \frac{(\vxi - \vx)(\vxi - \vy)^{\top}}{t^{2}}}\Bigg]\vv \\
        &= \frac{t - s}{tp_{t}(\vxi)^{2}}\vv^{\top}\Ex\rs{\frac{s}{t - s}\vI +\frac{(\vxi - \vx)(\vxi - \vx)^{\top}}{2t^{2}} + \frac{(\vxi - \vy)(\vxi - \vy)^{\top}}{2t^{2}} - \frac{(\vxi - \vx)(\vxi - \vy)^{\top}}{t^{2}}}\vv \\
        &= \frac{t - s}{tp_{t}(\vxi)^{2}}\vv^{\top}\Ex\rs{\frac{s}{t - s}\vI +\frac{1}{2t^{2}}\bp{(\vxi - \vx)(\vxi - \vx)^{\top} + (\vxi - \vy)(\vxi - \vy)^{\top} - 2(\vxi - \vx)(\vxi - \vy)^{\top}}}\vv \\
        &= \frac{t - s}{tp_{t}(\vxi)^{2}}\Ex\rs{\frac{s}{t - s}\norm{\vv}_{2}^{2} +\frac{1}{2t^{2}}\bp{[(\vxi - \vx)^{\top}\vv]^{2} + [(\vxi - \vy)^{\top}\vv]^{2} - 2[(\vxi - \vx)^{\top}\vv][(\vxi - \vy)^{\top}\vv]}} \\
        &= \frac{t - s}{tp_{t}(\vxi)^{2}}\Ex\rs{\frac{s}{t - s}\norm{\vv}_{2}^{2} +\frac{1}{2t^{2}}\bp{[(\vxi - \vx)^{\top}\vv] - [(\vxi - \vy)^{\top}\vv]}^{2}} \\
        &= \frac{t - s}{tp_{t}(\vxi)^{2}}\Ex\rs{\frac{s}{t - s}\norm{\vv}_{2}^{2} +\frac{1}{2t^{2}}[(\vy - \vx)^{\top}\vv]^{2}} \\
        &= \frac{s}{tp_{t}(\vxi)^{2}}\norm{\vv}_{2}^{2} + \frac{t - s}{2t^{3}p_{t}(\vxi)}\Ex[\{(\vy - \vx)^{\top}\vv\}^{2}]
    \end{align}
    Deoarece \(\vx\) și \(\vy\) sunt i.i.d., întreaga integrală (adică forma pătratică originală) este \(0\) dacă și numai dacă \(s = 0\) și \(\vx\) are suport conținut în întregime într-un subspațiu afin care este ortogonal pe \(\vv\). Dar aceasta este exclusă prin presupunere (adică că \(\vx\) are o densitate pe \(\R^{D}\)), deci jacobianul \(\bar{\vx}^{\prime}(\vxi)\) este simetric pozitiv definit.
\end{proof}

\begin{lemma}[Generalizare a \cite{Gribonval2011-pf} Corolar A.2, Partea 1]\label{lem:gribonval_A2}
    Fie \(f \colon \R^{D} \to \R^{D}\) orice funcție diferențiabilă al cărei jacobian \(f^{\prime}(\vx)\) este simetric pozitiv definit. Atunci \(f\) este injectivă, și prin urmare inversabilă ca funcție \(\R^{D} \to \Range(f)\) unde \(\Range(f)\) este imaginea lui \(f\).
\end{lemma}
\begin{proof}
    Să presupunem că \(f\) nu ar fi injectivă, adică există \(\vx, \vx^{\prime}\) astfel încât \(f(\vx) = f(\vx^{\prime})\) în timp ce \(\vx \neq \vx^{\prime}\). Definim \(\vv \doteq (\vx^{\prime} - \vx)/\norm{\vx^{\prime} - \vx}_{2}\). Definim funcția \(g \colon \R \to \R\) ca \(g(t) \doteq \vv^{\top}f(\vx + t\vv)\). Atunci \(g(0) = \vv^{\top}f(\vx) = \vv^{\top}f(\vx^{\prime}) = g(\norm{\vx^{\prime} - \vx}_{2})\). Deoarece \(f\) este diferențiabilă, \(g\) este diferențiabilă, deci derivata \(g^{\prime}\) trebuie să se anuleze pentru un \(t^{\ast} \in (0, \norm{\vx^{\prime} - \vx}_{2})\) prin teorema valorii medii. Totuși,
    \begin{equation}
        g^{\prime}(t^{\ast}) \doteq \vv^{\top}\bs{f^{\prime}(\vx + t^{\ast}\vv)}\vv > 0
    \end{equation}
    deoarece jacobianul este pozitiv definit. Așadar ajungem la o contradicție, așa cum s-a afirmat.
\end{proof}

Combinând cele două rezultate de mai sus, obținem următorul rezultat crucial.

\begin{corollary}[Generalizare a \cite{Gribonval2011-pf} Corolar A.2, Partea 2]\label{cor:gribonval_A2}
    Fie \(\vx\) orice variabilă aleatoare astfel încât \Cref{assumption:entropy_x_compact_support,assumption:entropy_x_density} sunt valabile, și fie \((\vx_{t})_{t \in [0, T]}\) procesul stocastic \eqref{eq:app_additive_gaussian_noise_model}. Fie \(s, t \in [0, T]\) astfel încât \(0 \leq s < t \leq T\), și fie \(\bar{\vx}(\vxi) \doteq \Ex[\vx_{s} \mid \vx_{t} = \vxi]\). Atunci \(\bar{\vx}\) este injectivă, și prin urmare inversabilă pe imaginea sa.
\end{corollary}
\begin{proof}
    Singurul lucru rămas de arătat este că \(\bar{\vx}\) este diferențiabilă, dar aceasta este imediat din formula lui Tweedie (\Cref{thm:tweedie}) care arată că \(\bar{\vx}\) este diferențiabilă dacă și numai dacă \(\nabla \log p_{t}\) este diferențiabilă, și aceasta este furnizată de \Cref{eq:p_t_representation}.
\end{proof}

\subsubsection{Controlul Laplacianului \(\Delta \log p_{t}\)}

În final, dezvoltăm o estimare tehnică care este necesară pentru demonstrația \Cref{thm:conditioning_reduces_entropy} și motivează de fapt presupunerea pentru \(t\) viabil.

\begin{lemma}\label{lem:app_diffusion_laplacian_control}
    Fie \(\vx\) orice variabilă aleatoare astfel încât \Cref{assumption:entropy_x_compact_support,assumption:entropy_x_density} sunt valabile, și fie \((\vx_{t})_{t \in [0, T]}\) procesul stocastic \eqref{eq:app_additive_gaussian_noise_model}. Fie \(p_{t}\) densitatea lui \(\vx_{t}\). Atunci, pentru \(t > 0\) avem 
    \begin{equation}
        \sup_{\vxi \in \R^{D}}\abs{\Delta \log p_{t}(\vxi)} \leq \max\rp{\frac{D}{t^{2}}, \abs*{\frac{R^{2}}{t^{4}} - \frac{D}{t^{2}}}}.
    \end{equation}
\end{lemma}
\begin{proof}
    Prin regula lanțului, un exercițiu simplu calculează 
    \begin{equation}
        \Delta \log p_{t}(\vxi) = \frac{\Delta p_{t}(\vxi)}{p_{t}(\vxi)} - \frac{\norm{\nabla p_{t}(\vxi)}_{2}^{2}}{p_{t}(\vxi)^{2}}.
    \end{equation}
    Folosind \Cref{prop:p_t_derivatives} pentru a scrie termenii din \(\Delta p_{t}(\vxi)\), obținem
    \begin{align}
        \frac{\Delta p_{t}(\vxi)}{p_{t}(\vxi)}
        &= \frac{\Ex\rs{\frac{\norm{\vxi - \vx}_{2}^{2} - Dt^{2}}{t^{4}} \cdot \phi_{t}(\vxi - \vx)}}{\Ex[\phi_{t}(\vxi - \vx)]} \\
        &= \frac{\int_{\R^{D}}\bc{\frac{\norm{\vxi - \vu}_{2}^{2} - Dt^{2}}{t^{4}}}\phi_{t}(\vxi - \vu)p(\vu)\odif{\vu}}{\int_{\R^{D}}\phi_{t}(\vxi - \vu)p(\vu)\odif{\vu}}.
    \end{align}
    Aceasta arată ca o marginalizare bayesiană, deci să definim densitatea normalizată corespunzătoare
    \begin{equation}
        q_{\vxi}(\vu) = \frac{\phi_{t}(\vxi - \vu)p(\vu)}{\int_{\R^{D}}\phi_{t}(\vxi - \vv)p(\vv)\odif{\vv}} = \frac{\phi_{t}(\vxi - \vu)p(\vu)}{\Ex[\phi_{t}(\vxi - \vx)]} = \frac{\phi_{t}(\vxi - \vu)p(\vu)}{p_{t}(\vxi)}
    \end{equation}
    Apoi, definând \(\vy_{\vxi} \sim q_{\vxi}\), putem scrie 
    \begin{equation}
        \frac{\Delta p_{t}(\vxi)}{p_{t}(\vxi)} = \int_{\R^{D}}\bc{\frac{\norm{\vxi - \vu}_{2}^{2} - Dt^{2}}{t^{4}}}q_{\vxi}(\vu)\odif{\vu} = \frac{1}{t^{4}}\Ex[\norm{\vxi - \vy_{\vxi}}_{2}^{2}] - \frac{D}{t^{2}}.
    \end{equation}
    Similar, scriind al doilea termen (nepătrat) obținem
    \begin{equation}
        \frac{\nabla p_{t}(\vxi)}{p_{t}(\vxi)} = -\frac{\vxi - \Ex[\vy_{\vxi}]}{t^{2}}.
    \end{equation}
    Notând \(\vz_{\vxi} \doteq \vy_{\vxi} - \vxi\), avem
    \begin{equation}
        \frac{\Delta p_{t}(\vxi)}{p_{t}(\vxi)} = \frac{\Ex[\norm{\vz_{\vxi}}_{2}^{2}]}{t^{4}} - \frac{D}{t^{2}}, \qquad \frac{\nabla p_{t}(\vxi)}{p_{t}(\vxi)} = \frac{\Ex[\vz_{\vxi}]}{t^{2}}.
    \end{equation}
    Astfel scriind \(\Delta \log p_{t}\) complet, avem
    \begin{align}
        \Delta \log p_{t}(\vxi)
        &= \frac{\Ex[\norm{\vz_{\vxi}}_{2}^{2}]}{t^{4}} - \frac{D}{t^{2}} - \frac{\norm{\Ex[\vz_{\vxi}]}_{2}^{2}}{t^{4}} \\
        &= \frac{\Ex[\norm{\vz_{\vxi}}_{2}^{2}] - \norm{\Ex[\vz_{\vxi}]}_{2}^{2}}{t^{4}} - \frac{D}{t^{2}} \\
        &= \frac{\tr(\Cov(\vz_{\vxi}))}{t^{4}} - \frac{D}{t^{2}} \\
        &= \frac{\tr(\Cov(\vy_{\vxi}))}{t^{4}} - \frac{D}{t^{2}}.
    \end{align}
    O limită inferioară trivială pentru această urmă este \(0\), deoarece matricele de covarianță sunt pozitiv semidefinite. Pentru a găsi o limită superioară, observăm că \(\vy_{\vxi}\) ia valori doar în suportul lui \(\vx\) (deoarece \(p\) este un factor al densității \(q_{\vxi}\) a lui \(\vy_{\vxi}\)), care prin \Cref{assumption:entropy_x_compact_support} este o mulțime compactă \(\cS\) cu raza \(R \doteq \sup_{\vxi \in \R^{D}}\norm{\vxi}_{2}\). Deci
    \begin{equation}
        \tr(\Cov(\vy_{\vxi})) = \Ex[\norm{\vy_{\vxi}}_{2}^{2}] - \norm{\Ex[\vy_{\vxi}]}_{2}^{2} \leq \Ex[\norm{\vy_{\vxi}}_{2}^{2}] \leq R^{2}.
    \end{equation}
    Prin urmare
    \begin{equation}
        -\frac{D}{t^{2}} \leq \Delta \log p_{t}(\vxi) \leq \frac{R^{2}}{t^{4}} - \frac{D}{t^{2}},
    \end{equation}
    ceea ce demonstrează afirmația.
\end{proof}

\subsubsection{Calcule de Derivate}

Aici calculăm unele derivate utile care vor fi reutilizate în întregul apendice.

\begin{proposition}\label{prop:p_t_derivatives}
    Fie \(\vx\) o variabilă aleatoare astfel încât \Cref{assumption:entropy_x_compact_support,assumption:entropy_x_density} sunt îndeplinite, și fie \((\vx_{t})_{t \in [0, T]}\) procesul stocastic \eqref{eq:app_additive_gaussian_noise_model}. Pentru \(t \geq 0\), fie \(p_{t}\) densitatea lui \(\vx_{t}\). Atunci
    \begin{align}
        \pdv{p_{t}}{t}(\vxi)
        &= \Ex\rs{\phi_{t}(\vxi - \vx)\cdot\frac{\norm{\vxi - \vx}_{2}^{2} - Dt^{2}}{t^{3}}} \\
        \nabla p_{t}(\vxi)
        &= -\Ex\rs{\phi_{t}(\vxi - \vx)\cdot\frac{\vxi - \vx}{t^{2}}} \\
        \nabla^{2}p_{t}(\vxi)
        &= \Ex\rs{\phi_{t}(\vxi - \vx)\cdot\frac{(\vxi - \vx)(\vxi - \vx)^{\top} - t^{2}\vI}{t^{4}}} \\ 
        \Delta p_{t}(\vxi)
        &= \Ex\rs{\phi_{t}(\vxi - \vx)\cdot\frac{\norm{\vxi - \vx}_{2}^{2} - Dt^{2}}{t^{4}}}.
    \end{align}
\end{proposition}
\begin{proof} 
    Folosim reprezentarea prin convoluție a lui \(p_{t}\), anume \eqref{eq:p_t_representation}. Luând mai întâi derivata în timp, un calcul arată că \Cref{prop:dutis} se aplică,\footnote{Folosim \(f_{t}(\vxi) = p(\vxi) \phi_{t}(\vxi - \vx)\), observând că este de două ori continuu diferențiabilă în \(\vxi\) și (mai mult de) două ori continuu diferențiabilă în \(t\). Apoi pentru a verifica integrabilitatea locală a lui \(f_{t}\) calculăm \(\pdv{f_{t}}{t}(\vxi) = f_{t}(\vxi)\cdot \frac{1}{t^{3}}(\norm{\vxi - \vx}_{2}^{2} - Dt^{2})\), care este ușor de verificat integrabilă peste \(\vxi\) și \(t \in [t_{\min}, t_{\max}]\) unde \(t_{\min} > 0\). (Într-adevăr, \(f_{t}\) are cozi care scad exponențial, deci termenul pătratic din produs nu este o problemă.)} deci putem aduce derivata înăuntrul integralei/așteptării ca:
    \begin{equation}
        \pdv{p_{t}}{t}(\vxi) = \pdv*{\Ex[\phi_{t}(\vxi - \vx)]}{t} = \Ex\rs{\pdv*{\phi_{t}(\vxi - \vx)}{t}} = \pdv{\phi_{t}}{t} * p.
    \end{equation}
    Între timp, prin proprietățile convoluțiilor (\Cref{prop:diff_convolution}) și folosind faptul că \(p\) are suport compact (\Cref{assumption:entropy_x_compact_support}),
    \begin{equation}
        p_{t} = \phi_{t} * p \implies \nabla p_{t} = \nabla \phi_{t} * p \implies \nabla^{2}p_{t} = \nabla^{2}\phi_{t} * p \implies \Delta p_{t} = \Delta \phi_{t} * p.
    \end{equation}
    Restul calculului urmează din \Cref{prop:normal_derivatives}.
\end{proof}


\begin{proposition}\label{prop:normal_derivatives}
    Pentru \(t > 0\) și \(\vxi \in \R^{D}\) avem
    \begin{align}
        \pdv*{\phi_{t}}{t}(\vxi)
        &= \phi_{t}(\vxi) \cdot \frac{\norm{\vxi}_{2}^{2} - Dt^{2}}{t^{3}} \\
        \nabla \phi_{t}(\vxi)
        &= -\phi_{t}(\vxi)\cdot\frac{\vxi}{t^{2}} \\ 
        \nabla^{2} \phi_{t}(\vxi)
        &= \phi_{t}(\vxi)\cdot\frac{\vxi\vxi^{\top} - t^{2}\vI}{t^{4}} \\
        \Delta \phi_{t}(\vxi)
        &= \phi_{t}(\vxi) \cdot \frac{\norm{\vxi}_{2}^{2} - Dt^{2}}{t^{4}}.
    \end{align}
\end{proposition}
\begin{proof}
    Calcul direct.
\end{proof}



\subsubsection{Diferențierea Sub Semnul Integralei}

În acest apendice, diferențiem sub semnul integralei de multe ori, și este important să știm când putem face acest lucru. Există două tipuri de diferențiere sub semnul integralei:
\begin{enumerate}
    \item Diferențierea unei integrale \(\int f_{t}(\vxi)\odif{\vxi}\) în raport cu parametrul auxiliar \(t\).
    \item Diferențierea unei convoluții \((f * g)(\vxi) = \int f(\vxi)g(\vxi - \vu)\odif{u}\) în raport cu variabila \(\vxi\).
\end{enumerate}

Pentru prima categorie, oferim un rezultat concret, enunțat fără demonstrație dar atribuibil \href{https://planetmath.org/differentiationundertheintegralsign}{sursei link-uite}, care derivează următorul rezultat ca un caz special al unei teoreme mai generale despre interacțiunea operatorilor diferențiali și distribuțiilor temperate, mult dincolo de scopul cărții. O referință formală completă poate fi găsită în \cite{jones1982theory}.
\begin{proposition}[\cite{jones1982theory}, Secțiunea 11.12]\label{prop:dutis}
    Fie \(f \colon (0, T) \times \R^{D} \to \R\) astfel încât:
    \begin{itemize}
        \item \(f\) este o funcție măsurabilă în comun de \((t, \vxi)\);
        \item Pentru aproape fiecare \(\vxi \in \R^{D}\) în sensul Lebesgue, funcția \(t \mapsto f_{t}(\vxi)\) este absolut continuă;
        \item \(\pdv{f_{t}}{t}\) este local integrabilă, adică pentru fiecare \([t_{\min}, t_{\max}] \subseteq (0, T)\) avem
        \begin{equation}
            \int_{t_{\min}}^{t_{\max}}\int_{\R^{D}}\abs*{\pdv{f_{t}}{t}(\vxi)}\odif{\vxi} < \infty.
        \end{equation}
    \end{itemize}
    Atunci \(t \mapsto \int_{\R^{D}}f_{t}(\vxi)\odif{\vxi}\) este o funcție absolut continuă pe \((0, T)\), și derivata sa este
    \begin{equation}
        \odv*{\int_{\R^{D}}f_{t}(\vxi)\odif{\vxi}}{t} = \int_{\R^{D}}\pdv*{f_{t}}{t}(\vxi)\odif{\vxi},
    \end{equation}
    definită pentru aproape fiecare \(t \in (0, T)\).
\end{proposition}

Pentru a doua categorie, oferim un alt rezultat concret, enunțat fără demonstrație dar complet formalizat în \cite{brezis2011functional}.
\begin{proposition}[\cite{brezis2011functional}, Propoziția 4.20]\label{prop:diff_convolution}
    Fie \(f\) de \(k\) ori continuu diferențiabilă cu suport compact, și fie \(g\) local integrabilă. Atunci convoluția \(f * g\) definită prin
    \begin{equation}
        (f * g)(\vxi) \doteq \int_{\R^{D}}f(\vu)g(\vxi - \vu)\odif{\vu}
    \end{equation}
    este de \(k\) ori continuu diferențiabilă, și derivata sa de ordinul \(k\) este
    \begin{equation}
        \nabla^{k}(f * g) =(\nabla^{k}f) * g. 
    \end{equation}
\end{proposition}
Deși nu este în carte, un argument simplu de integrare prin părți arată că dacă \(g\) este și ea de \(k\) ori diferențiabilă, atunci putem "face schimb" de regularitate:
\begin{equation}
    \nabla^{k}(f * g) = f * (\nabla^{k} g).
\end{equation}


\section{Codare cu Pierderi și Împachetarea Sferelor}\label{app:rate-distortion-covering}

În această secțiune, demonstrăm \Cref{thm:covering-number-rate-distortion}.
Urmând convențiile noastre din acest apendice, scriem $\cS = \Supp(\vx)$
pentru suportul compact al variabilei aleatoare $\vx$.

După cum am anticipat, vom face o presupunere de regularitate asupra mulțimii suport $\cS$
pentru a demonstra \Cref{thm:covering-number-rate-distortion}. O posibilitate pentru
a proceda sub presupuneri minime ar fi să instanțiem rezultatele din
\cite{Riegler2018-jh,Riegler2023-rr} în contextul nostru, deoarece aceste rezultate se aplică
mulțimilor $\cS$ cu regularitate foarte scăzută (de exemplu, mulțimi de tip Cantor cu structură
fractală). Cu toate acestea, am constatat că calcularea precisă a constantelor în aceste
rezultate, o încercare necesară pentru a afirma o concluzie precum
\Cref{thm:covering-number-rate-distortion}, este oarecum dificilă în contextul
nostru.
Abordarea noastră este, prin urmare, să adăugăm o presupunere de regularitate geometrică asupra
mulțimii $\cS$ care sacrifică ceva generalitate, dar permite dezvoltarea
unui argument mai transparent. Pentru a evita sacrificarea prea multă generalitate, trebuie
să ne asigurăm că dimensionalitatea scăzută în mulțimea $\cS$ nu este interzisă.
Prin urmare, considerăm exemplul curent pe care l-am folosit pe parcursul cărții,
amestecul de distribuții gaussiene de rang mic. În acest cadru geometric, vom
impune acest lucru presupunând că $\cS$ este o reuniune de hipersfere, ceea ce este
echivalent cu presupunerea gaussiană în dimensiuni înalte cu probabilitate
copleșitoare.

\begin{assumption}\label{assumption:union-of-spheres}
    Suportul $\cS \subset \R^D$ al variabilei aleatoare $\vx$ este o reuniune
    finită de $K$ sfere, fiecare cu dimensiunea $d_k$, $k \in [K]$.
    Probabilitatea ca $\vx$ să fie extrasă din a $k$-a sferă este dată de
    $\pi_k \in [0, 1]$, și condiționat de a fi extrasă din a $k$-a sferă,
    $\vx$ este distribuită uniform pe acea sferă.
    Suporturile satisfac că fiecare sferă este mutual ortogonală cu toate
    celelalte.
\end{assumption}

Procedăm sub \Cref{assumption:union-of-spheres} simplificatoare pentru a
simplifica tehnicitatea excesivă și pentru a ne conecta la un exemplu important
folosit pe parcursul monografiei. Credem că rezultatele noastre pot fi generalizate la
suportul $\cS$ din clasa \textit{mulțimilor cu atingere pozitivă} cu
efort tehnic suplimentar, dar lăsăm acest lucru pentru viitor.




\subsection{Demonstrația Relației Dintre Distorsiunea Ratei și Acoperire}

Schițăm pe scurt demonstrația, apoi procedăm la stabilirea a trei leme
fundamentale, apoi dăm demonstrația. Demonstrația va depinde de noțiunile introduse în
schița de mai jos.

Obținerea unei limite superioare pentru funcția de distorsiune a ratei
\eqref{eqn:rate-distortion-general} este simplă: prin caracterizarea ratei
(adică, funcția de distorsiune a ratei este rata minimă a
unui cod pentru $\vx$ cu distorsiune $\ell^2$ pătrată așteptată $\epsilon$), limitarea
superioară a $\cR_{\epsilon}(\x)$ necesită doar demonstrarea unui cod pentru $\x$ care
atinge această distorsiune țintă, și orice $\epsilon$-acoperire a $\Supp(\x)$
realizează acest lucru, cu rata egală cu logaritmul în baza 2 al cardinalității
acoperirii.
Limita inferioară este mai subtilă. Facem uz de limita inferioară Shannon,
discutată în \Cref{rem:slb}: calcularea constantelor în \cite[\S III,
(22)]{Linder1994-ej} oferă o versiune mai precisă a rezultatului citat în
\Cref{eq:slb} (în biți, desigur): pentru orice variabilă aleatoare $\vx$ cu suport
compact și o densitate, avem
\begin{equation}\label{eq:slb-appendix-1}
    \cR_{\epsilon}(\x)
    \geq
    h(\x)
    - \log \volume(B_{\epsilon})
    +
    \log
    \left(
    \frac{
    	2
    }
    {
    	D \Gamma(D/2)
    }
    \left(
    \frac{
    	D
    }
    {
    	2e
    }
    \right)^{D/2}
    \right),
\end{equation}
cu entropia (etc.) în nats în această expresie.
Constanta poate fi estimată ușor folosind aproximația lui Stirling.
O formă cantitativă a aproximației lui Stirling care este adesea utilă dă
pentru orice $x > 0$ \cite{Jameson2015-hy}
\begin{equation}
    \Gamma(x) \leq \sqrt{2\pi} x^{x - 1/2} e^{-x} e^{1/(12x)}.
\end{equation}
Vom aplica această limită la $\Gamma(D/2)$ în \Cref{eq:slb-appendix-1}.
Obținem
\begin{align}
    \log
    \left(
    \frac{
        2
    }
    {
        D \Gamma(D/2)
    }
    \left(
    \frac{
        D
    }
    {
        2e
    }
    \right)^{D/2}
    \right)
    &\geq
    -\frac{1}{6D}
    +
    \log\left(
        \frac{2}{D} 
        \left(
        \frac{
            D
        }
        {
            2e
        }
        \right)^{D/2}
        \cdot
        \sqrt{\frac{D}{4\pi}}
        \left(
        \frac{D}{2e}
        \right)^{-D/2}
    \right)
    \\
    &=
    -\frac{1}{6D}
    - \frac{1}{2}\log D\pi,\label{eq:slb-constant-est}
\end{align}
pe care o putem lua pentru valoarea explicită a constantei $C_D$ în \Cref{eq:slb}.
Rezumând limita inferioară Shannon complet cuantificată (în biți):
\begin{equation}\label{eq:slb-appendix}
    \cR_{\epsilon}(\x)
    \geq
    h(\x)
    - \log_2 \volume(B_{\epsilon})
    - O(\log D).
\end{equation}

Acum, constrângerea importantă pentru scopurile noastre actuale este că limita inferioară
Shannon necesită ca variabila aleatoare $\vx$ să aibă o densitate, ceea ce exclude multe
distribuții de interes cu dimensionalitate scăzută.
Dar să luăm în considerare momentan situația când $\vx$ admite o densitate.
Presupunerea că $\vx$ este distribuită uniform pe suportul său este ușor
formalizată în acest cadru: pentru orice mulțime Borel $A \subset \cS$, avem
\begin{equation}
    \Pr[\vx \in A] = \int_{A} \frac{1}{\volume(\cS)} \odif \vx.
\end{equation}
Atunci entropia $h(\vx)$ este doar
\begin{equation}
    h(\vx) = \log_2 \volume(\cS).
\end{equation}
Demonstrația se încheie apoi cu o lemă care leagă raportul $\volume(\cS)
/ \volume(B_{\epsilon})$ de numărul de $\epsilon$-acoperire a $\cS$ cu
bile de rază $\epsilon$.


Pentru a extinde programul de mai sus la distribuții degenerate care satisfac
\Cref{assumption:union-of-spheres},
demonstrația noastră a limitei inferioare în \Cref{thm:covering-number-rate-distortion} va
folosi un argument de aproximare a distribuției actuale cu dimensionalitate
scăzută $\vx$ prin distribuții "apropiate" care au
densități, similar dar nu exact la fel ca schița demonstrației care precede \Cref{thm:max_entropy}.
Vom lega apoi parametrul introdus în
secvența de aproximare de parametrul de distorsiune $\epsilon$ pentru a
obține concluzia dorită în \Cref{thm:covering-number-rate-distortion}.


\begin{definition}\label{def:thickening-set}
    Fie $\cS$ o mulțime compactă.
    Pentru orice $\delta > 0$, definim $\delta$-îngroșarea lui $\cS$, notată
    $\cS_{\delta}$, prin
    \begin{equation}
        \cS_{\delta} = \set*{\vxi \in \R^D \given \mathrm{dist}(\vxi, \cS) \leq
        \delta}.
    \end{equation}
\end{definition}
Funcția de distanță referită în \Cref{def:thickening-set} este definită prin
\begin{equation}\label{eq:dist-func}
    \dist(\vxi, \cS) = \inf_{\vxi' \in \cS} \norm*{\vxi - \vxi'}_2.
\end{equation}
Pentru o mulțime compactă $\cS$, teorema lui Weierstrass implică că pentru orice $\vxi \in
\R^D$, există întotdeauna un $\vxi' \in \cS$ care atinge infimumul în funcția de
distanță.
Compactitatea lui $\cS_{\delta}$ rezultă imediat din compactitatea lui $\cS$, deci
$\volume(\cS_{\delta})$ este finit pentru orice $\delta > 0$. Este atunci posibil să
facem următoarea definiție a unei variabile aleatoare îngroșate, specializată la
\Cref{assumption:union-of-spheres}.


\begin{definition}\label{def:thickening-rv-uos}
    Fie $\vx$ o variabilă aleatoare astfel încât $\Supp(\vx) = \cS$ este o reuniune de
    $K$ hipersfere, distribuită ca în \Cref{assumption:union-of-spheres}.
    Notăm suportul fiecărei componente a amestecului cu $\cS_k$.
    Definim variabila aleatoare îngroșată $\vx_{\delta}$ ca amestecul de
    măsuri unde fiecare măsură componentă este uniformă pe mulțimea îngroșată
    $\cS_{k, \delta}$ (\Cref{def:thickening-set}), pentru $k \in [K]$, cu ponderi de
    amestecare $\pi_k$.
\end{definition}


\begin{lemma}\label{lem:rate-distortion-lb-uos}
    Presupunem că variabila aleatoare $\vx$ satisface
    \Cref{assumption:union-of-spheres}. Atunci dacă $0 < \delta < \tfrac{1}{2}$,
    variabila aleatoare îngroșată $\vx_{\delta}$ (\Cref{def:thickening-rv-uos})
    satisface pentru orice $\epsilon > 0$
    \begin{equation}
        R_{\delta + \epsilon}(\vx_{\delta})
        \leq
        R_{\epsilon}(\vx).
    \end{equation}
\end{lemma}

Demonstrația \Cref{lem:rate-distortion-lb-uos} este amânată la
\Cref{sec:app-rate-dist-deferred-proofs}.
Folosind \Cref{lem:rate-distortion-lb-uos}, programul de mai sus poate fi realizat,
deoarece variabila aleatoare $\vx_{\delta}$ are o densitate care este uniformă în
raport cu măsura Lebesgue.

\begin{proof}[(Demonstrația \Cref{thm:covering-number-rate-distortion})]
    Limita superioară este ușor de arătat. Dacă $S$ este orice $\epsilon$-acoperire a
    suportului lui $\vx$ cu cardinalitate $\cN_{\epsilon}(\Supp(\vx))$, atunci considerăm
    schema de codare care atribuie fiecărui $\vxi \in \Supp(\vx)$ reconstrucția
    $\hat{\vxi} = \argmin_{\vxi' \in S}\, \norm{\vxi - \vxi'}_2$, cu egalitățile
    rezolvate arbitrar. Atunci egalitățile apar cu probabilitate zero, și faptul că
    $S$ acoperă $\Supp(\vx)$ la scara $\epsilon$ garantează distorsiune nu mai mare
    decât $\epsilon$; rata acestei scheme este $\log_2 \cN_{\epsilon}(\Supp(\vx))$.

    Pentru limita inferioară, fie $0 < \delta < \tfrac{1}{2}$, și considerăm
    variabila aleatoare îngroșată $\vx_{\delta}$. Prin
    \Cref{lem:rate-distortion-lb-uos}, avem
    \begin{equation}
        R_{\delta + \epsilon}(\vx_{\delta})
        \leq
        R_{\epsilon}(\vx).
    \end{equation}
    Deoarece $\vx_{\delta}$ are o densitate Lebesgue care este uniformă, putem apoi
    aplica limita inferioară Shannon, în forma \eqref{eq:slb-appendix}, pentru a obține
    \begin{equation}\label{eq:slb-proof}
        \log_2 \volume(\Supp(\vx_{\delta}))
        - \log_2 \volume(B_{\delta + \epsilon})
        - O(\log D)
        \leq
        R_{\epsilon}(\vx).
    \end{equation}
    În final, trebuie să limităm inferior raportul
    \begin{equation}
        \frac{
            \volume(\Supp(\vx_{\delta}))
        }
        {
            \volume(B_{\delta + \epsilon})
        }
    \end{equation}
    în termeni de numărul de acoperire.
    Deoarece $\Supp(\vx_{\delta}) = \Supp(\vx) + B_{\delta}$, unde $+$ aici
    denotă suma Minkowski, o aplicație standard a argumentelor de limită de volum
    (vezi de exemplu \cite[Propoziția 4.2.12]{Vershynin2018-br}) dă
    \begin{equation}
        \volume(\Supp(\vx_{\delta}))
        \geq
        \cN_{2\delta}(\Supp(\vx))
        \volume(B_{\delta}).
    \end{equation}
    Prin urmare
    \begin{align}
        \frac{
            \volume(\Supp(\vx_{\delta}))
        }
        {
            \volume(B_{\delta + \epsilon})
        }
        &\geq
        \cN_{2\delta}(\Supp(\vx))
        \frac{
            \volume(B_{\delta})
        }
        {
            \volume(B_{\delta + \epsilon})
        }
        \\
        &=
        \cN_{2\delta}(\Supp(\vx))
        \left(
            \frac{
                \delta
            } 
            {
                \delta + \epsilon
            }
        \right)^D.
    \end{align}
    Alegând $\delta = \epsilon / 2$ obținem din limita inferioară Shannon
    \eqref{eq:slb-proof} și estimările de mai sus
    \begin{equation}
        \log_2 \cN_{\epsilon}(\Supp(\vx))
        - O(D)
        \leq
        R_{\epsilon}(\vx),
    \end{equation}
    ceea ce trebuia demonstrat.

\end{proof}



\subsection{Demonstrația Lemei \ref{lem:rate-distortion-lb-uos}}\label{sec:app-rate-dist-deferred-proofs}

\begin{proof}[(Demonstrația \Cref{lem:rate-distortion-lb-uos})]
    Este suficient să arătăm că orice cod pentru $\vx$ cu distorsiune pătrată așteptată
    $\epsilon^2$ produce un cod pentru $\vx_{\delta}$ cu aceeași rată și
    distorsiune nu mult mai mare, pentru o alegere potrivită a lui $\delta$.
    Deci fixăm un astfel de cod pentru $\vx$, obținând rata $R$ și distorsiune pătrată
    așteptată $\epsilon^2$. Scriem $\hat{\vx}$ pentru variabila aleatoare
    reconstruită folosind acest cod, și $\mathrm{q} : \Supp(\vx) \to \Supp(\vx)$
    pentru maparea asociată de codare-decodare (adică, $\hat{\vx}
    = \mathrm{q}(\vx)$).

    Acum fie $\cS_k$ a $k$-a hipersferă în suportul lui $\vx$.
    Există o bază ortonormală $\vU_{k} \in \R^{D \times d_k}$ astfel încât
    $\Span(\cS_k) = \Span(\vU_k)$.
    Următoarea descompunere ortogonală a mulțimii suport $\cS$
    va fi folosită repetat pe parcursul demonstrației.
    Avem
    \begin{align}
        \cS_{\delta} 
        &= \set{\vxi \in \R^D \given \exists k \in [K] \::\: \dist(\vxi,
        \cS_k) \leq \delta}
        \\
        &= \bigcup_{k \in [K]} \set{\vxi \in \R^D \given \dist(\vxi,
        \cS_k) \leq \delta}.
    \end{align}
    Prin proiecție ortogonală, pentru orice $k \in [K]$ orice $\vxi \in \R^D$ poate fi
    scris ca $\vxi = \vxi^{\|} + \vxi^{\perp}$, cu $\vxi^{\|} \in \Span(\cS_k)$
    și $\ip{\vxi^{\|}}{\vxi^\perp} = 0$.
    Atunci pentru orice $\vxi' \in \cS_k$, avem
    \begin{align}
        \norm*{\vxi - \vxi'}_2^2 
        = 
        \norm*{\vxi^{\|} + \vxi^{\perp} - \vxi'}_2^2
        &=
        \norm*{\vxi^{\|}}_2^2 + \norm*{\vxi^{\perp}}_2^2 + \norm*{\vxi'}_2^2
        - 2 \ip*{\vxi^{\|}}{\vxi'}
        \\
        &\geq
        \norm*{\vxi^{\|}}_2^2 + \norm*{\vxi'}_2^2
        - 2 \ip*{\vxi^{\|}}{\vxi'}
        \\
        &=
        \norm*{\vxi^{\|} - \vxi'}_2^2.
    \end{align}
    Mai mult, se știe că pentru orice $\vxi^{\|} \in \Span(\cS_k)$ nenul,
    \begin{equation}
        \inf_{\vxi' \in \cS_k}\,
        \norm*{\vxi^{\|} - \vxi'}_2^2
        =
        \norm*{\vxi^{\|} - \frac{\vxi^{\|}}{\norm{\vxi^{\|}}_2}}_2^2.
    \end{equation}
    Dacă $\vxi^{\|}$ este zero, este clar că distanța de mai sus este egală cu $1$
    pentru fiecare $\vxi' \in \cS_k$.
    Prin urmare, dacă definim o mapare de proiecție
    $\pi_{\cS_k}(\vxi)$
    prin
    \begin{equation}
        \pi_{S_k}(\vxi) = \frac{\vU_k\vU_k^\top \vxi}{\norm{\vU_k^\top \vxi}_2}
    \end{equation}
    pentru orice $\vxi \in \R^D$ cu $\vU_k^\top \vxi \neq \mathbf{0}$, atunci
    $\pi_{\cS_k}(\vxi) = \argmin_{\vxi' \in \cS_k}\norm*{\vxi' - \vxi}_2$.
    Alegem $0 < \delta < 1$, astfel încât mulțimea îngroșată $\cS_{\delta}$
    să nu conțină puncte $\vxi \in \R^D$ la care oricare dintre mapările de proiecție
    $\pi_{\cS_k}$ nu este bine definită.
    Deci mulțimea îngroșată $\cS_{\delta}$ satisface
    \begin{align}
        \cS_{\delta} 
        &= \bigcup_{k \in [K]} \set*{\vxi \in \R^D \given 
        \norm*{\vxi - \frac{\vU_k\vU_k^\top \vxi}{\norm{\vU_k^\top \vxi}_2}}_2
        \leq \delta}.
    \end{align}
    Aceste distanțe pot fi rescrise în termenii descompunerii ortogonale ca
    \begin{align}
        \norm*{\vxi - \frac{\vU_k\vU_k^\top \vxi}{\norm{\vU_k^\top \vxi}_2}}_2^2
        &=
        \norm*{\vxi}_2^2 - 2 \norm{\vU_k^\top \vxi}_2 + 1
        \\
        &=
        \norm*{\vxi^{\|}}_2^2 
        + \norm*{\vxi^{\perp}}_2^2
        - 2 \norm{\vU_k^\top \vxi^{\|}}_2 + 1
        \\
        &=
        \norm*{\vxi^{\perp}}_2^2
        + \left( \norm*{\vxi^{\|}}_2 - 1 \right)^2.
        \label{eq:distance-to-hypersphere}
    \end{align}
    Vom arăta în continuare că fiecare astfel de $\vxi \in \cS_{\delta}$ poate fi
    asociat unic cu o proiecție pe un singur subspațiu în amestec,
    ceea ce ne va permite să definim o proiecție corespunzătoare pe $\cS$.
    Dat un $\vxi \in \cS_{\delta}$, prin cele de mai sus, putem găsi un subspațiu
    $\vU_k$ astfel încât descompunerea ortogonală $\vxi = \vxi^{\|}_k
    + \vxi^{\perp}_k$ satisface
    \begin{equation}
        \norm*{\vxi^{\perp}_k}_2^2
        + \left( \norm*{\vxi^{\|}_k}_2 - 1 \right)^2
        \leq
        \delta^2.
    \end{equation}
    Considerăm descompunerea $\vxi = \vxi_j^{\|} + \vxi_j^{\perp}$ pentru un $j
    \neq k$. Avem
    \begin{align}
        \norm*{\vxi_j^{\|}}_2
        =
        \norm*{
            \vU_j \vU_j^\top \vxi 
        }_2
        &=
        \norm*{
            \vU_j \vU_j^\top( \vU_k \vU_k^\top \vxi + (\vI - \vU_k \vU_k^\top)
            \vxi )
        }_2
        \\
        &=
        \norm*{
            \vU_j \vU_j^\top (\vI - \vU_k \vU_k^\top) \vxi
        }_2
        \\
        &\leq
        \norm*{
            (\vI - \vU_k \vU_k^\top) \vxi
        }_2
        =
        \norm*{
            \vxi_k^{\perp}
        }_2
        \leq \delta,
    \end{align}
    unde a doua linie folosește presupunerea de ortogonalitate asupra subspațiilor
    $\vU_k$, și a treia folosește faptul că proiecțiile ortogonale sunt
    neexpansive.
    Prin urmare, a $j$-a distanță satisface
    \begin{equation}
        \norm*{\vxi^{\perp}_j}_2^2
        + \left( \norm*{\vxi^{\|}_j}_2 - 1 \right)^2
        \geq
        (1 - \delta)^2.
    \end{equation}
    Aceasta implică că dacă $0 < \delta < 1/2$, fiecare $\vxi \in \cS_{\delta}$ are
    un subspațiu cel mai apropiat unic în amestec.
    Prin urmare, sub această condiție, următoarea mapare $\pi_{\cS} : \cS_{\delta}
    \to \cS$ este bine definită:
    \begin{equation}
        \pi_{\cS}(\vxi) 
        =
        \pi_{\cS_{k_\star}}(\vxi),
        \enspace\text{unde}\enspace
        k_{\star} = \argmin_{k \in [K]}\,
        \dist(\vxi, \cS_{k}).
    \end{equation}


    Acum, definim un cod pentru $\vx_{\delta}$ prin
    \begin{equation}
        \hat{\vx}_{\delta} = \mathrm{q}( \pi_{\cS}(\vx_{\delta}) ).
    \end{equation}
    În mod clar aceasta este asociată cu un cod de rată $R$ pentru $\vx_{\delta}$, deoarece
    folosește mapările de codare-decodare din codul de rată $R$ pentru $\vx$. Trebuie
    să arătăm că atinge distorsiune mică.
    Calculăm
    \begin{align}
        \bE\left[ \norm*{ \vx_{\delta} - \hat{\vx}_{\delta} }_2^2 \right]
        &=
        \bE\left[ \norm*{ \vx_{\delta} - \mathrm{q}(\pi_{\cS}(\vx_{\delta})) }_2^2 \right]
        \\
        &\leq
        \left(
        \bE\left[ \norm*{ \vx_{\delta} - \pi_{\cS}(\vx_{\delta}) }_2^2
        \right]^{1/2}
        + \bE\left[ \norm*{ \pi_{\cS}(\vx_{\delta})
        - \mathrm{q}(\pi_{\cS}(\vx_{\delta})) }_2^2 \right]^{1/2}
        \right)^2,\label{eq:distortion-decomposition}
    \end{align}
    unde inegalitatea folosește inegalitatea Minkowski.
    Acum, prin \Cref{def:thickening-set,def:thickening-rv-uos}, avem
    determinist
    \begin{equation}
        \norm*{ \vx_{\delta} - \pi_{\cS}(\vx_{\delta}) }_2^2
        \leq \delta^2,
    \end{equation}
    deci așteptarea satisface și această estimare.
    Pentru al doilea termen, va fi suficient să caracterizăm densitatea
    variabilei aleatoare $\pi_{\cS}(\vx_{\delta})$ ca fiind suficient de aproape de
    densitatea lui $\vx$---care, așa cum implică \Cref{assumption:union-of-spheres}, este
    un amestec de distribuții uniforme pe fiecare sub-sferă $\cS_k$.
    Prin argumentul de mai sus, fiecare punct $\vxi \in \cS_{\delta}$ poate fi asociat
    cu unul și numai un subspațiu $\vU_k$, ceea ce înseamnă că componentele
    amestecului în definiția lui $\cS_{\delta}$ (amintiți-vă
    \Cref{def:thickening-rv-uos}) nu se suprapun. Prin urmare, densitatea
    $\pi_{\cS}(\vx_{\delta})$ poate fi caracterizată studiind efectul lui
    $\pi_{\cS_k}$ asupra variabilei aleatoare condiționate $\vx_{\delta}$, condiționată
    să fie extrasă din $\cS_{k, \delta}$. Notăm această măsură cu $\mu_{k,
    \delta}$.
    Afirmăm că împingerea înainte a acestei măsuri sub $\pi_{\cS_k}$ este uniformă pe $\cS_k$.
    Pentru a vedea că aceasta este valabilă, ne amintim
    \Cref{eq:distance-to-hypersphere},
    care dă caracterizarea
    \begin{equation}
        \cS_{k, \delta} = \set*{
            \vxi^{\|} + \vxi^{\perp} 
            \given 
            \vxi^{\|} \in \Span(\vU_k),
            \vxi^{\perp} \in \Span(\vU_k)^\perp,
            \norm*{\vxi^{\perp}}_2^2
            + \left( \norm*{\vxi^{\|}}_2 - 1 \right)^2
            \leq
            \delta
        }.
    \end{equation}
    Distribuția condiționată în discuție este uniformă pe această mulțime; trebuie să
    arătăm că proiecția $\pi_{\cS_k}$ aplicată acestei variabile aleatoare
    condiționate produce o variabilă aleatoare care este uniformă pe $\cS_k$.
    În raport cu aceste coordonate, am văzut că $\pi_{\cS_k}(\vxi^\|
    + \vxi^\perp) = \vxi^\| / \norm{\vxi^{\|}}_2$.
    Prin urmare, pentru orice $\vxi \in \cS_{k}$, avem că preimaginea lui $\vxi$ în
    $\cS_{k, \delta}$ sub $\pi_{\cS_k}$ este
    \begin{equation}
        \pi_{\cS_k}^{-1}(\vxi) = \set*{
            r\vxi + \vxi^\perp 
            \given
            r > 0,
            \vxi^{\perp} \in \Span(\vU_k)^\perp,
            \norm*{\vxi^{\perp}}_2^2
            + \left( r - 1 \right)^2
            \leq
            \delta
        }.
        \label{eq:fiber-of-uos}
    \end{equation}
    Pentru a arăta că $(\pi_{\cS_k})_{\sharp} \mu_{k, \delta}$ este uniformă, trebuie să
    descompunem integrala densității uniforme pe $\cS_{k, \delta}$ într-un mod
    care face clar că fiecare dintre fibrele
    $\pi_{\cS_k}^{-1}(\vxi)$ (pentru fiecare $\vxi \in \cS_k$) "contribuie" în mod egal
    la integrală.\footnote{Mai riguros, aceasta corespunde descompunerii
    densității uniforme pe $\cS_{k, \delta}$ într-o densitate condiționată regulată
    corespunzătoare lui $\vxi \in \cS_{k}$, și arătând că densitatea
    corespunzătoare pe $\vxi$ este uniformă. Demonstrația face clar că aceasta este adevărat.}
    Avem prin \Cref{def:thickening-rv-uos}
    \begin{equation}
        \volume(\cS_{k, \delta})
        = \iint_{\Span(\vU_k) \times \Span(\vU_k)^\perp} \mathbf{1}_{
            \norm*{\vxi^{\perp}}_2^2
            + \left( \norm*{\vxi^{\|}}_2 - 1 \right)^2
            \leq
            \delta
        }
        \odif \vxi^{\|} \odif \vxi^{\perp}.
    \end{equation}
    În particular, integrarea peste coordonatele ortogonale se factorizează.
    Fie $\odif \vtheta^{d}$ măsura uniformă (Haar) pe sfera de
    rază $1$ în $\R^d$. Convertind integrala $\vxi^{\|}$ în coordonate
    polare, avem
    \begin{equation}
        \volume(\cS_{k, \delta})
        = \int_{[0, \infty)} \int_{\bS^{d_k-1}} \int_{\Span(\vU_k)^\perp} 
        r^{d_k-1}
        \mathbf{1}_{
            \norm*{\vxi^{\perp}}_2^2
            + \left( r - 1 \right)^2
            \leq
            \delta
        }
        \odif r \odif \vtheta^{d_k} \odif \vxi^{\perp}.
    \end{equation}
    Comparând cu reprezentarea fibrei \eqref{eq:fiber-of-uos},
    vedem că trebuie să "integrăm" peste componentele $r$ și $\vxi^\perp$
    ale integralei precedente pentru a verifica că împingerea înainte
    este uniformă.
    Dar aceasta este evidentă, deoarece expresia anterioară arată că valoarea acestei
    integrale este independentă de $\vxi^\|$---sau, echivalent în context,
    valoarea componentei sferice $\vtheta^{d_k}$.

    Astfel rezultă din argumentul de mai sus că $\pi_{\cS}(\vx_{\delta})$ este
    uniformă.
    Deoarece presupunerea asupra lui $\delta$ implică că componentele amestecului în
    distribuția lui $\vx_{\delta}$ nu se suprapun, ponderile de amestecare
    $\pi_k$ sunt de asemenea păstrate în imaginea $\pi_{\cS}(\vx_{\delta})$, și în
    particular, distribuția lui $\pi_{\cS}(\vx_{\delta})$ este egală cu
    distribuția lui $\vx$.
    Prin urmare, al doilea termen în \Cref{eq:distortion-decomposition} satisface
    \begin{equation}
        \bE\left[ \norm*{ \pi_{\cS}(\vx_{\delta}) - \mathrm{q}(\pi_{\cS}(\vx_{\delta})) }_2^2 \right]
        =
        \bE\left[ \norm*{ \vx - \mathrm{q}(\vx) }_2^2 \right]
        \leq
        \epsilon^2,
    \end{equation}
    deoarece $\mathrm{q}$ este un cod cu distorsiune $\epsilon$ pentru $\vx$.

    Am arătat astfel că codul ipotezat de rată $R$, distorsiune (pătrată așteptată)
    $\epsilon^2$ pentru $\vx$ produce un cod de rată $R$, distorsiune (pătrată
    așteptată) $\delta + \epsilon$ pentru $\vx_{\delta}$.
    Aceasta stabilește că
    \begin{equation}
        R_{\delta + \epsilon}(\vx_{\delta})
        \leq
        R_{\epsilon}(\vx),
    \end{equation}
    ceea ce trebuia demonstrat.


\end{proof}

\end{document}