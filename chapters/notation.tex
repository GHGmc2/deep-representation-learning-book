\chapter*{Notation}

%\DP{Can flesh out later. Please remark here if you want to add some notation.}

\section*{Linear Algebra Notation}
\begin{itemize}
    \item Scalars are (capital or lower-case) non-bold, e.g., \(x\), \(X\).
    \item Vectors are lower-case bold, e.g., \(\vx\), \(\vpi\). Entries of vectors are denoted \(x_{i}\) or \(\pi_{i}\), alternatively \((\vx)_{i}\).
    \item Matrices are capital bold, e.g., \(\vX, \vPi\). Entries of matrices are denoted \(X_{ij}\) or \(\Pi_{ij}\), alternatively \((\vX)_{ij}\).
    \item Use same notation for random vs deterministic.
    \item \(\ell_{p}\) norms on vectors are denoted by \(\norm{\vx}_{p}\), \(p \in \{0\} \cup [1, \infty]\).
    \item Euclidean operator norm on matrices is \(\norm{\vA}\).
    \item Frobenius norm on matrices is \(\norm{\vA}_{F}\).
    \item Euclidean inner product on vectors is \(\ip{\vx}{\vy}\).
    \item Frobenius inner product on matrices is \(\ip{\vX}{\vY}\).
    \item Transpose is \(\top\), e.g., \(\vA^{\top}\). %
    Adjoint is $\adj$, e.g., $\vA\adj$. %
    Pseudo-inverse is \(\dagger\), e.g., \(\vA^{\dagger}\).
    \item Projection onto set, say \(S\), is \(\proj_{S}\).
    \item Orthogonal matrices are \(\O(m, n)\) or \(\O(n) = \O(n, n)\).
    \item Symmetric matrices are \(\Sym(n)\), symmetric PSD matrices are \(\PSD(n)\), symmetric PD matrices are \(\PD(n)\).
    \item Sphere is \(\Sphere^{n - 1} \subseteq \R^{n}\), ball is \(\Ball^{n} \subseteq \R^{n}\).
    \item Rank of a matrix is \(\rank(\vX)\), trace is \(\tr(\vX)\), det is \(\det(\vX)\), log-det is \(\logdet(\vX)\).
    \item Eigenvalues of symmetric matrix \(\vX \in \Sym(n)\) are \(\lambda_{i}(\vX)\) such that \(\lambda_{1}(\vX) \geq \cdots \geq \lambda_{n}(\vX)\).
    \item Singular values of \(\vX \in \R^{n \times d}\) with \(\rank(\vX) = r\) are \(\sigma_{1}(\vX) \geq \cdots \geq \sigma_{r}(\vX) > 0\). By convention we set \(\sigma_{r + 1}(\vX) = \sigma_{r + 2}(\vX) = \cdots = 0\).
    \item Element-wise multiplication is \(\vX \hada \vY\). Element-wise squaring is 
    $\vX^{\hada 2}$.
    \item Kronecker product is $\vX \kron \vY$. Iterated Kronecker product is $\vX^{\kron 2}$.
    \item Elementwise function application is \(f[\vX]\), i.e., with square brackets.
    \item Ones vector/matrix given by \(\vone\). Zeros vector/matrix given by \(\vzero\). Identity matrix given by \(\vI\).
\end{itemize}

\section*{Probability Notation}

\begin{itemize}
    \item Probability is \(\Pr\).
    \item Expected value is \(\Ex\). 
    \item Covariance is \(\Cov\). 
    \item Correlation is \(\Corr\).
    \item Set of probability distributions on measurable space \(\vX\) is denoted \(\Delta(\vX)\). (For \(\cX = [n]\), this is the simplex).
\end{itemize}

\section*{Machine Learning Notation}

\begin{itemize}
    \item \(\vx \in \R^{D} = \cX\) is vector-valued data r.v. (when each sample is standalone). \(\vX \in \R^{N \times D} = \cX\) is matrix-valued data r.v.~(when each sample is a token set).
    \item \(\vz \in \R^{d} = \cZ\) is vector-valued features r.v.. \(\vZ \in \R^{n \times d} = \cZ\) is matrix-valued features r.v..
    \item The map \(f \colon \cX \mapsto \cZ\) is the representation mapping. \(\vZ = f(\vX)\) are the features of \(\vX\).
    \item A mapping \(g \colon \cZ \to \cX\) is the decoding mapping. \(\hat{\vX} = g(\vZ)\) is the autoencoding of \(\vX\). % \DP{Yi likes hat not widehat.}
    \item When \(f\) and \(g\) are implemented as neural networks, they will have the same number of layers without loss of generality; we write them as \(f = f^{L} \circ f^{L - 1} \circ \cdots \circ f^{2} \circ f^{1} \circ f^{\pre}\) and \(g = g^{\post} \circ g^{1} \circ g^{2} \circ \cdots \circ g^{L - 1} \circ g^{L}\). Usually, {\(g^{\ell} \circ f^{\ell} \approx \id\)} 
\end{itemize}

